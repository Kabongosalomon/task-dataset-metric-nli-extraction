<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One Shot Face Swapping on Megapixels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of IoTSC</orgName>
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<email>jian.wang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhong</forename><surname>Xu</surname></persName>
							<email>czxu@um.edu.mo</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of IoTSC</orgName>
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<email>znsun@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One Shot Face Swapping on Megapixels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Example of a swapped face. Left: source image that represents the identity; Middle: target image that provides the attributes; Right: the swapped face image. All images are in 1024 2 . * Corresponding author ment of our model for GPU memory can be satisfied for megapixel face swapping. In summary, complete face representation, stable training, and limited memory usage are the three novel contributions to the success of our method. Extensive experiments demonstrate the superiority of MegaFS and the first megapixel level face swapping database is released for research on DeepFake detection and face image editing in the public domain. The dataset is at this link.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Face swapping has both positive applications such as entertainment, human-computer interaction, etc., and negative applications such as DeepFake threats to politics, economics, etc. Nevertheless, it is necessary to understand the scheme of advanced methods for high-quality face swapping and generate enough and representative face swapping images to train DeepFake detection algorithms. This paper proposes the first Megapixel level method for one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face representation hierarchically by the proposed Hierarchical Representation Face Encoder (HieRFE) in an extended latent space to maintain more facial details, rather than compressed representation in previous face swapping methods. Secondly, a carefully designed Face Transfer Module (FTM) is proposed to transfer the identity from a source image to the target by a non-linear trajectory without explicit feature disentanglement. Finally, the swapped faces can be synthesized by StyleGAN2 with the benefits of its training stability and powerful generative capability. Each part of MegaFS can be trained separately so the require-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given two face images, face swapping refers to transferring the identity from the source image to the target image, while the facial attributes of the target image hold intact. It has attracted extensive attention in recent years for its broad application prospects in entertainment <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">24]</ref>, privacy protection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">33]</ref>, and theatrical industry <ref type="bibr" target="#b32">[34]</ref>.</p><p>Existing face swapping methods can be roughly divided into two categories: subject-specific and subject agnostic methods. Subject-specific face swapping methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b32">34]</ref> need to be trained and tested on the same pair of subjects, which restricts their potential applications. On the contrary, subject agnostic face swapping methods <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b34">36]</ref> can be applied to arbitrary identities without additional training procedures. In this paper, we focus on a more challenging topic: one shot face swapping, where only one image is given from the source and target identity for both training and testing.</p><p>With the rapid growth of high resolution image and video data on the web, it becomes increasingly popular to process high resolution samples. However, generating high resolution swapped faces is rather difficult because of the following problems. Firstly, information is insufficient for high-quality face generation due to the compressed representation in an end-to-end framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b26">28]</ref>. Secondly, adversarial training is unstable <ref type="bibr" target="#b7">[8]</ref>, which confines the resolution of previous methods only up to 256 2 . Thirdly, the GPU memory limitation makes the training untenable, or the training batch is bounded by a small size, which aggravates the collapse of the training process.</p><p>To this end, this paper proposes the first Megapixel level one shot Face Swapping method (MegaFS) by adopting the "divide and conquer" strategy in three steps. Firstly, to overcome the information loss in the encoder, we adopt GAN Inversion methods <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b57">59]</ref> and propose a Hierarchical Representation Face Encoder (HieRFE) to find the complete face representation in an extended latent space W ++ . Secondly, to modify face representations and resolve the problem of previous latent code manipulation methods <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b2">3]</ref> that only one attribute can be modified once a time, a novel swapping module, Face Transfer Module (FTM), is proposed to control multiple attributes synchronously without explicit feature disentanglement. Finally, the unstable adversarial training problem is evaded by exploiting StyleGAN2 <ref type="bibr" target="#b21">[23]</ref> as the decoder, which is fixed and the discriminator is not used for optimization. Each part of MegaFS can be trained separately so the GPU memory requirements are satisfied for megapixel face swapping. The contributions of this paper can be summarized as:</p><p>? To the best of our knowledge, the proposed MegaFS is the first method that can conduct one shot face swappings at megapixel level.</p><p>? For encoding and manipulating the complete face representation, faces are encoded by HieRFE hierarchically in the new extended latent space W ++ and a new multistep non-linear latent code manipulation module, FTM, is proposed to manage multiple attributes synchronously without explicit feature disentanglement.</p><p>? Experimental results on benchmark dataset have shown the effectiveness of the proposed MegaFS. Furthermore, the first megapixel face swapping database is released for research of DeepFake detection and face image editing in the public domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Swapping</head><p>Subject-specific face swapping methods are popular in recent years, where DeepFake <ref type="bibr" target="#b10">[11]</ref> and its variants are trained using pairwise samples. Besides, <ref type="bibr">Korshunova et al. [27]</ref> model different source identities separately, such as a CageNet for Nicolas Cage, or a SwiftNet for Taylor Swift. Recently, Disney Research realizes high resolution face swapping <ref type="bibr" target="#b32">[34]</ref>, but it requires training decoders for different subjects, which hinders its generalization. Besides, it is time consuming and difficult for subject-specific methods to train specific models for distinct pairs of faces <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b24">26]</ref>. Subsequently, subject agnostic face swapping methods break the limitations of previous subject-specific face swapping methods. Realistic Neural Talking Head <ref type="bibr" target="#b37">[39]</ref> adopts meta-learning to relieve the pain of fine-tuning on different individuals. FaceSwapNet <ref type="bibr" target="#b54">[56]</ref> proposes a landmark swapper to handle the identity leakage problem from landmarks. In the meanwhile, other mindsets follow the attribute disentanglement heuristic to explore new high fidelity face swapping frameworks. FS-Net <ref type="bibr" target="#b33">[35]</ref> represents the face region of the source image as a vector, which is combined with a non-face target image to generate the swapped face image. IPGAN <ref type="bibr" target="#b4">[5]</ref> disentangles identities and facial attributes as different vectorized representaions. Based on previous works, FSGAN <ref type="bibr" target="#b34">[36]</ref> and FaceShifter <ref type="bibr" target="#b26">[28]</ref> achieve state-of-the-art results by their outstanding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GAN Inversion</head><p>Based on a well-trained GAN, GAN Inversion, or Latent Space Embedding, tries to find the latent code that can accurately reconstruct a given image synthesized. To this end, two problems need to be settled: determining a proper latent space and designing an algorithm to search for the optimal latent code within that space. As for the latent space, early methods perform image inversion into W ? R 1?512 <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b15">17]</ref>, while later works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> extend the latent space to W + ? R 18?512 , which proves to have better reconstruction results. As for the inversion algorithms, they either train an encoder <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">16]</ref> to predict latent codes of images or minimize the error between predicted and given images by optimizing latent codes from random initializations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">31]</ref>. Some methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b57">59]</ref> combine both to optimize latent codes initialized by encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Latent Code Manipulation</head><p>Latent Code Manipulation, or Latent Control, is another attractive research area to manipulate latent codes based on the observation that semantic editing operations can be realized by adding high dimensional directions <ref type="bibr" target="#b41">[43]</ref>. Several linear semantic directions, or trajectories, of W are found   <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b46">48]</ref>. StyleRig <ref type="bibr" target="#b49">[51]</ref> and PIE <ref type="bibr" target="#b48">[50]</ref> propose to manipulate latent space through an existing 3D model <ref type="bibr" target="#b6">[7]</ref>, which successfully control facial poses, expressions, and illuminations. Previous methods <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b41">43]</ref> have found good controllability of StyleGAN based on the assumption that semantic directions in StyleGAN latent space are linear. Recently, StyleFlow <ref type="bibr" target="#b2">[3]</ref> achieves better manipulation results through non-linear trajectories. <ref type="figure" target="#fig_0">Fig.2</ref> demonstrates the overall pipeline and notations of the proposed MegaFS, which combines the identity information from a source image x s and attribute information from a target image x t to generate the final swapped face image y s2t . In the following, we will present the details of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hierarchical Representation for Face Swapping</head><p>In the first stage, face images are projected into latent space W ++ using Hierarchical Representation Face Encoder (HieRFE) to deposit complete face information. The structure of HieRFE is detailed in <ref type="figure" target="#fig_1">Fig.3</ref>.</p><p>Specifically, HieRFE consists of a ResNet50 backbone based on several residual blocks <ref type="bibr" target="#b16">[18]</ref>, a feature pyramid structure based on FPN <ref type="bibr" target="#b28">[30]</ref> for feature refinement, and eighteen lateral non-linear mapping networks for latent code prediction. Please refer to the corresponding papers for details of residual blocks and FPN. As for the nonlinear mapping network, it comprises repeated downsampling, convolution, batchnorm, and leakyReLU layers until the feature map can be pooled as a vector, i.e., l ? R 1?512 .</p><p>Then, the constant input of StyleGAN2 predicted by the backbone and four latent codes predicted by the smallest feature map, denoted as C ? R 4?4?512 and L low ? R 4?512 , represent low-level topology information. Other latent codes are gathered as L high ? R 14?512 to represent high-level semantic information. Finally, subscript s and t are adopted to represent the source and target images if it is necessary in the following paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Synchronized Control of Multiple Attributes</head><p>During the second stage, Face Transfer Module (FTM) is proposed to control multiple attributes of identity information in a synchronized manner for face swapping demands. In detail, FTM contains 14 Face Transfer Blocks, the number of which equals that of l high .</p><p>As shown in <ref type="figure" target="#fig_2">Fig.4</ref>, each Face Transfer Block contains three identical transfer cells. In each transfer cell, l high s and l high t are firstly concatenated to l high c , which collects all information from the source and target images. Then l high s is refined tol high s through a two-step non-linear trajectory:</p><formula xml:id="formula_0">T (l high c , l high s ) = T 2 (l high c , T 1 (l high c , l high s ))<label>(1)</label></formula><p>in which</p><formula xml:id="formula_1">T 1 (a, b) = sigmoid(K 1 (a)) ? b T 2 (a, b) = T anh(K 2 (a)) + b<label>(2)</label></formula><p>where K 1 (?) and K 2 (?) denote two linear layers. The trajectory is crafted based on the following illustrations. In the first step, the multiplication coefficients are scaled in range (0, 1) after sigmoid activation, where l high s is designed to discard irrelevant semantics except for the identity information. In the second step, l high s accepts a small amount of target semantic attributes by shifting in the latent space. Similarly, l high t is processed in parallel but for discarding target identity while holding other semantics. Finally, the transferred latent code l s2t ? L s2t can be predicted as</p><formula xml:id="formula_2">l s2t = ?(?)l high t + (1 ? ?(?))l high s (3)</formula><p>where ? ? R 1?512 is a trainable weight vector, and ? stands for the sigmoid activation. The transferred latent codes L s2t is composed by gathering all predicted l s2t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">High-Fidelity Face Generation</head><p>Finally in the third stage, C s and L low s are discarded since they contain negligible identity information from x s . The swapped face image y s2t can be generated by feeding StyleGAN2 generator with C t , L low t and L s2t . By taking StyleGAN2 as the decoder, face swapping through latent space differentiates our method from other face swapping frameworks. Firstly, it provides an extended latent space for complete face representation, which makes detailed face generation feasible. Secondly, it makes our method operating globally in W ++ instead of locally on feature maps, which is desirable as it can conduct nonlinear transformations through latent code manipulations without local distortions. Thirdly, it does not require explicit attributes disentanglement, which makes the training process straightforward without tricky loss functions and hyper-parameter settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Functions</head><p>For each part of MegaFS, HieRFE and FTM are trained sequentially, while StyleGAN2 generator remains intact.</p><p>Objective function of HieRFE: Following the previous work <ref type="bibr" target="#b42">[44]</ref>, we make use of three objectives for supervising a pair of input image x and its reconstruction imagex, including pixel-wise reconstruction loss L rec , Learned Perceptual Image Path Similarity (LPISP) loss L LP IP S <ref type="bibr" target="#b56">[58]</ref>, and identity loss L id as follows:</p><formula xml:id="formula_3">L rec = x ?x 2<label>(4)</label></formula><formula xml:id="formula_4">L LP IP S = F (x) ? F (x) 2<label>(5)</label></formula><formula xml:id="formula_5">L id = 1 ? cos(R(x), R(x))<label>(6)</label></formula><p>where ? 2 denotes 2 distance, F (?) denotes the perceptual feature extractor, R(?) denotes the ArcFace <ref type="bibr" target="#b11">[12]</ref> recognition model, cos(?, ?) denotes the cosine similarity of two face embeddings.</p><p>In addition, as face swapping needs pose and expression controllability, we introduce landmarks loss L ldm to measure 2 difference between the predicted landmarks of the input faces and the corresponding ones of reconstructed faces as following:</p><formula xml:id="formula_6">L ldm = P (x) ? P (x) 2<label>(7)</label></formula><p>where P (?) denotes the facial landmark predictor <ref type="bibr" target="#b52">[54]</ref>. The overall loss function for training HieRFE is</p><formula xml:id="formula_7">L inv = ? 1 L rec + ? 2 L LP IP S + ? 3 L id + ? 4 L ldm (8)</formula><p>where ? 1 , ? 2 , ? 3 and ? 4 are loss weights. Besides, x andx need resizing as the input of each model before calculating the loss function.</p><p>Objective function of FTM: For training FTM, four losses are proposed, including:</p><formula xml:id="formula_8">L rec = x s ?x s 2 + x t ?x t 2 (9) L LP IP S = F (x t ) ? F (y s2t ) 2 (10) L id = 1 ? cos(R(x s ), R(y s2t ))<label>(11)</label></formula><formula xml:id="formula_9">L ldm = P (x t ) ? P (y s2t ) 2<label>(12)</label></formula><p>Besides, L norm is leveraged to stabilizes the training process.</p><formula xml:id="formula_10">L norm = L high s ? L s2t 2<label>(13)</label></formula><p>Similarly, the overall loss function for training FTM is</p><formula xml:id="formula_11">L swap = ? 1 L rec +? 2 L LP IP S +? 3 L id +? 4 L ldm +? 5 L norm<label>(14)</label></formula><p>where ? 1 , ? 2 , ? 3 , ? 4 and ? 5 are loss weights. Finally, when FTM converges, the proposed method is ready for face swapping on megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will first show the effectiveness of the proposed method by comparing it with other state-of-the-art methods provided in FaceForensics++ <ref type="bibr" target="#b44">[46]</ref>. Then the superiority of our method is demonstrated by conducting face swapping on CelabA-HQ <ref type="bibr" target="#b19">[21]</ref>. Finally, an ablation study is presented to reveal the necessity of each component of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>CelebA <ref type="bibr" target="#b30">[32]</ref>: This dataset is built for face detection, facial landmark localization, attribute recognition and control, and face synthesis. It contains 202,599 celebrity images with 40 labeled attributes and 5 landmark location annotations. CelebA-HQ <ref type="bibr" target="#b19">[21]</ref>: It is a high-quality version of CelebA dataset. All 202,599 images in CelebA are processed by two pre-trained neural nets for denoising and super-resolution, resulting in 30,000 high-quality images. FFHQ <ref type="bibr" target="#b20">[22]</ref>: The dataset contains 70,000 megapixel face images collected from Flickr. FFHQ has considerable variations of age, ethnicity, gender, and background. FaceForensics++ <ref type="bibr" target="#b44">[46]</ref>: It is a forensics dataset consisting of 1,000 original video sequences from YouTube that have been manipulated with five automated face manipulation methods: Deepfakes, Face2Face, FaceSwap, NeuralTextures, and FaceShifter, in which Deepfakes, FaceSwap, and FaceShifter are face swapping methods, while Face2Face and NeuralTextures are reenactment algorithms. Implementation Details: In all experiments, learning rate of the Adam optimizer <ref type="bibr" target="#b23">[25]</ref> is set to 0.01. We set ? 1 , ? 2 , ? 3 and ? 4 to 1, 0.8, 1, and 1000. We set ? 1 , ? 2 , ? 3 , ? 4 and ? 5 to 8, 32, 24, 100000, and 32. In addition, 200,000 faces are randomly sampled as auxiliary data by running StyleGAN2. <ref type="figure">Figure 5</ref>. Qualitative comparison results of FaceSwap, DeepFakes, FaceShifter, and ours. FaceShifter and our method generate obviously better results than other methods. For FaceShifter, it generates wrong expressions in row 1 (fierce) and row 2 (fear), of which expressions are from source faces. Besides, FaceShifter keeps the beard of target faces in rows 3 and 4, which makes the swapped faces close to their target faces. In the last three rows, FaceShifter fails to swap faces. However, our method successfully preserves identity information from source images.</p><p>For experiments on FaceForensics++, HieRFE and FTM are sequentially trained ten epochs in total on CelebA, CelebA-HQ, FFHQ, and the auxiliary data. As for experiments on CelebA-HQ, HieRFE and FTM are sequentially trained seventeen epochs in total on FFHQ and the auxiliary data. As for training time, it takes about five days on three Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on FaceForensics++</head><p>Qualitative Comparison: As FaceForensics++ contains images generated by three face swapping methods: FaceSwap, DeepFakes, and FaceShifter, we extract frames of the same index from this dataset and compare them with the proposed MegaFS.</p><p>As shown in <ref type="figure">Fig.5</ref>, FaceShifter and our method generate more visually pleasant results than other methods. For example, FaceSwap and DeepFakes suffer from blending inconsistency, distortions, and artifacts. For FaceShifter, the disentanglement of identity information from other attributes is sub-optimal because of its fixed identity encoder. In <ref type="figure">Fig.5</ref>, FaceShifter generates unnatural expressions in the first and second rows, which seems to keep unnecessary ex-pressions from the source images. FaceShifter also fails to transfer identity information from source faces to target faces by incorrectly maintaining the beard from target faces in rows 3 and 4. Additionally as shown in rows 4, 5, and 6, FaceShifter tends to keep excessive attributes from target images, which makes the swapped faces similar to their target faces.</p><p>Quantitative Comparison: In order to make a fair comparison with other methods quantitatively, we follow the experiment settings introduced in FaceShifter <ref type="bibr" target="#b26">[28]</ref>.</p><p>Firstly, ten frames per original video are evenly sampled and processed by MTCNN <ref type="bibr" target="#b55">[57]</ref>, resulting in 10,000 aligned faces. Then, aligned faces are manually checked in case of incorrect detections. After data cleaning, all corresponding frames in manipulated videos are extracted for testing. However, as FaceForensics++ is not designed for face recognition, some videos display repeated identities. For example, videos numbered 043 and 343 show Vladimir Putin, and videos of 179, 183 and 826 show the same person Barack Obama. Thus, we manually categorize all videos into 889 identities. ID retrieval is measured as the top-1 matching rate of the swapped faces and their corresponding identities from source faces, serving to measure the identity preservation ability of different face swapping methods. As for pose and expression errors, an open-sourced pose estimator <ref type="bibr" target="#b45">[47]</ref> and a 3D facial model <ref type="bibr" target="#b12">[13]</ref> are used to extract pose and expression vectors. Then 2 distances between swapped faces and the corresponding target faces are measured and recorded in Tab.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ID retrieval ? pose ? expression ? DeepFakes <ref type="bibr" target="#b10">[11]</ref> 88. <ref type="bibr" target="#b37">39</ref>  As DeepFakes, FaceSwap, and FaceShifter are face swapping methods, while Face2Face and Neural Textures are face reenactment methods, different evaluation criterions should be considered. We report ID retrieval, pose error, and expression error for face swapping methods and neglect ID retrieval for face reenactment methods. As shown in Tab.1, our method achieves the highest ID retrieval thanks to the hierarchical representation for faces. However, our method performs inferior to FaceShifter and reenactment methods in terms of pose and expression errors. Aside from face reenactment methods are mainly designed to control facial movements and expression deformations while ne- glecting to swap the identity information, two possible reasons hide behind. Firstly, our method is trained on only 500,000 images, which is much less than 2,700,000 images used to train FaceShifter. Besides, the training set for FaceShifter contains VGGFace <ref type="bibr" target="#b39">[41]</ref>, which contains more pose and expression variations compared with CelebA-HQ and FFHQ. Secondly, StyleGAN2 is trained on FFHQ, which is proved to have data bias <ref type="bibr" target="#b47">[49]</ref>. Consequently, Style-GAN2 tends to generate smiling faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on CelebA-HQ</head><p>Qualitative Result: One superiority of our method is that it can achieve megapixel level face swapping. As shown in <ref type="figure" target="#fig_3">Fig.6</ref>, faces can be swapped across various expressions and poses. The swapped faces faithfully keep wrinkles, iris colors, eyebrow and nose shapes from source faces. To the best of our knowledge, no other methods can swap faces at the resolution of 1024 2 except for <ref type="bibr" target="#b32">[34]</ref>. However, <ref type="bibr" target="#b32">[34]</ref> needs to train different decoders for different identities, so it is not compared in this section.</p><p>Method ID similarity ? pose ? expression ? FID ? Ours 0.5014 3.58 2.87 10.16 <ref type="table">Table 2</ref>. Quantitative experimental results on CelebA-HQ. We report ID similarity, pose error, and expression error to demonstrate the megapixel level face swapping performance of the proposed MegaFS. FID is also reported as the similarity between the 300,000 swapped face images and CelebA-HQ dataset.</p><p>Quantitative Result: To quantify the capability of the proposed MegaFS on swapping megapixel face images, we randomly swapped 300,000 pairs of face images in CelebA-HQ for testing. For the reason that ID retrieval calculation between 30,000 original faces and 300,000 swapped faces requires Nine Billion times of matching, we report cosine similarity of swapped faces and the corresponding source faces using cosface as ID similarity to release the computational burden. Also, both pose error and expression error are measured under the same settings as experimented in subsection 4.2. In addition, Fr?chet Inception Distance (FID) is reported to quantify the similarity of the 300,000 swapped face images to CelebA-HQ dataset. The results are summarized in Tab.2 as the baseline for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we conduct ablation experiments on CelebA-HQ to evaluate the effectiveness of the key components in the proposed MegaFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">The Choise of Latent Space</head><p>In this part, we will verify the superiority of the extended latent space W ++ over W + . We trained another neural network, which has the same network structure as HieRFE, to project facial images into latent space W + .  For illustrating the information preservation ability, two widely used metrics in GAN Inversion, LPIPS 2 distance and image level MSE, are reported in Tab.3. Besides, the percentage of unsuccessful reconstructions is defined as the failure rate to quantify the robustness of two inversion models. From the reported results, we can conclude that HieRFE   outperforms its counterpart trained on W + for better information preservation ability as well as the robustness. As to the controllability, we use the same evaluation criterions in subsection 4.3 to evaluate different latent space. The quantitative results are shown in Tab.4, suggesting that W ++ is better than W + in terms of ID similarity, pose and expression preservation ability. The qualitative results of two inversion models are displayed in <ref type="figure" target="#fig_4">Fig.7</ref>. HieRFE and its counterpart can reconstruct easy cases well. However, the latter fails to recast sunglasses, eyeglasses, eye gazes, and faces under complex lighting conditions. Thus, the latent space W ++ is verified to be better than W + for both face reconstruction and face swapping tasks in all terms of information preservation ability, robustness, and controllability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">The Design of Latent Code Manipulator</head><p>As StyleGAN2 has a layer-wise representation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23]</ref>, it is heuristically feasible to manipulate latent codes by any network that operates on vectors. However, we argue that the design of the latent code manipulator needs to consider the applicability on vectorized information exchanging. To this end, we make use of</p><formula xml:id="formula_12">[C t , L high t , L high s ] in- stead of [C t , L high t ,</formula><p>L s2t ] for generation, named as Latent Code Replacement (LCR), to envisage the functionality of C, L low and L high . Afterwards, we follow the previous <ref type="figure">Figure 8</ref>. The design of ID Injection follows the SPADE ResBlk <ref type="bibr" target="#b38">[40]</ref>, with convolutional layers for 2D input subsititued by linear layers, indicated by black arrows, for vectors. method <ref type="bibr" target="#b38">[40]</ref> to inject identity information into latent codes. This design is detailed in <ref type="figure">Fig.8</ref>, namely ID Injection. Both designs are compared to the proposed FTM. For a fair comparison, other two sets of 300,000 swapped face images are generated by adopting LCR and ID Injection respectively. The quantitative results are summarized in Tab.5 and the qualitative comparison is shown in <ref type="figure">Fig.9</ref>. LCR achieves the best FID since it keeps excessive semantic information from source images. However, this is not favorable for face swapping since information from L high t is lost. As shown in the third column of <ref type="figure">Fig.9</ref>, LCR can swap faces while ignoring target attributes such as skin color and eye state. Thus, we can safely conclude that identity information, to a large extent, is encoded in L high . Thus, C s and L low s are discarded in the proposed pipeline. Based on this observation, ID Injection and FTM are proposed to process only on L high . For ID Injection, it has the lowest expression error as it reserves topology information from target images at the cost of other semantic parts from source images, such as identity information stated in Tab.5, and facial details as shown in the fourth column of <ref type="figure">Fig.9</ref>. Among them, FTM achieves the highest ID similarity level, lowest pose error, and makes a decent balance in terms of expression error, FID, and visual pleasantness. Thus, the proposed FTM shows to be better than the other two latent code manipulators for face swapping. <ref type="figure">Figure 9</ref>. Qualitative comparison results of different latent code manipulation methods: LCR, ID Injection, and FTM. LCR keeps skin color and eye state from the source image xs as shown in the first two rows. For ID Injection, attributes are dominated by the target image xt. For example, the red lip in row 2 and beard in the last row from source images are neglected. FTM achieves the best balance among the three latent code manipulation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have analyzed three unsettled key issues in previous works for high resolution face swapping and proposed a general face swapping pipeline named MegaFS to resolve these difficulties in a three-stage procedure. Hi-eRFE in the first stage projects faces into hierarchical representaions in an extended latent space W ++ for complete facial information deposit. FTM in the second stage transfers the identity from a source image to the target by a nonlinear trajectory without explicit feature disentanglement. Finally, StyleGAN2 is used to synthesize the swapped face and avoid unstable adversarial training. The modular design of MegaFS requires little GPU memory with a negligible performance cost and it performs comparatively when compared to other state-of-the-art face swapping methods at the resolution of 256 2 . Besides, to the best of our knowledge, MegaFS is the first method that can conduct one shot face swapping on megapixels. Finally, based on MegaFS, the first megapixel level face swapping database is built and released to the public for future research of forgery detection and face swapping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The proposed MegaFS consists of three stages: Face Encoding, Latent Code Manipulation, and Face Generation. Firstly, HieRFE projects two face images into latent space W ++ . Then FTM manipulates L high s and L high t in two hierarchical latent sets Ss and St to get Ls2t. Finally, the swapped face image ys2t can be synthesized by a pre-trained StyleGAN2 generator from Ct, L low t , and Ls2t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>HieRFE consists of a ResNet50 backbone based on residual blocks, a feature pyramid structure based on FPN, and eighteen lateral non-linear mapping networks, in which n? refers to the number of the corresponding parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Inside FTM, each Face Transfer Block contains three identical transfer cell. After being processed by three cells, two refined vectors are weighted by a learnable weight ? and summed as the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Face swapping results on CelabA-HQ. Images from right to left are source image xs which provides the identity, target image xt that offers the attributes, and the swapped face image ys2t. All images are in 1024 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison results of reconstructed images from latent space W + and W ++ . From top to bottom: source images, reconstructed images from W + and W ++ . HieRFE and its counterpart perform well in easy cases (the first column), but the latter fails to recast sunglasses, glasses, eye gazes, and faces under complex lighting conditions (from the second to the last columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This work was supported in part by the National Key R&amp;D Program of China under Grant 2020AAA0140002, in part by the Science and Technology Development Fund of Macau SAR under grant 0015/2019/AKP, in part by the Natural Science Foundation of China under Grant 62076240, Grant 61721004, Grant U1836217, and in part by the Macao Youth Scholars Program under Grant AM201913.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison results on FaceForensics++. The best two results are shown in red and blue respectively. ? means higher is better, and ? means lower is better.</figDesc><table><row><cell></cell><cell></cell><cell>4.64</cell><cell>3.33</cell></row><row><cell>FaceSwap [15]</cell><cell>72.69</cell><cell>2.58</cell><cell>2.89</cell></row><row><cell>Face2Face [53]</cell><cell>-</cell><cell>2.68</cell><cell>2.09</cell></row><row><cell>Neural Textures [52]</cell><cell>-</cell><cell>2.21</cell><cell>1.64</cell></row><row><cell>FaceShifter [28]</cell><cell>90.68</cell><cell>2.55</cell><cell>2.82</cell></row><row><cell>Ours</cell><cell>90.83</cell><cell>2.64</cell><cell>2.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison results of latent space W + and W ++ using GAN Inversion metrics. LPIPS 2 distance and image level MSE are measured to quantify the information preservation capabilities of W + and W ++ . The robustness is indicated by the failure rate of facial reconstruction. For reported metrics, HieRFE outperforms its counterpart trained on W + .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparison results of latent space W + and W ++ using face swapping metrics. HieRFE trained on W ++ beats its counterpart trained on W + in terms of ID similarity, pose error, and expression error.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Latent Control ID similarity ? pose ? exp ? FID ? Quantitative comparison results of different latent code manipulation methods ("exp" represents expression error). FTM achieves the best ID similarity and pose preservation results and makes a decent balance among expression error and FID.</figDesc><table><row><cell>LCR</cell><cell>0.3997</cell><cell>5.04</cell><cell>3.43</cell><cell>9.64</cell></row><row><cell>ID Injection</cell><cell>0.4447</cell><cell>3.67</cell><cell>2.82</cell><cell>10.32</cell></row><row><cell>FTM (Ours)</cell><cell>0.5014</cell><cell>3.58</cell><cell>2.87</cell><cell>10.16</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Im-age2stylegan++: How to edit the embedded images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8296" to="8305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Styleflow: Attribute-conditioned exploration of stylegangenerated images using conditional continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02401</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creating a photoreal digital actor: The digital emily project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lambeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference for Visual Media Production</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards open-set identity preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6713" to="6722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exchanging faces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="669" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Editing in style: Uncovering the local semantics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5771" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inverting the generator of a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1967" to="1974" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfakes</surname></persName>
		</author>
		<idno>Accessed: 2020-10-08. 1</idno>
		<ptr target="https://github.com/ondyari/FaceForensics/tree/master/dataset/DeepFakes" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The deepfake detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08854</idno>
		<ptr target="https://github.com/ondyari/FaceForensics/tree/master/dataset/FaceSwapKowalski.Accessed:2020-10-08.6" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">dfdc) preview dataset. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01758</idno>
		<title level="m">Collaborative learning for faster stylegan embedding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganspace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02546</idno>
		<title level="m">Discovering interpretable gan controls</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the &quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2886" to="2895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfiguring portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08685</idno>
		<title level="m">Deepfakes: a new threat to face recognition? assessment and detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast face-swap using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3677" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advancing high fidelity identity swapping for forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Celeb-df: A large-scale challenging dataset for deepfake forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3207" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photorealistic face de-identification by aggregating donors&apos; face components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Saleh Mosaddegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Highresolution neural face swapping for visual effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fsnet: An identity-aware generative model for image-based face swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Yatagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="117" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7184" to="7193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Disentangling in latent space by harnessing a pretrained generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07728</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07728</idno>
		<title level="m">Face identity disentanglement via latent spacemapping</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Realistic dynamic facial textures from a single image using gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5429" to="5438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?lvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951,2020.4</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R?ssler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09179</idno>
		<title level="m">Justus Thies, and Matthias Nie?ner. Faceforensics: A large-scale video dataset for forgery detection in human faces</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">FaceForen-sics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R?ssler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09635</idno>
		<title level="m">terfacegan: Interpreting the disentangled face representation learned by gans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pie: Portrait image embedding for semantic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09485</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stylerig: Rigging stylegan for 3d control over portrait images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Sun Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exposing deep fakes using inconsistent head poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8261" to="8265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Faceswapnet: Landmark guided many-to-many face reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Indomain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00049</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
