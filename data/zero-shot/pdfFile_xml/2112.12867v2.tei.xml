<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HSPACE: Synthetic Parametric Humans Animated in Complex Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
							<email>egbazavan@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
							<email>andreiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
							<email>mihaiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<email>wfreeman@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HSPACE: Synthetic Parametric Humans Animated in Complex Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. HSPACE contains dynamic scenes with multiple moving people, with diverse body shapes and poses, placed in realistic environments, under complex lighting. Human animations are driven by GHUM <ref type="bibr" target="#b40">[41]</ref>. For all frames we provide 3d pose and shape ground truth, as well as other rich image annotations including human segmentation, body part localisation semantics, and temporal correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Advances in the state of the art for 3d human sensing are currently limited by the lack of visual datasets with 3d ground truth, including multiple people, in motion, operating in real-world environments, with complex illumination or occlusion, and potentially observed by a moving camera. Sophisticated scene understanding would require estimating human pose and shape as well as gestures, towards representations that ultimately combine useful metric and behavioral signals with free-viewpoint photo-realistic visualisation capabilities. To sustain progress, we build a largescale photo-realistic dataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic indoor and outdoor environments. We combine a hundred diverse individuals of varying ages, gender, proportions, and ethnicity, with hundreds of motions and scenes, as well as parametric variations in body shape (for a total of 1,600 different humans), in order to generate an initial dataset of over 1 million frames. Human animations are obtained by fitting an expressive human body model, GHUM, to single scans of people, followed by novel re-targeting and positioning procedures that support the realistic animation of dressed humans, statistical variation of body proportions, and jointly consistent scene placement of multiple moving people. Assets are generated automatically, at scale, and are compatible with existing real time rendering and game engines. The dataset 1 with evaluation server will be made available for research. Our large-scale analysis of the impact of synthetic data, in connection with real data and weak supervision, underlines the considerable potential for continuing quality improvements and limiting the sim-to-real gap, in this practical setting, in connection with increased model capacity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Progress in 3d human pose and shape estimation has been sustained over the past years as several statistical 3d human body models <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>, as well as learning and inference techniques have been developed <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. More recently there has been interest in human interactions, self-contact <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b25">26]</ref>, and human-object interactions, as well as in frameworks to jointly reconstruct multiple people <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. As errors steadily decreased on some of the standard 3d estimation benchmarks like Hu-man3.6M <ref type="bibr" target="#b11">[13]</ref> and HumanEva <ref type="bibr" target="#b35">[36]</ref>, other laboratory benchmarks recently appeared <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b6">8]</ref>, more complex in terms of motion, occlusion and scenarios, e.g. capturing interactions.</p><p>While good quality laboratory benchmarks remain essential to monitor and track progress, as a rich source of motion data to construct pose and dynamic priors, or to initially bootstrap models trained on more complex imagery, overall there is an increasing need to bridge the gap between the inevitably limited subject, clothing, and scene diversity of the lab, and the complexity of the real world. It is also desirable to go beyond skeletons and 3d markers towards more holistic models of humans with estimates of shape, clothing, or gestures. While several recent self-supervised and weaklysupervised techniques emerged, with promising results in training with complex real-world image data <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b43">44]</ref>, their quantitative evaluation still is a challenge, as accurate 3d ground truth is currently very difficult to capture outside the lab. This either pushes quantitative assessment back to the lab, or makes it dominantly qualitative and inherently subjective. It is also difficult to design visual capture scenarios systematically in order to improve performance, based on identified failure modes.</p><p>HSPACE (Synthetic Parametric Humans Animated in Complex Environments) is a large-scale dataset that contains high resolution images and video of multiple people together with their ground truth 3d representation based on GHUM -a state-of-the art full-body, expressive statistical pose and shape model. HSPACE contains multiple people in diverse poses and motions (including hand gestures), at different scene positions and scales, and with different body shapes, ethnicity, gender and age. People are placed in synthetic complex scenes and under natural or artificial illumination, as simulated by accurate light transport algorithms. The dataset also features occlusion due to other people, objects, or the environment, and camera motion.</p><p>In order to produce HSPACE, we rely a corpus of 100 diverse 3d human scans (purchased from RenderPeople <ref type="bibr" target="#b31">[32]</ref>), with parametric varying shape, animated with over a 100 real human motion capture snippets (from the CMU human motion capture dataset), and placed in 100 different synthetic indoor and outdoor environments, available from various free and commercial sources. We automatically animate the static human scans and we consistently place multiple people and motions, sampled from our asset database, into various scenes. We then render the resulting scenes for different cinematic camera viewpoints at 4K/HDR, using a realistic, high-quality game-engine.</p><p>Our contribution is the construction of a large scale automatic system, which requires considerable time as well as human and material resources in order to perfect. The system supports our construction of a 3d dataset, HSPACE, unique in its large-scale, complexity and diversity, as well as accuracy and ground-truth granularity. Such features complement and considerably extend the current dataset portfolio of our research community, being essential for progress in the field. To make the approach practical and scalable we also develop: (1) procedures to fit GHUM to complex 3d human scans of dressed people with capacity to retarget and animate both the body and clothing, automatically, with realistic results, and (2) automatic 3d scene placement methodology to temporally avoid collisions between people, as well as between people and the environment. Finally, we present large-scale studies revealing insight into practical uses of synthetic data, the importance of using weaklysupervised real data in bridging the sim-to-real gap, and the potential for improvement as model capacity increases. The dataset and an evaluation server will be made available for research and performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are quite a few people datasets with various degrees of supervision: 2d joint annotations, semantic segmentations <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref>, or 3d by fitting a statistical body model or from multi-camera views <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref>, dense pose <ref type="bibr" target="#b33">[34]</ref>, indoor mocap datasets with 3d pose ground truth for single or multiple people <ref type="bibr">[6-9, 13, 36]</ref>, in the wild datasets where IMUs and mobile devices were used to recover 3d pseudo ground truth joints <ref type="bibr" target="#b39">[40]</ref>. All these datasets contain real images, however the variability of the scenes and the humans is limited and the 3d ground truth accuracy is subject to annotators bias, joint positioning errors (for mocap) or IMUs sensor data optimization errors. It is also difficult to increase the diversity of a real dataset, as one cannot capture the same exact sequence from e.g. a different camera viewpoint.</p><p>In order to address the above-mentioned issues, efforts have been made to generate data synthetically using photorealistic 3d assets (scenes, characters, motions). Some synthetic datasets compose statistical body meshes or 3d human scans with realistic human textures on top of random background images, HDRI backdrops or 3d scenes with limited variability <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>, or rely on game engine simulations to recover human motions and trajectories <ref type="bibr" target="#b2">[4]</ref>. <ref type="table">Table 1</ref> reviews some of the most popular datasets along several important diversity axes. Our HSPACE dataset addresses some of the limitations in the state of the art by diversifying over people, poses, motions and scenes, all within a realistic rendering environment and by providing a rich set of 2d and 3d annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our methodology consists of (1) procedures to fit the GHUM body model to a dressed human scan, as well as realistically repose and reshape it (repose and reshape logic), and (2) methods to place multiple moving (dressed) scans animated using GHUM, into a scene in a way that is physically consistent so that people do not collide with each other and with the environment (dynamic placement logic). Statistical GHUM Body Model. We rely on GHUM <ref type="bibr" target="#b40">[41]</ref>, a recently introduced statistical body model in order to represent and animate the human scans in the scene. The shape space ? of the model is represented by a variational autoencoder. The pose space ? = (? b , ? lh , ? rh ) is represented using normalizing flows <ref type="bibr" target="#b43">[44]</ref> with separate components for global rotation r ? R 6 <ref type="bibr" target="#b49">[50]</ref> and translation t ? R 3 . The output of the model is a mesh M (?, ?) = (V, F), where V ? R 10,168?3 are the vertices and F are the 20, 332 faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fitting GHUM to Clothed Human Scans</head><p>The first stage in our pipeline is to fit the GHUM [41] model to an initial 3d scan of a person M s = (V s , F s , T s ) containing vertices V s ? R Ns , faces F s ? N Nts?3 and texture information T s containing per vertex U V coordinates and normal, diffuse and specular maps. The task is to find a set of parameters (?, ?, r, t) such that the target GHUM <ref type="bibr" target="#b40">[41]</ref> </p><formula xml:id="formula_0">mesh M t (?, ?, r, t) = (V t , F t )</formula><p>is an accurate representation of the underlying geometry of M s . For the sake of simplicity, we drop the dependence on the parameters r and t. As illustrated in <ref type="figure">fig. 5</ref>, we uniformly sample camera views around the subject and render it using the texture information associated to M s . Image keypoints for the body, face, and hands are predicted for each view using a standard regressor <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b40">41]</ref> and we triangulate to obtain a 3d skeleton J s ? R Nj ?3 for the source mesh. The fitting procedure of the GHUM mesh M t (?, ?) to M s is formulated as a nonlinear optimization problem with the following ob-</p><formula xml:id="formula_1">jective L (?, ?) =? j L j (J t , J s ) + L m (V t , V s ) + l (?) + l (?) . (1) ? * , ? * = arg min(L (?, ?))<label>(2)</label></formula><p>In <ref type="formula" target="#formula_1">(2)</ref>, J t ? R Nj are the skeleton joints for the posed mesh M t (?, ?) and L j (J t , J s ) = 1 Nj Nj i=1 J s,i ? J t,i 2 is the 3d mean per joint position error between the 3d joints of the source and those of the target. L m (V t , V s ) is an adaptive iterative closest point loss between the target vertices V t and the source vertices V s . At each optimization step we split the vertices V t into two disjoint subsets: the vertices V i t that are inside M s and the vertices V o t which are outside of M s . In order to classify a vertex as inside or outside, we rely on a fast implementation of the generalized winding number test <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b12">14]</ref>. Given the closest distance d between a point p and a vertex set V</p><formula xml:id="formula_2">c(p, V) = min q?V d(p, q)<label>(3)</label></formula><p>we define L m as follows</p><formula xml:id="formula_3">L m = ? i p?V i t c(p, V s ) + ? o p?V o t c(p, V s )<label>(4)</label></formula><p>We set ? i &lt; ? o , enforcing the reconstructed mesh M t to be inside of M s , but close to the surface. We add regularization for pose and shape based on their native latent space priors l(?) = ? 2 2 , l(?) = ? 2 2 in order to penalize deviations from the mean of their Gaussian distributions. Reposing and Reshaping Clothed People. We compute displacements from GHUM to the scanned mesh in a local coordinate system. For each vertex of the scan, we consider its nearest neighbor point on the GHUM mesh. This point is parameterized by barycentric coordinates. When the GHUM mesh is generated for different pose and shape parameters, its local geometry rotates and scales. We want displacements between the scan and the updated GHUM geometry be preserved. We use a tangent space coordinate system that allows equivariance to rotations. Furthermore, due to the way the tangent space is computed, based on triangle surface area, we are also invariant to scale deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reposing and Reshaping Clothed People</head><p>We design an automated process of generating largescale animations of the same subject's scan, but with different shape characteristics. We need the animation process to be compatible with LBS pipelines, such as Unreal Engine, in order to automate the rigging and rendering process for large-scale data creation. This is a non-physical process in the absence of explicit clothing models, but we aim for automation and scalability, rather than perfect simulation fidelity. We aim not only for animation diversity, but also for shape diversification. We support transformations in the tangent-space of local surface geometry that can accommodate changes in both shape and pose -this is different from inverse skinning methods <ref type="bibr" target="#b10">[12]</ref> that only handle the latter.</p><p>Tangent-space representation. Given a source scan with mesh M s and its fitted GHUM mesh M t , we compute a displacement field D ? R Ns?3 from M s to M t . For each vertex v k ? V s we compute its closest point v k on M t and we denote by a k ? R 3 its barycentric coordinates on the projection face f k ? F t . From all values a k , k ? 1, . . . , N s we build a sparse connection matrix A ? R Ns?Nt . The displacement field D from M s to M t is defined as</p><formula xml:id="formula_4">D = V s ? V s ,<label>(5)</label></formula><p>where V s ? R Ns?3 are all the stacked closest points v k and V s = AV t . We want each of the displacement vectors d k in D to reside in a local coordinate system determined by the supporting local geometry {a k , f k ? F t }. Hence, we compute associated normal n k , tangent t k and bitangent b k vectors. The normals and tangents are interpolated given per-vertex information available for the faces f k ? F t , a k . Per-vertex tangents are a function of the UV coordinates. For more details on the usage of UV coordinates to obtain tangents, see <ref type="bibr" target="#b29">[30]</ref>. After Gram-Schmidt orthonormalization of tangents and normals, we derive a rotation matrix R k = [t k ; n k ; t k ? n k ] ? R 3?3 representing a local coordinate system for each displacement vector d k . We stack the rotation matrices for all displacement vectors and construct R ? R Ns?3?3 . Controlling shape and pose. For a target set of pose and shape parameters (? , ? ) of GHUM, let M t (? , ? ) = (V t , F t ) be the new target GHUM posed mesh with vertices V t . The task is to find M s (V s , F s ) which would correspond to the same change in pose and shape for M s . For that, we first compute V s = AV t . Using V s and M t we get updated local orientations R for each v k ? V s from the normal, tangent and bitangent vectors similarly to R. Note R R ?1 gives the change of orientation for the supporting faces f k ? F t from v k to v k . We use them to compute the change in orientation for the displacement field D</p><formula xml:id="formula_5">V s = V s + R R ?1 D<label>(6)</label></formula><p>and obtain the corresponding mesh M s (V s , F s ).</p><p>Rendering engine compatibility Rendering engines use linear blend skinning to display realtime realistic animations, so we cannot incorporate tangent-space transformations to drive the animation. Instead, we use tangent-space transformations to compute a new target rest mesh (this is equivalent to unposing and reshaping), with different body shapes sampled from the latent distribution of the GHUM model, and then continue the animation by LBS. We compute the skinning weights for M s as W s = AW t , where W t ? R Nt?Nj are the skinning weights for M t . The skeleton animation posing values, skinning matrix W s and updated rest mesh M s are sufficient for animation export. The limitations of our animation method lie in the hair or clothing simulation which lacks physical realism. However, this geometric animation process is efficient and easy to compute and, as can be seen in <ref type="figure" target="#fig_2">fig. 6</ref>, results are visually plausible within limits. Our quantitative experiments show that such synthesis methodology improves performance on challenging tasks like 3d pose and shape estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene Placement Logic</head><p>In order to introduce multiple animated scans into scenes, we develop a methodology for automatic scene placement based on free space calculations. Typically, we sample several people, their shape, and their motions as well as a bounded, square region of the synthetic scene, so it can be comfortably observed by 4 cameras placed in the corners of the square at different elevations. This is important as some synthetic scenes could be very large, and sampling may generate people spread too far apart or not even visible in any of the virtual cameras.</p><p>The union of tightly bounding parallelepipeds for each human shape at each timestep of their animation defines a motion volume. These are aligned with a global threedimensional grid. The objective is to estimate a set of positions and planar orientations for the motion volumes, such that no two persons occupy the same unit volume at the same motion timestep (as otherwise trajectories from different people at different timesteps can collide).</p><p>Given a scene (3d bounding boxes around any objects including the floor/ground), we sample a set of random motion volumes and initially place them into the scene such that the mid point of the motion paths is in the middle of the scene. We define a loss function which is the sum of a) number of collision between the sequences (defined as their time-varying 3d bounding boxes intersecting or intersecting with object bounding boxes) and b) the number of time steps when they are outside the scene bounding box.</p><p>The input to the loss function is a set of per-sequence translation variables, as well as rotations around the axis of ground normal. We then minimize this loss function using a non-differentiable covariance matrix adaptation optimization method (CMA) <ref type="bibr" target="#b9">[11]</ref> over the initial translation and rotation of the motion volumes, and only accept solutions where the physical loss is 0 (i.e. has no collisions and all sequences are inside the scene bounding box). While the scene placement model can be improved in a number of ways, including the use of physical models or environmental semantics it provides an automation baseline for initial synthesis. See <ref type="figure" target="#fig_1">fig.3</ref> for an illustration. Automatic Pipeline. We designed a pipeline, such that given a query for a specific body scan asset, animation and scene, we produce a high quality rendering placed automatically, at a physically plausible location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">HSPACE dataset</head><p>Dataset Statistics. Our proposed HSPACE dataset was created by using 100 unique photogrammetry scans of people from the commercial dataset RenderedPeople [1]. We reshape the scans using our proposed methodology (see section 3.2), with 16 uniformly sampled shape parameters sampled from GHUM's VAE shape space. For animation, we use 100 CMU motion capture sequences for which we have corresponding GHUM pose parameters. For background variation, we use 100 complex, good quality 3d scenes. These include both indoor and outdoor environments. To create a sequence in our dataset, we randomly sample from all factors of variation and place the animations in the scene using our scene placement method (see section 3.3). In total, we collect 1, 000, 000 unique rendered frames, each consisting of 5 subjects on average. An example of a scene with multiple dynamic people is shown in <ref type="figure">fig.1</ref>. Rendering. HSPACE images and videos are rendered using Unreal Engine 5 at 4k resolution. The rendering uses raytracing, high resolution light mapping, screen-space ambient occlusion, per-category shader models (e.g. Burley subsurface scattering for human skin), temporal anti-aliasing and motion blur. For each frame we capture the ground truth 3d pose of the various people inserted in the scene and save render passes for the finally rendered RGB output, as well as segmentation masks. On average, our system renders at 1 frame/s including saving data on disk. All of our dataset was rendered on 10 virtual machines with GPU support running in the cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We validate the utility of HSPACE for both training and evaluation of 3d human pose and shape reconstruction models. We split HSPACE 80/20% into training and testing subsets, respectively. We use different people and animation assets for each split.</p><p>We additionally employ a dataset with images in-thewild, Human Internet Images (HITI) (100,000 images), of more than 20,000 different people performing activities with highly complex poses (e.g. yoga, sports, dancing). This dataset was collected in-house and is annotated with both 2d keypoints and body part segmentation. We <ref type="figure">Figure 4</ref>. Three scans with different appearance and body mass index, synthesised using GHUM statistical shape parameters, based on a single scan of each subject. Notice plausible body shape variations and reasonable automatic clothing deformation as body mass varies. <ref type="figure">Figure 5</ref>. Main processing pipeline for our synthetic human animations. Given a single 3d scan of a dressed person, we automatically fit GHUM to the scan, and build a representation that supports the plausible animation of both the body and the clothing based on different 3d human motion capture signals. Shape can be varied too -notice also plausible positioning for the fringes of the long blouse outfit.  use it our experiments for training in a weakly supervised regime. The test version of this dataset, Human Internet Images (HITI-TEST), consists of 40,000 images with fitted GHUM parameters under multiple depth-ordering constraints that we can use as pseudo ground-truth for evaluation in-the-wild (see our Sup. Mat. for details). Evaluation of GHUM Fitting to Human Scans. In order to evaluate our GHUM fitting procedure, we compute errors of the nonlinear optimization fit in (2) with keypoints only (L j ), as well as for the full optimization (L j + L m ) with L m as well. Results are given in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fitting Method</head><p>V2V Chamfer Lj 10 13 Lj + Lm 8 11 In all experiments we train models for 3d human pose and shape estimation based on the THUNDR architecture <ref type="bibr" target="#b46">[47]</ref>. We report standard 3d reconstruction errors used in the literature: mean per joint position errors with and without Procrustes alignment (MPJPE, MPJE-PA) for the 3d pose and mean per vertex errors with and without Procrustes alignment (MPVPE, MPVPE-PA) for the 3d shape, as well as global translation errors.</p><p>We present the experimental results on the test set of HSPACE in table 4. First we report results for various state of the art 3d pose and shape estimation models such as HUND <ref type="bibr" target="#b44">[45]</ref>, THUNDR <ref type="bibr" target="#b46">[47]</ref>, SPIN <ref type="bibr" target="#b18">[20]</ref> and VIBE <ref type="bibr" target="#b17">[19]</ref>. The first two methods estimate GHUM mesh parameters, while the last two methods output SMPL mesh parameters. Both SPIN <ref type="bibr" target="#b18">[20]</ref> and VIBE <ref type="bibr" target="#b17">[19]</ref> use orthographic projection camera models so we can not report translation errors. We train a weakly supervised (WS) version of THUNDR on the HITI training dataset and fine tune it on HSPACE in a fully supervised (FS) regime. This model performs better than all other state of the art methods. The best reconstruction results are obtained by a modified temporal version of THUNDR (labeled as T-THUNDR in table 4) with the same number of parameters as the single-frame version. We provide details of this architecture in the Sup. Mat.</p><p>We also train and evaluate on a widely used dataset in the literature, the Human3.6M <ref type="bibr" target="#b11">[13]</ref> dataset. This is an indoor benchmark with ground-truth 3d joints obtained from a motion capture system. We report results on protocol P1 (100,000 images) where subjects S1, S5-S8 are used for training, and subjects S9 and S11 are used for testing. In <ref type="table" target="#tab_2">table 3</ref> we show that a refined variant of the THUNDR <ref type="bibr" target="#b46">[47]</ref> architecture on HSPACE training data achieves the lowest reconstruction errors under all metrics.</p><p>We also performed a comprehensive study in order to understand the impact of increasing the size of synthetic data on model performance. Other important factors are the simto-real gap, the importance of real data, and the influence of model capacity on performance. One of the most practical approaches would be to use large amounts of supervised synthetic data, as well as potentially large amounts of real images without supervision. The question is whether this combination would help and how would the different factors (synthetic data, real data, model capacity, initialisation and curriculum ordering) play on performance.</p><p>We trained a battery of models with different fractions of weakly supervised real data (10%, 30% or 100% of HITI-TRAIN), fully supervised synthetic data (0%, 10%, 30%, 60%, 100% of HSPACE-TRAIN), and for two model sizes (small THUNDR model with a transformer component of 1.9M parameters, and a big THUNDR model with a transformer component of 3.8M parameters). All models were evaluated on HSPACE-TEST (first and second columns in <ref type="figure">figure 8</ref>) as well as on HITI-TEST for complex real images. Results are presented in <ref type="figure">fig. 8</ref>.</p><p>Empirically we found that models trained on synthetic data alone do not perform the best, not even when tested on synthetic data. Moreover, we found that pre-training with real data and refining on synthetic data produces better results than vice-versa. Large volumes of synthetic data improve model performance in conjunction with increasing amounts of weakly annotated real data, which is important as this is a practical setting and the symbiosis of synthetic and real data during training appears to address the simto-real gap. An increase in model capacity seems however necessary in order to take advantage of larger datasets. Ethical Considerations. Our dataset creation methodology aims at diversity and coverage in order to build synthetic <ref type="figure">Figure 8</ref>. Performance on HSPACE-TEST set (plots in the first and second rows) and HITI-TEST set (plots in third and fourth rows) for THUNDR (WS+FS) models with different capacities (SMALL for a THUNDR model with a transformer component of 1.9M parameters and BIG for a THUNDR model with a transformer component of 3.8M parameters, see supplementary material for more details) trained with various percentages of HITI (real) and HSPACE (synthetic) data. The THUNDR models were first trained in weakly supervised (WS) regime on the percentage of HITI data indicated in the legend and then refined in a fully supervised (FS) regime on different amounts of HSPACE data as well. We report MPJPE-PA and MPJPE metrics. We observe performance improvements when adding greater amounts of both synthetic and real data, as well as when increasing the model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MPJPE-PA MPJPE MPJPE-T HMR <ref type="bibr" target="#b16">[18]</ref> 58.1 88.0 NR HUND <ref type="bibr" target="#b44">[45]</ref> 53 ground-truth for different human body proportions, poses, motions, ethnicity, age, or clothing. By generating people in new synthetic poses, and by controlling different body proportions in various scenes, we can produce considerable diversity by largely relying on synthetic assets and by varying the parameters of a statistical human pose and shape model (GHUM). This supports, in turn, our long-term goal to build inclusive models that work well for everyone especially in cases where real human data as well as forms of 3d ground truth are difficult to collect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have introduced HSPACE, a large-scale dataset of humans animated in complex synthetic indoor and outdoor environments. We combine diverse individuals of varying ages, gender, proportions, and ethnicity, with many motions and scenes, as well as parametric variations in body shape, as well as gestures, in order to generate an initial dataset of over 1 million frames. Human animations are obtained by fitting an expressive human body model, GHUM, to single scans of people, followed by re-targeting and re-posing procedures that support realistic animation, statistic variations of body proportions, and jointly consistent scene placement for multiple moving people. All assets are generated automatically, being compatible with existing real time rendering engines. The dataset and an evaluation server will be made available for research.</p><p>Our quantitative evaluation of 3d human pose and shape estimation in synthetic and mixed (sim-real) regimes, underlines (1) the importance of synthetic, large-scale datasets, but also (2) the need for real data, within weakly supervised training regimes, as well as (3) the effect of increasing (matching) model capacity, for domain transfer, and continuing performance improvement as datasets grow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE-PA MPJPE MPVPE-PA MPVPE MPJPE-T R#2D R#2D-3D</head><p>S#3D SPIN <ref type="bibr" target="#b18">[20]</ref> 79  <ref type="table">Table 4</ref>. Results on the HSPACE test set. All current state of the art methods do not perform well when tested on the HSPACE test set. However performance improves significantly when training on HSPACE. We report mean per joint positional errors (with and without Procrustes alignment) (MPJPE-PA, MPJPE), mean per joint vertex displacement error (with and without Procrustes alignment) (MPVPE-PA, MPVPE) computed against ground truth GHUM meshes and translation error (MPJPE-T) computed against the pelvis joint. We also report the number of real images and the type of annotations used during the training of the listed models, e.g. number of real images with 2d annotations (R#2D), number of real images with paired 2d-3d annotations (R#2D-3D) used during training and number of synthetic images with full 3d supervision (S#3D). See our Sup. Mat. for additional detail and for qualitative visualisations of 3d human pose and shape reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Reposing and Reshaping Clothed People. We compute displacements from GHUM to the scanned mesh in a local coordinate system. For each vertex of the scan, we consider its nearest neighbor point on the GHUM mesh. This point is parameterized by barycentric coordinates. When the GHUM mesh is generated for different pose and shape parameters, its local geometry rotates and scales. We want displacements between the scan and the updated GHUM geometry be preserved. We use a tangent space coordinate system that allows equivariance to rotations. Furthermore, due to the way the tangent space is computed, based on triangle surface area, we are also invariant to scale deformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Dynamic placement logic ensures that multiple moving people follow plausible human motions, and are positioned in a scene in way that is consistent with spatial occupancy from other objects or people. An optimization algorithm ensures no two people occupy the same scene location at the same motion timestep. Trajectories are shown in color, with start/end denoted by A/B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Frames from HSPACE sequences with companion GHUM ground truth. Highly dynamic motions work best with characters wearing tight fitted clothing, the animated sequences look natural and smooth (bottom rows) but also notice good performance for less tight clothing (top rows). See our Sup. Mat. for videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Human scans animated and placed in complexly lit 3d scenes with background motion (e.g. curtains, vegetation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Fitting evaluation with vertex to vertex errors and bidirectional Chamfer distance. Values are reported in mm. Please see fig. 6 and our Sup. Mat. for qualitative visualizations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results obtained when refining THUNDR [47] on the HSPACE training set and evaluated on Human3.6M under training/testing assumptions of protocol P1 (100K testing samples). Refining on HSPACE improves over the previous SOTA under MPJPE-PA, MPJPE and translation errors (MPJPE-T).</figDesc><table><row><cell></cell><cell>.0</cell><cell>72.0</cell><cell>160.0</cell></row><row><cell>THUNDR [47]</cell><cell>39.8</cell><cell>55.0</cell><cell>143.9</cell></row><row><cell>THUNDR (HSPACE)</cell><cell>39.0</cell><cell>53.3</cell><cell>132.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https : / / github . com / google -research / googleresearch/tree/master/hspace</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Blazepose: On-device real-time body pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Bazarevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raveendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno>ECCV. 2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Threedimensional reconstruction of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Threedimensional reconstruction of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning complex 3d human self-contact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conf. on Artificial Intelligence (AAAI&apos;21)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aifit: Automatic 3d human-interpretable feedback models for fitness training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Silviu Cristian Pirlea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">REMIPS: Physically consistent 3d reconstruction of multiple interacting people under weak supervision</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<editor>Mihai Fieraru, Mihai Zanfir, Teodor Alexandru Szente, Eduard Gabriel Bazavan, Vlad Olaru, and Cristian Sminchisescu</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The CMA Evolution Strategy: A Comparing Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="75" to="102" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arch: Animatable reconstruction of clothed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust inside-outside segmentation using generalized winding numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Kavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno>33:1-33:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (proceedings of ACM SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On self-contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">T</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative parallax mapping with slope information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?ty?s</forename><surname>Premecz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Central European Seminar on Computer Graphics</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling the geometry of dressed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sanchez-Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renderpeople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renderpeople</surname></persName>
		</author>
		<title level="m">people for renderings</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Iasonas Kokkinos Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neverova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Estimating Articulated Human Motion with Covariance Scaled Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page" from="371" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">GHUM &amp; GHUML: Generative 3D human shape and articulated pose models. CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ultrapose: Synthesizing dense pose with 1 billion points by human-body decoupling 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianhong</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Humbi: A large multiview dataset of human body expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2990" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06910</idno>
		<title level="m">Neural descent for visual 3d human pose and shape</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformer-based 3d human reconstruction with markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thundr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07035</idno>
		<title level="m">On the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Simpose: Effectively learning densepose and surface normals of people from simulated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
