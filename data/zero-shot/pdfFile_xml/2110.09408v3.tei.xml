<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HRFormer: High-Resolution Transformer for Dense Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<addrLine>5 Baidu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HRFormer: High-Resolution Transformer for Dense Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Transformer that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet [46]), along with local-window self-attention that performs self-attention over small non-overlapping image windows <ref type="bibr" target="#b20">[21]</ref>, for improving the memory and computation efficiency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the High-Resolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer [27] by 1.3 AP on COCO pose estimation with 50% fewer parameters and 30% fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> shows promising performance on ImageNet classification tasks. Many follow-up works boost the classification accuracy through knowledge distillation <ref type="bibr" target="#b41">[42]</ref>, adopting deeper architecture <ref type="bibr" target="#b42">[43]</ref>, directly introducing convolution operations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref>, redesigning input image tokens <ref type="bibr" target="#b53">[54]</ref>, and etc. Besides, some studies attempt to extend the transformer to address broader vision tasks such as object detection <ref type="bibr" target="#b3">[4]</ref>, semantic segmentation <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b36">37]</ref>, pose estimation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b22">23]</ref>, video understanding <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30]</ref>, and so on. This work focuses on the transformer for dense prediction tasks, including pose estimation and semantic segmentation.</p><p>Vision Transformer splits an image into a sequence of image patches of size 16 ? 16, and extracts the feature representation of each image patch. Thus, the output representations of Vision Transformer lose the fine-grained spatial details that are essential for accurate dense predictions. The Vision Transformer only outputs a single-scale feature representation, and thus lacks the capability to handle multi-scale variation. To mitigate the loss of feature granularity and model the multi-scale variation, we present High-Resolution Transformer (HRFormer) that contains richer spatial information and constructs multi-resolution representations for dense predictions.</p><p>The High-Resolution Transformer is built by following the multi-resolution parallel design that is adopted in HRNet <ref type="bibr" target="#b45">[46]</ref>. First, HRFormer adopts convolution in both the stem and the first stage as several concurrent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref> also suggest that convolution performs better in the early stages. Second, HRFormer maintains a high-resolution stream through the entire process with parallel medium-and low-resolution streams helping boost high-resolution representations. With feature maps of different resolutions, thus HRFormer is capable to model the multi-scale variation. Third,  <ref type="figure">Figure 1</ref>: Illustrating the HRFormer block. The HRFormer block is composed of (a) local-window selfattentionm and (b) feed-forward network (FFN) with depth-wise convolution. The local-window self-attention scheme is inspired by the interlaced sparse self-attention <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>HRFormer mixes the short-range and long-range attention via exchanging multi-resolution feature information with the multi-scale fusion module.</p><p>At each resolution, the local-window self-attention mechanism is adopted to reduce the memory and computation complexity. We partition the representation maps into a set of non-overlapping small image windows and perform self-attention in each image window separately. This reduces the memory and computation complexity from quadratic to linear with respect to spatial size. We further introduce a 3 ? 3 depth-wise convolution into the feed-forward network (FFN) that follows the local-window self-attention, to exchange information between the image windows which are disconnected in the local-window self-attention process. This helps to expand the receptive field and is essential for dense prediction tasks. <ref type="figure">Figure 1</ref> shows the details of an HRFormer block.</p><p>We conduct experiments on image classification, pose estimation, and semantic segmentation tasks, and achieve competitive performance on various benchmarks.  <ref type="bibr" target="#b12">[13]</ref> and the data-efficient image transformer (DeiT) <ref type="bibr" target="#b41">[42]</ref>, various techniques are proposed to improve the ImageNet classification accuracy of Vision Transformer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref>. Among the very recent advancements, the community has verified several effective improvements such as multi-scale feature hierarchies and incorporating convolutions.</p><p>For example, the concurrent works MViT <ref type="bibr" target="#b13">[14]</ref>, PVT <ref type="bibr" target="#b46">[47]</ref>, and Swin <ref type="bibr" target="#b26">[27]</ref> introduce the multi-scale feature hierarchies into transformer following the spatial configuration of a typical convolutional architecture such as ResNet-50. Different from them, our HRFormer incorporates the multi-scale feature hierarchies through exploiting the multi-resolution parallel design inspired by HRNet. CvT <ref type="bibr" target="#b47">[48]</ref>, CeiT <ref type="bibr" target="#b52">[53]</ref>, and LocalViT <ref type="bibr" target="#b24">[25]</ref> propose to enhance the locality of transformer via inserting depth-wise convolutions into either the self-attention or the FFN. The purpose of the inserted convolution within our HRFormer is different, apart from enhancing the locality, it also ensures information exchange across the non-overlapping windows.</p><p>Several previous studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19]</ref> have proposed similar local self-attention schemes for image classification. They construct the overlapped local windows following the strided convolution, resulting in heavy computation cost. Similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27]</ref>, we propose to apply the local-window self-attention scheme to divide the input feature map into non-overlapping windows. Then we apply the self-attention within each window independently so as to improve the efficiency significantly.</p><p>There are several concurrently-developed works <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b36">37]</ref> use the Vision Transformer to address the dense predict tasks such as semantic segmentation. They have shown that increasing the spatial resolution of the representations output by the Vision Transformer is important for semantic segmen-  tation. Our HRFormer provides a different path to address the low-resolution problem of the Vision Transformer via exploiting the multi-resolution parallel transformer scheme.</p><p>High-Resolution CNN for Dense Prediction. The high-resolution convolutional schemes have achieved great success on both pose estimation and semantic segmentation tasks. In the development of high-resolution convolutional neural networks, the community has developed three main paths including: (i) applying dilated convolutions to remove some down-sample layers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52]</ref>, (ii) recovering high-resolution representations from low-resolution representations with decoders <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, and (iii) maintaining high-resolution representations throughout the network <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20]</ref>. Our HRFormer belongs to the third path, and retains the advantages of both vision transformer and HRNet <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">High-Resolution Transformer</head><p>Multi-resolution parallel transformer. We follow the HRNet <ref type="bibr" target="#b45">[46]</ref> design and start from a highresolution convolution stem as the first stage, gradually adding high-to-low resolution streams one by one as new stages. The multi-resolution streams are connected in parallel. The main body consists of a sequence of stages. In each stage, the feature representation of each resolution stream is updated with multiple transformer blocks independently and the information across resolutions is exchanged repeatedly with the convolutional multi-scale fusion modules. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the overall HRFormer architecture. The design of convolutional multi-scale fusion modules exactly follows HRNet. We illustrate the details of the transformer block in the following discussion and more details are presented in <ref type="figure">Figure 1</ref>.</p><p>Local-window self-attention. We divide the feature maps X ? R N ?D into a set of non-overlapping small windows: X ? {X 1 , X 2 , ? ? ? , X P }, where each window is of size K ? K. We perform multi-head self-attention (MHSA) within each window independently. The formulation of multi-head self-attention on the p-th window is given as:</p><formula xml:id="formula_0">MultiHead(X p ) = Concat[head(X p ) 1 , ? ? ? , head(X p ) H ] ? R K 2 ?D ,<label>(1)</label></formula><formula xml:id="formula_1">head(X p ) h = Softmax (X p W h q )(X p W h k ) T D /H X p W h v ? R K 2 ? D H ,<label>(2)</label></formula><formula xml:id="formula_2">X p = X p + MultiHead(X p )W o ? R K 2 ? D H ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">W o ? R D?D , W h q ? R D H ?D , W h k ? R D H ?D , and W h v ? R D H ?D for h ? {1</formula><p>, ? ? ? , H}. H represents the number of heads, D represents the number of channels, N represents the input resolutions, and X p represents the output representation of MHSA. We also apply the relative position embedding scheme introduced in the T5 model <ref type="bibr" target="#b34">[35]</ref> to incorporate the relative position information into the local-window self-attention.</p><p>With MHSA aggregates information within each window, we merge them to compute the output X MHSA :</p><formula xml:id="formula_4">{ X 1 , X 2 , ? ? ? , X P } Merge ? ??? ? X MHSA .<label>(4)</label></formula><p>The left part of <ref type="figure">Figure 1</ref> illustrates how local-window self-attention updates the 2D input representations, where the multi-head self-attention operates within each window independently.  FFN with depth-wise convolution. Local-window self-attention performs self-attention over the non-overlapping windows separately. There is no information exchange across the windows. To handle this issue, we add a 3 ? 3 depth-wise convolution in between the two point-wise MLPs that form the FFN in Vision transformer: MLP(DW-Conv.(MLP())). The right part of <ref type="figure">Figure 1</ref> shows an example of how FFN with 3 ? 3 depth-wise convolution updates the 2D input representations.</p><formula xml:id="formula_5">? ? ?B1?M1 LSA,W1,H1 FFN-DW,R1 ?B2?M2 LSA,W1,H1 FFN-DW,R1 ?B3?M3 LSA,W1,H1 FFN-DW,R1 ?B4?M4 8? LSA,W2,H2 FFN-DW,R2 ?B2?M2 LSA,W2,H2 FFN-DW,R2 ?B3?M3 LSA,W2,H2 FFN-DW,R2 ?B4?M4 16? LSA,W3,H3 FFN-DW,R3 ?B3?M3 LSA,W3,H3 FFN-DW,R3 ?B4?M4 32? LSA,W4,H4 FFN-DW,R4 ?B4?M4</formula><p>Representation head designs. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the output of HRFormer consists of four feature maps of different resolutions. We illustrate the details of the representation head designs for different tasks as following: (i) ImageNet classification, we send the four-resolution feature maps into a bottleneck and the output channels are changed to 128, 256, 512, and 1024 respectively. Then, we apply the strided convolutions to fuse them and output a feature map of the lowest resolution with 2048 channels. Last, we apply a global average pooling operation followed by the final classifier. (ii) pose estimation, we only apply the regression head over the highest resolution feature map. (iii) semantic segmentation, we apply the semantic segmentation head over the concatenated representations, which are computed by first upsampling all the low-resolution representations to the highest resolution and then concatenate them together. Instantiation. We illustrate the overall architecture configuration of HRFormer in <ref type="table" target="#tab_1">Table 1</ref>. We</p><formula xml:id="formula_6">use (M 1 , M 2 , M 3 , M 4 ) and (B 1 , B 2 , B 3 , B 4 )</formula><p>to represent the number of modules and the number of blocks of {state1, stage2, stage3, stage4}, respectively. We use</p><formula xml:id="formula_7">(C 1 , C 2 , C 3 , C 4 ), (H 1 , H 2 , H 3 , H 4 ) and (R 1 , R 2 , R 3 , R 4 )</formula><p>to represent the number of channels, the number of heads and the MLP expansion ratios in transformer block associated with different resolutions. We keep the first stage unchanged following the original HRNet and use the bottleneck as the basic building block. We apply the transformer blocks in the other stages and each transformer block consists of a local-window self-attention followed by an FFN with 3 ? 3  depth-wise convolution. We have not included the convolutional multi-scale fusion modules in <ref type="table" target="#tab_1">Table 1</ref> for simplicity. In our implementation, we set the size of the windows on four resolution streams as (7, 7, 7, 7) by default. <ref type="table" target="#tab_2">Table 2</ref> illustrates the configuration details of three different HRFormer instances with increasing complexities, where the MLP expansion ratios (R 1 , R 2 , R 3 , R 4 ) are set as <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4)</ref> for all models and are not shown.</p><p>Analysis. The benefits of 3 ? 3 depth-wise convolution are twofold: one is enhancing the locality and the other one is enabling the interactions across windows. We illustrate how the FFN with depth-wise convolution is capable to expand the interactions beyond the non-overlapping local windows and model the relations between them in <ref type="figure" target="#fig_3">Figure 3</ref>. Therefore, based on the combination of the localwindow self-attention and the FFN with 3 ? 3 depth-wise convolution, we can build the HRFormer block that improves the memory and computation efficiency significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Pose Estimation</head><p>Training setting. We study the performance of HRFormer on the COCO <ref type="bibr" target="#b25">[26]</ref> human pose estimation benchmark, which contains more than 200K images and 250K person instances labeled with 17 keypoints. We train our model on COCO train 2017 dataset, including 57K images and 150K person instances. We evaluate our approach on the val 2017 set and test-dev 2017, containing 5K images and 20K images, respectively.</p><p>We follow most of the default training and evaluation settings of mmpose [8] 2 , and change the optimizer from Adam to AdamW. For the training batch size, we choose 256 for HRFormer-T and HRFormer-S and 128 for HRFormer-B due to limited GPU memory. Each HRFormer experiment on COCO pose estimation task takes 8? 32G-V100 GPUs.</p><p>Results. <ref type="table" target="#tab_3">Table 3</ref> reports the comparisons on COCO val set. We compare HRFormer to the representative convolutional method such as HRNet <ref type="bibr" target="#b40">[41]</ref> and several recent transformer methods, including PRTR <ref type="bibr" target="#b22">[23]</ref>, TransPose-H-A6 <ref type="bibr" target="#b50">[51]</ref>, and TokenPose-L/D24 <ref type="bibr" target="#b23">[24]</ref>. HRFormer-B gains 0.9% with 32% fewer parameters and 19% fewer FLOPs when compared to HRNet-W48 with an input size of 384 ? 288. Therefore, our HRFormer-B already achieves 77.2% w/o using any advanced techniques such as UDP <ref type="bibr" target="#b19">[20]</ref> and DARK <ref type="bibr" target="#b58">[59]</ref>. We believe that our HRFormer-B could achieve better results by exploiting either UDP or DARK scheme. We also report the comparisons on COCO test-dev set in <ref type="table" target="#tab_4">Table 4</ref>. Our HRFormer-B outperforms HRNet-W48 by around 0.7% with fewer parameters and FLOPs. <ref type="figure" target="#fig_4">Figure 4</ref> shows some example results of human pose estimation on COCO val set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Segmentation</head><p>Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b8">[9]</ref> is for urban scene understanding. There are a total of 30 classes and only 19 classes are used for parsing evaluation. The dataset contains 5K high-quality pixel-level finely annotated images and 20K coarsely annotated images. The finely annotated 5K images are divided into 2, 975 train images, 500 val images and 1, 525 test images. We set the initial learning rate as 0.0001, weight decay as 0.01, crop size as 1024 ? 512, batch size as 8, and training iterations as 80K by default. Each HRFormer + OCR experiment on Cityscapes takes 8? 32G-V100 GPUs. <ref type="table" target="#tab_5">Table 5</ref> reports the results on Cityscapes val. We choose to use HRFormer + OCR as our semantic segmentation architecture. We compare our method with several well-known Vision Transformer based methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b36">37]</ref> and CNN based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b54">55]</ref>. Specifically, SETR-PUP and SETR-MLA use the ViT-Large <ref type="bibr" target="#b12">[13]</ref> as the backbone. DPT-Hybrid uses the ViT-Hybrid <ref type="bibr" target="#b12">[13]</ref> that consists of a ResNet-50 followed by 12 transformer layers. Both ViT-Large and ViT-Hybrid are initialized with the weights pre-trained on ImageNet-21K, where both of them achieve around 85.1% top1 accuracy on ImageNet. DeepLabv3 <ref type="bibr" target="#b5">[6]</ref> and PSPNet <ref type="bibr" target="#b61">[62]</ref> are based on dilated ResNet-101 with output stride 8.</p><p>According to the fourth column of <ref type="table" target="#tab_5">Table 5</ref>, HRFormer + OCR achieves competitive performance overall. For example, HRFormer-B + OCR achieves comparable performance with SETR-PUP while saving 70% parameters and 50% FLOPs. COCO-Stuff. The COCO-Stuff dataset <ref type="bibr" target="#b2">[3]</ref> is a challenging scene parsing dataset that contains 171 semantic classes. The train set and test set consist of 9K and 1K images respectively. We set the initial learning rate as 0.0001, weight decay as 0.01, crop size as 520 ? 520, batch size as 16, and training iterations as 60K by default. We report the comparisons on the last column of <ref type="table" target="#tab_5">Table 5</ref> and HRFormer-B + OCR outperforms the previous best-performing HRNet-W48 + OCR by nearly 2%. Each HRFormer + OCR experiment on COCO-Stuff takes 8? 32G-V100 GPUs. <ref type="figure" target="#fig_5">Figure 5</ref> shows some example results on Cityscapes, PASCAL-Context, and COCO-Stuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ImageNet Classification</head><p>Training setting. We conduct the comparisons on ImageNet-1K, which consists of 1.28M train images and 50K val images with 1000 classes. We train all models with batch size 1024 for 300 epochs with AdamW <ref type="bibr" target="#b27">[28]</ref> optimizer, cosine decay learning rate schedule, weight decay as 0.05, and a bag of augmentation policies, including rand augmentation <ref type="bibr" target="#b9">[10]</ref>, mixup <ref type="bibr" target="#b59">[60]</ref>, cutmix <ref type="bibr" target="#b57">[58]</ref>, and so on. HRFormer-T and HRFormer-S require 8 ? 32G-V100 GPUs and HRFormer-B requires 32 ? 32G-V100 GPUs.</p><p>Results. We compare HRFormer to some representative CNN methods and vision transformer methods in <ref type="table" target="#tab_6">Table 6</ref>, where all methods are trained on ImageNet-1K only. The results of ViT-Large with larger dataset such as ImageNet-21K not included for fairness. According to <ref type="table" target="#tab_6">Table 6</ref>, HRFormer achieves competitive performance. For example, HRFormer-B gains 1.0% over DeiT-B while saving nearly 40% parameters and 20% FLOPs.  Influence of shifted window scheme &amp; 3?3 depth-wise convolution within FFN based on Swin-T. We compare our method with the shifted windows scheme of Swin transformer <ref type="bibr" target="#b26">[27]</ref> in <ref type="table" target="#tab_8">Table 8</ref>.</p><p>For fair comparisons, we construct a Intra-Window transformer architecture following the same  architecture configurations of Swin-T <ref type="bibr" target="#b26">[27]</ref> except that we do not apply shifted windows scheme. We see that applying 3?3 depth-wise convolution within FFN improves both Swin-T and Intrawin-T. Surprisingly, when equipped with 3? 3 depth-wise convolution within FFN, Intrawin-T even outperforms Swin-T.</p><p>Shifted window scheme v.s. 3?3 depth-wise convolution within FFN based on HRFormer-T. In <ref type="table" target="#tab_9">Table 9</ref>, we compare the 3 ? 3 depth-wise convolution within FFN scheme to the shifted window scheme based on HRFormer-T. According to the results, we see that applying 3?3 depthwise convolution within FFN significantly outperforms applying shifted window scheme across all different tasks.</p><p>Comparison to ViT, DeiT &amp; Swin on pose estimation. We report the COCO pose estimation results based on the two well-known transformer models, including ViT-Large <ref type="bibr" target="#b12">[13]</ref>, DeiT-B? <ref type="bibr" target="#b41">[42]</ref> and Swin-B <ref type="bibr" target="#b26">[27]</ref> in <ref type="table" target="#tab_1">Table 10</ref>. Notably, both ViT-Large and Swin-B ? are pre-trained on ImageNet21K in advance and then finetuned on ImageNet1K and achieve 85.1% and 86.4% top-1 accuracy respectively. DeiT-B? is trained on ImageNet1K for 1000 epochs and achieves 85.2% top-1 accuracy. We apply deconvolution modules to upsample the output representations of the encoder following the SimpleBaseline <ref type="bibr" target="#b48">[49]</ref> for three methods. The number of parameters and FLOPs are listed on the fourth and fifth columns of <ref type="table" target="#tab_1">Table 10</ref>. According to the results in <ref type="table" target="#tab_1">Table 10</ref>, we see that our HRFormer-B achieves better performance than all three methods with fewer parameters and FLOPs.</p><p>Comparison to HRNet. We compare our HRFormer to the convolutional HRNet with almost the same architecture configurations via replacing all the transformer blocks with the conventional basic block consisting of two 3 ? 3 convolutions. <ref type="table" target="#tab_1">Table 11</ref> shows the comparison results on ImageNet, PASCAL-Context, and COCO. We observe that HRFormer significantly outperforms HRNet under various configurations with much less model and computation complexity. For example, HRFormer-T outperforms HRNet-T by 2.0%, 1.5%, and 1.6% on three tasks while requiring only around 50% parameters and FLOPs, respectively. In summary, HRFormer achieves better performance via exploiting the benefits of transformers such as content-dependent dynamic interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present the High-Resolution Transformer (HRFormer), a simple yet effective transformer architecture, for dense prediction tasks, including pose estimation and semantic segmentation.</p><p>The key insight is to integrate the HRFormer block, which combines local-window self-attention and FFN with depth-wise convolution to improve the memory and computation efficiency, with the multi-resolution parallel design of the convolutional HRNet. Besides, HRFormer also benefits from adopting convolution in the early stages and mixing short-range and long-range attention with multi-scale fusion scheme. We empirically verify the effectiveness of our HRFormer on both pose estimation and semantic segmentation tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>More Visualization Results. We present additional visualizations of the example results of our method on both pose estimation and semantic segmentation tasks. <ref type="figure" target="#fig_6">Figure 6</ref> shows more pose estimation results of HRFormer-B on COCO val. <ref type="figure" target="#fig_7">Figure 7</ref> shows more semantic segmentation results on Cityscapes val, PASCAL-Context test and COCO-Stuff test.</p><p>Ablation of window sizes. We report the results with different window sizes at different resolutions on semantic segmentation tasks and we will add more results if necessary. We use (W 1 , W 2 , W 3 , W 4 ) to represent the window sizes associated with feature maps with different resolutions with stride 4, 8, 16, 32. We choose larger window sizes for higher resolution branches, thus, we have W 1 &gt; W 2 &gt; W 3 &gt; W 4 . According to these results, we can see that applying larger windows improves the performance, and applying different window sizes at different resolutions makes no big difference.  <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> 56.0M 1064G 57.4(58.5) <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11)</ref> 56.1M 1069G 56.6(57.6) <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref> 56.1M 1083G 57.0(58.1) <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15)</ref> 56.2M 1120G 57.5(58.5) <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9)</ref> 56.1M 1094G 56.9(57.9) <ref type="bibr" target="#b20">(21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9)</ref> 56.2M 1148G 56.9(57.9) <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11)</ref> 56.2M  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustrating the High-Resolution Transformer architecture. The multi-resolution parallel transformer modules are marked with light blue color areas. Each module consists of multiple successive multiresolution parallel transformer blocks. The first stage is constructed with convolution block and the remained three stages are constructed with transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustrating that FFN with 3 ? 3 depth-wise convolution connects the non-overlapping windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example results of HRFormer-B on COCO pose estimation val: containing occlusion, multiple persons, viewpoint and appearance change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Example results of HRFormer-B + OCR on Cityscapes val (left one), COCO-Stuff test (middle two), and PASCAL-Context test (right two).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the pose estimation results based on HRFormer-B on COCO val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the semantic segmentation results based on HRFormer-B + OCR on Cityscapes val, PASCAL-Context test, and COCO-Stuff test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The architecture configuration of HRFormer. LSA: local-window self-attention, FFN-DW: feed-forward network with a 3 ? 3 depth-wise convolution, (M1, M2, M3, M4): the number of modules, (B1, B2, B3, B4): the number of blocks, (W1, W2, W3, W4): the size of windows, (H1, H2, H3, H4): the number of heads, (R1, R2, R3, R4): the MLP expansion ratios.</figDesc><table><row><cell>Res.</cell><cell></cell><cell>Stage 1</cell><cell>Stage 2</cell><cell>Stage 3</cell><cell>Stage 4</cell></row><row><cell></cell><cell>?</cell><cell>1 ? 1,64</cell><cell></cell><cell></cell></row><row><cell>4?</cell><cell>?</cell><cell>3 ? 3,64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 ? 1,256</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>HRFormer instances. HRFormer-T, HRFormer-S, and HRFormer-B represents tiny, small, and base HRFormer model, respectively.</figDesc><table><row><cell>Model</cell><cell>#modules (M1, M2, M3, M4)</cell><cell>#blocks (B1, B2, B3, B4)</cell><cell>#channels (C1, C2, C3, C4)</cell><cell>#heads (H1, H2, H3, H4)</cell></row><row><cell>HRFormer-T</cell><cell>(1, 1, 3, 2)</cell><cell>(2, 2, 2, 2)</cell><cell>(18, 36, 72, 144)</cell><cell>(1, 2, 4, 8)</cell></row><row><cell>HRFormer-S</cell><cell>(1, 1, 4, 2)</cell><cell>(2, 2, 2, 2)</cell><cell>(32, 64, 128, 256)</cell><cell>(1, 2, 4, 8)</cell></row><row><cell>HRFormer-B</cell><cell>(1, 1, 4, 2)</cell><cell>(2, 2, 2, 2)</cell><cell>(78, 156, 312, 624)</cell><cell>(2, 4, 8, 16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison on the COCO pose estimation val set. The number of parameters and FLOPs for the pose estimation network are measured w/o considering neither human detection nor keypoint grouping. All results are based on ImageNet pretraining. ? means the numbers are not provided in the original paper. Method input size #param. FLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>HRNet-W32 [41]</cell><cell cols="2">256 ? 192 28.5M</cell><cell>7.1G</cell><cell cols="2">74.4 90.5</cell><cell>81.9</cell><cell>70.8</cell><cell>81.0 78.9</cell></row><row><cell>HRNet-W32 [41]</cell><cell cols="2">384 ? 288 28.5M</cell><cell cols="3">16.0G 75.8 90.6</cell><cell>82.7</cell><cell>71.9</cell><cell>82.8 81.0</cell></row><row><cell>HRNet-W48 [41]</cell><cell cols="2">256 ? 192 63.6M</cell><cell cols="3">14.6G 75.1 90.6</cell><cell>82.2</cell><cell>71.5</cell><cell>81.8 80.4</cell></row><row><cell>HRNet-W48 [41]</cell><cell cols="2">384 ? 288 63.6M</cell><cell cols="3">32.9G 76.3 90.8</cell><cell>82.9</cell><cell>72.3</cell><cell>83.4 81.2</cell></row><row><cell>PRTR [23]</cell><cell cols="2">512 ? 384 57.2M</cell><cell cols="3">37.8G 73.3 89.2</cell><cell>79.9</cell><cell>69.0</cell><cell>80.9 80.2</cell></row><row><cell>TransPose-H-A6 [51]</cell><cell cols="2">256 ? 192 17.5M</cell><cell cols="2">21.8G 75.8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>80.8</cell></row><row><cell cols="3">TokenPose-L/D24 [24] 256 ? 192 27.5M</cell><cell cols="3">11.0G 75.8 90.3</cell><cell>82.5</cell><cell>72.3</cell><cell>82.7 80.9</cell></row><row><cell>HRFormer-T</cell><cell>256 ? 192</cell><cell>2.5M</cell><cell>1.3G</cell><cell cols="2">70.9 89.0</cell><cell>78.4</cell><cell>67.2</cell><cell>77.8 76.6</cell></row><row><cell>HRFormer-T</cell><cell>384 ? 288</cell><cell>2.5M</cell><cell>1.8G</cell><cell cols="2">72.4 89.3</cell><cell>79.0</cell><cell>68.2</cell><cell>79.7 77.9</cell></row><row><cell>HRFormer-S</cell><cell>256 ? 192</cell><cell>7.8M</cell><cell>2.8G</cell><cell cols="2">74.0 90.2</cell><cell>81.2</cell><cell>70.4</cell><cell>80.7 79.4</cell></row><row><cell>HRFormer-S</cell><cell>384 ? 288</cell><cell>7.8M</cell><cell>6.2G</cell><cell cols="2">75.6 90.3</cell><cell>82.2</cell><cell>71.6</cell><cell>82.5 80.7</cell></row><row><cell>HRFormer-B</cell><cell cols="2">256 ? 192 43.2M</cell><cell cols="3">12.2G 75.6 90.8</cell><cell>82.8</cell><cell>71.7</cell><cell>82.6 80.8</cell></row><row><cell>HRFormer-B</cell><cell cols="2">384 ? 288 43.2M</cell><cell cols="3">26.8G 77.2 91.0</cell><cell>83.6</cell><cell>73.2</cell><cell>84.2 82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>HRNet-W32 [41]</cell><cell cols="2">384 ? 288 28.5M</cell><cell cols="2">16.0G 74.9 92.5</cell><cell>82.8</cell><cell>71.3</cell><cell cols="2">80.9 80.1</cell></row><row><cell>HRNet-W48 [41]</cell><cell cols="2">384 ? 288 63.6M</cell><cell cols="2">32.9G 75.5 92.5</cell><cell>83.3</cell><cell>71.9</cell><cell cols="2">81.5 80.5</cell></row><row><cell>PRTR [23]</cell><cell cols="2">512 ? 384 57.2M</cell><cell cols="2">37.8G 72.1 90.4</cell><cell>79.6</cell><cell>68.1</cell><cell cols="2">79.0 79.4</cell></row><row><cell>TransPose-H-A6 [51]</cell><cell cols="2">256 ? 192 17.5M</cell><cell cols="2">21.8G 75.0 92.2</cell><cell>82.3</cell><cell>71.3</cell><cell>81.1</cell><cell>?</cell></row><row><cell cols="3">TokenPose-L/D24 [24] 384 ? 288 29.8M</cell><cell cols="2">22.1G 75.9 92.3</cell><cell>83.4</cell><cell>72.2</cell><cell cols="2">82.1 80.8</cell></row><row><cell>HRFormer-S</cell><cell>384 ? 288</cell><cell>7.8M</cell><cell>6.2G</cell><cell>74.5 92.3</cell><cell>82.1</cell><cell>70.7</cell><cell cols="2">80.6 79.8</cell></row><row><cell>HRFormer-B</cell><cell cols="2">384 ? 288 43.2M</cell><cell cols="2">26.8G 76.2 92.7</cell><cell>83.8</cell><cell>72.5</cell><cell cols="2">82.3 81.2</cell></row></table><note>Comparison on the COCO pose estimation test-dev set. The number of parameters and FLOPs for the pose estimation network are measured w/o considering neither human detection nor keypoint grouping. All results are based on ImageNet pretraining.Method input size #param. FLOPs AP AP 50 AP 75 AP M AP L AR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the recent SOTA on semantic segmentation tasks. We report the mIoUs on Cityscapes val, PASCAL-Context test, COCO-Stuff test, and ADE20K val. The number of parameters and FLOPs are measured on the image size of 1024 ? 1024, and the output label map size of 19 ? 1024 ? 1024. All results are evaluated with multi-scale testing. ?: the results are obtained with extra pre-training on ADE20K. The PASCAL-Context dataset<ref type="bibr" target="#b28">[29]</ref> is a challenging scene parsing dataset that contains 59 semantic classes and 1 background class. The train set and test set consist of 4, 998 and 5, 105 images respectively. We set the initial learning rate as 0.0001, weight decay as 0.01, crop size as 520 ? 520, batch size as 16, and training iterations as 60K by default. We report the comparisons on the fifth column ofTable 5. Accordingly, HRFormer-B + OCR gains 1.1%, 1.5% over HRNet-W48 + OCR, SETR-MLA with fewer parameters and FLOPs, respectively. Notably, DPT-Hybrid achieves the best performance through extra pre-training the models on ADE20K in advance. Each HRFormer + OCR experiment on PASCAL-Context takes 8? 32G-V100 GPUs.</figDesc><table><row><cell>Method</cell><cell cols="6">#params. FLOPs Cityscapes PASCAL-Context COCO-Stuff ADE20K</cell></row><row><cell>Transformer backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SETR-PUP [63]</cell><cell cols="2">317.8M 2326.7G</cell><cell>82.2</cell><cell>55.3</cell><cell>?</cell><cell>50.1</cell></row><row><cell>SETR-MLA [63]</cell><cell cols="2">309.5M 2138.6G</cell><cell>?</cell><cell>55.8</cell><cell>?</cell><cell>50.3</cell></row><row><cell>Swin-S + UperNet [27]</cell><cell cols="2">81.16M 1036.50G</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>49.5</cell></row><row><cell>Swin-B + UperNet [27]</cell><cell cols="2">121.18M 1187.90G</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>49.7</cell></row><row><cell>PVT-Large + Semantic FPN [47]</cell><cell>65.1M</cell><cell>?G</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>43.5</cell></row><row><cell>CNN backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deeplabv3 [7]</cell><cell cols="2">87.1M 1394.0G</cell><cell>80.7</cell><cell>54.1</cell><cell>?</cell><cell>?</cell></row><row><cell>PSPNet [62]</cell><cell cols="2">68.0M 1028.8G</cell><cell>80.0</cell><cell>54.0</cell><cell>43.3</cell><cell>?</cell></row><row><cell>HRNet-W48 + OCR [55]</cell><cell cols="2">74.5M 924.7G</cell><cell>?</cell><cell>56.2</cell><cell>40.5</cell><cell>45.7</cell></row><row><cell>CNN+Transformer backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPT-Hybrid [37]</cell><cell cols="2">124.0M 1231.5G</cell><cell>?</cell><cell>60.5  ?</cell><cell>?</cell><cell>49.0</cell></row><row><cell>HRFormer-B + OCR</cell><cell cols="2">56.2M 1119.9G</cell><cell>82.6</cell><cell>58.5</cell><cell>43.3</cell><cell>50.0</cell></row><row><cell cols="3">HRFormer-B + OCR + SegFix [57] 56.2M 1119.9G</cell><cell>83.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>PASCAL-Context.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons on ImageNet-1K val.</figDesc><table><row><cell>Method</cell><cell>image size</cell><cell>#param.</cell><cell>FLOPs</cell><cell>Top-1 acc.</cell></row><row><cell>ResNet-18 [18]</cell><cell>224 ? 224</cell><cell>11M</cell><cell>1.8G</cell><cell>69.8</cell></row><row><cell>ResNet-50 [18]</cell><cell>224 ? 224</cell><cell>26M</cell><cell>4.1G</cell><cell>78.5</cell></row><row><cell>ResNet-101 [18]</cell><cell>224 ? 224</cell><cell>45M</cell><cell>7.9G</cell><cell>79.8</cell></row><row><cell>HRNet-W18 [46]</cell><cell>224 ? 224</cell><cell>21.3M</cell><cell>4.0G</cell><cell>76.8</cell></row><row><cell>HRNet-W32 [46]</cell><cell>224 ? 224</cell><cell>41.2M</cell><cell>8.3G</cell><cell>78.5</cell></row><row><cell>HRNet-W48 [46]</cell><cell>224 ? 224</cell><cell>77.5M</cell><cell>16.1G</cell><cell>79.3</cell></row><row><cell>RegNetY-4G [34]</cell><cell>224 ? 224</cell><cell>21M</cell><cell>4.0G</cell><cell>80.0</cell></row><row><cell>RegNetY-8G [34]</cell><cell>224 ? 224</cell><cell>39M</cell><cell>8.0G</cell><cell>81.7</cell></row><row><cell>RegNetY-16G [34]</cell><cell>224 ? 224</cell><cell>84M</cell><cell>16.0G</cell><cell>82.9</cell></row><row><cell>ViT-B/16 [13]</cell><cell>224 ? 224</cell><cell>86M</cell><cell>55.4G</cell><cell>77.9</cell></row><row><cell>ViT-L/16 [13]</cell><cell>224 ? 224</cell><cell>307M</cell><cell>190.7G</cell><cell>76.5</cell></row><row><cell>DeiT-T [42]</cell><cell>224 ? 224</cell><cell>5M</cell><cell>1.3G</cell><cell>72.2</cell></row><row><cell>DeiT-S [42]</cell><cell>224 ? 224</cell><cell>22M</cell><cell>4.6G</cell><cell>79.8</cell></row><row><cell>DeiT-B [42]</cell><cell>224 ? 224</cell><cell>86M</cell><cell>17.5G</cell><cell>81.8</cell></row><row><cell>DeiT-B? [42]</cell><cell>384 ? 384</cell><cell>86M</cell><cell>55.4G</cell><cell>83.4</cell></row><row><cell>Conformer-T [33]</cell><cell>224 ? 224</cell><cell>23.5M</cell><cell>5.2G</cell><cell>81.3</cell></row><row><cell>Conformer-S [33]</cell><cell>224 ? 224</cell><cell>37.7M</cell><cell>10.6G</cell><cell>83.4</cell></row><row><cell>Conformer-B [33]</cell><cell>224 ? 224</cell><cell>83.3M</cell><cell>23.3G</cell><cell>84.1</cell></row><row><cell>PVT-T [47]</cell><cell>224 ? 224</cell><cell>13.2M</cell><cell>1.9G</cell><cell>75.1</cell></row><row><cell>PVT-S [47]</cell><cell>224 ? 224</cell><cell>24.5M</cell><cell>3.8G</cell><cell>79.8</cell></row><row><cell>PVT-M [47]</cell><cell>224 ? 224</cell><cell>44.2M</cell><cell>6.7G</cell><cell>81.2</cell></row><row><cell>PVT-L [47]</cell><cell>224 ? 224</cell><cell>61.4M</cell><cell>9.8G</cell><cell>81.7</cell></row><row><cell>Swin-T [27]</cell><cell>224 ? 224</cell><cell>29M</cell><cell>4.5G</cell><cell>81.3</cell></row><row><cell>Swin-S [27]</cell><cell>224 ? 224</cell><cell>50M</cell><cell>8.7G</cell><cell>83.0</cell></row><row><cell>Swin-B [27]</cell><cell>224 ? 224</cell><cell>88M</cell><cell>15.4G</cell><cell>83.5</cell></row><row><cell>Swin-B [27]</cell><cell>384 ? 384</cell><cell>88M</cell><cell>47G</cell><cell>84.5</cell></row><row><cell>HRFormer-T</cell><cell>224 ? 224</cell><cell>8.0M</cell><cell>1.8G</cell><cell>78.5</cell></row><row><cell>HRFormer-S</cell><cell>224 ? 224</cell><cell>13.5M</cell><cell>3.6G</cell><cell>81.2</cell></row><row><cell>HRFormer-B</cell><cell>224 ? 224</cell><cell>50.3M</cell><cell>13.7G</cell><cell>82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Study of the 3?3 depth-wise convolution in FFN. We report the top1 acc., mIoU, and AP on ImageNet val, PASCAL-Context test, and COCO pose estimation val, respectively. Results on PASCAL-Context are evaluated with single-scale testing. The number of parameters and FLOPs are measured on ImageNet.</figDesc><table><row><cell>Method</cell><cell>#param.</cell><cell>FLOPs</cell><cell>ImageNet</cell><cell>PASCAL-Context</cell><cell>COCO</cell></row><row><cell>FFN w/o 3?3 DW-Conv.</cell><cell>7.9M</cell><cell>1.76G</cell><cell>77.83</cell><cell>46.84</cell><cell>66.88</cell></row><row><cell>FFN w/ 3? 3 DW-Conv.</cell><cell>8.0M</cell><cell>1.83G</cell><cell>78.48</cell><cell>49.74</cell><cell>70.92</cell></row><row><cell>4.4 Ablation Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Influence of 3 ? 3 depth-wise convolution within FFN We study the influence of the 3 ? 3 depth-</cell></row><row><cell cols="6">wise convolution within FFN based on HRFormer-T in Table 7. We observe that applying 3 ? 3</cell></row><row><cell cols="6">depth-wise convolution in FFN significantly improves the performance on multiple tasks, including</cell></row><row><cell cols="6">ImageNet classification, PASCAL-Context segmentation, and COCO pose estimation. For example,</cell></row><row><cell cols="6">HRFormer-T + FFN w/ 3? 3 depth-wise convolution outperforms HRFormer-T + FFN w/o 3? 3</cell></row><row><cell cols="6">depth-wise convolution by 0.65%, 2.9% and 4.04% on ImageNet, PASCAL-Context and COCO,</cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Influence of shifted window scheme &amp; 3?3 depth-wise convolution within FFN based on Swin-T.</figDesc><table><row><cell>Method</cell><cell>3? 3 depth-wise convolution in FFN</cell><cell>#param.</cell><cell>FLOPs</cell><cell>ImageNet top1 acc.</cell></row><row><cell>Swin-T</cell><cell></cell><cell>28.3M</cell><cell>4.5G</cell><cell>81.3</cell></row><row><cell>Swin-T</cell><cell></cell><cell>28.5M</cell><cell>4.6G</cell><cell>82.2</cell></row><row><cell>IntraWin-T</cell><cell></cell><cell>28.3M</cell><cell>4.5G</cell><cell>80.2</cell></row><row><cell>IntraWin-T</cell><cell></cell><cell>28.5M</cell><cell>4.6G</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Shifted window scheme v.s. 3? 3 depth-wise convolution within FFN based on HRFormer-T.</figDesc><table><row><cell>shifted window scheme</cell><cell>3?3 depth-wise convolution within FFN</cell><cell cols="2">#param. FLOPs</cell><cell>ImageNet top1 acc.</cell><cell>PASCAL-Context mIoU</cell><cell>COCO AP</cell></row><row><cell></cell><cell></cell><cell>8.0M</cell><cell>1.8G</cell><cell>78.5</cell><cell>49.7</cell><cell>70.9</cell></row><row><cell></cell><cell></cell><cell>7.9M</cell><cell>1.6G</cell><cell>76.6</cell><cell>43.3</cell><cell>67.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparisons to ViT &amp; DeiT on COCO pose estimation val. ? marks the methods pretrained on ImageNet-22K.</figDesc><table><row><cell>Method</cell><cell>image size</cell><cell>#param.</cell><cell>FLOPs</cell><cell>COCO</cell></row><row><cell>ViT-Large  ?</cell><cell>256 ? 192</cell><cell>308.5M</cell><cell>60.1G</cell><cell>69.2</cell></row><row><cell>DeiT-B?</cell><cell>256 ? 192</cell><cell>90.0M</cell><cell>17.9G</cell><cell>69.0</cell></row><row><cell>Swin-B  ?</cell><cell>256 ? 192</cell><cell>93.2M</cell><cell>17.6G</cell><cell>74.3</cell></row><row><cell>HRFormer-B</cell><cell>256 ? 192</cell><cell>43.2M</cell><cell>12.2G</cell><cell>75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparisons to HRNet. We report the top1 acc., mIoU, and AP on ImageNet val, PASCAL-Context test, and COCO pose estimation val, respectively. Results on PASCAL-Context are based on single-scale testing. The number of parameters and FLOPs are measured on ImageNet.</figDesc><table><row><cell>Method</cell><cell>#param.</cell><cell>FLOPs</cell><cell>ImageNet</cell><cell>PASCAL-Context</cell><cell>COCO</cell></row><row><cell>HRNet-T</cell><cell>15.6M</cell><cell>2.7G</cell><cell>76.5</cell><cell>47.8</cell><cell>69.3</cell></row><row><cell>HRFormer-T</cell><cell>8.0M</cell><cell>1.8G</cell><cell>78.5</cell><cell>49.3</cell><cell>70.9</cell></row><row><cell>HRNet-S</cell><cell>24.5M</cell><cell>5.0G</cell><cell>78.7</cell><cell>52.3</cell><cell>73.1</cell></row><row><cell>HRFormer-S</cell><cell>13.5M</cell><cell>3.6G</cell><cell>81.2</cell><cell>53.8</cell><cell>74.0</cell></row><row><cell>HRNet-B</cell><cell>85.3M</cell><cell>20.3G</cell><cell>81.4</cell><cell>55.2</cell><cell>75.1</cell></row><row><cell>HRFormer-B</cell><cell>50.3M</cell><cell>13.7G</cell><cell>82.8</cell><cell>58.5</cell><cell>75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Influence of the size of windows (W 1 , W 2 , W 3 , W 4 ) in HRFormer-B on PASCAL Context.</figDesc><table><row><cell>Method</cell><cell>(W1, W2, W3, W4)</cell><cell>#param.</cell><cell>FLOPs</cell><cell>ss result (ms result)</cell></row><row><cell></cell><cell>(7, 7, 7, 7)</cell><cell>56.0M</cell><cell>1051G</cell><cell>56.3(57.3)</cell></row><row><cell>HRFormer-B + OCR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/open-mmlab/mmpose, Apache License 2.0</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Openmmlab pose estimation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmpose</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmpose" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Residual conv-deconv grid network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Tr?meau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Levit: A vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03516</idno>
		<title level="m">Tokenpose: Learning keypoint tokens for human pose estimation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03889</idno>
		<title level="m">Conformer: Local features coupling global representations for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Convolutional neural fabrics. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4053" to="4061" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Piotr Doll?r, and Ross Girshick. Early convolutions help transformers see better</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transpose: Towards explainable human pose estimation by transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14214</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Interlinked convolutional neural networks for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISNN</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
