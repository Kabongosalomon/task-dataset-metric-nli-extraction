<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Aware Transformer for Graph Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>O&amp;apos;bray</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
						</author>
						<title level="a" type="main">Structure-Aware Transformer for Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph Transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and Transformers. Our code is available at https: //github.com/BorgwardtLab/SAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph neural networks (GNNs) have been established as powerful and flexible tools for graph representation learning, with successful applications in drug discovery <ref type="bibr">(Gaudelet et al., 2021)</ref>, protein design <ref type="bibr" target="#b9">(Ingraham et al., 2019)</ref>, social network analysis <ref type="bibr">(Fan et al., 2019)</ref>, and so on. A large class of GNNs build multilayer models, where each layer operates on the previous layer to generate new representations using a message-passing mechanism <ref type="bibr" target="#b3">(Gilmer et al., 2017)</ref> to aggregate local neighborhood information.</p><p>While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs <ref type="bibr" target="#b37">(Xu et al., 2019;</ref><ref type="bibr" target="#b24">Morris et al., 2019)</ref>, as well as known problems such as over-smoothing <ref type="bibr" target="#b18">(Li et al., 2018;</ref><ref type="bibr">Chen et al., 2020;</ref><ref type="bibr">Oono &amp; Suzuki, 2020)</ref> and oversquashing <ref type="bibr">(Alon &amp; Yahav, 2021)</ref>. Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain "bottlenecks" in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.</p><p>Transformers <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, which have proved to be successful in natural language understanding <ref type="bibr" target="#b33">(Vaswani et al., 2017</ref><ref type="bibr">), computer vision (Dosovitskiy et al., 2020</ref>, and biological sequence modeling <ref type="bibr">(Rives et al., 2021)</ref>, offer the potential to address these issues. Rather than only aggregating local neighborhood information in the messagepassing mechanism, the Transformer architecture is able to capture interaction information between any node pair via a single self-attention layer. Moreover, in contrast to GNNs, the Transformer avoids introducing any structural inductive bias at intermediate layers, addressing the expressivity limitation of GNNs. Instead, it encodes structural or positional information about nodes only into input node features, albeit limiting how much information it can learn from the graph structure. Integrating information about the graph structure into the Transformer architecture has thus gained growing attention in the graph representation learning field. However, most existing approaches only encode positional relationships between nodes, rather than explicitly encoding the structural relationships. As a result, they may not identify structural similarities between nodes and could fail to model the structural interaction between nodes (see <ref type="figure">Figure 1</ref>). This could explain why their performance arXiv:2202.03036v3 [stat.ML] 13 Jun 2022 u G 1 v G 2 <ref type="figure">Figure 1</ref>: Position-aware vs. structure-aware: Using a positional encoding based on shortest paths in G 1 and G 2 respectively (assuming all edges have equal weight), node u and v would receive identical encodings since their shortest paths to all other nodes are the same in both graphs. However, their structures are different, with v forming a triangle with its red neighbors.</p><p>was dominated by sparse GNNs in several tasks <ref type="bibr">(Dwivedi et al., 2022)</ref>.</p><p>Contribution In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:</p><p>? We reformulate the self-attention mechanism in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref> as a kernel smoother and extend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node. ? We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations. ? We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than stateof-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN. ? Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding.</p><p>We will present the related work and relevant background in Sections 2 and 3 before presenting our method in Section 4 and our experimental findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We present here the work most related to ours, namely the work stemming from message passing GNNs, positional representations on graphs, and graph Transformers.</p><p>Message passing graph neural networks Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>, which was based on performing convolutions on the graph. <ref type="bibr" target="#b3">Gilmer et al. (2017)</ref> reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples <ref type="bibr" target="#b4">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2019;</ref><ref type="bibr">Corso et al., 2020;</ref><ref type="bibr">Hu et al., 2020b;</ref><ref type="bibr" target="#b34">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b16">Li et al., 2020a;</ref><ref type="bibr">Yang et al., 2022)</ref>. However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.</p><p>Absolute encoding Because of the limited expressiveness of GNNs, there has been some recent research into the use of absolute encoding <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref>, which consists of adding or concatenating positional or structural representations to the input node features. While it is often called an absolute positional encoding, we refer to it more generally as an absolute encoding to include both positional and structural encoding, which are both important in graph modeling. Absolute encoding primarily considers position or location relationships between nodes. Examples of position-based methods include the Laplacian positional encoding <ref type="bibr">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr">Kreuzer et al., 2021)</ref>, Weisfeiler-Lehman-based positional encoding <ref type="bibr">(Zhang et al., 2020)</ref>, and random walk positional encoding (RWPE) <ref type="bibr">(Li et al., 2020b;</ref><ref type="bibr">Dwivedi et al., 2022)</ref>, while distance-based methods include distances to a predefined set of nodes <ref type="bibr">(You et al., 2019)</ref> and shortest path distances between pairs of nodes <ref type="bibr">(Zhang et al., 2020;</ref><ref type="bibr">Li et al., 2020b)</ref>. <ref type="bibr">Dwivedi et al. (2022)</ref> extend these ideas by using a trainable absolute encoding.</p><p>Graph Transformers While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer <ref type="bibr">(Dwivedi &amp; Bresson, 2021)</ref> provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN <ref type="bibr">(Kreuzer et al., 2021)</ref> also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref> in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. <ref type="bibr" target="#b22">Mialon et al. (2021)</ref> propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations <ref type="bibr">(Ying et al., 2021)</ref> or by using GNNs to integrate the graph structure <ref type="bibr" target="#b29">(Rong et al., 2020;</ref><ref type="bibr">Jain et al., 2021;</ref><ref type="bibr" target="#b22">Mialon et al., 2021;</ref><ref type="bibr">Shi et al., 2021)</ref>.</p><p>In this work, we combine the best of both worlds from message passing GNNs and from the Transformer architecture.</p><p>We incorporate both an absolute as well as a novel relative encoding that explicitly incorporates the graph structure, thereby designing a Transformer architecture that takes both local and global information into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In the following, we refer to a graph as G = (V, E, X), where the node attributes for node u ? V is denoted by x u ? X ? R d and the node attributes for all nodes are stored in X ? R n?d for a graph with n nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformers on Graphs</head><p>While GNNs use the graph structure explicitly, Transformers remove that explicit structure, and instead infer relations between nodes by leveraging the node attributes. In this sense, the Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> ignores the graph structure and rather considers the graph as a (multi-) set of nodes, and uses the self-attention mechanism to infer the similarity between nodes. The Transformer itself is composed of two main blocks: a self-attention module followed by a feed-forward neural network. In the self-attention module, the input node features X are first projected to query (Q), key (K) and value (V) matrices through a linear projection such that Q = XW Q , K = XW K and V = XW V respectively. We can compute the self-attention via</p><formula xml:id="formula_0">Attn(X) := softmax( QK T ? d out )V ? R n?dout ,<label>(1)</label></formula><p>where d out refers to the dimension of Q, and W Q , W K , W V are trainable parameters. It is common to use multi-head attention, which concatenates multiple instances of Eq. (1) and has shown to be effective in practice <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. Then, the output of the selfattention is followed by a skip-connection and a feedforward network (FFN), which jointly compose a Trans-former layer, as shown below:</p><formula xml:id="formula_1">X = X + Attn(X), X = FFN(X ) := ReLU(X W 1 )W 2 .<label>(2)</label></formula><p>Multiple layers can be stacked to form a Transformer model, which ultimately provides node-level representations of the graph. As the self-attention is equivariant to permutations of the input nodes, the Transformer will always generate the same representations for nodes with the same attributes regardless of their locations and surrounding structures in the graph. It is thus necessary to incorporate such information into the Transformer, generally via absolute encoding.</p><p>Absolute encoding Absolute encoding refers to adding or concatenating the positional or structural representations of the graph to the input node features before the main Transformer model, such as the Laplacian positional encoding <ref type="bibr">(Dwivedi &amp; Bresson, 2021)</ref> or <ref type="bibr">RWPE (Dwivedi et al., 2022)</ref>. The main shortcoming of these encoding methods is that they generally do not provide a measure of the structural similarity between nodes and their neighborhoods.</p><p>Self-attention as kernel smoothing As noticed by <ref type="bibr" target="#b22">Mialon et al. (2021)</ref>, the self-attention in Eq.</p><p>(1) can be rewritten as a kernel smoother</p><formula xml:id="formula_2">Attn(x v ) = u?V ? exp (x v , x u ) w?V ? exp (x v , x w ) f (x u ), ?v ? V,</formula><p>(3) where f (x) = W V x is the linear value function and ? exp is a (non-symmetric) exponential kernel on R d ? R d parameterized by W Q and W K :</p><formula xml:id="formula_3">? exp (x, x ) := exp W Q x, W K x / d out ,<label>(4)</label></formula><p>where ?, ? is the dot product on R d . With this form, <ref type="bibr" target="#b22">Mialon et al. (2021)</ref> propose a relative positional encoding strategy via the product of this kernel and a diffusion kernel on the graph, which consequently captures the positional similarity between nodes. However, this method is only positionaware, in contrast to our structure-aware encoding that will be presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structure-Aware Transformer</head><p>In this section, we will describe how to encode the graph structure into the self-attention mechanism and provide a class of Transformer models based on this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Structure-Aware Self-Attention</head><p>As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only  <ref type="figure">Figure 2</ref>: Overview of an example SAT layer that uses the k-subgraph GNN extractor as its structure extractor. The structure extractor generates structure-aware node representations which are used to compute the query (Q) and key (K) matrices in the Transformer layer. Structure-aware node representations are generated in the k-subgraph GNN extractor by first extracting the k-hop subgraph centered at each node (here, k = 1) and then using a GNN on each subgraph to generate a node representations using the full subgraph information. While the structure extractor can use any class of subgraphs, the one illustrated here defined on the class of k-hop subgraphs has a reasonable computation-expressiveness trade-off.</p><p>captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:</p><formula xml:id="formula_4">SA-attn(v) := u?V ? graph (S G (v), S G (u)) w?V ? graph (S G (v), S G (w)) f (x u ),<label>(5)</label></formula><p>where S G (v) denotes a subgraph in G centered at a node v associated with node features X and ? graph can be any kernel that compares a pair of subgraphs. This new selfattention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property.</p><p>In the rest of the paper, we will consider the following form of ? graph that already includes a large class of expressive and computationally tractable models:</p><formula xml:id="formula_5">? graph (S G (v), S G (u)) = ? exp (?(v, G), ?(u, G)), (6)</formula><p>where ?(u, G) is a structure extractor that extracts vector representations of some subgraph centered at u with node features X. We provide several alternatives of the structure extractor below. It is worth noting that our structure-aware self-attention is flexible enough to be combined with any model that generates representations of subgraphs, including GNNs and (differentiable) graph kernels. For notational simplicity, we assume there are no edge attributes, but our method can easily incorporate edge attributes as long as the structure extractor can accommodate them. The edge attributes are consequently not considered in the self-attention computation, but are incorporated into the structure-aware node representations. In the structure extractors presented in this paper, this means that edge attributes were included whenever the base GNN was able to handle edge attributes. k-subtree GNN extractor A straightforward way to extract local structural information at node u is to apply any existing GNN model to the input graph with node features X and take the output node representation at u as the subgraph representation at u. More formally, if we denote by GNN <ref type="bibr">(k)</ref> G an arbitrary GNN model with k layers applied to G with node features X, then</p><formula xml:id="formula_6">?(u, G) = GNN (k) G (u).<label>(7)</label></formula><p>This extractor is able to represent the k-subtree structure rooted at u <ref type="bibr" target="#b37">(Xu et al., 2019)</ref>. While this class of structure extractors is fast to compute and can flexibly leverage any existing GNN, they cannot be more expressive than the Weisfeiler-Lehman test due to the expressiveness limitation of message passing GNNs <ref type="bibr" target="#b37">(Xu et al., 2019)</ref>. In practice, a small value of k already leads to good performance, while not suffering from over-smoothing or over-squashing.</p><p>k-subgraph GNN extractor A more expressive extractor is to use a GNN to directly compute the representation of the entire k-hop subgraph centered at u rather than just the node representation u. Recent work has explored the idea of using subgraphs rather than subtrees around a node in GNNs, with positive experimental results <ref type="bibr">(Zhang &amp; Li, 2021;</ref><ref type="bibr" target="#b36">Wijesinghe &amp; Wang, 2022)</ref>, as well as being strictly more powerful than the 1-WL test <ref type="bibr">(Zhang &amp; Li, 2021)</ref>. We follow the same setup as is done in <ref type="bibr">Zhang &amp; Li (2021)</ref>, and adapt our GNN extractor to utilize the entire k-hop subgraph. The k-subgraph GNN extractor aggregates the updated node representations of all nodes within the k-hop neighborhood using a pooling function such as summation.</p><p>Formally, if we denote by N k (u) the k-hop neighborhood of node u including itself, the representation of a node u is:</p><formula xml:id="formula_7">?(u, G) = v?N k (u) GNN (k) G (v).<label>(8)</label></formula><p>We observe that prior to the pooling function, the ksubgraph GNN extractor is equivalent to using the k-subtree GNN extractor within each k-hop subgraph. So as to capture the attributed similarity as well as structural similarity, we augment the node representation from k-subgraph GNN extractor with the original node features via concatenation.</p><p>While this extractor provides more expressive subgraph representations than the k-subtree extractor, it requires enumerating all k-hop subgraphs, and consequently does not scale as well as the k-subtree extractor to large datasets.</p><p>Other structure extractors Finally, we present a list of other potential structure extractors for different purposes. One possible choice is to directly learn a number of "hidden graphs" as the "anchor subgraphs" to represent subgraphs for better model interpretability, by using the concepts introduced in <ref type="bibr">Nikolentzos &amp; Vazirgiannis (2020)</ref>. While Nikolentzos &amp; Vazirgiannis (2020) obtain a vector representation of the input graph by counting the number of matching walks between the whole graph and each of the hidden graphs, one could extend this to the node level by comparing the hidden graphs to the k-hop subgraph centered around each node. The adjacency matrix of the hidden graphs is a trainable parameter in the network, thereby enabling end-to-end training to identify which subgraph structures are predictive. Then, for a trained model, visualizing the learned hidden graphs provides useful insights about the structural motifs in the dataset.</p><p>Furthermore, more domain-specific GNNs could also be used to extract potentially more expressive subgraph representations. For instance, Bodnar et al. <ref type="formula" target="#formula_0">(2021)</ref> recently proposed a new kind of message passing scheme operating on regular cell complexes which benefits from provably stronger expressivity for molecules. Our self-attention mechanism can fully benefit from the development of more domain-specific and expressive GNNs.</p><p>Finally, another possible structure extractor is to use a nonparametric graph kernel (e.g. a Weisfeiler-Lehman graph kernel) on the k-hop subgraphs centered around each node. This provides a flexible way to combine graph kernels and deep learning, which might offer new theoretical insights into the link between the self-attention and kernel methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structure-Aware Transformer</head><p>Having defined our structure-aware self-attention function, the other components of the Structure-Aware Transformer follow the Transformer architecture as described in Section 3.1; see <ref type="figure">Figure 2</ref> for a visual overview. Specifically, the self-attention function is followed by a skip-connection, a FFN and two normalization layers before and after the FFN. In addition, we also include the degree factor in the skip-connection, which was found useful for reducing the overwhelming influence of highly connected graph components <ref type="bibr" target="#b22">(Mialon et al., 2021)</ref>, i.e.,</p><formula xml:id="formula_8">x v = x v + 1/ d v SA-attn(v),<label>(9)</label></formula><p>where d v denotes the degree of node v. After a Transformer layer, we obtain a new graph with the same structure but different node features G = (V, E, X ), where X corresponds to the output of the Transformer layer.</p><p>Finally, for graph property prediction, there are various ways to aggregate node-level representations into a graph representation, such as by taking the average or sum. Alternatively, one can use the embedding of a virtual [CLS] node <ref type="bibr">(Jain et al., 2021)</ref> that is attached to the input graph without any connectivity to other nodes. We compare these approaches in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combination with Absolute Encoding</head><p>While the self-attention in Eq. (5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE <ref type="bibr">(Dwivedi et al., 2022)</ref>, though any other absolute positional representations, including learnable ones, can also be used.</p><p>We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:</p><p>Theorem 1. Assume that f is a Lipschitz mapping with the Lipschitz constant denoted by Lip(f ) and the structure extractor ? is bounded by a constant C ? on the space of subgraphs. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ) with the same number of nodes |V | = |V |, the distance between their representations after the structure-aware attention is bounded by:</p><formula xml:id="formula_9">SA-attn(v) ? SA-attn(v ) ? C 1 [ h v ? h v +D(H, H )] + C 2 D(X, X ),<label>(10)</label></formula><p>where C 1 , C 2 &gt; 0 are constants depending on |V |, Lip(f ), C ? and spectral norms of the parameters in SA-attn, whose expressions are given in the Appendix, and h w := ?(w, G) denotes the subgraph representation at node w for any w ? V and h w := ?(w , G ) similarly, and H = (h w ) w?V and H = (h w ) w ?V denote the multiset of subgraph representations in G and G respectively. Denoting by ?(V, V ) the set of permutations from V to V , D is an optimal matching metric between two multisets of representations with the same cardinality, defined as</p><formula xml:id="formula_10">D(X, X ) := inf ???(V,V ) sup w?V x w ? x ?(w) .</formula><p>The proof is provided in the Appendix. The metric D is an optimal matching metric between two multisets which measures how different they are. This theorem shows that two node representations from the SA-attn are similar if the graphs that they belong to have similar multisets of node features and subgraph representations overall, and at the same time, the subgraph representations at these two nodes are similar. In particular, if two nodes belong to the same graph, i.e. G = G , then the second and last terms on the right side of Eq. (10) are equal to zero and the distance between their representations is thus constrained by the distance between their corresponding subgraph representations. However, for Transformers with absolute positional encoding, the distance between two node representations is not constrained by their structural similarity, as the distance between two positional representations does not necessarily characterize how structurally similar two nodes are. Despite stronger inductive biases, we will show that our model is still sufficiently expressive in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Expressivity Analysis</head><p>The expressive power of graph Transformers compared to classic GNNs has hardly been studied, since the soft structural inductive bias introduced in absolute encoding is generally hard to characterize. Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor, following the injectivity of the attention function with respect to the query:</p><p>Theorem 2. Assume that the space of node attributes X is countable. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ), assume that there exist a node u 1 in V such that x u1 = x w for any w ? V and a node u 2 in V such that its subgraph representation ?(u 2 , G) = ?(w, G) for any w ? V . Then, there exists a set of parameters and a mapping f : X ? R dout such that their representations after the structure-aware attention are different, i.e. SA-attn(v) = SA-attn(v ), if their subgraph representations are different, i.e. ?(v, G) = ?(v , G ).</p><p>Note that the assumptions made in the theorem are mild as one can always add some absolute encoding or random noise to make the attributes of one node different from all other nodes, and similarly for subgraph representations. The countable assumption on X is generally adopted for expressivity analysis of GNNs (e.g. <ref type="bibr" target="#b37">Xu et al. (2019)</ref>). We assume f to be any mapping rather than just a linear function as in the definition of the self-attention function since it can be practically approximated by a FFN in multi-layer Transformers through the universal approximation theorem <ref type="bibr" target="#b6">(Hornik, 1991)</ref>. Theorem 2 suggests that if the structure extractor is sufficiently expressive, the resulting SAT model can also be at least equally expressive. Furthermore, more expressive extractors could lead to more expressively powerful SAT models and thus better prediction performance, which is also empirically confirmed in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate SAT models versus several SOTA methods for graph representation learning, including GNNs and Transformers, on five graph and node prediction tasks, as well as analyze the different components of our architecture to identify what drives the performance. In summary, we discovered the following aspects about SAT:</p><p>? The structure-aware framework achieves SOTA performance on graph and node classification tasks, outperforming SOTA graph Transformers and sparse GNNs. ? Both instances of the SAT, namely k-subtree and ksubgraph SAT, always improve upon the base GNN it is built upon, highlighting the improved expressiveness of our structure-aware approach. ? We show that incorporating the structure via our structure-aware attention brings a notable improvement relative to the vanilla Transformer with RWPE that just uses node attribute similarity instead of also incorporating structural similarity. We also show that a small value of k already leads to good performance, while not suffering from over-smoothing or over-squashing. ? We show that choosing a proper absolute positional encoding and a readout method improves performance, but to a much lesser extent than incorporating the structure into the approach.</p><p>Furthermore, we note that SAT achieves SOTA performance while only considering a small hyperparameter search space. Performance could likely be further improved with more hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Experimental Setup</head><p>We assess the performance of our method with five medium to large benchmark datasets for node and graph property prediction, including ZINC (Dwivedi et al., 2020), <ref type="bibr">CLUSTER (Dwivedi et al., 2020)</ref>, <ref type="bibr">PATTERN (Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type="bibr">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type="bibr">(Hu et al., 2020a)</ref>.</p><p>We compare our method to the following GNNs: GCN <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b4">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b34">(Veli?kovi? et al., 2018)</ref> All results for the comparison methods are either taken from the original paper or from Dwivedi et al. <ref type="formula" target="#formula_1">(2020)</ref> if not available. We consider k-subtree and k-subgraph SAT equipped with different GNN extractors, including GCN, GIN, GraphSAGE and PNA. For OGBG-PPA and OGBG-CODE2, we do not run experiments for k-subgraph SAT models due to large memory requirements. Full details on the datasets, experimental setup, and hyperparameters are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to State-of-the-Art Methods</head><p>We show the performance of SATs compared to other GNNs and Transformers in <ref type="table" target="#tab_2">Table 1</ref> and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard. <ref type="table" target="#tab_4">Table 3</ref> summarizes the performance of SAT relative to the sparse GNN it uses to extract the subgraph representations, across different GNNs. We observe that both variations of SAT consistently bring large performance gains to its base GNN counterpart, making it a systematic enhancer of any GNN model. Furthermore, PNA, which is the most expressive GNN we considered, has consistently the best performance when used with SAT, empirically validating our theoretical finding in Section 4.4. k-subgraph SAT also outperforms or performs equally as k-subtree SAT in almost all the cases, showing its superior expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">SAT Models vs. Sparse GNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Hyperparameter Studies</head><p>While <ref type="table" target="#tab_4">Table 3</ref> showcases the added value of the SAT relative to sparse GNNs, we now dissect the components of SAT on the ZINC dataset to identify which aspects of the architecture bring the biggest performance gains.</p><p>Effect of k in SAT The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. <ref type="figure">Figure 3a</ref> shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k = 0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k = 3 for both k-subtree and k-subgraph extractors. As k increases beyond k = 4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>. We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of absolute encoding</head><p>We assess here whether the absolute encoding brought complementary information to SAT. In <ref type="figure">Figure 3b</ref>, we conduct an ablation study showing the results of SAT with and without absolute positional encoding, including RWPE and Laplacian PE <ref type="bibr">(Dwivedi et al., 2020)</ref>. Our SAT with a positional encoding outperforms its counterpart without it, confirming the complementary nature of the two encodings. However, we also note that the performance gain brought by the absolute encoding is far less than the gain obtained by using our structure-aware attention, as shown in <ref type="figure">Figure 3a</ref> (comparing the instance of k = 0 to k &gt; 0), emphasizing that our structure-aware attention is the more important aspect of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of readout methods</head><p>Finally, we compare the performance of SAT models using different readout methods for aggregating node-level representations on the ZINC dataset in <ref type="figure">Figure 3c</ref>, including the CLS pooling discussed in Section 4.2. Unlike the remarkable influence of the readout method in GNNs <ref type="bibr" target="#b37">(Xu et al., 2019)</ref>, we observe very little impact in SAT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Model Interpretation</head><p>In addition to performance improvement, we show that SAT offers better model interpretability compared to the classic Transformer with only absolute positional encoding. We respectively train a SAT model and a Transformer with a CLS readout on the Mutagenicity dataset, and visualize the attention scores between the [CLS] node and other nodes learned by SAT and the Transformer in <ref type="figure" target="#fig_1">Figure 4</ref>. The salient difference between the two models is that SAT has structureaware node embeddings, and thus we can attribute the following interpretability gains to that. While both models manage to identify some chemical motifs known for mutagenicity, such as NO 2 and NH 2 , the attention scores learned by SAT are sparser and more informative, meaning that SAT puts more attention weights on these known mutagenic motifs than the Transformer with RWPE. The vanilla Transformer even fails to put attention on some important atoms such as the H atoms in the NH 2 group. The only H atoms highlighted by SAT are those in the NH 2 group, suggesting that our SAT indeed takes the structure into account. More focus on these discriminative motifs makes the SAT model less influenced by other chemical patterns that commonly exist in the dataset, such as benzene, and thus leads to overall improved performance. More results are provided in the Appendix.  <ref type="figure">Figure 3</ref>: We provide an analysis of the different drivers of performance in SAT on the ZINC dataset (lower is better). In <ref type="figure">Figure 3a</ref>, we show how changing the size of k affects performance (k=0 is equivalent to a vanilla Transformer that is not structure-aware). <ref type="figure">Figure 3b</ref> shows the effect of different absolute encoding methods, and <ref type="figure">Figure 3c</ref> shows the effect of different readout methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer.</p><p>Limitations As mentioned above, k-subgraph SAT has higher memory requirements than k-subtree SAT, which can restrict its applicability if access to high memory GPUs is restricted. We see the main limitation of SAT is that it suffers from the same drawbacks as the Transformer, namely the quadratic complexity of the self-attention computation.</p><p>Future work Because SAT can be combined with any GNN, a natural extension of our work is to combine SAT with structure extractors which have shown to be strictly more expressive than the 1-WL test, such as the recent topological GNN introduced by <ref type="bibr" target="#b5">Horn et al. (2021)</ref>. Additionally, the SAT framework is flexible and can incorporate any structure extractor which produces structure-aware node representations, and could even be extended beyond using GNNs, such as differentiable graph kernels.</p><p>Another important area for future work is to focus on reducing the high memory cost and time complexity of the self-attention computation, as is being done in recent efforts for developing a so-called linear transformer, which has linear complexity in both time and space requirements <ref type="bibr" target="#b32">(Tay et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020;</ref><ref type="bibr" target="#b27">Qin et al., 2022)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background on Graph Neural Networks</head><p>The overarching idea of a graph neural network is to iteratively update a node's embedding by incorporating information sent from its neighbors. <ref type="bibr" target="#b37">Xu et al. (2019)</ref> provide a general framework of the steps incorporated in this process by generalizing the different frameworks into AGGREGATE, COMBINE and READOUT steps. The various flavors of GNNs can be typically understood as variations within these three functions. For a given layer l, the AGGREGATE step aggregates (e.g. using the sum or mean) the representations of the neighbors of a given node, which is then combined with the given node's representation from the previous layer in the COMBINE step. This is followed by a non-linear function, such as ReLU, and the updated node representations are then passed to the next layer. These two steps are repeated for as many layers as there are in the network. It is worth noting that the output of these two steps provides representations of nodes which accounts for local sub-structures of size only increased by one, which would thus require a very deep network to capture interactions between the given node and all other nodes (the depth should not be smaller than the diameter of the graph). At the end of the network, the READOUT function provides a pooling function to convert the representations to the appropriate output-level granularity (e.g. node-level or graph-level). Both the AGGREGATE and READOUT steps must be invariant to node permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theoretical Analysis</head><p>B.1. Controllability of the Representations from the Structure-Aware Attention Theorem 1. Assume that f is a Lipschitz mapping with the Lipschitz constant denoted by Lip(f ) and the structure extractor ? is bounded by a constant C ? on the space of subgraphs. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ) with the same number of nodes |V | = |V | = n, the distance between their representations after the structure-aware attention is bounded by:</p><formula xml:id="formula_11">SA-attn(v) ? SA-attn(v ) ? C 1 [ h v ? h v + D(H, H )] + C 2 D(X, X ),<label>(11)</label></formula><p>where h w := ?(w, G) denotes the subgraph representation at node w for any w ? V and h w := ?(w , G ) similarly, and H = (h w ) w?V and H = (h w ) w ?V denote the multiset of subgraph representations in G and G respectively. Denoting by ?(V, V ) the set of permutations between V and V , D is a matching metric between two multisets of representations with the same cardinality, defined as</p><formula xml:id="formula_12">D(X, X ) := inf ???(V,V ) sup w?V x w ? x ?(w) .</formula><p>C 1 and C 2 are constants given by:</p><formula xml:id="formula_13">C 1 = 2 d out nLip(f )C ? W Q ? W K ? , C 2 = Lip(f ).</formula><p>Proof. Let us denote by</p><formula xml:id="formula_14">z v = ( W Q h v , W K h w ) w?V ? R n , z v = ( W Q h v , W K h w ) w ?V ? R n ,</formula><p>and by softmax(z) ? R n for any z ? R n with its i-th coefficient</p><formula xml:id="formula_15">softmax(z) i = exp(z i / ? d out ) n j=1 exp(z j / ? d out ) .</formula><p>Then, we have</p><formula xml:id="formula_16">SA-Attn(v) ? SA-Attn(v ) = w?V softmax(z v ) w f (x w ) ? w ?V softmax(z v ) w f (x w ) = w?V (softmax(z v ) w ? softmax(z v ) ?(w) )f (x w ) + w?V softmax(z v ) ?(w) f (x w ) ? w ?V softmax(z v ) w (f (x w )) ? w?V (softmax(z v ) w ? softmax(z v ) ?(w) )f (x w ) + w ?V softmax(z v ) w (f (x ? ?1 (w ) ) ? f (x w ))</formula><p>where ? : V ? V is an arbitrary permutation and we used the triangle inequality. Now we need to bound the two terms respectively. We first bound the second term:</p><formula xml:id="formula_17">w ?V softmax(z v ) w (f (x ? ?1 (w ) ) ? f (x w )) ? w ?V softmax(z v ) w f (x ? ?1 (w ) ) ? f (x w ) ? w ?V softmax(z v ) w Lip(f ) x ? ?1 (w ) ? x w = Lip(f ) w ?V softmax(z v ) w x ? ?1 (w ) ? x w ? Lip(f ) sup w ?V x ? ?1 (w ) ? x w = Lip(f ) sup w?V x w ? x ?(w)</formula><p>where the first inequality is a triangle inequality, the second inequality uses the Lipschitzness of f . And for the first term, we can upper-bound it by</p><formula xml:id="formula_18">w?V (softmax(z v ) w ? softmax(z v ) ?(w) )f (x w ) ? softmax(z v ) ? softmax((z v ) ? ) w?V f (x w ) 2 ? 1 ? d out z v ? (z v ) ? ? nLip(f ),</formula><p>where by abuse of notation, (z) ? ? R n denotes the vector whose w-th entry is z ?(w) for any z ? R n . The first inequality comes from a simple matrix norm inequality, and the second inequality uses the fact that softmax function is 1/ ? d out -Lipschitz (see e.g. Gao &amp; Pavel <ref type="formula" target="#formula_0">(2017)</ref>). Then, we have</p><formula xml:id="formula_19">z v ? (z v ) ? ) 2 = w?V W Q h v , W K h w ? W Q h v , W K h ?(w) 2 = w?V W Q h v , W K (h w ? h ?(w) ) + W Q (h v ? h v ), W K h ?(w) 2 ? 2 w?V W Q h v , W K (h w ? h ?(w) ) 2 + W Q (h v ? h v ), W K h ?(w) 2 ? 2 w?V W Q h v 2 W K (h w ? h ?(w) ) 2 + W Q (h v ? h v ) 2 W K h ?(w) 2 ? 2 w?V C 2 ? W Q 2 ? W K 2 ? h w ? h ?(w) 2 + W Q 2 ? h v ? h v 2 C 2 ? W K 2 ? ? 2nC 2 ? W Q 2 ? W K 2 ? h v ? h v 2 + sup w?V h w ? h ?(w) 2 ,</formula><p>where the first inequality comes from (a + b) 2 ? 2(a 2 + b 2 ), the second one uses the Cauchy-Schwarz inequality and the third one uses the definition of spectral norm and the bound of the structure extractor function. Then, we obtain the following inequality</p><formula xml:id="formula_20">w?V (softmax(z v ) w ? softmax(z v ) ?(w) )f (x w ) ? 2 d out nLip(f )C ? W Q ? W K ? h v ? h v + sup w?V h w ? h ?(w)</formula><p>By combining the upper bounds of the first and the second term, we obtain an upper bound for the distance between the structure-aware attention representations:</p><formula xml:id="formula_21">SA-attn(v) ? SA-attn(v ) ? C 1 h v ? h v + sup w?V h w ? h ?(w) + C 2 sup w?V x w ? x ?(w) ,</formula><p>for any permutation ? ? ?(V, V ), where</p><formula xml:id="formula_22">C 1 = 2 d out nLip(f )C ? W Q ? W K ? C 2 = Lip(f ).</formula><p>Finally, by taking the infimum over the set of permutations, we obtain the inequality in the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Expressivity Analysis</head><p>Here, we assume that f can be any continuous mapping and it is approximated by an MLP network through the universal approximation theorem <ref type="bibr" target="#b6">(Hornik, 1991)</ref> in practice.</p><p>Theorem 2. Assume that the space of node attributes X is countable. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ), assume that there exists a node u 1 in V such that x u1 = x w for any w ? V and a node u 2 in V such that its subgraph representation ?(u 2 , G) = ?(w, G) for any w ? V . Then, there exists a set of parameters and a mapping f : X ? R dout such that their representations after the structure-aware attention are different, i.e. SA-attn(v) = SA-attn(v ), if their subgraph representations are different, i.e. ?(v, G) = ?(v , G ).</p><p>Proof. This theorem amounts to showing the injectivity of the original dot-product attention with respect to the query, that is to show</p><formula xml:id="formula_23">Attn(h v , x v , G) = u?V ? exp (h v , h u ) w?V ? exp (h v , h w ) f (x u ) is injective in h v , where ? exp (h, h ) := exp W Q h + b Q , W K h + b K / d out .<label>(12)</label></formula><p>Here we consider the offset terms that were omitted in Eq. (1). Let us prove the contrapositive of the theorem. We assume that Attn(h v , x v , G) = Attn(h v , x v , G ) for any set of parameters and any mapping f and want to show that</p><formula xml:id="formula_24">h v = h v .</formula><p>Without loss of generality, we assume that G and G have the same number of nodes, that is |V | = |V | = n. Otherwise, one can easily add some virtual isolated nodes to the smaller graph. Now if we take W Q = W K = 0, all the softmax coefficients will be identical and we have</p><formula xml:id="formula_25">w?V f (x w ) = w ?V f (x w ).</formula><p>Thus, by Lemma 5 of <ref type="bibr" target="#b37">Xu et al. (2019)</ref>, there exists a mapping f such that the multisets X and X are identical.</p><p>As a consequence, we can re-enumerate the nodes in two graphs by a sequence V (by abuse of notation, we keep using V here) such that x u = x u for any u ? V . Then, we can rewrite the equality Attn</p><formula xml:id="formula_26">(h v , x v , G) = Attn(h v , x v , G ) as u?V ? exp (h v , h u ) w?V ? exp (h v , h w ) ? ? exp (h v , h u ) w?V ? exp (h v , h w ) f (x u ) = 0.</formula><p>Now since there exists a node u 1 in V such that its attributes are different from all other nodes, i.e. x u1 = x w for any w ? V , we can find a mapping f such that f (x u1 ) is not in the span of (f (x w )) w?V,w =u1 . Then, by their independence we have</p><formula xml:id="formula_27">? exp (h v , h u1 ) w?V ? exp (h v , h w ) = ? exp (h v , h u1 ) w?V ? exp (h v , h w ) , for any W Q , W K , b Q and b K .</formula><p>On the one hand, if we take W Q = 0, we have for any W K , b Q and b K that</p><formula xml:id="formula_28">exp ( b Q , W K h u1 + b K / ? d out ) w?V exp ( b Q , W K h w + b K / ? d out ) = exp ( b Q , W K h u1 + b K / ? d out ) w?V exp ( b Q , W K h w + b K / ? d out ) .</formula><p>On the other hand if we take b Q = 0 we have for any W Q , W K and b K that</p><formula xml:id="formula_29">exp ( W Q h v , W K h u1 + b K / ? d out ) w?V exp ( W Q h v , W K h w + b K / ? d out ) = exp ( W Q h v , W K h u1 + b K / ? d out ) w?V exp ( W Q h v , W K h w + b K / ? d out ) = exp ( W Q h v , W K h u1 + b K / ? d out ) w?V exp ( W Q h v , W K h w + b K / ? d out ) ,</formula><p>where the second equality is obtained by replacing b Q with W Q h v in the above equality. Then, we can rewrite the above equality as below:</p><formula xml:id="formula_30">w?V exp W Q h v , W K (h w ? h u1 ) ? d out = w?V exp W Q h v , W K (h w ? h u1 ) ? d out .</formula><p>If we denote by ? : R dout ? H the feature mapping associated with the dot product kernel ? exp (t, t ) = exp( t, t / ? d out ) and H the correspond reproducing kernel Hilbert space, we then have for any W Q and W K that</p><formula xml:id="formula_31">?(W Q h v ) ? ?(W Q h v ), w?V ?(W K (h w ? h u1 )) H = 0.</formula><p>Since by assumption there exists a u 2 ? V such that h u2 ? h u1 = 0 and ? exp is a universal kernel <ref type="bibr" target="#b23">(Micchelli et al., 2006)</ref>,</p><formula xml:id="formula_32">W K ? ?(W K (h u2 ? h u1 )) is dense in H and we have ?(W Q h v ) = ?(W Q h v ).</formula><p>We can then conclude, by the injectivity of ?, that</p><formula xml:id="formula_33">W Q h v = W Q h v ,</formula><p>for any W Q , and thus h v = h v . Now by taking h v = ?(v, G) and h v = ?(v , G ), we obtain the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Details and Additional Results</head><p>In this section, we provide implementation details and additional experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Computation Details</head><p>All experiments were performed on a shared GPU cluster equipped with GTX1080, GTX1080TI, GTX2080TI and TITAN RTX. About 20 of these GPUs were used simultaneously, and the total computational cost of this research project was about 1k GPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Datasets Description</head><p>We provide details of the datasets used in our experiments, including ZINC <ref type="bibr" target="#b10">(Irwin et al., 2012)</ref>, <ref type="bibr">CLUSTER (Dwivedi et al., 2020)</ref>, <ref type="bibr">PATTERN (Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type="bibr">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type="bibr">(Hu et al., 2020a)</ref>. For each dataset, we follow their respective training protocols and use the standard train/validation/test splits and evaluation metrics.</p><p>ZINC. The ZINC dataset is a graph regression dataset comprised of molecules, where the task is to predict constrained solubility. Like <ref type="bibr">Dwivedi et al. (2020)</ref>, we use the subset of 12K molecules and follow their same splits.  <ref type="bibr" target="#b0">(Abbe, 2018)</ref>. The goal for both datasets is node classification, with PATTERN focused on detecting a given pattern in the dataset, and with CLUSTER focused on identifying communities within the graphs. For PATTERN, the binary class label corresponds to whether a node is part of the predefined pattern or not; for CLUSTER, the multi-class label indicates membership in a community. We use the splits as is used in <ref type="bibr">Dwivedi et al. (2020)</ref>.</p><p>OGBG-PPA. PPA <ref type="bibr">(Hu et al., 2020a)</ref> is comprised of protein-protein association networks where the goal is to correctly classify the network into one of 37 classes representing the category of species the network is from. Nodes represent proteins and edges represent associations between proteins. Edge attributes represent information relative to the association, such as co-expression. We use the standard splits provided by <ref type="bibr">Hu et al. (2020a)</ref>.</p><p>OGBG-CODE2. CODE2 <ref type="bibr">(Hu et al., 2020a</ref>) is a dataset containing source code from the Python programming language. It is made up of Abstract Syntax Trees where the task is to correctly classify the sub-tokens that comprise the method name. We use the standard splits provided by <ref type="bibr">Hu et al. (2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Hyperparameter Choices and Reproducibility</head><p>Hyperparameter choice. In general, we perform a very limited hyperparameter search to produce the results in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>. The hyperparameters for training SAT models on different datasets are summarized in <ref type="table" target="#tab_7">Table 4</ref>, where only the dropout rate and the size of the subgraph k are tuned (k ? {1, 2, 3, 4}). We use fixed <ref type="bibr">RWPE (Dwivedi et al., 2022)</ref> with SAT on ZINC, PATTERN and CLUSTER. In all experiments, we use the validation set to select the dropout rate and the size of the subtree or subgraph k ? {1, 2, 3, 4}. All other hyperparameters are fixed for simplicity, including setting the readout method to mean pooling. We did not use RWPE on OGBG-PPA and OGBG-CODE2 as we observed very little performance improvement. Note that we only use k = 1 for the k-subgraph SAT models on CLUSTER and PATTERN due to its large memory requirement, which already leads to performance boost compared to the k-subtree SAT using a larger k. Reported results are the average over 4 seeds on ZINC, PATTERN and CLUSTER, as is done in <ref type="bibr">Dwivedi et al. (2020)</ref>, and averaged over 10 seeds on OGBG-PPA and OGBG-CODE2.</p><p>Optimization. All our models are trained with the AdamW optimizer <ref type="bibr" target="#b20">(Loshchilov &amp; Hutter, 2018)</ref> with a standard warm-up strategy suggested for Transformers in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>. We use either the L1 loss or the cross-entropy loss depending on whether the task is regression or classification. The learning rate scheduler proposed in the Transformer is used on the ZINC, PATTERN and CLUSTER datasets and a cosine scheduler <ref type="bibr" target="#b19">(Loshchilov &amp; Hutter, 2016)</ref> is used on the larger OGBG-PPA and OGBG-CODE2 datasets.  Number of parameters and computation time. In <ref type="table" target="#tab_8">Table 5</ref>, we report the number of parameters and the training time per epoch for SAT with k-subtree GNN extractors using the hyperparameters selected from <ref type="table" target="#tab_7">Table 4</ref>. Note that the number of parameters used in our SAT on OGB datasets is smaller than most of the state-of-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Additional Results</head><p>We provide additional experimental results on ZINC, OGBG-PPA and OGBG-CODE2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1. ADDITIONAL RESULTS ON ZINC</head><p>We report a more thorough comparison of SAT instances using different structure extractors and different readout methods in <ref type="table" target="#tab_9">Table 6</ref>. We find that SAT models with PNA consistently outperform other GNNs. Additionally, the readout methods have very little impact on the prediction performance.</p><p>C.4.2. ADDITIONAL RESULTS ON OGBG-PPA <ref type="table">Table 7</ref> summarizes the results for k-subtree SAT with different GNNs compared to state-of-the-art methods on OGBG-PPA. All the results are computed from 10 runs using different random seeds.</p><p>C.4.3. ADDITIONAL RESULTS ON OGBG-CODE2 <ref type="table">Table 8</ref> summarizes the results for k-subtree SAT with different GNNs compared to state-of-the-art methods on OGBG-CODE2. All the results are computed from 10 runs using different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Interpretation</head><p>In this section, we provide implementation details about the model visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Dataset and Training Details</head><p>We use the Mutagenicity dataset <ref type="bibr" target="#b12">(Kersting et al., 2016)</ref>, consisting of 4337 molecular graphs labeled based on their mutagenic effect. We randomly split the dataset into train/val/test sets in a stratified way with a proportion of 80/10/10. We first train a two-layer vanilla Transformer model using RWPE. The hidden dimension and the number of heads are fixed to 64 and 8 respectively. The CLS pooling as described in Section 4.2 is chosen as the readout method for visualization purpose. We also train a k-subtree SAT using exactly the same hyperparameter setting except that it does not use any absolute positional encoding. k is fixed to 2. For both models, we use the AdamW optimizer and the optimization strategy described in Section C.3. We train enough epochs until both models converge. While the classic Transformer with RWPE achieves a test accuracy of 78%, the k-subtree SAT achieves a 82% test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Additional Results</head><p>Visualization of attention scores. Here, we provide additional visualization examples of attention scores of the [CLS] node from the Mutagenicity dataset, learned by SAT and a vanilla Transformer. <ref type="figure" target="#fig_2">Figure 5</ref> provides several examples of attention learned weights. SAT generally learns sparser and more informative weights even for very large graph as shown in the left panel of the middle row. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relationship to Subgraph Neural Networks and Graph Pooling</head><p>In the following we clarify the relationship (and differences) of SAT to Subgraph Neural Networks <ref type="bibr">(Alsentzer et al., 2020)</ref> as well as to the general topic of graph pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Differences to Subgraph Neural Networks</head><p>Subgraph Neural Networks (SNN) <ref type="bibr">(Alsentzer et al., 2020)</ref> explores explicitly incorporating position, neighborhood and structural information, for the purpose of solving the problem of subgraph prediction. SNN generates representations at the level of subgraph (rather than node). SAT, on the other hand, is instead motivated by modeling the structural interaction (through the dot-product attention) between nodes in the Transformer architecture by generating node representations that are structure-aware. This structure-aware aspect is achieved via a structure extractor, which can be any function that extracts local structural information for a given node and does not necessarily need to explicitly extract subgraphs. For example, the k-subtree GNN extractor does not explicitly extract the subgraph, but rather only uses the node representation generated from a GNN. This aspect also makes the k-subtree SAT very scalable. In contrast to SNN, the input to the resulting SAT is not subgraphs but rather the original graph, and the structure-aware node representations are computed as the query and key for the dot-product attention at each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Relationship to Graph Pooling</head><p>In GNNs, structural information is traditionally incorporated into node embeddings via the neighborhood aggregation process. A supplemental way to incorporate structural information is through a process called local pooling, which is typically based on graph clustering <ref type="bibr">(Ying et al., 2018)</ref>. Local pooling coarsens the adjacency matrix at layer l in the network by, for example, applying a clustering algorithm to cluster nodes, and then replacing the adjacency matrix with the cluster assignment matrix, where all nodes within a cluster are connected by an edge. An alternative approach to the pooling layer is based on sampling nodes <ref type="bibr">(Gao &amp; Ji, 2019)</ref>. While <ref type="bibr">Mesquita et al. (2020)</ref> found that these local pooling operations currently do not improve performance relative to more simple operations, local pooling could in theory be incorporated into the existing SAT's k-subtree and k-subgraph GNN extractors, as it is another layer in a GNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, GIN<ref type="bibr" target="#b37">(Xu et al., 2019)</ref>,PNA (Corso et al., 2020), DeeperGCN<ref type="bibr" target="#b16">(Li et al., 2020a), and</ref> ExpC (Yang et al., 2022). Our comparison partners also include several recently proposed Transformers on graphs, including the original Transformer withRWPE (Dwivedi  et al., 2022), Graph Transformer (Dwivedi &amp; Bresson,  2021, SAN(Kreuzer et al., 2021), Graphormer (Ying et al.,  2021 and GraphTrans(Jain et al., 2021), a model that uses the vanilla Transformer on top of a GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Attention visualization of SAT and the Transformer. The center column shows the attention weights of the [CLS] node learned by our SAT model and the right column shows the attention weights learned by the classic Transformer with the random walk positional encoding (RWPE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Attention visualization of SAT and the Transformer. The middle column shows the attention weights of the [CLS] node learned by our SAT model and the right column shows the attention weights learned by the classic Transformer with RWPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 Department of Biosystems Science and Engineering, ETH Z?rich, Switzerland 2 SIB Swiss Institute of Bioinformatics, Switzerland. Correspondence to: Dexiong Chen &lt;dexiong.chen@bsse.ethz.ch&gt;, Leslie O'Bray &lt;leslie.obray@bsse.ethz.ch&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of SAT to SOTA methods on graph regression and classification tasks. ZINC results use edge weights where applicable, otherwise without edge weights.indicates we obtained the results ourselves by adapting the code provided by the original paper. means that higher is better for the performance metric; indicates lower is better.</figDesc><table><row><cell></cell><cell>ZINC</cell><cell>CLUSTER</cell><cell>PATTERN</cell></row><row><cell># GRAPHS</cell><cell>12,000</cell><cell>12,000</cell><cell>14,000</cell></row><row><cell>AVG. # NODES</cell><cell>23.2</cell><cell>117.2</cell><cell>118.9</cell></row><row><cell>AVG. # EDGES</cell><cell>49.8</cell><cell>4,303.9</cell><cell>6,098.9</cell></row><row><cell>METRIC</cell><cell>MAE</cell><cell>ACCURACY</cell><cell>ACCURACY</cell></row><row><cell>GIN</cell><cell cols="3">0.387?0.015 64.716?1.553 85.590?0.011</cell></row><row><cell>GAT</cell><cell cols="3">0.384?0.007 70.587?0.447 78.271?0.186</cell></row><row><cell>PNA</cell><cell cols="2">0.188?0.004 67.077?0.977</cell><cell>86.567?0.075</cell></row><row><cell cols="4">TRANSFORMER+RWPE 0.310?0.005 29.622?0.176 86.183?0.019</cell></row><row><cell cols="4">GRAPH TRANSFORMER 0.226?0.014 73.169?0.622 84.808?0.068</cell></row><row><cell>SAN</cell><cell cols="3">0.139?0.006 76.691?0.650 86.581?0.037</cell></row><row><cell>GRAPHORMER</cell><cell>0.122?0.006</cell><cell>-</cell><cell>-</cell></row><row><cell>K-SUBTREE SAT</cell><cell cols="3">0.102?0.005 77.751?0.121 86.865?0.043</cell></row><row><cell>K-SUBGRAPH SAT</cell><cell cols="3">0.094?0.008 77.856?0.104 86.848?0.037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of SAT to SOTA methods on OGB datasets.</figDesc><table><row><cell></cell><cell>OGBG-PPA</cell><cell>OGBG-CODE2</cell></row><row><cell># GRAPHS</cell><cell>158,100</cell><cell>452,741</cell></row><row><cell>AVG. # NODES</cell><cell>243.4</cell><cell>125.2</cell></row><row><cell>AVG. # EDGES</cell><cell>2,266.1</cell><cell>124.2</cell></row><row><cell>METRIC</cell><cell>ACCURACY</cell><cell>F1 SCORE</cell></row><row><cell>GCN</cell><cell>0.6839?0.0084</cell><cell>0.1507?0.0018</cell></row><row><cell cols="2">GCN-VIRTUAL NODE 0.6857?0.0061</cell><cell>0.1595?0.0018</cell></row><row><cell>GIN</cell><cell>0.6892?0.0100</cell><cell>0.1495?0.0023</cell></row><row><cell cols="2">GIN-VIRTUAL NODE 0.7037?0.0107</cell><cell>0.1581?0.0026</cell></row><row><cell>DEEPERGCN</cell><cell>0.7712?0.0071</cell><cell>-</cell></row><row><cell>EXPC</cell><cell>0.7976?0.0072</cell><cell>-</cell></row><row><cell>TRANSFORMER</cell><cell>0.6454?0.0033</cell><cell>0.1670?0.0015</cell></row><row><cell>GRAPHTRANS</cell><cell>-</cell><cell>0.1830?0.0024</cell></row><row><cell>K-SUBTREE SAT</cell><cell>0.7522?0.0056</cell><cell>0.1937?0.0028</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Since SAT uses a GNN to extract structures, we compare the performance of the original sparse GNN to SAT which uses that GNN ("base GNN"). Across different choices of GNNs, we observe that both k-subtree and k-subgraph SATs always outperform the original sparse GNN it uses. The evaluation metrics are the same as inTable 1.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ZINC</cell><cell>CLUSTER</cell><cell>PATTERN</cell></row><row><cell></cell><cell></cell><cell cols="2">W/ EDGE ATTR. W/O EDGE ATTR.</cell><cell>ALL</cell><cell>ALL</cell></row><row><cell>GCN</cell><cell cols="2">BASE GNN K-SUBTREE SAT K-SUBGRAPH SAT 0.114?0.005 0.192?0.015 0.127?0.010</cell><cell>0.367?0.011 0.174?0.009 0.184?0.002</cell><cell cols="2">68.498?0.976 71.892?0.334 77.247?0.094 86.749?0.065 77.682?0.098 86.816?0.028</cell></row><row><cell>GIN</cell><cell cols="2">BASE GNN K-SUBTREE SAT K-SUBGRAPH SAT 0.095?0.002 0.209?0.009 0.115?0.005</cell><cell>0.387?0.015 0.166?0.007 0.162?0.013</cell><cell cols="2">64.716?1.553 85.590?0.011 77.255?0.085 86.759?0.022 77.502?0.282 86.746?0.014</cell></row><row><cell>GRAPHSAGE</cell><cell>BASE GNN K-SUBTREE SAT K-SUBGRAPH SAT</cell><cell>---</cell><cell>0.398?0.002 0.164?0.004 0.168?0.005</cell><cell cols="2">63.844?0.110 50.516?0.001 77.592?0.074 86.818?0.043 77.657?0.185 86.838?0.010</cell></row><row><cell>PNA</cell><cell cols="2">BASE GNN K-SUBTREE SAT K-SUBGRAPH SAT 0.094?0.008 0.188?0.004 0.102?0.005</cell><cell>0.320?0.032 0.147?0.001 0.131?0.002</cell><cell cols="2">67.077?0.977 86.567?0.075 77.751?0.121 86.865?0.043 77.856?0.104 86.848?0.037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Information Processing Systems, NeurIPS, 2020. (Cited on 21) Bodnar, C., Frasca, F., Otter, N., Wang, Y. G., Li?, P., Montufar, G. F., and Bronstein, M. Weisfeiler and lehman go cellular: Cw networks. In Advances in Neural Infor-Briefings in Bioinformatics, 22(6):bbab159, 2021. (Cited on 1) Yang, M., Wang, R., Shen, Y., Qi, H., and Yin, B. Breaking the expression bottleneck of graph neural networks. IEEE Transactions on Knowledge and Data Engineering, pp. 1-1, 2022. doi: 10.1109/TKDE.2022.3168070.</figDesc><table><row><cell>mation Processing Systems (NeurIPS), 2021. (Cited on 5) Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI Conference on Artificial Intelli-gence, 2020. (Cited on 1) Corso, G., Cavalleri, L., Beaini, D., Li?, P., and Veli?kovi?, P. Principal neighbourhood aggregation for graph nets. In Advances in Neural Information Processing Systems (NeurIPS), 2020. (Cited on 2, 7) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2020. (Cited on 1) Dwivedi, V. P. and Bresson, X. A generalization of trans-former networks to graphs. In AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021. (Cited on 2, 3, 7) Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. (Cited on 7, 8, 16, 17) Dwivedi, V. P., Luu, A. T., Laurent, T., Bengio, Y., and Bres-son, X. Graph neural networks with learnable structural and positional representations. In International Confer-ence on Learning Representations, 2022. (Cited on 2, 3, 5, 7, 17) Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., and Yin, D. Graph neural networks for social recommendation. In The World Wide Web Conference, 2019. (Cited on 1) Gao, B. and Pavel, L. On the properties of the softmax func-tion with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017. (Cited on 14) Gao, H. and Ji, S. Graph u-nets. In International Conference on Machine Learning, pp. 2083-2092, 2019. (Cited on 21) 2, 7) Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y. Do transformers really perform badly for graph representation? In Advances in Neural Information Processing Systems (NeurIPS), 2021. (Cited on 3, 7) Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J. Hierarchical graph representation learning with differentiable pooling. Advances in neural informa-tion processing systems, 31, 2018. (Cited on 21) You, J., Ying, R., and Leskovec, J. Position-aware graph neural networks. In International Conference on Machine Learning (ICML), 2019. (Cited on 2) Zhang, J., Zhang, H., Xia, C., and Sun, L. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020. (Cited on 2) Zhang, M. and Li, P. Nested graph neural networks. In Pro-ceedings of the 35th Conference on Neural Information Gaudelet, T.(Cited on Processing Systems (NeurIPS), 2021. (Cited on 4, 5)</cell></row></table><note>, Day, B., Jamasb, A. R., Soman, J., Regep, C., Liu, G., Hayter, J. B., Vickers, R., Roberts, C., Tang, J., et al. Utilizing graph machine learning within drug discovery and development.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for SAT models trained on different datasets. RWPE-p indicates using p steps in the random walk positional encoding, which results in a p-dimensional vector as the positional representation for each node.</figDesc><table><row><cell>Hyperparameter</cell><cell>ZINC</cell><cell cols="4">CLUSTER PATTERN OGBG-PPA OGBG-CODE2</cell></row><row><cell>#Layers</cell><cell>6</cell><cell>16</cell><cell>6</cell><cell>3</cell><cell>4</cell></row><row><cell>Hidden dimensions</cell><cell>64</cell><cell>48</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>FFN hidden dimensions</cell><cell></cell><cell></cell><cell cols="2">2?Hidden dimensions</cell><cell></cell></row><row><cell>#Attention heads</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>{4, 8}</cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell cols="2">{0.0, 0.1, 0.2, 0.3, 0.4}</cell><cell></cell></row><row><cell>Size of subgraphs k</cell><cell></cell><cell></cell><cell>{1, 2, 3, 4}</cell><cell></cell><cell></cell></row><row><cell>Readout method</cell><cell>mean</cell><cell>None</cell><cell>None</cell><cell>mean</cell><cell>mean</cell></row><row><cell>Absolute PE</cell><cell>RWPE-20</cell><cell>RWPE-3</cell><cell>RWPE-7</cell><cell>None</cell><cell>None</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0003</cell><cell>0.0003</cell><cell>0.0001</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>#Epochs</cell><cell>2000</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>30</cell></row><row><cell>Warm-up steps</cell><cell>5000</cell><cell>5000</cell><cell>5000</cell><cell>10 epochs</cell><cell>2 epochs</cell></row><row><cell>Weight decay</cell><cell>1e-5</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-6</cell></row><row><cell cols="6">PATTERN and CLUSTER. PATTERN and CLUSTER Dwivedi et al. (2020) are synthetic datasets that were created</cell></row><row><cell>using Stochastic Block Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Number of parameters and training time per epoch for k-subtree SAT models using the hyperparameters inTable 4. Various GNNs are used as the base GNN in SAT.</figDesc><table><row><cell></cell><cell cols="5">ZINC CLUSTER PATTERN OGBG-PPA OGBG-CODE2</cell></row><row><cell>Base GNN</cell><cell></cell><cell></cell><cell cols="2">#Parameters</cell><cell></cell></row><row><cell>GCN</cell><cell>421k</cell><cell>571k</cell><cell>380k</cell><cell>766k</cell><cell>14,030k</cell></row><row><cell>GIN</cell><cell>495k</cell><cell>684k</cell><cell>455k</cell><cell>866k</cell><cell>14,554k</cell></row><row><cell>PNA</cell><cell>523k</cell><cell>741k</cell><cell>493k</cell><cell>1,088k</cell><cell>15,734k</cell></row><row><cell>Base GNN</cell><cell></cell><cell cols="3">GPU time on a single TITAN RTX/epoch</cell><cell></cell></row><row><cell>GCN</cell><cell>6s</cell><cell>142s</cell><cell>40s</cell><cell>308s</cell><cell>40min</cell></row><row><cell>GIN</cell><cell>6s</cell><cell>144s</cell><cell>62s</cell><cell>310s</cell><cell>40min</cell></row><row><cell>PNA</cell><cell>9s</cell><cell>178s</cell><cell>90s</cell><cell>660s</cell><cell>55min</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Test MAE for SAT models using different structure extractors and readout methods on the ZINC dataset. 131?0.002 0.129?0.003 0.128?0.004 0.094?0.008 0.089?0.002 0.093?0.009</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>W/O EDGE ATTRIBUTES</cell><cell></cell><cell></cell><cell>W/ EDGE ATTRIBUTES</cell></row><row><cell></cell><cell>BASE GNN</cell><cell>MEAN</cell><cell>SUM</cell><cell>CLS</cell><cell>MEAN</cell><cell>SUM</cell><cell>CLS</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">0.174?0.009 0.170?0.010 0.167?9.005 0.127?0.010 0.117?0.008 0.115?0.007</cell></row><row><cell>K-SUBTREE SAT</cell><cell cols="7">GIN GRAPHSAGE 0.164?0.004 0.165?0.008 0.156?0.005 0.166?0.007 0.162?0.010 0.157?0.002 0.115?0.005 0.112?0.008 0.104?0.003 ---</cell></row><row><cell></cell><cell>PNA</cell><cell cols="6">0.147?0.001 0.142?0.008 0.135?0.004 0.102?0.005 0.102?0.003 0.098?0.008</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">0.184?0.002 0.186?0.007 0.184?0.007 0.114?0.005 0.103?0.002 0.103?0.008</cell></row><row><cell>K-SUBGRAPH SAT</cell><cell cols="7">GIN GRAPHSAGE 0.168?0.005 0.165?0.005 0.169?0.005 0.162?0.013 0.158?0.007 0.162?0.005 0.095?0.002 0.097?0.002 0.098?0.010 ---</cell></row><row><cell></cell><cell>PNA</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung (K.B.). The authors would also like to thank Dr. Bastian Rieck and Dr. Carlos Oliver for their insightful feedback on the manuscript, which greatly improved it.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides both theoretical and experimental materials and is organized as follows: Section A provides a more detailed background on graph neural networks. Section B presents proofs of Theorem 1 and 2. Section C provides experimental details and additional results. Section D provides details on the model interpretation and additional visualization results.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: Recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">177</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subgraph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural</title>
		<meeting>Neural</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 1, 2)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 2, 7)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Topological graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on 9</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Cited on 6, 15</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2020a</title>
		<imprint/>
	</monogr>
	<note>Cited on 7, 16, 17</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR), 2020b</title>
		<imprint/>
	</monogr>
	<note>Cited on 2</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zinc: A free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Cited on 16</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 3, 5, 7</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 2, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 2, 7)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Cited on 2, 7)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2020b</title>
		<imprint/>
	</monogr>
	<note>Cited on 2</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking pooling in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 21</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Encoding graph structure in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on 3, 5</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random walk graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 5</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">cosformer: Rethinking softmax in attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Cited on 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on largescale molecular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Cited on 3</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)</title>
		<editor>Zhou, Z.-H.</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 3</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on 9</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 3, 17</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 2, 7)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Cited on 9</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new perspective on &quot;how graph neural networks go beyond weisfeiler-lehman?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Cited on 4</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 4, 6, 7, 8, 13, 15</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
