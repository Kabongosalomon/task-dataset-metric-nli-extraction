<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Models for Situation Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
							<email>amallya2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Models for Situation Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes Recurrent Neural Network (RNN) models to predict structured 'image situations' -actions and noun entities fulfilling semantic roles related to the action. In contrast to prior work relying on Conditional Random Fields (CRFs), we use a specialized action prediction network followed by an RNN for noun prediction. Our system obtains state-of-the-art accuracy on the challenging recent imSitu dataset, beating CRF-based models, including ones trained with additional data. Further, we show that specialized features learned from situation prediction can be transferred to the task of image captioning to more accurately describe human-object interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognition of actions and human-object interactions in still images has been widely studied in computer vision. Early datasets and approaches focused on identifying a relatively small number of actions, such as 10 in PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> and 40 in the Stanford Dataset <ref type="bibr" target="#b29">[30]</ref>. Newer and larger datasets such as MPII Human Pose <ref type="bibr" target="#b18">[19]</ref> have enlarged the number of action classes to around 400. The COCO-A <ref type="bibr" target="#b20">[21]</ref> and HICO <ref type="bibr" target="#b3">[4]</ref> datasets aim to recognize interactions between multiple humans, and humans and objects, expanding the scope of recognition to outputs such as human-riding-bicycle, human-repairing-bicycle, humanriding-horse, etc.</p><p>Of late, the focus has shifted to predicting even more structured outputs, tackling higher-level questions such as who is doing what and with which object. The recently introduced imSitu Dataset <ref type="bibr" target="#b32">[33]</ref> generalizes the task of action recognition to 'situation recognition' -the recognition of all entities fulfilling semantic roles in an instance of an action performed by a human or non-human actor. Given a particular action, situations are represented by a set of relevant (semantic role: noun entity) pairs. An example image and associated situation from imSitu are shown in <ref type="figure">Fig. 1</ref>, where "a woman arranging flowers in a vase on the countertop" is represented by Action: arranging, {(Agent: woman), (Item: flowers), (Tool: vase), (Place: countertop)}. As an-  <ref type="figure">Figure 1</ref>: Each image in imSitu is labeled with an action verb (orange), and each verb is associated with a unique set of semantic roles (bold black) which are fulfilled by noun entities present in the image (green). Each image has multiple annotations to account for the intrinsic ambiguity of the task. Our approach first uses the fusion network of <ref type="bibr" target="#b15">[16]</ref> to predict the action verb. Then it feeds the verb and a visual feature from a separate network into an RNN to predict the noun roles in a fixed sequence conditioned on the action. other example, "A horse rearing outside" can be mapped to Action: rearing, {(Agent: horse), (Place: outside)}. imSitu consists of 504 actions, 1,700 semantic roles, and 11,000 noun entities resulting in around 200,000 unique situations. Along with the dataset, Yatskar et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> also introduced Conditional Random Field (CRF) models to predict situations given an image. In our work, we propose and train Recurrent Neural Networks (RNNs) to predict such situations and outperform the previously state of the art CRFs. Our use of RNNs for situation prediction is motivated by their popularity for tasks like image caption generation, where they have proven to be successful at capturing grammar and forming coherent sentences linking multiple concepts. The standard framework for caption generation involves feeding high-level features from a CNN, often trained for image classification on ImageNet <ref type="bibr" target="#b21">[22]</ref>, into an RNN that proceeds to generate one word of the caption at a time <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>. Situation recognition involves the prediction of a sequence of noun entities for a particular action, so it can be viewed as a more structured version of the captioning task with a grammar that is fixed given an action. <ref type="figure">Figure 1</ref> gives an overview of our best proposed system. First, we predict the action verb using the specialized action recognition architecture of <ref type="bibr" target="#b15">[16]</ref>, which fuses features from a detected person box with a global representation of the image. Conditioned on the action, we treat the prediction of noun entities as a sequence generation problem and use an RNN. Details of our model, along with several baselines, will be given in Section 2. Through extensive experiments (Section 3) we found that using separate networks for predicting the action verb and the noun entities produces higher accuracy than jointly training a visual representation for the two tasks. Finally, in Section 4 we explore how knowledge gained from situation prediction can obtain meaningful improvements for image captioning on the MSCOCO dataset <ref type="bibr" target="#b14">[15]</ref> through feature transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Situation Prediction Task and Methods</head><p>Situations are based on a discrete set of action verbs V , noun entities N , and semantic roles R. Each verb v ? V is paired with a unique frame f ? F derived from FrameNet <ref type="bibr" target="#b7">[8]</ref>, a lexicon for semantic role labeling. A frame is a collection of semantic roles R v ? R which are associated with the verb v. For example, the semantic roles {Agent, Item, Tool, Place} ? R are associated with the verb arranging. In an instantiation of an action in an image, each semantic role is fulfilled by some noun n ? N ?{?}, where ? indicates that the value is either not known or does not apply. The set of nouns N is derived from WordNet <ref type="bibr" target="#b16">[17]</ref>. An instance of an action v in an image I forms a realized frame F <ref type="bibr">(I,v)</ref> in which each semantic role is associated with some noun n, i.e. F (I,v) = {(r i , n i ) : r i ? R v , n i ? N ?{?}, i = 1, ? ? ? , |R v |}. Finally, a situation S is the pair of action and realized frame for that action, S = {v, F (I,v) }. The task of situation prediction is to predict an action verb and its associated realized frame given an image. Though each image is annotated with a single verb, multiple situations might be applicable for an image due to the choice of nouns used to form a realized frame. For example, one might use the term countertop instead of kitchen as the noun associated with the semantic role of Place in <ref type="figure">Fig. 1</ref>. To account for this multiplicity, the imSitu dataset provides three independently labeled situations per image.</p><p>The authors who introduced situtation prediction also proposed a CRF-based approach for the task <ref type="bibr" target="#b32">[33]</ref>. They decompose the structured prediction of a situation, S = {v, F (I,v) }, over the verb v and semantic role value pairs (r, n) in the realized frame F (I,v) . They learn a potential function ? v (v; ?) for every verb, and a potential function for every verb, semantic role, noun entity tuple ? r (v, r, n; ?)</p><formula xml:id="formula_0">(v ? V , r ? R v , n ? N ? {?}),</formula><p>where ? denotes the parameters of the deep neural network used to predict these potentials. The probability of a particular situation S given input image I can thus be represented by:</p><formula xml:id="formula_1">p(S|I; ?) = 1 Z ? ? v (v|I; ?) ? (ri,ni) ri?Rv,ni?N ?{?} ? r (v, r i , n i |I; ?). (1)</formula><p>The CRF normalization constant Z required for computing the loss during training is obtained by predicting the potentials for all valid tuples found in the training set and then summing them. The potentials are predicted using a fully connected layer on top of the fc7 layer of the VGG-16 network <ref type="bibr" target="#b22">[23]</ref>. During inference time, all valid tuples are scored and ranked. A difficulty with this approach is the large number of potentials that need to be predicted: 504 for all possible verbs and 121,381 for all valid verb, semantic role, noun entity tuples. Further, this model does not explicitly account for the fact that nouns are shared across semantic roles, though it is possible that the deep neural network implicitly learns such representations. In order to explicitly enforce the sharing of information and reduce the number of parameters, the follow-up work by Yatskar et al. <ref type="bibr" target="#b31">[32]</ref> further decomposes the potentials as a tensor product over verbs, semantic roles, and noun entities. This makes for a complex model, details of which can be found in <ref type="bibr" target="#b31">[32]</ref>.</p><p>We take an alternate view of situation prediction by observing that given a verb v, the set of semantic roles R v associated with it is fixed. For example, given the verb arranging, we know that we have to predict relevant noun entities for the semantic roles of R arranging ={Agent, Item, Tool, Place} (see <ref type="figure">Fig. 1</ref>). Conditioned on a given verb, if we assume some arbitrary but fixed ordering over these semantic roles, we can reduce the problem to that of sequential prediction of noun entities corresponding to the semantic roles. We decompose p(S|I; ?) as: p(S|I; ?) = p v, (r 1 , n 1 ), ? ? ? , (r |Rv| , n |Rv| )|I; ?</p><p>= p v, n 1 , ? ? ? , n |Rv| |I; ?</p><p>= p(v|I; ?) |Rv| t=1 p (n t |v, n 1 , ? ? ? , n t?1 , I; ?) . (4)</p><p>Note that if an arbitrary but fixed ordering is chosen for semantic roles belonging to every verb, then Eq. (3) follows from Eq. (2) as the correspondence of nouns to roles is implicit. In our implementation, we use the semantic role ordering provided in the dataset, which was derived from FrameNet <ref type="bibr" target="#b7">[8]</ref>. We explore the sensitivity of methods to the specific ordering in the experiments of Section 3, and find that the accuracy is affected only to a very small degree.</p><p>We represent each p (n t |v, n 1 , ? ? ? , n t?1 , I; ?) in Eq. (4) with a softmax over all the noun entities in the training dataset, referred to as the noun vocabulary. This is a standard formulation first introduced for natural language translation <ref type="bibr" target="#b23">[24]</ref> and widely adopted for image caption- The four approaches used for action and noun entity prediction: a) The baseline no-vision model, which only tries to predict noun entities n 1 , ? ? ? , n 4 in the chosen arbitrary but fixed semantic role ordering, given the ground truth verb v. b) Training an RNN which takes image features as input and predicts action, followed by noun entities, c) Training a VGG-16 network for action prediction, and feeding its features to the RNN that predicts nouns associated with the semantic roles, and d) Using separate networks for action and noun entity prediction. Bold colored text (orange and green) indicates training targets.</p><p>ing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Similar to these works, we use a softmax classification loss with the corresponding ground truth noun entity as the target at every prediction step.</p><p>It is worth pointing out that both formulations, those of CRF-based structured prediction (Eq. (1)) and sequential prediction (Eq. (4)), are equally powerful in their representational abilities as both model the joint probability of the verb and noun entities in a proposed situation. At inference time, in the CRF approach of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, all valid tuples of verb and noun entities are evaluated and the most likely one is reported, while in our sequential approach, we perform approximate inference by selecting the most likely noun entity at each step. Despite this limitation, we obtain satisfactory empirical results (we also experimented with beam search but did not see an improvement).</p><p>Next, we present the progression of models we developed, starting with a language-only baseline and ending in our highest-performing method illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) No vision, RNN for Nouns.</head><p>In order to verify that sequential situation prediction can actually work and that an RNN can memorize the specific ordering of semantic roles for each verb, we propose a basic language-only model that only tries to predict noun entities given the ground truth verb. This model also acts as a strong baseline by exploiting bias in the dataset labeling as it does not use any visual feature input. This model is depicted in <ref type="figure" target="#fig_1">Fig. 2a</ref>. The ground truth verb is fed in at the first time step. Note that it is essential to feed in the verb at the first time step as the ordering and number of semantic roles for which noun entities are produced is decided by the choice of verb. At the following time step, the RNN tries to predict the noun entity associated with the first semantic role in the arbitrarily selected but fixed ordering, and so on, until a noun entity is predicted for each semantic role for that verb. In line with prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>, we feed in the initial verb and the output of the previous time step as a one-hot vector through a word embedding layer. As will be discussed in the next section, this RNN can indeed memorize the arbitrary semantic role ordering to make noun entity predictions in the appropriate order. B) Shared network, RNN for Actions &amp; Nouns. The next natural step is to extend the above no-vision model to use image features and predict the action as well. This model is shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>. After consuming the fc7 image features from a VGG-16 network at the first time step, the model predicts the action at the second time step and then continues on to predict noun entities. The noun vocabulary (space of all noun entities) is extended with that of possible actions to allow the prediction of both. Note that we use the ground truth action as input during training and the predicted action during testing. At inference time, we enforce that only an action can be predicted at the second time step, followed by noun entities only thereafter. C) Shared network, Actions classifier, RNN for Nouns. Since situation recognition has such a strong up-front dependence on the action verb, the next question we want to explore is whether we can improve performance by breaking off the action prediction into a specialized task, instead of treating it the same as the other roles. It also helps that imSitu has many fewer verbs (504) than noun entities (11K), giving us enough data to train a dedicated action classifier. Accordingly, our second model predicts actions using a separate fully-connected classification layer on top of the fc7 layer of the VGG-16 network as shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>. At the first step of the RNN, we feed in the one-hot representation of the action (at training time, we use the ground truth action and at test time, the predicted action). At the second time step, we feed in the fc7 image features to the RNN to predict noun entities. Our experiments will investigate how to train the VGG network to get the highest ac-curacy for the overall task. One option is to train it solely for action prediction and another is to jointly train it for both action and noun prediction. Interestingly, our results in Section 3 will show that the former strategy works better. D) Separate networks, Actions classifier, RNN for Nouns. The lack of success of joint training leads to the question of whether we can do even better by not sharing parameters between action and noun entity prediction. Accordingly, our final model decouples the two tasks and uses two separate networks that are independently fine-tuned, as depicted in <ref type="figure" target="#fig_1">Fig. 2d</ref>. For predicting actions, we use the feature fusion network of <ref type="bibr" target="#b15">[16]</ref> which obtained state-of-the-art performance on the HICO dataset <ref type="bibr" target="#b3">[4]</ref>. This network (called Fusion in the following) combines local features from detected human boxes and global features from the whole image to make predictions that are then pooled. It defaults to the full image in case no human is detected in the image. As a large number of images in the imSitu dataset feature humans, this is a reasonable choice of architecture. Along with a vanilla RNN for predicting noun entities, we will also report experiments with an attention model based on <ref type="bibr" target="#b28">[29]</ref> which consumes image features through a soft attention module at each time step. Note that instead of the fc7 features, the attention-based RNN uses the conv5 feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Situation Prediction Experiments</head><p>Implementation Details. We use the simplified Long-Short Term Memory (LSTM) cell <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref> as our RNN model. We use a single-layer LSTM and with input and hidden layer sizes of 512. We did not observe any significant improvement by using larger layer sizes or more layers. The imSitu dataset has a total of 504 actions and 11,790 noun entities, leading to an LSTM output layer size of 11,790 in the case of models A, C, and D and 11,790+504 in the case of model B. We train all our RNNs with Adam <ref type="bibr" target="#b12">[13]</ref> using an initial learning rate of 4e-4, decayed by a factor of 10 every 28,800 iterations using a batch size of 64. For noun entity prediction, we first train the RNN for 60k iterations. We then turn on fine-tuning for the CNN with an initial learning rate of 1e-5 and use Adam with the same learning rate decay scheme for an additional 100k iterations. The Fusion network <ref type="bibr" target="#b15">[16]</ref> is trained using stochastic gradient descent with momentum using a learning rate of 5e-5 for 70k iterations. Person boxes are detected using the Faster-RCNN <ref type="bibr" target="#b19">[20]</ref> with a confidence threshold of 0.8. Similar to <ref type="bibr" target="#b15">[16]</ref>, we use a weighted loss during action prediction, unless otherwise specified. The weight for a class is inversely proportional to its frequency in the training set. Using weighted loss or beam search for noun entity prediction did not help. We only train on the imSitu train set of 75k images. During training, we evaluate the model on the dev set of 25k images and retain the best-performing model.</p><p>Finally, we evaluate the best model on the imSitu test set of 25k images. All hyperparameters are tuned on the dev set.</p><p>Metrics. We evaluate performance on action verb predictions (verb), and (semantic role: noun entity) pair predictions (value, value-all) as well as the average across all measures (mean), as proposed in <ref type="bibr" target="#b31">[32]</ref>. Value-all measures the percentage of predictions for which all of the (semantic role: noun entity) pairs of an action verb matched with at least 1 of the 3 ground truth (GT) annotations, while Value measures the percentage of pairs which matched at least one of the three GT annotations. We report accuracy at top-1, top-5 action verb predictions and given the GT verb. Similar to <ref type="bibr" target="#b31">[32]</ref>, we also report performance on examples with ten or fewer samples in the imSitu training set (rare setting).</p><p>Results. We report results on the full dev set in <ref type="table" target="#tab_0">Table 1</ref>. Section I of the table presents results from prior work of Yatskar et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref>. Their baseline, a method they call the Discrete Classifier, restricts its output space to the 10 most frequent realized frames for each verb. The Image Regression CRF uses the formulation of Eq. (1) with an output space of 121,381 for (verb, semantic role, noun entity) tuples + 504 for actions, while Tensor Composition CRF uses a tensor-based potential decomposition in an attempt to reduce the number of parameters. The authors had to combine the potentials produced by both models in order to improve performance, leading to the Tensor Comp. + Reg. CRF method. Finally, by using five million web-sourced images based on semantic querying <ref type="bibr" target="#b31">[32]</ref> in addition to the 75k train set images, they were able to slightly improve performance.</p><p>Our baseline presented in Section II of <ref type="table" target="#tab_0">Table 1</ref>, corresponding to the architecture of <ref type="figure" target="#fig_1">Fig. 2a</ref>, shows that RNNs can indeed memorize an arbitrary ordering of semantic roles for each verb and produce relevant noun entities in the correct and corresponding order. Further, by simply exploiting the labeling bias, it beats the Discrete Classifier baseline by a large margin, given the ground truth action verb.</p><p>Section III shows results from our next model <ref type="figure" target="#fig_1">(Fig. 2b)</ref>, which tries to predict both the action and noun entities using the same RNN. It improves the value metric by over 16% given the ground truth verb over our no-vision baseline model, by using information from visual features.</p><p>Section IV reports the results of separating the action prediction parameters from those of the noun entity predicting RNN (see <ref type="figure" target="#fig_1">Fig. 2c</ref>). We see a large improvement in action verb prediction accuracy (26.52% to 35.35%) as long as we first fine-tune the network for the action task. By simply using features from the network trained for action prediction, we only observe a very small drop in the value metric given ground truth verbs, as compared to jointly fine-tuning for verb and noun entity prediction (68.98% to 68.44%). Here, we also try predicting the noun entities in a reversed order so as to determine whether the order affects perfor-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>top-1 predicted verb top-5 predicted verbs ground truth verbs mean verb value value-all verb value value-all value value-all</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I)</head><p>Discrete Classifier <ref type="bibr" target="#b32">[33]</ref> 26   <ref type="table">Table 3</ref>: Situation prediction results on the rare portion of the imSitu test set. Along with better verb prediction accuracy, our method also produces more accurate role values given GT verbs, indicating better generalization probably due to the use of shared parameters and word embeddings.</p><p>mance. We clearly see that this has very little effect on accuracy (0.1-0.2%). However, we cannot rule out that some optimal ordering of semantic roles might exist for every verb. We find that joint fine-tuning, either from the start or later, is detrimental for action verb prediction, leading us to the final models of Sections V and VI, which use separate networks for action and noun entity prediction.</p><p>In Section V of <ref type="table" target="#tab_0">Table 1</ref>, we compare various methods of separately predicting actions and noun entities. The Fusion network of <ref type="bibr" target="#b15">[16]</ref> outperforms the VGG-16 network at action prediction and using a weighted softmax loss helps in both cases. By using a stand-alone action prediction network, we obtain a top-1 and top-5 accuracy of 36.11% and 63.11% in contrast to the previous best of 32.91% and 59.92% from <ref type="bibr" target="#b32">[33]</ref>, respectively. Even the method from <ref type="bibr" target="#b32">[33]</ref> that uses an additional 5 Million images only obtains 34.20% and 62.21% accuracies, respectively. also try using the soft attention-based architecture of Xu et al. <ref type="bibr" target="#b28">[29]</ref>. The attention-based RNN works better, as long as we do not fine-tune the underlying VGG-16 network. Turning on fine-tuning makes the simple LSTM architecture work better, in line with results obtained on image captioning <ref type="bibr" target="#b26">[27]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows some predicted situations and associated attention maps. Qualitatively, attention produces plausible results in simple cases, but is unable to make fine distinctions, e.g., between multiple instances of a noun entity in different roles (bottom row of the <ref type="figure">figure)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apart from using LSTMs for predicting noun entities, we</head><p>Finally, we combine our best action prediction and our best noun entity prediction networks to propose our final method referred to as Fusion, VGG+RNN <ref type="figure" target="#fig_1">(Fig. 2d)</ref> in Section VI of <ref type="table" target="#tab_0">Table 1</ref>. We beat the previous state-of-the-art method trained on the imSitu train set on every metric. Additionally, we also beat the method trained on the extra 5M images, except on the value given ground truth verb metric, on which we lag by just 0.32%. <ref type="table" target="#tab_1">Table 2</ref> compares our best-performing method against the previous work on the full imSitu test set. We observe a trend similar to that on the imSitu dev test. We improve upon both the top-1 and top-5 verb prediction accuracies by around 3% and by 1% (value) and 2.3% (value-all) on noun entity prediction given ground truth verbs, for methods trained on the imSitu train set.</p><p>Most interestingly, <ref type="table">Table 3</ref> shows that we also do well on the rare portion of the imSitu test set. We improve upon the top-1 and top-5 verb prediction accuracies by around 2% and by 3% respectively. We improve by 3% (value) and 3.5% (value-all) on noun entity prediction given ground truth verbs, for methods trained on the imSitu train set. We believe that embedding nouns in a common continuous space during input to RNNs helps to overcome the lack of data and aids in generalization more effectively than the 'semantic augmentation' with additional data in the previous method <ref type="bibr" target="#b31">[32]</ref>.</p><p>Finally, <ref type="figure">Figure 4</ref> shows some correctly and incorrectly predicted situations on the imSitu test set by our best-performing method. While most of the mistakes are due to incorrect action predictions, we observe that mistakes are often reasonable, e.g., 'arresting' instead of 'misbehaving' in the bottom row, middle image. By analyzing the verb prediction results, we find that we obtain the worst performance on bothering, intermingling, and imitating, which are very contextual and semantic in nature, while those with a clear visual nature such as erupting, shearing, and taxiing obtain high accuracies. The worst noun prediction performance is obtained in cases where multiple nouns can fulfill semantic roles, such as distributing, prying, repairing; while ballooning, taxiing, scoring obtain high accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application to Image Captioning</head><p>One of the key motivations of proposing the task of image situation prediction was to better understand and learn the semantic content of images, beyond mere action recognition <ref type="bibr" target="#b32">[33]</ref>. A more structured and nuanced understanding of image semantics is expected to help high-level reasoning tasks such as image captioning and Visual Question Answering (VQA) <ref type="bibr" target="#b2">[3]</ref>. In this work, we try to leverage our new state-of-the-art models for action verb and noun entity recognition to improve image captioning performance on the MSCOCO dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>We modify an off-the-shelf image captioning model, NeuralTalk2 <ref type="bibr" target="#b0">[1]</ref>, by providing it features from our networks as an additional input, as shown in <ref type="figure">Figure 5</ref>. The vanilla NeuralTalk2 network takes in fc7 features from a VGG-16 network as input to an RNN through an image embedding layer W i . It then proceeds to output words of the caption one by one till the &lt;END&gt; token is predicted or a maximum length (typically 16) is reached. We feed in features from networks trained on imSitu at the second time step, similar to the method proposed in <ref type="bibr" target="#b30">[31]</ref>. We try two types of features: fc7 features from the VGG-16 network used for noun entity prediction (green network in <ref type="figure" target="#fig_1">Fig. 2</ref>) and fc7 features from the VGG-16 network trained for action verb prediction (VGG, fc for Actions of Section IV of <ref type="table" target="#tab_0">Table 1</ref>). We use features from the VGG-16 network for action prediction instead of the better performing Fusion network because the former produces features from the whole image, while the latter produces features for each detected person box. Implementation Details and Results. We use a singlelayer LSTM with 512 hidden units and input size of 512. We train our captioning networks on the MSCOCO split of Karpathy et al. <ref type="bibr" target="#b10">[11]</ref> which has 113,287 training, 5k validation, and 5k test images. We train the RNN and VGG-16 CNN using Adam, with an initial learning rate of 4e-4 and 1e-5 respectively. We train the baseline network in the following recommended stages <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>: 1) Fine-tune RNN only for 100k iterations, 2) Fine-tune RNN and VGG-16 network for 150k iterations. As shown in  <ref type="figure">Figure 5</ref>: The modified NeuralTalk2 <ref type="bibr" target="#b0">[1]</ref> recurrent neural network that accepts the fc7 feature vector from the networks trained on the imSitu situation prediction task at time step 2. All units with the same color share weights. Bold words w 1 , ? ? ? , w N are targets at training time.  set. We then modify the baseline model to accept an additional imSitu-based feature as input, as shown in <ref type="figure">Fig. 5</ref> and fine-tune the whole RNN+CNN for another 100k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Beam search of 2 and 3 was found to help the baseline and improved model respectively (recall that it did not help in situation prediction). We see that feeding in imSitu-based features improves the CIDEr score by 2.2 points. Feeding features from the network that produces noun entity predictions (Image+Nouns) works better than features from the action prediction network (Image+Actions). Similar improvements are also observed on the held-out MSCOCO test set as shown in <ref type="table">Table 5</ref>. Note that competing methods listed in that table use ensembles and improved architectures to obtain better captioning performance. While the quantitative improvements afforded by our additional semantic features are small (and automatic captioning metrics have well-known limitations <ref type="bibr" target="#b1">[2]</ref>), we have qualitatively observed that our captions can describe interactions with objects more accurately, as can be seen from images and captions in the top row of <ref type="figure">Figure 6</ref>. For example, we can correctly identify that a person is holding a baseball bat instead of a frisbee, or a hairbrush instead of a phone. When our model goes wrong <ref type="figure">(Figure 6</ref>, bottom row), it is prone to hallucinating interactions with people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper framed the recently introduced task of situation recognition as sequential prediction and conducted an extensive evaluation of RNN-based models on the imSitu dataset <ref type="bibr" target="#b32">[33]</ref>. Our most important findings are below.</p><p>? RNNs-based methods are a straightforward fit for the task and work quite well. ? Accurate action prediction is one of the main keys to beating the CRF methods of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, which do not train an explicit action classifier but predict actions jointly  <ref type="table">Table 5</ref>: Caption generation model performance on the COCO test2014 online leaderboard. We list results that have been published and highlight our implemented baseline and methods. Note that the top methods use ensembles, better model architectures, and other engineering tricks such as scheduled sampling, beyond the scope of this work. The c5 test setting uses 5 reference captions and c40 uses 40 reference captions. ? indicates an ensemble of models, ? indicates unspecified if ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG:</head><p>A man sitting on a couch with a cat VGG+imSitu: A man sitting on a chair with a cell phone GT: An old man is trying to use his cell phone VGG: A woman is holding a frisbee in a park VGG+imSitu: A young girl is holding a baseball bat on a field GT: A girl with a bat standing in a field VGG: A man with a beard and a tie VGG+imSitu: A man is holding a pair of scissors GT: A person holding a pair of scissors open intently VGG: A herd of elephants walking across a lush green field VGG+imSitu: A group of people standing around a large elephant GT: A herd of elephants walking across a grass covered field VGG: A truck is parked on the side of the road VGG+imSitu: A man standing next to a blue truck GT: A truck is parked on the side of a street VGG: A woman holding a cell phone in her hand VGG+imSitu: A woman is brushing her hair in a bathroom GT: A little girl is brushing her hair in a bathroom VGG: A man and a woman are playing a video game VGG+imSitu: Two men standing in front of a kitchen counter GT: A man and a woman are playing video games <ref type="figure">Figure 6</ref>: Sample images from COCO test set of Karpathy et al. <ref type="bibr" target="#b10">[11]</ref> for which adding imSitu features provided the largest gain (top row) and largest drop (bottom row) in CIDEr scores. We also show one of the five ground truth captions that is most similar to the produced captions. We notice that adding imSitu features helps identify and better describe interactions with objects. At the same time, in some of the failure cases, it hallucinates interactions with humans or misidentifies actions. with all the other roles. Further, we found that training a separate action classifier that does not share parameters with noun entity prediction works best. This suggests that the representations needed to predict actions and nouns may be different in non-trivial ways, as it was difficult to fine-tune them jointly.</p><p>? Weakly-supervised attention gives minor improvements but is hard to fine-tune, limiting its absolute accuracy. This is consistent with findings from captioning <ref type="bibr" target="#b26">[27]</ref>. Qualitatively, we found this form of attention to have limited ability to distinguish between entities, indicating the need for advanced attention mechanisms <ref type="bibr" target="#b11">[12]</ref>. ? We have preliminary evidence that situations can help improve captioning quality, though the improvement is currently small. In the future, we will explore better methods to integrate the external knowledge provided by the imSitu dataset into captioning.</p><p>A limitation of the RNN-based models over CRF-based models is that they cannot produce outputs for verbs unseen at train time as they are unaware of the semantic role ordering associated with the verb. We believe that this can be fixed by making the RNN also output semantic roles, which will be explored in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The four approaches used for action and noun entity prediction: a) The baseline no-vision model, which only tries to predict noun entities n 1 , ? ? ? , n 4 in the chosen arbitrary but fixed semantic role ordering, given the ground truth verb v. b) Training an RNN which takes image features as input and predicts action, followed by noun entities, c) Training a VGG-16 network for action prediction, and feeding its features to the RNN that predicts nouns associated with the semantic roles, and d) Using separate networks for action and noun entity prediction. Bold colored text (orange and green) indicates training targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Erasing -(Agent : man), (Erased : word), (Source : blackboard), (Place : ?)Talking -(Agent : woman), (Listener : woman), (Place : office) Predicted situations and attention maps associated with produced noun entities. In the top row, attention focuses on the correct regions. In the bottom example, attention cannot distinguish between the Agent and Listener women instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image Regression CRF [33] 32.25 24.56 14.28 58.64 42.68 22.75 65.90 29.50 36.32 Tensor Composition CRF [32] 31.73 24.04 13.73 58.06 42.64 22.70 68.73 32.14 36.72 Tensor Comp. + Image Reg. CRF [32] 32.91 25.39 14.87 59.92 44.50 24.04 69.39 Fusion for Actions, VGG+RNN for Nouns (ft) 36.11 27.74 16.60 63.11 47.09 26.48 70.48 35.56 40.40 henceforth ref. to as Fusion, VGG+RNN Situation prediction results on the full imSitu dev set (see text for detail).</figDesc><table><row><cell>.4 4.0</cell><cell>0.4</cell><cell>51.1 7.8</cell><cell>0.6</cell><cell>14.4</cell><cell>0.9</cell><cell>13.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.17</cell><cell>38.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Situation prediction results on the full imSitu test set.</figDesc><table><row><cell></cell><cell cols="2">top-1 predicted verb verb value value-all</cell><cell cols="2">top-5 predicted verbs verb value value-all</cell><cell cols="2">ground truth verbs value value-all</cell><cell>mean</cell></row><row><cell>Image Regression CRF [33]</cell><cell>20.61 11.79</cell><cell>3.07</cell><cell>44.75 24.85</cell><cell>5.98</cell><cell>50.37</cell><cell>9.31</cell><cell>21.34</cell></row><row><cell cols="2">Tensor Comp. + Image Reg. CRF [32] 19.96 11.57</cell><cell>2.30</cell><cell>44.89 25.26</cell><cell>4.87</cell><cell>53.39</cell><cell>10.15</cell><cell>21.55</cell></row><row><cell>Above + Extra 5M Images [32]</cell><cell>20.32 11.87</cell><cell>2.52</cell><cell>47.07 27.50</cell><cell>6.35</cell><cell>55.72</cell><cell>12.28</cell><cell>22.95</cell></row><row><cell>Fusion, VGG+RNN</cell><cell>22.07 12.96</cell><cell>3.37</cell><cell>47.83 27.89</cell><cell>6.85</cell><cell>56.38</cell><cell>13.79</cell><cell>23.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Correct (top row) and wrong (bottom row) predictions on the imSitu test set. One of the three groundtruth labels (GT) is shown to the top right of each image. The top 2 predictions (as numbered) are shown below the ground truth. Mistakes can be due to incorrect action verb prediction (bottom row first two images) or incorrect noun entity prediction (bottom right image).</figDesc><table><row><cell>, this base-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ours) 71.5 54.6 40.9 30.9 24.7 94.5 17.6 Image + Nouns (Ours) 71.5 54.6 41.1 31.1 24.8 95.2 17.7</figDesc><table><row><cell></cell><cell cols="2">B@1 B@2 B@3 B@4 M C</cell><cell>S</cell></row><row><cell>LRCN [6]</cell><cell>62.8 44.2 30.4 21.0 -</cell><cell>-</cell><cell>-</cell></row><row><cell>img-gLSTM [10]</cell><cell cols="3">64.7 45.9 31.1 21.4 20.4 67.7 -</cell></row><row><cell>NIC [26]  ?,?</cell><cell>66.6 46.1 32.9 24.6 -</cell><cell>-</cell><cell>-</cell></row><row><cell>img-gLSTM [10]</cell><cell cols="3">67.0 49.1 35.8 26.4 22.7 81.3 -</cell></row><row><cell>Hard-Attention [29]</cell><cell cols="2">71.8 50.4 35.7 25.0 23.0 -</cell><cell>-</cell></row><row><cell>Soft-Attention [29]</cell><cell cols="2">70.7 49.2 34.4 24.3 23.9 -</cell><cell>-</cell></row><row><cell>ATT-FCN [34] ?</cell><cell cols="2">70.9 53.7 40.2 30.4 24.3 -</cell><cell>-</cell></row><row><cell cols="4">NeuralTalk2 [1] (Ours) 70.8 53.7 40.1 30.1 24.5 93.0 17.3</cell></row><row><cell>Image + Actions (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Caption generation model performance on the COCO test set (5000 images) of Karpathy et al. [11]. B@N, M, C, and S indicate BLEU@N [18], METEOR [14], CIDEr [25], and SPICE [2] respectively. ? indicates a different split of 4000 images and ? indicates an ensemble of models. Bold values indicate the highest value for metrics obtained using a single model.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Mark Yatskar for his help with the imSitu dataset. This work was partially supported by the National Science Foundation under Grants CIF-1302438 and IIS-1563727, Xerox UAC, the Sloan Foundation, and a Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neuraltalk2</surname></persName>
		</author>
		<idno>2017-03-03. 6</idno>
		<ptr target="https://github.com/karpathy/neuraltalk2" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Background to framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of lexicography</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>1997. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guiding the long-short term memory model for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>GCPR. 2014. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01646</idno>
		<title level="m">Boosting image captioning with attributes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Commonly uncommon: Semantic sparsity in situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00901</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
