<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Chandra Chhipa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">EISLAB</orgName>
								<orgName type="institution" key="instit2">Lule? Tekniska Universitet</orgName>
								<address>
									<settlement>Lule?</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Upadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">EISLAB</orgName>
								<orgName type="institution" key="instit2">Lule? Tekniska Universitet</orgName>
								<address>
									<settlement>Lule?</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Grund Pihlgren</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">EISLAB</orgName>
								<orgName type="institution" key="instit2">Lule? Tekniska Universitet</orgName>
								<address>
									<settlement>Lule?</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajkumar</forename><surname>Saini</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">EISLAB</orgName>
								<orgName type="institution" key="instit2">Lule? Tekniska Universitet</orgName>
								<address>
									<settlement>Lule?</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
							<email>uchida@ait.kyushu-u.ac.jp*correspondingauthor-prakash.chandra.chhipa@ltu.se</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Human Interface Laboratory</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<settlement>Fukuoka</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">EISLAB</orgName>
								<orgName type="institution" key="instit2">Lule? Tekniska Universitet</orgName>
								<address>
									<settlement>Lule?</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>self-supervised learning</term>
					<term>contrastive learning</term>
					<term>repre- sentation learning</term>
					<term>breast cancer</term>
					<term>histopathological images</term>
					<term>transfer learning</term>
					<term>medical images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a novel self-supervised pre-training method to learn efficient representations without labels on histopathology medical images utilizing magnification factors. Other state-of-theart works mainly focus on fully supervised learning approaches that rely heavily on human annotations. However, the scarcity of labeled and unlabeled data is a long-standing challenge in histopathology. Currently, representation learning without labels remains unexplored in the histopathology domain. The proposed method, Magnification Prior Contrastive Similarity (MPCS), enables self-supervised learning of representations without labels on small-scale breast cancer dataset BreakHis by exploiting magnification factor, inductive transfer, and reducing human prior. The proposed method matches fully supervised learning state-of-the-art performance in malignancy classification when only 20% of labels are used in fine-tuning and outperform previous works in fully supervised learning settings for three public breast cancer datasets, including BreakHis. Further, It provides initial support for a hypothesis that reducing humanprior leads to efficient representation learning in self-supervision, which will need further investigation. The implementation of this work is made available online on GitHub 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Cancer diagnosis by analyzing histopathological wholeslide images (WSI) is an active research field in machine learning <ref type="bibr" target="#b32">[33]</ref>.</p><p>A challenge for the supervised learning approaches applied to histopathological WSI is the scarcity of labeled data. Furthermore, label information for digital WSI is also limited and does not provide details of the affected region at different magnifications, as in dataset BreakHis <ref type="bibr" target="#b51">[52]</ref>. Representations learned through supervised learning might suffer as such methods typically require a large amount of labeled data. This can lead to sub-optimal performance on downstream tasks.</p><p>Exploring efficient representation learning on small-scale histopathological WSI data using objectives that do not require 1 https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method <ref type="figure">Fig. 1</ref>: The proposed approach comprises three steps: (1) Parameters initialization with supervised ImageNet weights.</p><p>(2) Self-supervised pre-training on unlabeled BreakHis histopathology images using proposed method Magnification Prior Contrastive Similarity to provide positive pairs by exploiting supervision signal, e.g., magnification from data and reducing human prior. (3) Fine-tuning on histopathology images for the downstream task.</p><p>labels is a promising approach as it requires a lower amount of labeled data needed to learn successful downstream models. This work proposes a novel self-supervised learning (SSL) method based on contrastive joint embedding called Magnification Prior Contrastive Similarity (MPCS), to learn efficient representations without labels. The proposed method uses a magnification factor (a signal from data) to construct positive pairs for contrastive similarity. MPCS uses magnification factors to enable SSL on the small-scale dataset. This work also hypothesizes that reducing human inducted prior in SSL methods enhances representation learning.</p><p>The proposed method MPCS conducts self-supervised pretraining on histopathological WSI on the BreakHis <ref type="bibr" target="#b51">[52]</ref> dataset with two different backbone encoders Efficient-net <ref type="bibr" target="#b54">[55]</ref> and dilated ResNet-50 <ref type="bibr" target="#b60">[61]</ref>, pre-trained on ImageNet <ref type="bibr" target="#b14">[15]</ref>. The  <ref type="bibr" target="#b50">[51]</ref> AlexNet <ref type="bibr" target="#b34">[35]</ref> variant No No No 5 trials MI <ref type="bibr" target="#b5">[6]</ref> Custom CNN Simple Custom No 5-fold GLPB <ref type="bibr" target="#b2">[3]</ref> TCNN <ref type="bibr" target="#b0">[1]</ref> Custom No No 5-fold MIL <ref type="bibr" target="#b52">[53]</ref> Various MIL <ref type="bibr" target="#b30">[31]</ref> No No 5 trials A-MIL <ref type="bibr" target="#b41">[42]</ref> Custom CNN Simple No No Unclear MRN <ref type="bibr" target="#b23">[24]</ref> ResNet <ref type="bibr" target="#b26">[27]</ref> variant Simple No No Unclear MV <ref type="bibr" target="#b20">[21]</ref> Various No No Voting 5 trials SM <ref type="bibr" target="#b21">[22]</ref> DenseNet <ref type="bibr" target="#b28">[29]</ref> Simple Pretrained XGBoost <ref type="bibr" target="#b9">[10]</ref> 3 trials PI <ref type="bibr" target="#b22">[23]</ref> ResNet <ref type="bibr" target="#b26">[27]</ref> Simple Pretrained Custom 3 trials TL <ref type="bibr" target="#b15">[16]</ref> AlexNet <ref type="bibr" target="#b34">[35]</ref> &amp; VGG16 <ref type="bibr" target="#b47">[48]</ref> No Pretrained Two networks 5-fold RPDB <ref type="bibr" target="#b38">[39]</ref> DenseNet <ref type="bibr" target="#b28">[29]</ref> Simple AnoGAN <ref type="bibr" target="#b44">[45]</ref> No 5-fold CV MIM <ref type="bibr" target="#b6">[7]</ref> Various Simple Pretrained No Unclear MPCS Efficient-net b2 <ref type="bibr" target="#b54">[55]</ref> Simple Custom No 5-fold stratified CV effectiveness of the learned representations is assessed by finetuning for the downstream task of malignancy classification on multiple public datasets e.g. BACH <ref type="bibr" target="#b1">[2]</ref>, Breast Cancer Cell Dataset <ref type="bibr" target="#b17">[18]</ref> including BreakHis dataset. The complete approach is depicted in <ref type="figure">Figure 1</ref>. Following are the main contributions: 1) Enables self-supervised learning on small-scale histopathology dataset by exploiting a data prior 2) Hypothesizes a relation between human inducted prior and data prior for self-supervised representation learning With contributions mentioned above, this work demonstrates significantly improved performance on downstream tasks on three public datasets. It provides preliminary empirical support that 1) reducing human-prior leads to efficient representation learning and 2) learning being magnification invariant by cross-magnification evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Most efforts in machine learning for histopathological analysis have been using supervised learning. However, current supervised learning methods struggle when labeled data is scarce <ref type="bibr" target="#b32">[33]</ref>. Other methods, e.g. pseudo-label and transfer learning, are often used with supervised learning to make up for the lack of labeled data. Examples of such methods are augmentation <ref type="bibr" target="#b36">[37]</ref> for the first category and feature-extraction and selection <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b57">[58]</ref> for the second.</p><p>One public histopathological image dataset that poses such a challenge of data-and label scarcity is BreakHis <ref type="bibr" target="#b51">[52]</ref>. Many different methods have been applied to the BreakHis <ref type="bibr" target="#b51">[52]</ref> dataset, most of which utilize a Convolutional Neural Network (CNN) in conjunction with one or both of the methods for handling data scarcity above. <ref type="table" target="#tab_0">Table I</ref> contains a summary of a few such approaches along with some of the strategies used on BreakHis <ref type="bibr" target="#b51">[52]</ref>. In the table, custom means a specialized or novel method introduced. Simple augmentations refer to one or more operations among rotation, flipping, cropping, shifting, and zooming. Another breast cancer dataset is BACH <ref type="bibr" target="#b1">[2]</ref>, on which previous works in transfer learning TL <ref type="bibr" target="#b56">[57]</ref>, patch-based PT <ref type="bibr" target="#b43">[44]</ref>, and hybrid networks HN <ref type="bibr" target="#b59">[60]</ref> shows performance improvements. Further, the small-scale dataset Breast Cancer Cell Dataset <ref type="bibr" target="#b17">[18]</ref> is also being evaluated for binary tasks in various previous works based on shearlet transform ST <ref type="bibr" target="#b42">[43]</ref> and attention methods are ATN <ref type="bibr" target="#b29">[30]</ref> and MATN <ref type="bibr" target="#b33">[34]</ref>.</p><p>Another learning paradigm to effectively counter earlier stated data scarcity challenges is self-supervised learning. Representation learning from self-supervised learning paradigms for computer vision can be categorized majorly as (i) Joint Embedding Architecture &amp; Method (JEAM) ( <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b61">[62]</ref>), (ii) Prediction Methods ( <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b16">[17]</ref>), and loosely (iii) Reconstruction Methods ( <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b18">[19]</ref>). Specifically, JEAM can be divided further with each subdivision providing many interesting works; (i) Contrastive Methods (PIRL <ref type="bibr" target="#b39">[40]</ref>, Sim-CLR <ref type="bibr" target="#b10">[11]</ref>, SimCLRv2 <ref type="bibr" target="#b11">[12]</ref>, MoCo <ref type="bibr" target="#b25">[26]</ref>), (ii) Distillation (BYOL <ref type="bibr" target="#b19">[20]</ref>, SimSiam <ref type="bibr" target="#b12">[13]</ref>), (iii) Quantization (SwAV <ref type="bibr" target="#b8">[9]</ref>, DeepCluster <ref type="bibr" target="#b7">[8]</ref>), and (iv) Information Maximization (Barlow Twins <ref type="bibr" target="#b61">[62]</ref>, VICReg <ref type="bibr" target="#b4">[5]</ref>). Of these divisions, this work focuses on contrastive methods.</p><p>Recently, contrastive JEAM has been tailored for medical images. Contrastive learning on digital pathology DPCL <ref type="bibr" target="#b13">[14]</ref> applies contrastive learning and shows improvement in the breast cancer dataset <ref type="bibr" target="#b1">[2]</ref>. In MICLe <ref type="bibr" target="#b3">[4]</ref>, which is based on SimCLR <ref type="bibr" target="#b10">[11]</ref>, multiple instance contrastive learning is applied by enabling input views from several image instances of the same patient. Another work from contrastive methods on histopathology is DRL <ref type="bibr" target="#b58">[59]</ref>. Other applications making use of contrastive JEAM are chest X-rays <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b37">[38]</ref>, CT scans for COVID-19 <ref type="bibr" target="#b27">[28]</ref>, 3d-Radiomic <ref type="bibr" target="#b35">[36]</ref>, and Radiograph <ref type="bibr" target="#b62">[63]</ref>. A work using contrastive JEAM on the BreakHis dataset is SMSE <ref type="bibr" target="#b53">[54]</ref>, which trains the network using pair and triplet losses. SSL methods including JEAM require large-size data. Thus applying the contrastive JEAM paradigm on small datasets with the reduced human dependency of prior is an open challenge and interest of this work. ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The primary focus of this work is to introduce a novel self-supervised pre-training method with the aim is to learn representations from data without labels, while using supervision signals from data, e.g., magnification factor and using inductive transfer from ImageNet <ref type="bibr" target="#b14">[15]</ref> pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inductive Transfer Learning</head><p>Given the fact that BreakHis <ref type="bibr" target="#b51">[52]</ref> is a small-scale and classimbalanced dataset, this work hypothesizes a constraint case of inductive transfer for representation learning by initializing encoder ImageNet <ref type="bibr" target="#b14">[15]</ref> pre-trained weights. In this work, the inductive transfer (i) helps to obtain improved performance on the downstream task of malignancy classification and (ii) enables self-supervised pre-training using the proposed method on the small-scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised Method -Magnification Prior Contrastive Similarity</head><p>Magnification Prior Contrastive Similarity (MPCS) method formulates self-supervised pre-training to learn representations on microscopic Histopathology WSI without labels on smallscale data. The main objective of MPCS is to lower the amount of labeled data needed for the downstream task to address challenges in supervised learning.</p><p>MPCS construct pairs of distinct views considering characteristics of microscopic histopathology WSI (H-WSI) for contrastive similarity-based pre-training. Microscopic H-WSI  <ref type="bibr" target="#b14">[15]</ref> (vehicles, cats, or dogs) in terms of location, size, shape, background-foreground, and concrete definition of objects. Unlike SimCLR <ref type="bibr" target="#b10">[11]</ref> where pairs of distinct views from the input image is constructed by humancentered augmentations, MPCS constructs pair of distinct views using pair sampling methods based on the signal from data itself i.e. magnification factor in BreakHis <ref type="bibr" target="#b51">[52]</ref>. Two H-WSI from different magnification factors of the same sample makes a pair. Utilizing prior from data (magnification factor) enables meaningful contrastive learning on histopathology H-WSI and reduces dependency over human inducted prior. Further, tumor-affected regions in H-WSI are characterized by format and highly abnormal amounts of nuclei. Such affected regions are promising in all the H-WSIs of different magnification for the same sample. Thus, affected regions being common and size invariant in positive pair of a sample allow learning contrastive similarity by region attentions. Current work also hypothesizes that reduced human-prior in the pre-training method provides an enhanced degree of freedom to the method that can increase the potential of the network to learn efficient representations in a self-supervised approach. To investigate, three strategies for pair sampling are formulated based on inducted human prior. The number of human decisions defines the level of inducted human-prior (HP) during pair sampling. As explained in <ref type="figure" target="#fig_1">Figure 3</ref>, in Fixed Pair, decisions to choose magnification factor for both views are by human, thus making strong human-prior. In Ordered Pair, only the second view of the pair is chosen by a human using a look-up table, making weaker human prior. In Random Pair, no human prior inducted and magnification factor for both views are sampled randomly. Further, <ref type="figure" target="#fig_2">Figure 4</ref> demonstrates the Degree of Freedom (DoF) for the method where the Fixed Pair strategy provides no DoF, Order Pair provides one DoF, and Random Pair provides 2 DoF to the method. In MPCS, to formulate a batch of 2N views, randomly sampled batch of N sets of input</p><formula xml:id="formula_0">X = {X (1) , X (2) , ..., X (N ) } are considered where each set of input X (i) = {x (i) 40 , x (i) 100 , x (i) 200 , x (i)</formula><p>400 } contains the images corresponding to four magnification factors. Positive pair of views is constructed based on a selection strategy of pair sampling which contains two views from the same example of different magnification. Further, similarity maximizes (loss minimizes) by an objective defined by the contrastive loss in Eq. 1). MPCS is demonstrated in <ref type="figure" target="#fig_0">Figure 2</ref> and components are explained below. </p><formula xml:id="formula_1">h MF1 = f (x MF1 ) = encoder-network(x MF1 ) and h MF2 = f (x MF2 ) = encoder-network(x MF2 ) where h MF1 , h MF2 ? R d</formula><p>are the output after the respective average pooling layers, shown in step 3 of <ref type="figure" target="#fig_0">Figure 2</ref>. ? A small-scale MLP projection head g(?) that maps representations to the latent space where contrastive loss is applied, shown in step 4 of <ref type="figure" target="#fig_0">Figure 2</ref>. A multi-layer perceptron with single hidden layer to obtain</p><formula xml:id="formula_2">z MF1 = g(h MF1 ) = W (2) ?(W (1) h MF1 ) and z MF2 = g(h MF2 ) = W (2) ?(W (1) h MF2 )</formula><p>where ? is ReLU. ? A contrastive loss function, normalized temperaturescaled cross entropy loss (NT-Xent) from SimCLR is defined for a contrastive prediction, shown in step 5 of <ref type="figure" target="#fig_0">Figure 2</ref>. For given a setx k including a positive pair examplesx MF1 andx MF2 , the contrastive prediction task</p><formula xml:id="formula_3">tends to findx MF2 in {x k } k =M F 1 for the givenx MF1 .</formula><p>The loss function for a positive pair of examples (MF1, MF2) is defined as</p><formula xml:id="formula_4">L MF1,MF2 = ? log exp(sim(z MF1 , z MF2 )/? ) 2N k=1 1 [k =M F 1] exp(sim(z MF1 , z k )/? ) (1) In Eq. (1), where 1 [k =M F 1] ? 0, 1 is an indicator evaluating to 1 if k = i.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATIONS</head><p>This section investigates the representation learning capabilities of the proposed method MPCS on two encoder networks with detailed experimentation on three public datasets.</p><p>A. Datasets 1) BreakHis: BreakHis <ref type="bibr" target="#b51">[52]</ref> dataset consists of 2,480 benign and 5,429 malignant histopathological microscopic images from 82 patients at four magnification levels (40?, 100?, 200?, 400?). Each image in the BreakHis dataset is of size 700?460, stained with hematoxylin and eosin (HE). Following the previous works, two evaluation metrics are used, imagelevel accuracy(ILA) and patient-level accuracy (PLA). PLA shows patient-wise classification performance, calculated as the mean over total no. of patients using patient-score. Patientscore is correctly classified images of the patient over a total number of images of that patient. ILA disregards patientlevel details and thus serves as standard image classification accuracy.</p><p>2) BACH: The second dataset, Breast Cancer Histology Images (BACH) <ref type="bibr" target="#b1">[2]</ref> is from the ICIAR2018 Grand Challenge and contains 400 histopathology slides. The BACH dataset has four classes, normal, benign, in-situ, and invasive. The slide size is relatively large, 2048 ? 1536 pixels; thus, patches of size 512x512. Two evaluation metrics, patch-wise accuracy and image-wise accuracy, are used, whereas image-wise accuracy is calculated based on majority voting over the patches of the respective image.</p><p>3) Breast Cancer Cell Dataset: The third dataset, Breast Cancer Cell Dataset <ref type="bibr" target="#b17">[18]</ref> is from the University of California, Santa Barbara Biosegmentation Benchmark. This dataset contains 58 HE-stained histopathology 896x768 size images of breast tissue, of which 26 are malignant, and 32 are benign. Patches of size 224x224 were created, and image-wise accuracy was calculated using majority voting over patches of the respective image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder Architectures</head><p>In this current work, the proposed method MPCS investigated two different CNN encoder architectures. ResNet-50 <ref type="bibr" target="#b60">[61]</ref> and Efficient-net b2 <ref type="bibr" target="#b54">[55]</ref> are used for pre-training and fine-tuning. SSL-specific MLP projection head used for Efficient-net b2 is three layers network of 2048-1204-128 units, whereas ResNet-50 is the most common backbone encoder, used projection head adapted from SimCLR, having 1024-128 units. encoder and projection heads are demonstrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Protocol</head><p>This section shares parameter configurations used in pretraining and fine-tuning.</p><p>1) SSL pre-training: Self-supervised pre-trainings of both encoders take place on the BreakHis dataset for 1000 epochs with temperature parameter 0.01, learning rate 1e-05, and a set of augmentation methods such as color-jitter, flip, and rotation. Efficient-net b2 encoder pre-trained using Adam optimizer with a batch size of 128 and image input of (341, 341). However, ResNet-50 adapted standard configurations of selfsupervised practices and pre-trained using the LARS optimizer with a batch size of 1024 and input image size of 224x224.</p><p>2) Fine-tuning: The common training configurations across datasets for both encoders are learning rate of 2e-05, batch size of 32, image input of 224x224, augmentations methods such as random crop, flip, affine, rotation, and color-jitter, and using adam as optimizer. A dropout of 0.3 is used in the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimentation Details</head><p>To ensure the reliability and consistency of the models, this work follows 5-cross validation data split scheme. This is applied to all three datasets in which each fold contained 20% data, following class distribution from whole data. Four out of five folds are used for training &amp; validation, and the remaining one for testing. Thus all the results reported are in terms of mean value with standard deviation. In the abovestated 5-cross validation settings, both backbone encoders, ResNet-50 and Efficient-net b2, are being pre-trained on the first dataset BreakHis with all three variants (ordered pair, random pair, and fixed pair) of the proposed SSL method MPCS for learning domain-specific representations. Further, downstream-task-specific fine-tuning experiments are carried out to investigate the impact of learned representations for all three datasets e.g., BreakHis, BACH, and Breast Cancer Cell. Following are the details of experimentation for each dataset.   on proposed MPCS methods against ImageNet <ref type="bibr" target="#b14">[15]</ref> pretrained model to analyze the effect of learned representations. Preferred names in discussion used as MPCS-X for ImageNet ? MPCS-X. Experiments(Exp-9 to Exp-12) for the second dataset BACH <ref type="bibr" target="#b1">[2]</ref> are described in <ref type="table" target="#tab_0">Table III</ref> based on the ResNet-50 encoder. All the images are divided into 512X512 size patches; thus, performance is measured patch-wise and image-wise (using majority voting suggested in <ref type="bibr" target="#b56">[57]</ref>). The major objectives are 1) evaluating pre-trained models from the proposed method against the ImageNet-based transfer learning approach <ref type="bibr" target="#b1">[2]</ref> 2) evaluating the ability to learn downstream tasks with limited labels ranging from 5% to 100% of labels from the train data portion. Finally, to evaluate the effect of learned domain-specific representations on small-scale data, a series of fine-tuning (all layers trained) and linear evaluation experiments (only fully-connected layers trainable) Exp-13 to Exp-16 are conducted Breast Cancer Cell dataset and described in <ref type="table" target="#tab_0">Table IV</ref>. Similar to BACH dataset, images for this dataset were also divided into sizes 224X224 for training and test, and performance was measured likewise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS &amp; DISCUSSIONS</head><p>The quantitative results and qualitative analysis from extensive experiments on three datasets validate the efficiency of learned representations for the proposed self-supervised pretraining method MPCS. Preliminary investigation on data prior also supports the hypothesis of reducing human prior in selfsupervised representation learning.</p><p>Results on BreakHis dataset for the experiments Exp-1 to Exp-4 in <ref type="table" target="#tab_0">Table II</ref> in limited labels setting (20% labels only) are described in <ref type="table" target="#tab_5">Table V</ref>. It shows that all the variants (ordered pair, random pair, and fixed pair) of MPCS pre-trained models obtain significant (across magnification, p &lt;0.05) improvement (1.55 ? 2.52)% over the ImageNet transfer learning model for all four magnifications and results are competitive with other state-of-the-art methods that have been trained in the fully supervised setting using all labels. Further, <ref type="table" target="#tab_0">Table VI</ref> compares the performance of experiments Exp-5 to Exp-8 in <ref type="table" target="#tab_0">Table II</ref> in the fully supervised setting (using all labels). The MPCS pre-trained models outperform several state-of-the-art methods with (3.5 ? 8.0)% higher accuracy in malignancy classification. Both encoder architectures, Efficient-net b2 and ResNet-50 perform consistently. The t-SNE visualization of the pre-trained ResNet-50 encoder in <ref type="figure" target="#fig_6">Figure 9</ref> and class activation maps (CAM) <ref type="bibr" target="#b45">[46]</ref> in <ref type="figure" target="#fig_3">Figure 5</ref> show robust representation learning in the pre-tuning and fine-tuning phases, respectively. Additionally, cross-magnification evaluation is also performed as suggested in previous work <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b46">[47]</ref>, results support performance generalization across magnifications, and results can be found in the supplementary material. MPCS pre- trained ResNet-50 models on the BreakHis dataset are further evaluated on two additional datasets. Results on the BACH dataset for the experiments Exp-9 to Exp-12 in <ref type="table" target="#tab_0">Table III</ref> are described in <ref type="table" target="#tab_0">Table VII</ref> and VIII. Results in <ref type="table" target="#tab_0">Table VII</ref> show the performance improvement on four-class classification on image-wise and patch-wise accuracy while comparing with other state-of-the-arts for the ResNet-50 encoder and other architectures. Specifically, compared with other recent work based on contrastive learning-based self-supervised method DPCL <ref type="bibr" target="#b13">[14]</ref>, the proposed method MPCS consistently outperforms over a varying range of label usage as shown in <ref type="figure">Figure 8</ref> and obtains an improvement of 4.85% when using 100% labels. To compare with ImageNet pre-trained ResNet-50 encoder in identical settings, the method suggested in <ref type="bibr" target="#b56">[57]</ref> is re-implemented and results are reported in <ref type="table" target="#tab_0">Table VII</ref> for comparison and qualitative analysis through CAM is also shown for samples from the BACH dataset in <ref type="figure" target="#fig_4">Figure 6</ref>. Results on the third Breast Cell Cancer dataset for experiments Exp-13 to Exp-15 from <ref type="table" target="#tab_0">Table IV are compared with other  methods in Table IX</ref>, shows significant improvement over other methods and obtains 98.18% in fine-tuning and 96.36% in linear evaluation. CAM in <ref type="figure" target="#fig_5">Figure 7</ref> also supports the method qualitatively. Reproducible source code for all the results reported is added in supplementary content and shall be made available on GitHub.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-supervised method MPCS demonstrates label efficiency</head><p>All three variants of the proposed method MPCS demonstrate label efficiency on downstream tasks. Results in <ref type="table" target="#tab_5">Table V</ref> On BreakHis dataset, MPCS fine-tuned models obtain (significant improvement p &lt;0.01) proportionally bigger margins of improvement of (2.52?0.02)% over ImageNet pre-trained     <ref type="table" target="#tab_0">Table VI</ref> which have been trained on 100% labels. Following the trend, MPCS pretrained models consistently outperform on the BACH dataset compared with the recent contrastive learning-based method DPCL <ref type="bibr" target="#b13">[14]</ref> in a complete range of labels from 5% to 100%, shown in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data prior enables self-supervision on small-scale datasets</head><p>The proposed MPCS method enables self-supervised representation learning on the small-scale dataset by extensively using a data prior (supervision signal from data) e.g. magnification factors (40X, 100X, 200X, and 400x). It decreases the dependence on human-curated priors e.g. augmentation method choices during self-supervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MPCS learns robust self-supervised representations</head><p>Explicitly focusing on the robustness of learned representations, <ref type="figure" target="#fig_6">Figure 9</ref> strongly supports the fact that MPCS pretrained models can capture and learn discriminative features across the classes during the self-supervised pre-training phase itself without knowing about actual human-provided labels. It is worth mentioning that data points of different classes are <ref type="figure">Fig. 8</ref>: Comparison with DPCL <ref type="bibr" target="#b13">[14]</ref> for label efficiency on classification task on BACH dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Preliminary support for the hypothesis about reducing human inducted priors</head><p>The MPCS-Ordered Pair inducts weaker human-prior in pair sampling. Thus, the MPCS method obtains one DoF for randomly choosing the first input view. In comparison, the MPCS-Fixed Pair, which inducts stronger human prior by choosing both views, 200x and 400x by human-prior, gives zero DoF to the MPCS method. The MPCS-Random Pair, in which the MPCS method obtains the highest degree of freedom since the human-prior is absent. <ref type="figure">Figure 10</ref> explains the trends when fewer labels are used, that both encoders that weaker human-prior based pair sampling tends to outperform over extreme cases of stronger human prior or the absence of it. However, it requires detailed explorations of different datasets and tasks. Similar patterns were also observed on the BACH dataset that the ordered pair outperforms over other two variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>The novel MPCS method enables self-supervised pretraining for comparatively small-scale breast cancer micro- <ref type="figure">Fig. 10</ref>: Comparison for human priors -Indicates weaker human-prior(moderate DoF for method) outperforms stronger and no human-prior during limited (20% labels) data scopic image dataset BreakHis <ref type="bibr" target="#b51">[52]</ref> for efficient representation learning by exploiting supervision signals from data. Results on three public datasets have confirmed the excellence of the MPCS method. MPCS learns better representations by exploiting magnification priors (OP is best among all) and distinguishing among different cells in downstream tasks. It could be better to investigate on a continuous scale of magnification scales considered in this paper. In the future, we intend to investigate magnification prior to other selfsupervised learning approaches focusing on redundancy reduction and Siamese networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SUPPLEMENTARY MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-supervised methods learn magnification invariant representations</head><p>The MPCS methods not only outperform for magnificationspecific tasks stated in Tables V and VI but representations learned through the proposed method also demonstrate a consistent edge in classification performance in crossmagnification evaluation over the ImageNet model. These experiments were conducted on the Efficient-net b2 encoder only to prevent additional computation usage. However, the ResNet-50 encoder can be benchmarked, if needed. <ref type="table" target="#tab_10">Table X</ref>    evaluation). Interestingly, type-2 cross magnification evaluation also shows similar trends except in 400x, in which the ImageNet model obtained high PLA performance. Empirical analysis on type-1 and type-2 cross magnification suggests that MPCS self-supervised pre-trained models perform better than the ImageNet model by learning magnification invariant representations.</p><p>B. Additional ablation results on using 80%, 60%, and 40% of labels of the training set on BreakHis dataset</p><p>This section describes the extended ablation study on using labels in an incremental manner. The main section of the results describes and compares all three variants of the MPCS method with ImageNet pre-trained models when fine-tuned on 20% and 100% labels of the training set are used. To continue the trend for completeness of analysis, this section adds the results for the same setting considering 40%, 60%, and 80% label utilization in fine-tuning, results described in Tables XII, XIII, and XIV, respectively. The most important observation is that MPCS methods consistently outperform the ImageNet model over the range of labels provided and specifically, the ordered pair method remains best performing in largely. <ref type="figure" target="#fig_7">Figure 11</ref> and 12 shows the comparisons for Efficient-net b2 encoder for ILA and PLA accuracy. Similarly, <ref type="figure" target="#fig_1">Figure 13</ref> and 14 shows the comparisons for ResNet-50 encoder for ILA and PLA accuracy. A common trend is evident that MPCS methods based models consistently performs better than ImageNet based model for entire range of labels.</p><p>It clearly shows that self-supervised learned representations improve fine-tuning task performance overall range of available labels, similar to the trend observed on the BACH dataset. Besides being able to obtain relatively higher accuracy on limited label settings, more label additions are largely beneficial to self-supervised pre-trained models than ImageNet pre-trained models.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Magnification Prior Contrastive Similarity method explained structural properties are different from natural visual macroscopic images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Strategies for pair sampling based on inducted Human Prior(HP). Added measure to prevent mode collapse in pretraining by 1 st view = 2 nd view in all strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Relation between inducted Human Prior (HP) of magnification and Degree of Freedom (DoF) for method ? A domain specific human prior module P h (X ? X MF ): X = {x 40 , x 100 , x 200 , x 400 } that exploits supervision signal from data i.e. magnification and samples the two views X MF =(x MF1 , x MF2 ) of different magnifications to construct pair based on employed strategy of pair sampling shown in step 1 of Figure 2. ? A uniform stochastic transformation based module T U (X MF ?X MF ) that uniformly transforms both views from X MF =(x MF1 , x MF2 ) toX MF = (x MF1 ,x MF2 ) of positive pair by sampled augmentation transformations scheme shown in step 2 of Figure 2. ? A neural network base encoder f (?) which yields representations from transformed views of pair. It obtains</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Comparing CAM from ImageNet and MPCS-OP on BreakHis dataset. Only OP variant CAM is shown for space constraint. Activation -red (darker) color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>CAM from ImageNet and MPCS-RP, OP, FP on BACH dataset (MPCS specifically learns to prevent activation on cells present on normal class images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Comparing CAM among MPCS-RP, OP, and FP on Breast Cancer Cell dataset. Activation -red (darker) color of-the-art methods mentioned in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>tsne-visualization of the features from MPCS model (on BreakHis) without fine-tuning. Blue(benign); red(malignant). easily separable by either linear or non-linear boundaries for all four magnifications for all variants of the MPCS method. The CAM of fine-tuned models depicted in Figures 5 and 6 also indicate that MPCS pre-trained models activate regions of interest (dark red colors show strong activation) more efficiently than the ImageNet pre-trained model for BreakHis and BACH datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Performance comparison (ILA accuracy, Efficient-net b2 model) for MPCS pre-trained models with ImageNet pretrained model over range of labels used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Performance comparison (PLA accuracy, Efficientnet b2 model) for MPCS pre-trained models with ImageNet pre-trained model over range of labels used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Performance comparison (ILA accuracy, ResNet-50 model) for MPCS pre-trained models with ImageNet pretrained model over range of labels used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Performance comparison (PLA accuracy, ResNet-50 model) for MPCS pre-trained models with ImageNet pretrained model over range of labels used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Methodologies Used on the BreakHis Dataset.</figDesc><table><row><cell>Work</cell><cell>Model</cell><cell>Augmented Extra Training</cell><cell>Ensemble</cell><cell>Evaluation</cell></row><row><cell>Deep</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc>II describes fine-tuning experiments for the first dataset BreakHis. All the mentioned experiments for malignancy classification are conducted for both encoders, Efficient-net b2 and ResNet-50 for all four magnifications</figDesc><table /><note>(40X, 100X, 200X, and 400X). Experiments (Exp-1 to Exp-4) in Limited Labeled Data Setting evaluate model performance while using only 20% labels, whereas experiments (Exp-5 to Exp-8) in Fully Supervised Setting use all labels. The sole objective is to compare the performance of models pre-trained</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Experiment details for BreakHis dataset</figDesc><table><row><cell>No.</cell><cell>Pre-training Method</cell><cell>BreakHis data (%)for SSL</cell><cell cols="2">Finetuning on BreakHis Dataset Train (%) Test (%)</cell></row><row><cell></cell><cell cols="2">Limited Labeled Data Setting</cell><cell></cell><cell></cell></row><row><cell>Exp-1</cell><cell>ImageNet</cell><cell>-</cell><cell>20%</cell><cell>20%</cell></row><row><cell>Exp-2</cell><cell>ImageNet ? MPCS-Fixed Pair</cell><cell>60%</cell><cell>20%</cell><cell>20%</cell></row><row><cell>Exp-3</cell><cell>ImageNet ? MPCS-Ordered Pair</cell><cell>60%</cell><cell>20%</cell><cell>20%</cell></row><row><cell>Exp-4</cell><cell>ImageNet ? MPCS-Random Pair</cell><cell>60%</cell><cell>20%</cell><cell>20%</cell></row><row><cell></cell><cell cols="2">Fully Supervised Data Setting</cell><cell></cell><cell></cell></row><row><cell>Exp-5</cell><cell>ImageNet</cell><cell>-</cell><cell>80%</cell><cell>20%</cell></row><row><cell>Exp-6</cell><cell>ImageNet ? MPCS-Fixed Pair</cell><cell>60%</cell><cell>80%</cell><cell>20%</cell></row><row><cell>Exp-7</cell><cell>ImageNet ? MPCS-Ordered Pair</cell><cell>60%</cell><cell>80%</cell><cell>20%</cell></row><row><cell>Exp-8</cell><cell>ImageNet ? MPCS-Random Pair</cell><cell>60%</cell><cell>80%</cell><cell>20%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Experiment details for BACH dataset</figDesc><table><row><cell>No.</cell><cell>Pre-training method/weights</cell><cell>Fine-tuning on BACH dataset Labels(%) from Train Data</cell><cell>Test data (%)</cell></row><row><cell>Exp-9</cell><cell>ImageNet [57]-re-implement</cell><cell>100% ( 80% train data)</cell><cell>20%</cell></row><row><cell>Exp-10</cell><cell>MPCS-Fixed Pair (BreakHis)</cell><cell>[5%, 10%, 20%, 40%, 60%, 80%, 100%]</cell><cell>20%</cell></row><row><cell>Exp-11</cell><cell>MPCS-Ordered Pair(BreakHis)</cell><cell>[5%, 10%, 20%, 40%, 60%, 80%, 100%]</cell><cell>20%</cell></row><row><cell>Exp-12</cell><cell>MPCS-Random Pair(BreakHis)</cell><cell>[5%, 10%, 20%, 40%, 60%, 80%, 100%]</cell><cell>20%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Experiment details for Breast Cancer Cell Dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Fine-tuning on</cell><cell cols="2">Linear-evaluation on</cell></row><row><cell>No.</cell><cell>Pre-training method/weights</cell><cell cols="2">Breast Cancer Cell Dataset Train data(%) Test data (%)</cell><cell cols="2">Breast Cancer Cell Dataset Train data(%) Test data(%)</cell></row><row><cell>Exp-13</cell><cell>MPCS-Fixed Pair (BreakHis)</cell><cell>80%</cell><cell>20%</cell><cell>80%</cell><cell>20%</cell></row><row><cell>Exp-14</cell><cell>MPCS-Ordered Pair(BreakHis)</cell><cell>80%</cell><cell>20%</cell><cell>80%</cell><cell>20%</cell></row><row><cell>Exp-15</cell><cell>MPCS-Random Pair(BreakHis)</cell><cell>80%</cell><cell>20%</cell><cell>80%</cell><cell>20%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance evaluation of the proposed methods in limited labelled data setting when fine-tuning only on 20% labeled data. (across magnification, p &lt;0.05 for all three self-supervised methods for PLA). Ablation results for 40%, 60%, and 80% labels are provided in supplementary content.</figDesc><table><row><cell>Encoder</cell><cell>Metric</cell><cell>Method</cell><cell>40X</cell><cell>100X</cell><cell>200X</cell><cell>400X</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell>ImageNet</cell><cell>86.36?6.13</cell><cell>87.80?4.15</cell><cell>86.82?5.74</cell><cell>84.98?6.05</cell><cell>86.49?5.63</cell></row><row><cell></cell><cell>Image Level</cell><cell>FixedPair</cell><cell>87.26?4.46</cell><cell>87.45?2.35</cell><cell>89.38?2.18</cell><cell>88.12?3.84</cell><cell>88.05 ?3.20</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ordered Pair</cell><cell>87.40?3.73</cell><cell>89.30?2.74</cell><cell>90.50?2.19</cell><cell>88.35?3.30</cell><cell>88.89?2.99</cell></row><row><cell>Efficient-net b2</cell><cell></cell><cell>Random Pair ImageNet</cell><cell>86.21?4.20 86.13?5.15</cell><cell>89.55?2.84 87.76?6.03</cell><cell>89.18?4.05 85.79?4.10</cell><cell>87.34?3.44 85.51?5.27</cell><cell>88.07?3.63 86.30?5.14</cell></row><row><cell></cell><cell>Patient Level</cell><cell>FixedPair</cell><cell>86.90?4.14</cell><cell>87.64?3.05</cell><cell>89.60?3.28</cell><cell>88.26?3.05</cell><cell>88.10 ?3.38</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ordered Pair</cell><cell>87.19?3.20</cell><cell>88.86?2.58</cell><cell>90.20?3.26</cell><cell>88.96?3.22</cell><cell>88.80?3.07</cell></row><row><cell></cell><cell></cell><cell>Random Pair</cell><cell>87.17?3.88</cell><cell>88.36?2.84</cell><cell>88.58?4.01</cell><cell>88.66?3.13</cell><cell>88.19?3.48</cell></row><row><cell></cell><cell></cell><cell>ImageNet</cell><cell>87.40?4.88</cell><cell>86.22?5.71</cell><cell>86.02?4.74</cell><cell>85.30?5.95</cell><cell>86.24?5.32</cell></row><row><cell></cell><cell>Image Level</cell><cell>FixedPair</cell><cell>86.69?3.96</cell><cell>86.94?3.05</cell><cell>88.76?2.28</cell><cell>88.81?2.77</cell><cell>87.68 ?3.01</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ordered Pair</cell><cell>87.56?3.48</cell><cell>88.60?3.01</cell><cell>89.77?2.19</cell><cell>87.61?3.48</cell><cell>88.38?3.04</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>Random Pair ImageNet</cell><cell>87.06?3.40 87.10?4.80</cell><cell>87.96?3.44 88.06?5.11</cell><cell>88.55?3.15 84.19?4.28</cell><cell>86.64?2.98 85.01?5.27</cell><cell>87.55?3.24 86.09?4.86</cell></row><row><cell></cell><cell>Patient Level</cell><cell>FixedPair</cell><cell>87.45?3.96</cell><cell>86.38?3.12</cell><cell>88.18?3.00</cell><cell>88.89?2.98</cell><cell>87.72 ?3.27</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ordered Pair</cell><cell>87.88?2.89</cell><cell>88.21?3.21</cell><cell>89.52?3.26</cell><cell>87.90?3.03</cell><cell>88.38?3.09</cell></row><row><cell></cell><cell></cell><cell>Random Pair</cell><cell>87.17?2.98</cell><cell>87.96?3.02</cell><cell>88.76?3.55</cell><cell>88.06?3.00</cell><cell>87.99?3.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of proposed methods with state-of-the-art methods on downstream task of classifying histopathological images at different magnification factors in fully supervised setting(using 100% train set labels). RN-50 indicates ResNet-50 and Eff-net b2 indicates Efficient-net b2 encoder. Also, OP indicates Ordered Pair, FP indicates Fixed Pair, and RP indcates Random Pair.</figDesc><table><row><cell>Method</cell><cell>40X</cell><cell cols="2">Patient Level Accuracy (RR) 100X 200X</cell><cell>400X</cell><cell>Mean</cell><cell>40X</cell><cell cols="2">Image Level Accuracy 100X 200X</cell><cell>400X</cell><cell>Mean</cell></row><row><cell>Original-GLCM[51]</cell><cell>74.7?1.0</cell><cell>78.6?2.6</cell><cell>83.4?3.3</cell><cell>81.7?3.3</cell><cell>79.60?2.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PFTAS[25]</cell><cell>83.80?2.0</cell><cell>82.10?4.9</cell><cell>85.10?3.1</cell><cell>82.30?3.8</cell><cell>83.33?3.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MIL-NP[53]</cell><cell>92.1?5.9</cell><cell>89.1?5.2</cell><cell>87.2?4.3</cell><cell>82.7?3.0</cell><cell>87.77?4.6</cell><cell>87.8?5.6</cell><cell>85.6?4.3</cell><cell>80.8?2.8</cell><cell>82.9?4.1</cell><cell>84.28?4.20</cell></row><row><cell>SW[51]</cell><cell>88.6?5.6</cell><cell>84.5?2.4</cell><cell>85.3?3.8</cell><cell>81.7?4.9</cell><cell>85.02?4.17</cell><cell>89.6?6.58</cell><cell>85.0?4.8</cell><cell>84.0?3.2</cell><cell>80.8?3.1</cell><cell>84.85?4.42</cell></row><row><cell>MI[6]</cell><cell>83.08?2.08</cell><cell>83.17?3.51</cell><cell>84.63?2.72</cell><cell>82.10?4.42</cell><cell>83.25?3.18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep[50]</cell><cell>84.0?6.9</cell><cell>83.9?5.9</cell><cell>86.3?3.5</cell><cell>82.1?2.4</cell><cell>84.07?4.67</cell><cell>84.6?2.9</cell><cell>84.8?4.2</cell><cell>84.2?1.7</cell><cell>81.6?3.7</cell><cell>83.80?3.13</cell></row><row><cell>MILCNN[53]</cell><cell>86.9?5.4</cell><cell>85.7?4.8</cell><cell>85.9?3.9</cell><cell>83.4?5.3</cell><cell>85.47?4.85</cell><cell>86.1 ? 4.28</cell><cell>83.8?3.0</cell><cell>80.2?2.6</cell><cell>80.6?4.6</cell><cell>82.68?3.62</cell></row><row><cell>GLPB[3]</cell><cell>84.5?4.2</cell><cell>83.5?2.0</cell><cell>89.6?5.0</cell><cell>88.2?4.0</cell><cell>86.45?3.8</cell><cell>82.1?6.4</cell><cell>81.4?4.8</cell><cell>88.4?5.0</cell><cell>87.2?4.5</cell><cell>84.78?5.18</cell></row><row><cell>RPDB[39]  *</cell><cell>92.02?0.9</cell><cell>90.21?2.40</cell><cell>81.94?1.70</cell><cell>80.09?0.70</cell><cell>88.06?1.4</cell><cell>94.26?3.2</cell><cell>92.71?0.4</cell><cell>83.90?2.8</cell><cell>82.74?1.5</cell><cell>88.40?1.98</cell></row><row><cell>SMSE[54]</cell><cell>87.51?4.07</cell><cell>89.12?2.86</cell><cell>90.83?3.31</cell><cell>87.10?3.80</cell><cell>88.64?3.51</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ImageNet (Eff-net b2)</cell><cell>91.91?4.25</cell><cell>91.93?4.20</cell><cell>91.46?5.17</cell><cell>88.10?3.88</cell><cell>90.85?4.36</cell><cell>92.12?4.18</cell><cell>92.66?4.20</cell><cell>91.83?4.55</cell><cell>88.35?5.21</cell><cell>91.24?4.54</cell></row><row><cell>MPCS-FP (Eff-net b2)</cell><cell>92.23?3.50</cell><cell>92.72?3.68</cell><cell>91.94?3.80</cell><cell>88.40?3.26</cell><cell>91.33?3.56</cell><cell>92.23?3.80</cell><cell>93.57?3.23</cell><cell>92.23?2.98</cell><cell>88.40?3.90</cell><cell>91.61?3.48</cell></row><row><cell>MPCS-OP (Eff-net b2)</cell><cell>92.45?3.25</cell><cell>93.47?2.98</cell><cell>92.44?3.30</cell><cell>89.00?3.05</cell><cell>91.84?3.15</cell><cell>92.67?3.36</cell><cell>93.63?3.38</cell><cell>92.72?2.80</cell><cell>88.74?3.90</cell><cell>91.94?3.36</cell></row><row><cell>MPCS-RP (Eff-net b2)</cell><cell>93.26?3.48</cell><cell>93.57? 3.36</cell><cell>92.23?3.21</cell><cell>89.57?3.79</cell><cell>92.15?3.46</cell><cell>93.45?3.55</cell><cell>93.38?2.80</cell><cell>92.28?3.49</cell><cell>89.81?3.15</cell><cell>92.23?3.24</cell></row><row><cell>ImageNet (RN-50)</cell><cell>91.46?4.30</cell><cell>91.24?5.1</cell><cell>90.72?4.68</cell><cell>87.90?4.12</cell><cell>90.33?4.55</cell><cell>91.83?5.12</cell><cell>92.23?4.15</cell><cell>91.61?4.00</cell><cell>87.88?4.80</cell><cell>90.89?4.52</cell></row><row><cell>MPCS-FP (RN-50)</cell><cell>91.83?3.88</cell><cell>92.67?2.72</cell><cell>91.61?3.40</cell><cell>89.00?3.15</cell><cell>91.28?3.29</cell><cell>92.24?3.48</cell><cell>92.66?3.88</cell><cell>91.91?3.68</cell><cell>88.40?3.66</cell><cell>91.30?3.68</cell></row><row><cell>MPCS-OP (RN-50)</cell><cell>93.00?3.66</cell><cell>93.26?3.08</cell><cell>92.28?2.88</cell><cell>88.74?3.60</cell><cell>91.82?3.31</cell><cell>93.26?3.40</cell><cell>93.45?2.89</cell><cell>92.45?3.77</cell><cell>89.57?2.96</cell><cell>92.18?3.26</cell></row><row><cell>MPCS-RP (RN-50)</cell><cell>92.72?3.50</cell><cell>93.57? 2.88</cell><cell>92.23?3.90</cell><cell>88.40?3.05</cell><cell>91.73?3.33</cell><cell>92.72?3.38</cell><cell>92.72?4.02</cell><cell>91.91?3.21</cell><cell>88.56?3.89</cell><cell>91.48?3.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="5">: Performance comparison of proposed method</cell></row><row><cell cols="5">MPCS (ResNet-50 encoder) on BACH dataset with other</cell></row><row><cell cols="5">state-of-the-arts for four class classification. RN-50 indicates</cell></row><row><cell>ResNet-50.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Image-wise accuracy validation test</cell><cell cols="2">Patch-wise accuracy validation test</cell></row><row><cell>PT [44]</cell><cell>-</cell><cell>90.00</cell><cell>-</cell><cell>77.40</cell></row><row><cell>HN [60] (RN-50)</cell><cell>-</cell><cell>81.60</cell><cell>-</cell><cell>-</cell></row><row><cell>HN [60]</cell><cell>-</cell><cell>91.30</cell><cell>-</cell><cell>82.10</cell></row><row><cell>DPCL [14]</cell><cell>-</cell><cell>87.00</cell><cell>-</cell><cell>-</cell></row><row><cell>ImageNet [57] re-implement</cell><cell>92.40?2.04</cell><cell>90.50?2.10</cell><cell>80.56?3.06</cell><cell>80.00?2.64</cell></row><row><cell>MPCS-FP</cell><cell>92.50?1.90</cell><cell>90.55?2.05</cell><cell>84.25?1.88</cell><cell>82.79?2.05</cell></row><row><cell>MPCS-OP</cell><cell cols="2">93.31?1.85 91.85?1.77</cell><cell cols="2">83.90?1.89 83.13?2.00</cell></row><row><cell>MPCS-RP</cell><cell>93.00?1.88</cell><cell>91.00?2.32</cell><cell>83.78?2.09</cell><cell>82.90?2.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Performance evaluation (image-wise accuracy and f1 score) of proposed method MPCS on BACH dataset for limited label range for ResNet-50 encoder.</figDesc><table><row><cell>Label(%) from train data</cell><cell cols="3">F1 score (test data)</cell><cell cols="3">Accuracy (test data)</cell></row><row><cell></cell><cell>MPCS-FP</cell><cell>MPCS-OP</cell><cell>MPCS-RP</cell><cell>MPCS-FP</cell><cell>MPCS-OP</cell><cell>MPCS-RP</cell></row><row><cell>5%</cell><cell>0.50?0.05</cell><cell>0.50?0.05</cell><cell>0.51?0.05</cell><cell cols="3">51.25?5.02 50.00?4.80 53.00?5.06</cell></row><row><cell>10%</cell><cell>0.60?0.05</cell><cell>0.61?0.04</cell><cell>0.61?0.05</cell><cell cols="3">60.50?4.88 61.75?3.93 62.50?4.60</cell></row><row><cell>20%</cell><cell>0.65?0.03</cell><cell>0.70?0.04</cell><cell>0.68?0.02</cell><cell cols="3">69.25?2.92 71.00?3.90 69.00?2.89</cell></row><row><cell>40%</cell><cell>0.79?0.04</cell><cell>0.81?0.04</cell><cell>0.80?0.03</cell><cell cols="3">80.75?3.40 81.75?3.52 81.50?3.05</cell></row><row><cell>60%</cell><cell>0.87?0.03</cell><cell>0.87?0.03</cell><cell>0.86?0.03</cell><cell cols="3">87.70?3.48 87.75?3.10 86.50?3.00</cell></row><row><cell>80%</cell><cell>0.89?0.03</cell><cell>0.90?0.02</cell><cell>0.89?0.02</cell><cell cols="3">89.25?3.02 90.75?2.17 89.75?0.02</cell></row><row><cell>100%</cell><cell>0.90?0.02</cell><cell>0.91?0.02</cell><cell>0.90?0.02</cell><cell cols="3">90.55?2.05 91.85?1.77 91.00?2.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX</head><label>IX</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: Performance measure of proposed method MPCS</cell></row><row><cell cols="6">(ResNet-50 encoder) on Breast Cancer Cell Dataset</cell><cell></cell></row><row><cell>Method</cell><cell>accuracy</cell><cell>Fine-tuned precision</cell><cell>recall</cell><cell>accuracy</cell><cell>Linear-evaluation precision</cell><cell>recall</cell></row><row><cell>ST [43]</cell><cell>86.00?3.00</cell><cell>-</cell><cell>1.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MATN [34]</cell><cell>91.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ATN [30]</cell><cell>75.50?1.60</cell><cell>0.73?0.01</cell><cell>0.73?0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MPCS-FP</cell><cell>98.14?2.05</cell><cell>0.99?0.01</cell><cell>0.98?0.01</cell><cell>96.29?1.90</cell><cell>0.97?0.01</cell><cell>0.96?0.01</cell></row><row><cell>MPCS-OP</cell><cell>98.18?1.80</cell><cell>0.99?0.01</cell><cell>0.98?0.01</cell><cell>96.36?1.88</cell><cell>0.97?0.01</cell><cell>0.96?0.01</cell></row><row><cell>MPCS-RP</cell><cell>98.10?2.00</cell><cell cols="2">0.985?0.01 0.98?0.01</cell><cell>96.22?2.02</cell><cell>0.965?0.01</cell><cell>0.96?0.01</cell></row><row><cell cols="7">model while only 20 % labels are used for all magnification</cell></row><row><cell cols="7">scales. These results match the performance of other state-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X :</head><label>X</label><figDesc></figDesc><table><row><cell cols="5">Type 1 cross magnification performance compar-</cell></row><row><cell cols="5">ison of proposed methods (Leave one magnification out on</cell></row><row><cell cols="5">which model trained). The values represent mean performance</cell></row><row><cell cols="5">of remaining magnifications (e.g. train on 40x and evaluated</cell></row><row><cell cols="3">(mean) on 100x, 200x, and 400x.)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Trained</cell><cell>Trained</cell><cell>Trained</cell><cell>Trained</cell></row><row><cell>Method</cell><cell cols="4">on 40X Mean Cross-Magnification Image Level Accuracy on 100X on 200X on 400X</cell></row><row><cell>ImageNet</cell><cell>79.56?11.74</cell><cell>79.56?11.74</cell><cell>82.97?6.77</cell><cell>84.16?4.98</cell></row><row><cell>MPCS-Ordered Pair</cell><cell>80.99?8.91</cell><cell>80.99?8.91</cell><cell>84.40?3.81</cell><cell>84.20?5.58</cell></row><row><cell>MPCS-Random Pair</cell><cell>80.13?9.60</cell><cell>80.13?9.60</cell><cell>84.84?5.30</cell><cell>84.83?5.30</cell></row><row><cell></cell><cell cols="4">Mean Cross-Magnification Patient Level Accuracy</cell></row><row><cell>ImageNet</cell><cell>81.01?9.59</cell><cell>84.38?5.78</cell><cell>81.45?6.89</cell><cell>83.31?7.31</cell></row><row><cell>MPCS-Ordered Pair</cell><cell>81.49?6.96</cell><cell>84.02?5.78</cell><cell>82.19?3.83</cell><cell>83.10?7.44</cell></row><row><cell>MPCS-Random Pair</cell><cell>81.20?7.09</cell><cell>85.05?7.15</cell><cell>82.97?5.84</cell><cell>83.76?6.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI</head><label>XI</label><figDesc>accuracy where the model is evaluated on other magnifications except the magnification on which the model was trained. The MPCS-Order Pair methods outperform the ImageNet model and other methods with a mean crossmagnification ILA of 80.99% and PLA of 81.49% when the model was trained on 40x magnification and evaluated on 100x, 200x, and 400x. Whereas MPCS-Random Pair outperforms with mean cross-magnification ILA 84.84% and PLA 82.97% when trained on 200x and ILA 84.83% and PLA 83.76% when trained on 400x. For 100x, MPCS-Ordered Pair obtains ILA 80.99%, and MPCS-Random Pair obtains PLA 85.05%. Further, Table XI evaluates the mean performance of models trained on other magnifications except on which evaluation is performed (type-2 mean cross magnification</figDesc><table><row><cell cols="5">: Type 2 cross magnification performance com-</cell></row><row><cell cols="5">parison of proposed methods (select one magnification in on</cell></row><row><cell cols="5">which model was not trained). The values represent mean</cell></row><row><cell cols="5">performance of a magnification whereas trained on other</cell></row><row><cell cols="5">magnifications (e.g. evaluated on 40x and trained on 100x,</cell></row><row><cell>200x, and 400x.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Evaluated</cell><cell>Evaluated</cell><cell>Evaluated</cell><cell>Evaluated</cell></row><row><cell>Method</cell><cell cols="4">on 40X Mean Cross-Magnification Image Level Accuracy on 100X on 200X on 400X</cell></row><row><cell>ImageNet</cell><cell>84.33 ? 4.32</cell><cell>83.65 ? 6.07</cell><cell>84.31 ? 8.93</cell><cell>78.86 ? 10.62</cell></row><row><cell>MPCS-Ordered Pair</cell><cell>85.35 ? 4.29</cell><cell>84.56 ? 6.06</cell><cell>85.28 ? 8.20</cell><cell>78.98 ? 7.82</cell></row><row><cell>MPCS-Random Pair</cell><cell>86.55 ? 5.10</cell><cell>84.82 ? 5.54</cell><cell>84.99 ? 7.79</cell><cell>79.17 ? 9.60</cell></row><row><cell></cell><cell cols="4">Mean Cross-Magnification Patient Level Accuracy</cell></row><row><cell>ImageNet</cell><cell>81.71 ? 6.25</cell><cell>83.76 ? 6.82</cell><cell>84.35 ? 8.40</cell><cell>80.33 ? 8.10</cell></row><row><cell>MPCS-Ordered Pair</cell><cell>81.97 ? 6.25</cell><cell>84.54 ? 6.52</cell><cell>84.63 ? 7.80</cell><cell>79.51 ? 5.20</cell></row><row><cell>MPCS-Random Pair</cell><cell>83.24 ? 6.89</cell><cell>84.98 ? 6.12</cell><cell>84.84 ? 7.04</cell><cell>79.92 ? 6.53</cell></row><row><cell cols="5">shows (type-1 mean cross magnification evaluation) mean</cell></row><row><cell cols="2">cross-magnification</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">100x, 200x, and 400x</ref><p>), 5-cross folds, and on a wide range of labels (5% to 100% train set labels). One hundred forty downstream task training experiments were performed on the BACH dataset using BreakHis MPCS pretrained ResNet-50 models. Finally, 30 downstream tasks experiments were performed for the Breast Cell Cancer Dataset using ResNet-50 pre-trained models covering fine-tuning and linear evaluation. In this way, 1003 experiments are performed in the current work. Details are mentioned in <ref type="table">Table XV.</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using filter banks in convolutional neural networks for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul F Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="63" to="69" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><surname>Aresta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Ara?jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scotty</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Saketh Chennamsetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Safwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varghese</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Marami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Donovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Grand challenge on breast cancer histology images. Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="122" to="139" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data augmentation for histopathological images based on gaussian-laplacian pyramid blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Tsham Mpinda Ataky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>De Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alceu</forename><surname>Britto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05224</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vicreg: Varianceinvariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for magnification independent breast cancer histopathology image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Bayramoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International conference on pattern recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2440" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Breakhis based breast cancer automatic diagnosis using deep learning: Taxonomy, survey and insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassir</forename><surname>Benhammou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boujemaa</forename><surname>Achchab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="9" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xgboost: Reliable large-scale tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd SIGKDD Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semisupervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self supervised contrastive learning for digital histopathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">Louise</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning with Applications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">100198</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Transfer learning based histopathologic image classification for breast cancer detection. Health information science and systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkan</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehra</forename><surname>Abdulkadir ? Eng?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Kadiroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation and benchmark for biological image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyun</forename><surname>Elisa Drelie Gelasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boguslaw</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Obara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1816" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breast cancer histopathological image classification: is magnification important?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibha</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnav</forename><surname>Bhavsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequential modeling of deep features for breast cancer histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibha</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnav</forename><surname>Bhavsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2254" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Partially-independent framework for breast cancer histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibha</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnav</forename><surname>Bhavsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Breast cancer detection from histopathology images using modified residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Vasudev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1272" to="1287" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Amit Doegar, and Nitigya Sambyal</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast automated cell phenotype image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholas A Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radosav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Pantelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohan D Teasdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sample-efficient deep learning for covid-19 diagnosis based on ct scans. medrxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrated segmentation and recognition of hand-printed numerals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Keeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Leow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R. P. Lippmann, J. Moody, and D. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Machine learning methods for histopathological image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumpei</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Structural Biotechnology Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-attention multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><forename type="middle">V</forename><surname>Konstantinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Utkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Imbalance-aware self-supervised learning for 3d radiomic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengda</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ezhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Menze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04167</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scannet: A fast and dense scanning framework for metastastic breast cancer detection from whole-slide image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangjing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Align, attend and locate: Chest x-ray diagnosis via contrast induced attention network with limited supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10632" to="10641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classification of breast cancer histopathological images using discriminative patches screened by generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="155362" to="155377" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image classification and localization using multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipesh</forename><surname>Tamboli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swati</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International WIE Conference on Electrical and Computer Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
		<respStmt>
			<orgName>WIECON-ECE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microscopic medical image classification framework via deep learning and shearlet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Rezaeilouyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44501</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Debotosh Bhattacharjee, and Mita Nasipuri. Patch-based system for classification of breast histology images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushiki</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debapriya</forename><surname>Banik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Magnification generalization for histopathology image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Sikaroudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyamin</forename><surname>Ghojogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fakhri</forename><surname>Karray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1864" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Moco pretraining improves representation and transferability of chest x-ray models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Sowrirajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="728" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep features for breast cancer histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><forename type="middle">R</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Heutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1868" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Breast cancer histopathological image classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Alexandre</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2560" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dataset for breast cancer histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Heutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1455" to="1462" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiple instance learning for histopathological breast cancer image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pj Sudharshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">Eduardo</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Heutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Magnification-independent histopathological image classification with similarity-based multi-scale embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianni</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rotation equivariant cnns for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Classification of breast cancer histology images using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sulaiman</forename><surname>Vesal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirabbas</forename><surname>Davari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference image analysis and recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="812" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Breast cancer image classification via multi-network features and dual-network orthogonal low-rank learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiying</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elazab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Leng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="27779" to="27792" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data-efficient histopathology image analysis with deformation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Breast cancer histopathological image classification using a hybrid deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhou</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Comparing to learn: Surpassing imagenet pretraining on radiographs by comparing image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
