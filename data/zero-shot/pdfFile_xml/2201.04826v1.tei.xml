<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-level Relation Extraction with Context Guided Mention Integration and Inter-pair Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhao</surname></persName>
							<email>zhaochao@stu.csust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Hunan Normal University</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Changsha University of Science &amp; Technology</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
							<email>zengdj@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hunan Normal University</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hunan Normal University</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Dai</surname></persName>
							<email>daijianhua@hunnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Hunan Normal University</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-level Relation Extraction with Context Guided Mention Integration and Inter-pair Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level Relation Extraction (DRE)</head><p>aims to recognize the relations between two entities. The entity may correspond to multiple mentions that span beyond sentence boundary. Few previous studies have investigated the mention integration, which may be problematic because coreferential mentions do not equally contribute to a specific relation. Moreover, prior efforts mainly focus on reasoning at entity-level rather than capturing the global interactions between entity pairs. In this paper, we propose two novel techniques, Context Guided Mention Integration and Inter-pair Reasoning (CGM2IR), to improve the DRE. Instead of simply applying average pooling, the contexts are utilized to guide the integration of coreferential mentions in a weighted sum manner. Additionally, inter-pair reasoning executes an iterative algorithm on the entity pair graph, so as to model the interdependency of relations. We evaluate our CGM2IR model on three widely used benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-ofthe-art models.</p><p>[1] Britain 's Prince Harry is engaged to his US partner Meghan Markle ? [2] Harry spent 10 years in the army and has this year, with his elderly brother William , ? [3] The last major royal wedding took place in 2011, when Kate Middleton and Prince William were married ... Relations: royalty_of(Harry,Britain), sibling_of(William, Harry), spouse_of(kate,William), royalty_of(kate,Britain) ...</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a fundamental problem in natural language processing, which aims to identify the semantic relation between a pair of entities mentioned in the text. Recent progress in neural relation extraction has achieved great success <ref type="bibr" target="#b23">(Zeng et al., 2015;</ref><ref type="bibr" target="#b0">Baldini Soares et al., 2019)</ref>, but these approaches usually focus on binary relations (relations that only involve two entities) within a single sentence. While in practice, a large number of relations in entity pairs span sentence boundaries 1 . Many recent works <ref type="bibr" target="#b21">(Yao et al., 2019;</ref><ref type="bibr" target="#b32">Zhou et al., 2021)</ref> pay emphasis on document-level scene that requires a larger context to identify relations, making it a more practical but also more challenging task.</p><p>Document-level Relation Extraction (DRE) poses unique challenges compared to its sentencelevel counterpart. First, it is more complex to model a document with rich entity structure for relation extraction. The entities engaged in a relation may appear in different sentences, and some entities are repeated with the same phrases or aliases, the occurrences of which are often named entity mentions. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, Britain and Kate appear in the first and third sentences, respectively. Harry and William also appear more than once in this example. We are therefore confronted to deal with cross-sentence dependencies and synthesizing the information of multiple mentions, in contrast to two entities in one sentence. Second, there are intrinsic interactions among relational facts. The identification of relations between two entities requires reasoning beyond the contextual features. Specifically, in <ref type="figure" target="#fig_0">Figure 1</ref>, we can determine that the royalty_of relation exists between William and Britain from the context word Prince. Kate is also a member of the royal family, as she is married to William. Logical reasoning plays a dominant role when extract the fact Kate; royalty_of ; Britain .</p><p>Many previous works have tried to fulfill DRE and tackle the above challenges. In order to exploit the document structure and capture cross-sentence dependencies, most current approaches construct a delicately designed document graph with syntactic structures (coreference, dependency, etc.) <ref type="bibr" target="#b12">(Sahu et al., 2019)</ref>, heuristics rules 2 , or structured attention <ref type="bibr" target="#b10">(Nan et al., 2020)</ref>. The constructed graphs bridge entities that spread far apart in the document. Besides, as Transformers for NLP can be considered as a graph neural network with multihead attention as the neighbourhood aggregation function. It implicitly models long-distance dependencies. There are also some works <ref type="bibr" target="#b18">(Xu et al., 2021a;</ref><ref type="bibr" target="#b32">Zhou et al., 2021</ref>) that attempt to use Pretrained Models (PTMs) directly for DRE without involving graph structure. Afterwards, researchers simply apply average (max) pooling to the representation of coreferential entity mentions. Unfortunately, this is obviously not in accordance with intuition and fact. All mentions are equally treated, ignoring the corresponding mention pair contexts for a specific relation.</p><p>In this paper, instead of simply synthesizing multiple coreferential mentions, we propose a novel context guided attention mechanism for mention integration. Similarly to <ref type="bibr" target="#b32">Zhou et al. (2021)</ref>, after encoding through PTMs, we directly get the contexts for each entity pair from the attention heads. Then, the contexts are guided as query to obtain the weights of mentions through cross-attention. This process makes the representation of an entity change dynamically according to the entity pair in which it is located.</p><p>In light of the necessity of reasoning, message passing algorithms on graph are employed to update the entity representations accordingly. Thus, it conducts reasoning in an implicit way . Otherwise, a special reasoning network is designed for relation inference <ref type="bibr" target="#b26">(Zeng et al., 2020;</ref><ref type="bibr" target="#b7">Li et al., 2021</ref>). Despite their great success, these methods mainly focus on entity-level or contextual information propagation rather than entity pair interactions, ignoring the global interdependency among multiple relational facts.</p><p>In this paper, we propose a novel inter-pair reasoning approach to achieve this purpose. The head and tail entity representations obtained by context guided integration are merged with their contextual information to get the representations of multiple entity pairs. Then, the entity pair representations are formed as the nodes of Graph Neural Networks (GNNs). The inter-pair interactions are captured through an iterative algorithm over entity pairs, so as to complete reasoning.</p><p>By combining the proposed two techniques, we propose a simple yet effective document level relation extraction model, dubbed CGM2IR (Context Guided Mention Integration and Interpair Reasoning), to fully utilize the power of PTMs. To demonstrate the effectiveness of the proposed approach, we conduct comprehensive experiments on three widely used document level relation extraction datasets. The experimental results reveal that our CGM2IR model significantly outperforms the state-of-the-art methods. Our contributions can be summarized as follows:</p><p>? We propose a context guided attention mechanism to dynamically merge mentions that refer to the same entity in a weighted sum manner. Our approach innovatively uses contextual information to guide the entity representation.</p><p>? We propose an inter-pair reasoning approach to model interactions among entity pairs rather than entities. Reasoning based on entity pairs is more rational and consistent with the human way of intelligence and learning.</p><p>? We conduct experiments on three public DRE datasets. Experimental results demonstrate the effectiveness of our CGM2IR model that achieves the new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation extraction, also known as relational facts extraction, plays an essential role in a variety of applications in Natural Language Processing (NLP), especially for the automatic construction of Knowledge Graph (KG). Early researchers mainly concentrate on the sentence-level task, i.e. predicting the relations between two entities within a sentence. Many approaches <ref type="bibr" target="#b24">(Zeng et al., 2014;</ref><ref type="bibr" target="#b2">Cai et al., 2016)</ref> have been proposed to effectively fulfill Sentence-level Relation Extraction (SRE), especially the pre-training-then-fine-tuning paradigm of PTMs <ref type="bibr" target="#b30">(Zheng et al., 2021)</ref>. SRE faces an inevitable restriction in practice, where many relation facts can only be extracted from multiple sentences. Recently, researchers gradually push SRE forward to DRE <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>.</p><p>In DRE, an entity may correspond to multiple mentions, which are scattered in different sentences. We need to classify the relations of multiple entity pairs all at once, which usually requires complex reasoning skills and inter-sentential information. DRE can be cast as a problem with multiple entity pairs to classify and multiple labels to assign <ref type="bibr" target="#b32">(Zhou et al., 2021</ref>). To fulfill this task, most current approaches  adopt appropriate models to first learn the contextual representation of an input document and encode the tokens in it. Then the representation of entity pairs is obtained by different strategies. Finally, a sigmoid classifier is used for multi-label classification.</p><p>There are two types of mainstream for document representation. On the one hand, researchers construct a delicately designed document graph. <ref type="bibr" target="#b11">Quirk and Poon (2017)</ref> attempted a first step toward constructing document-level graph that augments conventional intra-sentential dependencies with new dependencies introduced by adjacent sentences and discourse relations. Afterwards, graph based algorithm was employed to pass messages and conduct reasoning in an implicit way . Otherwise, a special reasoning network was designed for relation inference <ref type="bibr" target="#b26">(Zeng et al., 2020;</ref><ref type="bibr" target="#b20">Xu et al., 2021c;</ref><ref type="bibr" target="#b7">Li et al., 2021;</ref><ref type="bibr" target="#b19">Xu et al., 2021b;</ref><ref type="bibr" target="#b25">Zeng et al., 2021)</ref>. On the other hand, as Transformers for NLP can be considered as a graph neural network with multihead attention as the neighbourhood aggregation function, it implicitly models long-distance dependencies. There are also some works <ref type="bibr" target="#b16">(Wang et al., 2019;</ref>) that attempt to use PTMs directly for DRE without involving graph structure. <ref type="bibr" target="#b18">Xu et al. (2021a)</ref> incorporated entity structure dependencies within Transformers encoding part and throughout the overall system. <ref type="bibr" target="#b32">Zhou et al. (2021)</ref> proposed an adaptive-thresholding loss and a localized context pooling to improve the performance. Transformer-based approaches implicitly integrate reasoning into the encoding process. These methods are simple but very effective, and have yielded the state-of-the-art performance.</p><p>Among the various amounts of prior works, <ref type="bibr" target="#b32">Zhou et al. (2021)</ref> and  are the most two relevant to our approach. <ref type="bibr" target="#b32">Zhou et al. (2021)</ref> also considered context to enhance the entity representation.  captured global interdependency among relation facts at entity pair level. However, the differences are substantial. First and foremost, these two approaches both equally treated all the mentions. In contrast, we use a context guided intra-pair attention mechanism to weigh the mentions. Moreover, we adopt a GNN that forms entity pairs as nodes to learn the inter-pair interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe the proposed model CGM2IR that incorporates context guided mention integration and inter-pair reasoning to improve DRE. As illustrated in figure 2, CGM2IR mainly consists of four parts, namely (i) the document encoding; (ii) the context guided mention integration; (iii) the inter-pair reasoning; (iv) and the final classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Encoding Module</head><p>To model the semantics of input document better, CGM2IR adopts BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> as the document encoder, which has recently been proven surprisingly effective by presenting state-of-the-art results in various NLP tasks.</p><p>Given a document D as input, it is comprised of l tokens x = {t i } l 1 and a set of annotated entities e i = {m j } t 1 where entity e i may have multiple mentions that scatter across the document. Borrowing the idea of entity marker (Baldini Soares et al., 2019), we first insert a special marker "*" at the start and end of mentions to mark the mention's span by the entity's annotation. Then, the document encoder is responsible to map each token and mention markers to a sequence of contextualized embedding representations H = {h 1 , h 2 , ? ? ? , h n }.</p><formula xml:id="formula_0">H = P T M s({x 1 , x 2 , ? ? ? , x n })<label>(1)</label></formula><p>where n is the length of tokens with all markers.</p><p>For each mention, we take the embedding of start marker as the mention embeddings. Limited by the input length of BERT, we use a dynamic window <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref> to sequentially encode the whole documents when n &gt; 512.</p><p>[ Inter-pair reasoning <ref type="figure">Figure 2</ref>: The overall architecture of CGM2IR. First, the input document is viewed as a long sequence of words, which are subsequently encoded through BERT. Then, the context guided mention integration module dynamically generates the head and tail entity embeddings for each entity pair. Next, we construct a homogeneous entity pair graph and use GNNs to model the inter-pair interaction. Finally, the classifier predicts relations of all the entity pairs in a parallel way.</p><formula xml:id="formula_1">( , ) H W ( , ) H B ( , ) B K ( , ) W K ( , ) B W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Guided Mention Integration Module</head><p>As argued in Section 1, an entity may be mentioned under the same phrase or alias in multiple sentences throughout the document. To obtain entity-level representation, previous works usually synthesize the embeddings of all mentions of an entity. These methods equally treat each mention and only generate one global embedding for an entity. Then, the entity embedding is used in the relation classification of all entity pairs. Unfortunately, it is obvious that some mentions may not be relevant to the relation when categorizing a particular entity pair. Therefore, we propose a context guided attention mechanism that can generate fine-grained entity representations for each pair. Different from the previous approaches, our motivation is to first get the entity-aware context through the average of mention attention matrices. Then, the contexts involved in both head and tail entities are located to steer the model for mention integration. Following <ref type="bibr" target="#b32">Zhou et al. (2021)</ref>, we explicitly use the token-level attention score A in the last encoder block of BERT to compute the pair-specific context embedding c h,t for entity pair (e h , e t ) as follows:</p><formula xml:id="formula_2">c (h,t) = Ha (h,t) a (h,t) = A h ? At 1 (A h ? At)<label>(2)</label></formula><p>where A h = avg m i ?e h (A m i ), A m i is the attention matrix for i-th mention of head entity e h to all tokens in the document. A similar operation yields A t for the tail entity. Since the transformer-based PTMs have learned token-level dependencies well by training in a large-scale corpus, we attend all the tokens that are important to both entities in pair (e h , e t ) by multiplying their entity-level attentions score with a normalization.</p><p>After obtaining the contextual features of entity pairs in the first step, we use them as queries and perform cross-attention to pool the entity representations related to the entity pair from the mention embeddings of head or tail entity. Specifically, given an entity pair (e h , e t ) and a sequence of mention embeddings h m 1 , h m 2 , ? ? ? , h mp of the head or tail entity, where h m i ? R d , p is the number of mentions. Guided by context feature c (h,t) ? R d for this pair, the head entity e h (h,t) is computed as follows:</p><formula xml:id="formula_3">e h (h,t) = p i=1 ? i (h,t) hm i a i (h,t) = WQc (h,t) WK hm i ? d ? i (h,t) = exp (a i (h,t) ) p j=1 exp (a j (h,t) )<label>(3)</label></formula><p>where W Q ? R d?d , W K ? R d?d denotes the query and key transformation matrixes, d is the dimension of hidden states. In a similar way, we can obtain the representation of the tail entity e t (h,t) . We can observe that the representation of each entity is not fixed. It is guided by the trade-off between the context and the entity pair in which it is located. The head entity and the tail entity are combined to dynamically determine the respective representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-pair Reasoning Module</head><p>To model interactions among entity pairs in a document, we construct a homogeneous entity pair graph and use GNNs to perform reasoning.</p><p>For each document D with m entities, we formulate a graph G = (V, E), where m ? (m ? 1) entity pairs form the nodes of the graph. Each node representation is computed by the following steps: Given the embeddings (e h (h,t) , e t (h,t) ) of an entity pair (e h , e t ) and its context features c (h,t) , we first combine the entity embeddings with their context embedding, and then map them to hidden representations z h (h,t) and z t (h,t) respectively with a feedforward neural network. Finally, the entity pair embedding, p (h,t) , is calculated through a group bilinear 3 function as follows:</p><formula xml:id="formula_4">z h (h,t) = tanh(W h e h (h,t) + Wc 1 c (h,t) ) z t (h,t) = tanh(Wte t (h,t) + Wc 2 c (h,t) ) p (h,t) = ?( k i=1 z h i (h,t) W i p z t i (h,t) )<label>(4)</label></formula><p>where W h ? R d?d , W t ? R d?d , W c 1 ? R d?d , W c 2 ? R d?d and W i p ? R d/k?d/k are learnable parameters. Furthermore, we concatenate the entity pair embedding with coreference embedding for head and tail entities to get the initial node representations following <ref type="bibr" target="#b21">Yao et al. (2019)</ref>:</p><formula xml:id="formula_5">P 0 (h,t) = [p h ; p (h,t) ; p t ]<label>(5)</label></formula><p>In contrast to the fully-connected case, we link each node to the nodes that have overlapping entities with it, since the clues for logical reasoning are usually passed on the chain of entities as it is approved in <ref type="bibr" target="#b19">Xu et al. (2021b)</ref> and <ref type="bibr" target="#b25">Zeng et al. (2021)</ref>. After the graph is constructed, We use GNNs to learn the inter-pair interactions. In each layer l, The GNNs selectively aggregate all entity pair embeddings passed from neighbors through an attention mechanism to update its representation in the next 3 Group bilinear <ref type="bibr" target="#b29">(Zheng et al., 2019)</ref> splits the embedding dimensions into k equal-sized groups and applies bilinear within the groups. layer l + 1. Formally, we have:</p><formula xml:id="formula_6">P l+1 u = F F N (Wr v?N (u) ? (u,v) P l u ) ? (u,v) = exp[QP l v (KP l u ) ] v ?N (u) exp[QP l v (KP l u ) ]<label>(6)</label></formula><p>where W r ? R d?d , Q ? R d?d , K ? R d?d are learnable weight matrices, F F N () denotes a feedforward network, N (u) is the set of neighbor nodes to the vertex u. In addition, we employ residual connection between two layers and perform layer normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification Module</head><p>To determine the semantic relations for an entity pair (e h , e t ), we first concatenate two pair-specific entity representations and the corresponding final entity pair representation.</p><formula xml:id="formula_7">r (h,t) = [e h (h,t) ; e t (h,t) ; P (h,t) ]<label>(7)</label></formula><p>Then, we use a feed-forward neural network to calculate the probability for each relation:</p><formula xml:id="formula_8">P (r|e h , et) = sigmoid(W b ?(War (h,t) + ba) + b b ) (8) where W a ? R 3d?d , W b ? R d?r , b a , b b</formula><p>are learnable parameters, ? is an elementwise activation function (e.g., tanh).</p><p>To address the multi-label and sample imbalance problem more effectively, we adopt an adaptivethresholding loss <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref> as the classification loss to train our model in an end-to-end way. Specifically, it introduces an additional threshold relation category TH, and optimizes the loss by increasing the logits of the positive relations P T higher than the TH relation and decreasing the logits of the negative relations N T lower than the TH relation.</p><formula xml:id="formula_9">L = ? r?P T log( exp(logit r )) r ?P T ?{T H} exp(logit r ) ) ? log( exp(logit T H ))</formula><p>r ?N T ?{T H} exp(logit r ) ) (9) where logit is the output in the last layer before Sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the effectiveness of our CGM2IR model on three public DRE datasets: DocRED, CDR, and GDA. The dataset statistics are shown in <ref type="table">Table 1</ref>.</p><p>DocRED is a large-scale human-annotated dataset for document-level RE proposed by <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>. It contains 97 types of relations and 5,053 annotated documents in total which are constructed from Wikipedia and Wikidata. Documents in DocRED contain about 12.6 positive relational facts on average, which is several times that of the common sentence-level RE dataset. CDR (Chemical-Disease Reactions) <ref type="bibr" target="#b6">(Li et al., 2016)</ref> and GDA (Gene-Disease Associations) <ref type="bibr" target="#b17">(Wu et al., 2019)</ref> are two widely-used DRE datasets in the biomedical domain. They both contain only one type of positive relation, Chemical-Induced-Disease between chemical and disease entities and Gene-Induced-Disease between gene and disease entities respectively. For a fair comparison, We follow the standard split of the three datasets as <ref type="bibr" target="#b26">Zeng et al. (2020)</ref> and <ref type="bibr" target="#b32">Zhou et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings and Evaluation Metrics</head><p>In our CGM2IR implementation, we use cased BERT-base <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> or RoBERTa-large  the encoder on DocRED and cased SciBERT-base <ref type="bibr" target="#b1">(Beltagy et al., 2019)</ref> on CDR and GDA. AdamW <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2019)</ref> is used to optimize the neural networks with a linear warmup and cosine decay learning rate schedule. We set the initial learning rate for all encoder modules to 2e ?5 , the initial learning rate for other modules to 1e ?4 , the embedding dimension, and the hidden dimension to 768. The GNNs have 3 layers and the hidden size of node embedding is 768. All hyper-parameters are tuned based on the development set. Other parameters in the network are all obtained by random orthogonal initialization <ref type="bibr" target="#b13">(Saxe et al., 2014)</ref> and updated during training. All the experiments are trained with an NVIDIA RTX 3090 GPU. Following <ref type="bibr" target="#b21">Yao et al. (2019)</ref> and previous works, we use the micro F 1 and micro Ign F 1 as the evaluation metrics for DocRED. Ign F 1 denotes the result after excluding the common relational facts that appear in both training set and development/test sets. For CDR and GDA, in addition to using micro F 1 , we also report the Intra F 1 and Inter F 1 metrics to evaluate the model's performance on intra-sentential relations and inter-sentential relations on the dev set, since they strictly annotate these two types of facts but DocRED does not. In our experiments, a triplet is taken as correct when the two corresponding entities and the relation type are all correct and we exclude all triplets with relation of "None".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on DocRED</head><p>We conduct comprehensive and comparable experiments on DocRED dataset. The results are shown in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>We compare our CGM2IR model with lots of methods from two categories. The first one is graph-based methods, including LSR <ref type="bibr" target="#b10">(Nan et al., 2020)</ref>, GEDA , GCGCN-BERT <ref type="bibr" target="#b31">(Zhou et al., 2020)</ref>, GLRE , GAIN <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref>, HeterGSAN <ref type="bibr" target="#b20">(Xu et al., 2021c)</ref>, SIRE <ref type="bibr" target="#b25">(Zeng et al., 2021)</ref> and DRE <ref type="bibr" target="#b19">(Xu et al., 2021b)</ref>. The second one is non-graph-based methods including BERT <ref type="bibr" target="#b16">(Wang et al., 2019)</ref>, HIN-BERT , CorefBERT , SSAN <ref type="bibr" target="#b18">(Xu et al., 2021a)</ref>, ATLOP <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref>, MRN <ref type="bibr" target="#b7">(Li et al., 2021)</ref> and DocuNet . The baselines we selected all use BERT as their encoder.</p><p>As shown in <ref type="table" target="#tab_5">Table 2</ref>, we observe that CGM2IR outperforms all baseline methods on both development and test sets. Compared with the models in these two categories, both F 1 and Ign F 1 of our model are significantly improved. Among the various amounts of baselines, ATLOP <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref> and DocuNet  are the most two relevant to our approach. Compared to ATLOP-BERT base , the performance of CGM2IR-BERT base improves roughly about 0.8% for Ign F 1 and 0.92% for F 1 . CGM2IR-BERT base also brings about 0.2% ign F 1 enhancement compared to DocuNet-BERT base , which verifies the effectiveness of our proposed method. Furthermore, CGM2IR-RoBERTa large obtains better results than baselines with BERT-large or RoBERTa-large as well. For example, CGM2IR-RoBERTa large achieves 0.71% Ign F 1 /0.77% F 1 gain compared <ref type="bibr" target="#b10">(Nan et al., 2020)</ref> 52.43 59.00 56.97 59.05 GEDA-BERT base  54.52 56.16 53.71 55.74 GCGCN-BERT base <ref type="bibr" target="#b31">(Zhou et al., 2020)</ref> 55.43 57.35 54.53 56.67 GLRE-BERT base  --55.40 57.40 HeterGSAN-BERT base <ref type="bibr" target="#b20">(Xu et al., 2021c)</ref> 58.13 60.18 57.12 59.45 GAIN-BERT base <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref> 59.14 61.22 59.00 61.24 DRE-BERT base <ref type="bibr" target="#b19">(Xu et al., 2021b)</ref> 59.33 61.39 59.15 61.37 SIRE-BERT base <ref type="bibr" target="#b25">(Zeng et al., 2021)</ref> 59   54.29 56.31 53.70 55.60 CorefBERT base  55.32 57.51 54.54 56.96 SSAN-BERT base <ref type="bibr" target="#b18">(Xu et al., 2021a)</ref> 57.03 59.19 55.84 58.16 ATLOP-BERT base <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref> 59.22 61.09 59.31 61.30 MRN-BERT base <ref type="bibr" target="#b7">(Li et al., 2021)</ref> 59.74 61.61 59.52 61.74 DocuNet-BERT base  59  <ref type="bibr" target="#b18">(Xu et al., 2021a)</ref> 60.25 62.08 59.47 61.42 GAIN-BERT large <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref> 60.87 63.09 60.31 62.76 ATLOP-RoBERTa large <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref> 61   to ATLOP-RoBERTa large on the development set. In general, these results demonstrate both the effectiveness of context guided mention integration and the usefulness of inter-pair reasoning. <ref type="table" target="#tab_6">Table 3</ref> depicts the comparisons with state-of-theart models on CDR and GDA. We compare our CGM2IR model with five baselines, including EoG , DHG , LSR <ref type="bibr" target="#b10">(Nan et al., 2020)</ref>, MRN <ref type="bibr" target="#b7">(Li et al., 2021)</ref>, ATLOP <ref type="bibr" target="#b32">(Zhou et al., 2021)</ref>. Our model adopts SciBERT base for its superiority when deal-  ing with biomedical domain texts. It can be observed that CGM2IR achieves the new state-of-the-art F 1 score on these two datasets in the biomedical domain. On CDR test set, CGM2IR obtains +4.6 F 1 gain, which significantly outperforms all other approaches. On GDA test set, similar improvements can also be observed. These results demonstrate the effectiveness and generality of our approach.</p><formula xml:id="formula_10">Model Dev Test Ign F1 F1 Ign F1 F1 LSR-BERT base</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on CDR and GDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We also conduct a thorough ablation study as shown in <ref type="table" target="#tab_8">Table 4</ref> to study the contribution of two key modules: context guided mention integration module and inter-pair reasoning module. From <ref type="table" target="#tab_8">Table 4</ref>, we can observe that:</p><p>(1) When the context guided mention integration module is discarded and replaced with the logsumexp pooling layer, the performance of our model on the DocRED dev set drops by 0.38% in both F 1 and Ign F 1 score. Similarly, removal of the inter-pair reasoning module results in a 0.27% drop in F 1 and 0.14% in Ign F 1 . This phenomenon indicates the effectiveness of context guided mention integration module and inter-pair reasoning module.</p><p>(2) Removal of both modules leads to a more considerable decrease. The F 1 score decreases from 62.01% to 60.89% and the Ign F 1 score decreases from 60.02% to 59.12%. This study demonstrates that all components work together in synergy with the final relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Intra-and Inter-sentence Triplet Extraction</head><p>To further evaluate the performance, we report the results of intra-and inter-sentence relation extraction on CDR and GDA, since they explicitly annotate these two types of facts. The experimental results are listed in <ref type="table" target="#tab_6">Table 3</ref>, from which we can find that CGM2IR outperforms the current best models on these two datasets in regard to both intra-and inter-F 1 . For example, Our model obtains +5.0 intra-F 1 /+2.5 inter-F 1 and +1.0 intra-F 1 /+6.1 inter-F 1 gain compared with ATLOP on the test set of these two datasets. The improvements indicate that our model can effectively capture the complex interactions among entity pairs across the document. The intra-sentence relations contained in local text can be well considered, as well as the long-distance dependent inter-sentence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effect Analysis for Context Guided Cross-Attention</head><p>To assess the effectiveness of context guided crossattention in modeling entity representations, we compare five different strategies for generating entity representations including global mean pooling, global max pooling, global attention pooling, global logsumexp pooling, and our context guided cross-attention. For simplicity, after encoding the document, we directly concat the representations of the head entity and the tail entity then send them to the final classifier. The results on the development set of DocRED are illustrated in <ref type="table" target="#tab_10">Table 5</ref>, from which we can observe that the context guided cross-attention is absolutely superior to the global strategies. This result indicates that context guided cross-attention is reasonable and effective, which drives the head and tail entities together to dynamically determine their respective representations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Effect Analysis for Inter-pair Reasoning</head><p>In addition, we evaluate the reasoning ability of our model on the development set of DocRED in <ref type="table" target="#tab_11">Table 6</ref>. Following <ref type="bibr" target="#b25">Zeng et al. (2021)</ref>, we use infer-F 1 as a metric that only considers instances of the two-hop positive relations in the development set of DocRED. More specifically, we only evaluate the golden relational facts r 1 , r 2 and r 3 when there exists e h r 1 ?? e o r 2 ?? e t and e h r 3 ?? e t . As illustrated in <ref type="table" target="#tab_11">Table 6</ref>, CGM2IR outperforms all the baselines in infer-F 1 . Specifically, CGM2IR-BERT base improves roughly about 1.15% for infer-F 1 score compared with GAIN-BERT base . This reveals that the inter-pair reasoning module plays an important role in capturing intrinsic clues and performing logic reasoning on entities chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose CGM2IR that incorporates context guided mention integration and interpair reasoning to improve DRE. Instead of simply synthesizing multiple coreferential mentions at once, CGM2IR dynamically generates fine-grained entity representations for each entity pair. Moreover, we construct a homogeneous entity pair graph and employ GNNs to capture intrinsic clues and perform reasoning among entity pairs. Experimental results on three widely used DRE datasets demonstrate that our CGM2IR model is effective and outperforms previous state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of DRE. Note that mentions of the same entity are marked with identical color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Following this, Christopoulou et al. (2019) built a document graph with heterogeneous types of nodes and edges. Nan et al. (2020) proposed a latent structure induction to induce the dependency tree in the document dynamically. Wang et al. (2020), Zeng et al. (2020), Li et al. (2020), Zhang et al. (2020) integrated similar structural dependencies to model documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1] Britain 's Prince Harry is engaged to his US partner Meghan Markle ? [2] Harry spent 10 years in the army and has this year, with his elderly brother William , ? [3] The last major royal wedding took place in 2011, when Kate Middleton and Prince William were married ...</figDesc><table><row><cell cols="3">Context guided mention integration</cell><cell>0.2</cell><cell>( e Harry</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>( C Harry William ,</cell><cell>)</cell><cell></cell><cell>0.7</cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell>( e William</cell><cell>)</cell></row><row><cell cols="2">...</cell><cell cols="3">Harry spent 10 years in the army and has this year, with his elderly brother William</cell><cell>...</cell></row><row><cell cols="3">attention score</cell><cell>Harry</cell><cell>William</cell></row><row><cell>Encoder</cell><cell></cell><cell></cell><cell cols="2">Pre-trained Language Model (e.g. BERT)</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row><row><cell>Document</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on the development and test set of DocRED. We separate graph-based and non-graph-based methods into two groups. The results of baselines are from their related papers.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell cols="2">intra-F1 inter-F1</cell></row><row><cell>? CDR Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EoG</cell><cell>63.6</cell><cell>68.2</cell><cell>50.9</cell></row><row><cell>LSR</cell><cell>64.8</cell><cell>68.9</cell><cell>53.1</cell></row><row><cell>DHG-BERT base</cell><cell>65.9</cell><cell>70.1</cell><cell>54.6</cell></row><row><cell>MRN</cell><cell>65.9</cell><cell>70.4</cell><cell>54.2</cell></row><row><cell>ATLOP-SciBERT base</cell><cell>69.2</cell><cell>74.2</cell><cell>52.6</cell></row><row><cell cols="2">CGM2IR-SciBERT base 73.8</cell><cell>79.2</cell><cell>55.1</cell></row><row><cell>? GDA Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EoG</cell><cell>81.5</cell><cell>85.2</cell><cell>50.0</cell></row><row><cell>LSR</cell><cell>82.2</cell><cell>85.4</cell><cell>51.1</cell></row><row><cell>MRN</cell><cell>82.9</cell><cell>86.1</cell><cell>53.5</cell></row><row><cell>DHG-BERT base</cell><cell>83.1</cell><cell>85.6</cell><cell>58.8</cell></row><row><cell>ATLOP-SciBERT base</cell><cell>83.9</cell><cell>87.3</cell><cell>52.9</cell></row><row><cell cols="2">CGM2IR-SciBERT base 84.7</cell><cell>88.3</cell><cell>59.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on CDR and GDA datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of CGM2IR on the development set of DocRED, where "w/o" indicates without.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Results of different strategies for generating entity representations on DocRED.</figDesc><table><row><cell>Model</cell><cell>Infer F1</cell><cell>P</cell><cell>R</cell></row><row><cell>BERT-RE  *  base</cell><cell>39.62</cell><cell cols="2">34.12 47.23</cell></row><row><cell>GAIN-GloVe  ?</cell><cell>40.82</cell><cell cols="2">32.76 54.14</cell></row><row><cell>RoBERTa-RE  *  base</cell><cell>41.78</cell><cell cols="2">37.97 46.45</cell></row><row><cell>SIRE-GloVe  ?</cell><cell>42.72</cell><cell cols="2">34.83 55.22</cell></row><row><cell>GAIN-BERT  *  base</cell><cell>46.89</cell><cell cols="2">38.71 59.45</cell></row><row><cell>CGM2IR-BERT base</cell><cell>48.04</cell><cell cols="2">39.54 61.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Infer-F 1 results on dev set of DocRED. Results with * are reported in<ref type="bibr" target="#b26">Zeng et al. (2020)</ref>, ? are reported in<ref type="bibr" target="#b25">Zeng et al. (2021)</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to<ref type="bibr" target="#b21">Yao et al. (2019)</ref>, at least 40.7% of relations can only be identified from multiple sentences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, EoG builds a graph network with sentences, entities, and entity mentions as nodes and edges connected between different nodes according to heuristical rules.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph enhanced dual attention network for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw068</idno>
	</analytic>
	<monogr>
		<title level="j">Database J. Biol. Databases Curation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MRN: A locally and globally mention-based reasoning network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.117</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1359" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_16</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Finetune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RENET: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-17083-7_17</idno>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology -23rd Annual International Conference, RECOMB 2019</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-05-05" />
			<biblScope unit="volume">11467</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="14149" to="14157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.144</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1653" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="14167" to="14175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.582</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7170" to="7186" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
	<note>Dublin City University and Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SIRE: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.47</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="524" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
		<ptr target="ij-cai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-27" />
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with dual-tier heterogeneous graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.143</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1641" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="4279" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PRGC: Potential relation and global correspondence based joint relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6225" to="6235" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global context-enhanced graph convolutional networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
