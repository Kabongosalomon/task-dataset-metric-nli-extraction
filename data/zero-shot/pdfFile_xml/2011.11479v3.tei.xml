<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
							<email>humam.alwassel@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent stateof-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding is thriving in the computer vision community, and it manifests in several challenging tasks such as action classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b75">76]</ref>, activity localization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b92">93]</ref>, and video captioning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b94">95</ref>]. Yet, the success of video research has been lagging behind that of its counterpart in the image domain. In many aspects, this is due to the exponentially larger amount of data in videos compared to images, not fitting in commodity hardware. Image encoders have the privilege to process batches of complete images at once, thus exploiting the rich contextual information from all pixels. Empowered by such capability, many image models are trained in an end-to-end manner for complex tasks such as object detection <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74]</ref>, semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, and image caption- . We train video encoders to be temporally-sensitive through a novel supervised pretraining paradigm. A fixed-sized clip is sampled from an untrimmed video and passed through the encoder to obtain a local clip feature (blue). A global video feature (red) is pooled from the local features of all clips in the untrimmed video. The local and global features are used to train the encoder on the task of classifying the label of foreground clips (action label) and classifying whether a clip is inside or outside the action (temporal region).</p><p>ing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b52">53]</ref>. In contrast, the long and variable length of untrimmed videos makes it impractical to encode a complete video on current hardware accelerators <ref type="bibr" target="#b82">[83]</ref>. While a few recent localization works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b93">94]</ref> attempt to train endto-end for untrimmed video tasks, such as temporal action localization, they need to resort to aggressive spatial and temporal downsampling to remain computationally practical. Instead, most state-of-the-art localization methods for untrimmed videos choose to learn models atop precomputed clip features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref>.</p><p>In this work, we focus on improving the precomputed features used for temporal localization tasks, which we de-fine as tasks that require predictions related to the time dimension of the video. Specifically, we target three important localization problems: Temporal Action Localization (TAL), Action Proposal Generation (Proposals), and Dense Video Captioning (Dense-Captioning). State-of-theart methods for these localization tasks use features extracted from video encoders typically pretrained for the task of Trimmed Action Classification (TAC) on large-scale datasets, such as Kinetics <ref type="bibr" target="#b36">[37]</ref> and Sports-1M <ref type="bibr" target="#b35">[36]</ref>. However, this pretrained representation is not necessarily suitable for localization tasks. In particular, we observe that TAC-pretrained features tend to be temporally-insensitive, i.e. background (no action) segments can have quite similar representations to foreground (action) segments from the same untrimmed video. We provide an analysis study of TAC-pretrained features in Section 5 that shows evidence of the high cosine similarity between features of background and foreground clips. These temporally-insensitive features make it harder for the localization algorithm to learn the target task, and thus, negatively impact the final performance.</p><p>To circumvent these drawbacks, we propose a novel, supervised pretraining paradigm for video clip representation that not only trains to classify foreground activities but also considers background clips and global video information to improve temporal sensitivity. We refer to our pretraining approach as Temporally-Sensitive Pretraining (TSP). <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of TSP. We conduct extensive experiments to show that features extracted by clip encoders pretrained with TSP are more discriminative, and that training state-of-the-art localization algorithms atop TSP features results in significant performance gains on three temporal localization tasks: TAL, Proposals, and Dense-Captioning. Moreover, TSP gives consistent performance boosts regardless of the video encoder architecture, pretraining dataset, or the localization algorithm learned atop our features. Interestingly, we observe that localization performance on short instances greatly improves when using TSP pretrained features. This aligns well with our hypothesis that temporally-sensitive features allow localization algorithms to draw sharper contrast between foreground and background context in long untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (I)</head><p>We propose TSP, a temporally-sensitive supervised pretraining task for video encoders. TSP trains an encoder to explicitly discriminate between foreground and background clips in untrimmed videos. (II) We show with comprehensive experiments that using features pretrained with the TSP task significantly improves performance across three video localization problems. Additionally, we show the generalization capability of our pretraining strategy on three encoder architectures and two pretraining datasets. We also demonstrate consistent performance gains for multiple localization algorithms trained on the same target problem. (III) We provide an extensive analy-sis study of our features. Interestingly, we observe that TSP pretraining boosts temporal action localization performance on short action instances. The study also demonstrates that our features are in fact temporally-sensitive and can encode background clips differently from foreground clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition. Large-scale video datasets, such as UCF-101 <ref type="bibr" target="#b72">[73]</ref>, Sports-1M <ref type="bibr" target="#b35">[36]</ref>, and Kinetics <ref type="bibr" target="#b36">[37]</ref>, have accelerated the development of action classification models. Simonyan and Zisserman <ref type="bibr" target="#b71">[72]</ref> introduced a two-stream encoder to represent appearance with RGB frames and motion with stacked optical flow vectors. Wang et al. <ref type="bibr" target="#b80">[81]</ref> proposed the Temporal Segment Network (TSN) encoder to capture long-term temporal information. Pretrained on TAC, TSN along with other recent architectures (e.g. R(2+1)D <ref type="bibr" target="#b76">[77]</ref>, I3D <ref type="bibr" target="#b9">[10]</ref>, and C3D <ref type="bibr" target="#b74">[75]</ref>) have become the de facto feature extractors for temporal action localization (TAL) <ref type="bibr" target="#b61">[62]</ref>, action segmentation <ref type="bibr" target="#b17">[18]</ref>, and event captioning <ref type="bibr" target="#b86">[87]</ref>. Since TAC-pretraining is not necessarily suitable for these localization tasks, we propose a pretraining that learns from both foreground and background clips in untrimmed videos.</p><p>Temporal action localization and proposal generation. Many algorithms have been developed for TAL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b89">90]</ref>. While the majority has been on fully-supervised TAL <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref>, recent works have also studied TAL under weak supervision <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b68">69]</ref>, singleframe supervision <ref type="bibr" target="#b53">[54]</ref>, and self-supervision <ref type="bibr" target="#b34">[35]</ref>. The first generation of algorithms applied complex action classifiers in a sliding window fashion <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref>. To alleviate the expensive cost of sliding an action classifier over long videos, the second generation of algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b91">92]</ref> followed a two-stage approach that first learns action proposals to limit the number of candidates passed to the action classifier. A third set of algorithms jointly learn action proposals and action classifiers in one stage <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b93">94]</ref>. A few works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b93">94]</ref> learn TAL end-to-end by drastically downsampling videos to be computationally practical, e.g. PBR-Net <ref type="bibr" target="#b48">[49]</ref> uses only 3 frames per second on ActivityNet and SSN <ref type="bibr" target="#b93">[94]</ref> uses only 9 clips per proposal. In contrast, most state-of-the-art methods build atop precomputed features from TAC-pretrained encoders. Since experiments show that such features are not best suited for TAL and Proposals, we propose to replace them with temporally-sensitive pretrained features that can significantly boost performance.</p><p>Dense video captioning. Krishna et al. <ref type="bibr" target="#b39">[40]</ref> introduced the task of Dense-Captioning along with the ActivityNet Captions benchmark. Dense-Captioning aims at both localizing and textually describing all events in a video. This problem branched out from video captioning <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b60">61]</ref>, where a full video is captioned without localizing events. <ref type="bibr" target="#b39">[40]</ref> uses a variant of DAPs <ref type="bibr" target="#b14">[15]</ref> to generate proposals and employs an LSTM-based captioning module to describe these proposals. Subsequent works use bidirectional attentive fusion <ref type="bibr" target="#b78">[79]</ref>, masked transformers <ref type="bibr" target="#b95">[96]</ref>, and reinforcement learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b83">84]</ref>. A line of multi-modal Dense-Captioning methods combine visual cues with signals from audio <ref type="bibr" target="#b63">[64]</ref>, speech/subtitles <ref type="bibr" target="#b66">[67]</ref>, or both <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Similar to TAL and Proposals, Dense-Captioning algorithms rely on temporally-insensitive TAC-pretrained features, which do not perform as well as the TSP pretrained ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Traditional Pretraining Strategies</head><p>Since it is impractical to fit entire untrimmed videos into commodity GPUs without drastically downsampling space or time, current state-of-the-art localization algorithms share a common practice in that they do not finetune their video encoders directly on the target task (e.g. TAL). Instead, they use pretrained encoders as fixed feature extractors <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref>. Trimmed action classification (TAC) has been the traditional approach to pretrain these encoders. The TAC task aims to classify clips from short videos, where the action spans the entire video. While TAC has been successful in providing features that discriminate between different action classes, it often fails to distinguish between the action instance and its nearby background context. For example, recent diagnostic studies <ref type="bibr" target="#b0">[1]</ref> have shown that state-of-the-art TAL methods are quite sensitive to the context around action instances and that their inability to distinguish between an action and its temporal background context is the main roadblock to improving localization performance. We argue that the features used in these stateof-the-art localization methods, pretrained on TAC, are a source of such confusion. Thus, we propose to depart from the traditional strategy and render the features temporallysensitive through a novel pretraining task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">How to Incorporate Temporal Sensitivity?</head><p>A limiting aspect of TAC-pretrained encoders is that they only learn from positive samples (foreground/action clips). Intuitively, learning from negative samples (background/no action clips) is expected to improve the temporal discriminative ability of these encoders. Given an untrimmed video, a good encoder for localization problems should be able to distinguish between the semantics of different actions as well as between actions and their background context. Intuitively, clip features that have an idea of whether the clip is inside or outside an action can directly help localization methods find better activity/proposal boundaries for TAL and Proposals and find better captions for Dense-Captioning. Thus, we propose to pretrain encoders on the task of (1) classifying the label of foreground clips and (2) classifying whether a clip is inside or outside the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporally-Sensitive Pretraining (TSP)</head><p>Input data. We pretrain our model using untrimmed videos with temporal annotations. The encoder is learned in an end-to-end fashion from the raw video input. In particular, given an untrimmed video, we sample a fixed-size input clip X of size 3?L?H?W , where 3 refers to the RGB channels, L is the number of frames, and H and W are the frame height and width. We assign X two labels: (1) the action class label y c if this clip is from a foreground segment, and (2) the binary temporal region label y r that indicates if the clip is from a foreground/action (y r = 1) or background/no action (y r = 0) region of the video. Local and global feature encoding. Let E be the video encoder that transforms a clip X into a feature vector f of size F . We refer to f as the local clip feature. Let {X i } be the set of clips from an untrimmed video. We refer to the max-pooled feature f g = max(E(X i )) as the global video feature (GVF). Given only a short clip X, it is challenging to classify whether X is inside or outside an action. The challenge stems from the fact that we only have access to local context, while the task we wish to solve inherently requires global understanding of the video content. To overcome this challenge, we combine the GVF with the local clip feature to better learn the task. We can think of the GVF as a conditioning vector for deciding foreground vs. background. We study other GVF pooling functions in the appendix. Two classification heads. We employ two classification heads to pretrain the encoder. Specifically, the first head (action label head) consists of a fully-connected (FC) layer W c of size F ? C, where C is the number of action classes in the dataset. W c transforms the local features f to an action label logits vector? c . The second head (temporal region head) is an FC layer W r of size 2F ?2, which takes as input the concatenation of the local and global features, f ? f g , to produce a temporal region logits vector? r . Loss. We optimize our loss for each input clip X:</p><formula xml:id="formula_0">loss = ? r L(? r , y r ) + ? c L(? c , y c ), if y r = 1 ? r L(? r , y r ), otherwise,<label>(1)</label></formula><p>where L is the cross-entropy loss and (? c , ? r ) are trade-off coefficients to weigh the losses of the two heads. The loss is the sum of the two head losses when the clip is from the foreground, i.e. y r = 1, and is the loss from the second head when the clip is from the background. Optimization details. Temporally annotated video datasets have a natural imbalance between the temporal duration of foreground vs. background. To mitigate this imbalance, we subsample clips from videos in such a way that we train on the same number of foreground and background clip samples. We initialize our encoder weights with those pretrained on Kinetics-400 <ref type="bibr" target="#b36">[37]</ref>. Many of the recent video ar-chitectures have publicly released their Kinetics-pretrained weights, and we make use of these models in our experiments. Ideally, we wish to backpropagate the loss (Equation 1) through the GVF portion of our model. However, and as mentioned earlier, it is impractical to treat entire untrimmed videos in commodity GPUs. Thus, we freeze the GVF during training, i.e. we precompute the GVF of each video from the Kinetics-pretrained initialized encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Pretraining datasets. To pretrain with our TSP strategy, we need a dataset of untrimmed videos with temporal boundary annotations. Thus, we leverage two standard datasets: ActivityNet v1.3 <ref type="bibr" target="#b8">[9]</ref> and THUMOS14 <ref type="bibr" target="#b33">[34]</ref>. We conduct experiments using two architectures: ResNet3D and R(2+1)D <ref type="bibr" target="#b76">[77]</ref>. We select these backbones for their recognized good performance, speed, and efficiency. ResNet3D: This is the 3D version of the 2D ResNet <ref type="bibr" target="#b27">[28]</ref> CNN for images. ResNet3D is composed of a series of 3D convolution layers with residual skip connections. In our experiments and for simplicity, we consider the 18-layer variant of ResNet3D. R(2+1)D: This encoder is also a ResNet-based backbone. It decomposes each spatiotemporal 3D convolution kernel into a 2D (spatial) and a 1D (temporal) convolution. Compared to ResNet3D, R(2+1)D is more efficient and light-weight, and it has been shown to maintain high performance on video tasks. In our experiments, we use the 18 and 34-layer versions of R(2+1)D. Implementation details. In order to cope with the diversity of video formats present in ActivityNet and THU-MOS14, we re-encode all videos in MP4 format with a constant frame rate of 30 fps. We sample clips of L = 16 frames with a stride of 2 frames, such that each clip covers a temporal receptive field of approximately one second. While keeping the aspect ratio fixed, frames are resized such that the smallest dimension is 128 pixels and then cropped to H ? W = 112 ? 112 pixels, randomly in training but deterministically centered during testing. The videos are split into temporally contiguous segments, representing foreground (action) and background (no action) content. We select 5 clips per segment, sampled randomly (temporal jittering) during training and uniformly in testing. We set ? c = ? r = 1 in Equation <ref type="formula" target="#formula_0">(1)</ref>, and use a distributed SGD optimizer with different learning rates per module: 10 ?4 for the video encoder and a grid search among [0.002, 0.004, 0.006, 0.008, 0.01] for the two classification heads. We train for 8 epochs with a batch size of 32 clips per GPU. We use two V100 GPUs and scale the learning rate linearly with the number of GPUs. We use a linear learning rate warm up strategy over the first 2 epochs and decay factor of ? = 0.01 at epochs 4 and 6. We select the best model among learning rates and training epochs based on the average validation clip accuracy of the two classification heads.</p><p>Baselines. We compare our pretraining approach with TAC pretraining. In particular, we consider the following baselines: TAC on Kinetics, TAC on ActivityNet, and TAC on THUMOS14. The models from the second and third baselines are finetuned from a Kinetics-pretrained model.</p><p>Target tasks and evaluation metrics. We consider three localization tasks to evaluate TSP pretrained features: TAL on both ActivityNet and THUMOS14, Proposals on Activi-tyNet, and Dense-Captioning on ActivityNet Captions <ref type="bibr" target="#b39">[40]</ref>. For the TAL tasks, the performance is measured using the mean Average Precision (mAP) metric, where a predicted temporal segment is considered a true positive, if it satisfies a temporal Intersection over Union (tIoU) threshold with a ground truth instance of the correct action label. Following standard practice, we use the average mAP over tIoUs [0.5 : 0.05 : 0.95] as the main metric for ActivityNet and the mAP at tIoU=0.5 (mAP@0.5) for THUMOS14. For the Proposals task, the main evaluation metric is the area under the curve (AUC) of the average recall (AR) vs. average number of proposals per video. Following common practice in ActivityNet, we limit the number of proposals to 100 per video when computing the AUC. We also report AR at 1, 10, and 100 proposals as additional metrics. Following common practice in the Dense-Captioning task, we use BLEU@3, BLEU@4, and METEOR averaged over tIoUs [0.3, 0.5, 0.7, 0.9] to evaluate performance.</p><p>Algorithms for the target tasks. In order to showcase the benefits of TSP pretrained features compared to the baselines, we retrain a variety of state-of-the-art algorithms for each target task atop features extracted from TSP pretrained encoders as well as the baseline encoders. We select the algorithms based on (1) their strong performance on the target tasks and <ref type="formula">(2)</ref> the availability of open-sourced code. Here, we briefly discuss each algorithm and how we apply it to our features. It is essential to note that we do not innovate in any of these algorithms, and we use their default hyperparameter settings unless otherwise stated below. We simply swap the visual features they originally use with ours or those of the encoder baselines we compare against. G-TAD [88]: We <ref type="table">Table 1</ref>: Effects of TSP on target tasks. We compare features pretrained with our TSP task vs. those pretrained with TAC on Kinetics and TAC on ActivityNet. We use R(2+1)D-34 encoders and pretrain on ActivityNet. We use G-TAD <ref type="bibr" target="#b87">[88]</ref>, BMN <ref type="bibr" target="#b46">[47]</ref>, and BMT <ref type="bibr" target="#b31">[32]</ref> as algorithms for the ActivityNet TAL, Proposals, and Dense-Captioning tasks, respectively. The column corresponding to the main evaluation metric for each task is highlighted in grey and the best performance is in bold. TSP significantly outperforms the baselines on all tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Here, we extensively ablate TSP along four dimensions: target localization task, encoder architecture, localization algorithm, and pretraining dataset.</p><p>Study 1: Effects of TSP on target tasks. This study aims to compare features pretrained with TSP vs. those pretrained with the baselines, TAC on Kinetics and TAC on ActivityNet, on multiple target tasks. Specifically, we pretrain with the ActivityNet dataset and use an R(2+1)D-34 for the baseline encoders as well as our own. We use G-TAD, BMN, and BMT as the algorithms for the ActivityNet TAL, Proposals, and Dense-Captioning tasks, respectively. <ref type="table">Table 1</ref> summarizes the results. Observations: (I) TAC on Activi-tyNet outperforms TAC on Kinetics for all three tasks. This makes sense given the fact that the former baseline is pretrained on the same dataset used in the target tasks. However, TSP features consistently show the best performance across all tasks. Specifically, TSP outperforms both base-  <ref type="table" target="#tab_2">Table 2</ref> studies the contribution of each TSP classification head to the target task performance. We observe that the performance boost comes mostly from the temporal region head, validating the importance of pretraining on foreground and background clips to attain temporal-sensitivity. Study 2: TSP for different video encoders. This experiment explores TSP pretraining for different video architec-   on THUMOS14), and its transferability across datasets (i.e. TSP pretrained on ActivityNet and tested for TAL on THU-MOS14). Specifically, we pretrain R(2+1)D-34 on THU-MOS14 and on ActivityNet, then apply P-GCN and G-TAD atop TSP features for the TAL task on THUMOS14. <ref type="table" target="#tab_5">Table 5</ref> compares the two TSP features with the baselines, TAC on Kinetics and TAC on THUMOS14. Observations: (I) THU-MOS14 is different from ActivityNet in two key aspects: THUMOS14 is much smaller, and it has a higher background to foreground ratio (i.e. actions are sparser in THU-MOS14). Despite these differences, TSP on THUMOS14 improves over the TAC-based baselines by significant margins, regardless of the localization algorithm. Specifically when using P-GCN, TSP on THUMOS14 features improve  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">State-of-the-Art (SOTA) Comparison</head><p>While the previous ablations shed light on the generalization of TSP across multiple tasks, video encoders, algorithms, and datasets, this subsection puts our results in perspective and compares them with SOTA algorithms for each localization task. We report the comparative results in Tables 6 and 7. Note that we build TSP upon the bestperforming publicly available code for each task, namely G-TAD <ref type="bibr" target="#b87">[88]</ref>, P-GCN <ref type="bibr" target="#b91">[92]</ref>, BMN <ref type="bibr" target="#b46">[47]</ref>, and BMT <ref type="bibr" target="#b31">[32]</ref>. In TAL on ActivityNet (Table 6(a)), we reach SOTA performance with TSP. We achieve 35.81% in average mAP, a boost of 0.80% w.r.t. the previous SOTA PBRNet <ref type="bibr" target="#b48">[49]</ref> and a boost of 1.72% w.r.t. our baseline G-TAD <ref type="bibr" target="#b87">[88]</ref>. Moreover, TSP (with RGB features only) outperforms SOTA methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref> that use RGB and Flow features. In TAL on THUMOS14 (Table 6(b)), we achieve 53.5% in mAP@0.5, a boost of 0.5% w.r.t. the previous SOTA TSA-Net <ref type="bibr" target="#b25">[26]</ref> and a boost of 4.4% w.r.t. our baseline P-GCN <ref type="bibr" target="#b91">[92]</ref>. The results display different improvements on both datasets, focusing on higher tIoU for ActivityNet and lower tIoU on THUMOS14. We argue that this dis- crepancy originates from the different activity densities in both datasets. In Action Proposal Generation <ref type="table" target="#tab_7">(Table 7)</ref>, we reach 69.04% in AUC, a boost of 1.96% w.r.t. our baseline BMN <ref type="bibr" target="#b46">[47]</ref>, but fall short of RapNet <ref type="bibr" target="#b19">[20]</ref> (?0.89%). In Dense Video Captioning (Table 6(c)), we reached 8.75% in average METEOR, a 0.31% improvement over the baseline BMT <ref type="bibr" target="#b31">[32]</ref>, but fall short of SDVC <ref type="bibr" target="#b55">[56]</ref> (?0.07%). We argue that SDVC <ref type="bibr" target="#b55">[56]</ref> uses a reinforcement learning paradigm that optimizes for the METEOR metrics directly, trading off BLEU performances to overfit on METEOR. In contrast, the TSP-empowered BMT model achieves balanced performances in both BLEU and METEOR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Self-Supervised Encoders</head><p>Recent self-supervised learning (SSL) methods have shown impressive performance on video tasks such as action classification <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b84">85]</ref>. Here, we compare TSP features with SOTA SSL features for temporal localization tasks. Specifically, we compare with XDC <ref type="bibr" target="#b2">[3]</ref>, a recent SOTA SSL method that learns video and audio features via cross-modal deep clustering. <ref type="table" target="#tab_8">Table 8</ref> compares TSP and XDC features for TAL on THUMOS14 under the same settings: R(2+1)D-18 encoder and G-TAD algorithm. Although XDC impressively outperforms the supervised TAC baselines, it falls short of TSP performance by 2.8% in mAP@0.5. While it is expected that SSL requires more video data for pretraining than supervised pretraining, it is worthwhile to point out that XDC pretrains on 65M videos from IG-Kinetics <ref type="bibr" target="#b23">[24]</ref>, i.e. 260 times more videos than TSP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Feature Analysis</head><p>We further analyze TSP pretrained features on ActivityNet. DETAD analysis. Following DETAD <ref type="bibr" target="#b0">[1]</ref>, we analyze the TAL on ActivityNet performance (average mAP) for five different groups of activities based on their length (  <ref type="figure" target="#fig_0">&gt; 180s)</ref>. The extra short instances, XS, are known to be the most challenging to localize <ref type="bibr" target="#b0">[1]</ref>, and they represent more than half of the annotated instances (53.7%). Their temporal extent is limited as is the information available to recognize the activity. Such instances might be hidden among a significant amount of background. It is clear that TAC performs well in localizing long activities, in particular because they are predominant in their corresponding videos. Yet, TAC achieves the worst performance on the challenging shorter activities. We argue that their localization is more sensitive to the classification of each single clip, since TAC is unaware of what an activity does not look like in its temporal surrounding. In contrast, TSP features outperform the TAC ones for the short activity instances (XS and S). We believe our learned clip feature is more aware of background, and thus more perceptive of temporal activity boundaries for localization. As a trade-off, it appears that TSP does not perform as well on extra long activities. We believe those long activities might include intermediate clips with content leaning toward a background activity, thus misleading the localization and resulting in slightly worse performance. Nevertheless, the XL activities merely represent 4.0% of the dataset, so the overall impact on performance is insignificant. Feature similarity among video clips. Here, we analyze the similarity between video clip features within the same video. We expect the clip features from the same activity to be very similar (consensus), yet very different from the background clips in its temporal surrounding (sharpness). <ref type="figure" target="#fig_1">Figure 2</ref> visualizes the cosine similarity between clips of the same video using TAC on Kinetics vs. TSP features (more examples are in the supplementary material). In (a), TAC on Kinetics shows a high similarity between the activity and the background. This will inevitably make localization more difficult. In comparison, TSP better discriminates between background and activity. In (b), it appears that TAC on Kinetics is trying to split the activity in two. By learning what background is and what it is not, TSP homogenizes the similarity between all clip features in the foreground activity. In (c), the TSP video encoder increases the differences between background and foreground features. It homogenizes the features within both activities (bottom left and top right corners), yet it does not enforce background features to be similar, resulting in an increase in dissimilarity within the background (see the apparent diagonal in the central square). In (d), TAC on Kinetics displays a high similarity for the clips of the foreground activity, yet they might look similar to the remaining background. TSP learns obvious dissimilarity between background and foreground. The TAC pretraining is unaware of the existence of background clips. As a result, it might recognize the class of some actions but is unable to localize them precisely. In contrast, TSP makes the encoder aware of the existence of background, and so the clip features across the video tend to be more informative for localization. Thus, TSP improves the encoder's discriminative ability, reduces the smoothing over the temporal axis, and leads to sharper localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present TSP, a novel temporally-sensitive supervised pretraining for video encoders, which not only trains to classify actions, but also considers background clips and global information to gain temporal sensitivity. We show that TSP features improve SOTA methods on the TAL, Proposals, and Dense-Captioning tasks. We argue TSP features can be preferred over other features to build more accurate models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Pooling Function for GVF <ref type="table">Table 10</ref> compares between the performance of TSP with max-pooled vs. average-pooled GVF on the three target tasks: TAL, Proposals, and Dense-Captioning. TSP with max-pooled GVF offers better performance across all the tasks. <ref type="table">Table 10</ref>: Effects of GVF pooling function on target tasks. We compare features pretrained with TSP using average-pooled vs. maxpooled GVF. We use R(2+1)D-34 encoders and pretrain on ActivityNet. We use G-TAD, BMN, and BMT as the methods for the ActivityNet TAL, Proposals, and Dense-Captioning tasks, respectively. TSP with max-pooled GVF is better on all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Task</head><p>Temporal Action Localization Action Proposal Generation Dense Video Captioning Feature Pretraining </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended Ablation Study Results</head><p>In this section, we provide further statistical analysis for our studies on TSP. Statistical analyses are of great importance when comparing the performances of different algorithms. In particular, it helps us to understand whether a given improvement is significant or it is within the noise range. For each ablation study in the main paper, we reported the maximum value over 5 runs, following common practice in the field. However, such practice might be misleading under certain scenarios, in particular if a proposed approach has high performance variance but has on average worse performance. To alleviate any doubts, and in an effort of transparency, we share here the mean and standard deviation performances for our proposed approach along with those of the TAC-pretrained baselines. Refer to <ref type="bibr">Tables 11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>, and 14 for the extended statistical results of Study 1, 2, 3, and 4, respectively. These tables report the performance on all tIoUs as well. The extended results show that our improvements are consistent with those reported in the main paper, and that such improvements do not lie within the noise range. With such analysis, we can confidently say that TSP is statistically better than the TAC-pretrained baselines. <ref type="table" target="#tab_5">Tables 15, 16</ref>, 17, and 18 present extended SOTA comparison with more methods and all tIoUs for TAL on ActivityNet, TAL on THUMOS14, Dense-Captioning on ActivityNet Captions, and Proposals on ActivityNet, respectively. For the Dense-Captioning task, we report additional results for captioning ground truth proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extended State-of-the-Art Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extended Feature Analysis Study</head><p>Here, we extended the feature similarity study to compare with TAC on ActivityNet. <ref type="figure" target="#fig_3">Figure 3</ref> provides more examples comparing TSP features with those of TAC on Kinetics and TAC on ActivityNet. Not only does TSP show better temporal sensitivity compared to TAC on Kinetics (as we have shown in the main paper), but it also presents a better distinguishing of background vs. foreground representation compared to TAC on ActivityNet. <ref type="table">Table 11</ref>: Effects of TSP on target tasks (extended results). Each experiment in Study 1 is repeated five times, and we report the the mean, standard deviation (std), and max values over those five runs. Each table entry is given by mean ? std (max). The row/column corresponding to the main evaluation metric for each task is highlighted in grey and the best (mean) performance is in bold.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Temporally-Sensitive Pretraining (TSP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Feature similarity. Each column shows the similarity matrices for clips in a single video using TAC on Kinetics (top) and TSP (bottom) features. The green lines represent the temporal extent of ground truth actions. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) TAL on ActivityNet using G-TAD with R(2+1)D-34.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Feature similarity (extended results). Each column (set of three matrices) shows the similarity matrices of one video using TAC on Kinetics (top), TAC on ActivityNet (middle), and TSP on ActivityNet (bottom) features. The green lines next to each matrix represent the temporal extent of ground truth actions. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TAD for TAL on both ActivityNet and THUMOS14. G-TAD originally uses a Kinetics-pretrained TSN<ref type="bibr" target="#b80">[81]</ref> encoder to extract RGB and Flow features, then trains on their concatenation. For G-TAD on THUMOS14, we increase the very small default learning rate by ?10 (i.e. to 0.0004) to speed up the training. BMN<ref type="bibr" target="#b46">[47]</ref>: BMN is used for both Proposals and TAL on ActivityNet. BMN did not release code for THUMOS14, and it uses the same precomputed features as G-TAD. P-GCN<ref type="bibr" target="#b91">[92]</ref>: We employ P-GCN for TAL on THUMOS14. P-GCN did not release code for Ac-tivityNet. P-GCN extracts features from an RGB and Flow I3D Kinetics-pretrained encoder. Then, two RGB and Flow localization models are trained independently and their results are combined at inference time. We keep the Flow model unchanged and only retrain the RGB model with our features. BMT<ref type="bibr" target="#b31">[32]</ref>: BMT is used for the Dense-Captioning task on the ActivityNet Captions dataset. BMT uses visual and audio features. The visual features are the summation of RGB and Flow features from I3D Kinetics-pretrained encoders, and the audio features are from a VGG-like encoder pretrained on AudioSet<ref type="bibr" target="#b22">[23]</ref>. We keep the audio features as is and replace the visual features with ours.</figDesc><table><row><cell>Video Task</cell><cell cols="4">Temporal Action Localization</cell><cell cols="4">Action Proposal Generation</cell><cell>Dense Video Captioning</cell></row><row><cell>Feature Pretraining</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell cols="5">Avg. AR@1 AR@10 AR@100 AUC BLEU@3 BLEU@4 METEOR</cell></row><row><cell>TAC on Kinetics</cell><cell cols="5">48.54 34.24 7.85 33.32 34.19</cell><cell>57.52</cell><cell>75.56</cell><cell>67.91</cell><cell>3.42</cell><cell>1.58</cell><cell>8.17</cell></row><row><cell cols="6">TAC on ActivityNet 49.76 34.87 8.65 34.08 34.67</cell><cell>57.89</cell><cell>75.65</cell><cell>68.08</cell><cell>3.63</cell><cell>1.74</cell><cell>8.21</cell></row><row><cell>TSP w/o GVF</cell><cell cols="5">51.45 36.87 9.11 35.75 34.97</cell><cell>59.35</cell><cell>76.47</cell><cell>68.88</cell><cell>3.75</cell><cell>1.83</cell><cell>8.42</cell></row><row><cell cols="6">TSP on ActivityNet 51.26 37.12 9.29 35.81 34.99</cell><cell>58.96</cell><cell>76.63</cell><cell>69.04</cell><cell>4.16</cell><cell>2.02</cell><cell>8.75</cell></row><row><cell>use G-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Contribution of each TSP classification head to the target task performance. We pretrain R(2+1)D-34 on Activi-tyNet and test the features on ActivityNet TAL using G-TAD<ref type="bibr" target="#b87">[88]</ref>. Temporal Region 49.<ref type="bibr" target="#b75">76</ref> 34.87 8.65 34.08 TSP w/o Action Label 51.23 36.79 9.91 35.72 TSP 51.26 37.12 9.29 35.81 lines by at least +1.73% in average mAP on TAL, +0.96% in AUC on Proposals, and +0.54% in average METEOR on</figDesc><table><row><cell>Feature Pretraining</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg</cell></row><row><cell>TSP w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Dense-Captioning. These significant gains underscore the effectiveness of TSP pretraining in encoding better temporal representations for untrimmed videos. (II) On the TAL task, TSP features significantly boost performance at high tIoU thresholds (e.g. mAP@0.75 is 37.12% for TSP vs. 34.87% for TAC on ActivityNet). Better mAP at high tIoUs signi- fies tighter temporal predictions around the ground truth action instances. This indicates that TSP pretrained fea- tures can encode better boundary contrast between the ac- tion and its nearby background context. (III) While TSP pretraining without the GVF (TSP w/o GVF in the table) outperforms the baselines, using GVF for the second clas- sification head consistently boosts performance across all tasks (e.g. 8.75% vs. 8.42% in average METEOR on Dense- Captioning). This validates our design choice and shows the importance of GVF in helping the local features be more temporally-sensitive. Given this observation, we omit TSP w/o GVF from the remaining ablation studies. (IV) While the Dense-Captioning experiment is conducted on the same pretraining videos, the ActivityNet Captions temporal anno- tations [40] used for training the Dense-Captioning meth- ods do not necessarily align with the ActivityNet tempo- ral action annotations used for our pretraining. Neverthe- less, TSP still provides an improvement over the baselines. (V)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>TSP for different video encoders. We pretrain ResNet3D-18, R(2+1)D-18, and R(2+1)D-34 on ActivityNet and compare the features on the ActivityNet TAL task using G-TAD<ref type="bibr" target="#b87">[88]</ref> as the TAL algorithm. Our TSP features consistently outperform the baselines for every encoder type, indicating the generalizability of our pretraining to different backbone architectures.<ref type="bibr" target="#b32">33</ref>.21 8.96 32.78 47.57 33.11 8.10 32.46 48.54 34.24 7.85 33.32 TAC on ActivityNet 48.71 34.22 8.82 33.40 49.00 34.56 9.42 33.87 49.76 34.87 8.65 34.08 TSP on ActivityNet 49.81 34.81 8.63 34.10 50.07 35.61 8.96 34.71 51.26 37.12 9.29 35.81 Both BMN and G-TAD originally use the same features (TSN pretrained on Kinetics) and have a 0.24% gap in average mAP. However, when both are trained using TSP features, BMN bridges the performance gap with G-TAD to be only 0.14%. This highlights the importance of having temporally-sensitive video features for localization tasks.</figDesc><table><row><cell>Backbone Architecture</cell><cell></cell><cell cols="2">ResNet3D-18</cell><cell></cell><cell></cell><cell cols="2">R(2+1)D-18</cell><cell></cell><cell></cell><cell cols="2">R(2+1)D-34</cell><cell></cell></row><row><cell>Feature Pretraining</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg.</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg.</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg.</cell></row><row><cell>TAC on Kinetics</cell><cell>47.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>tures. Specifically, we pretrain ResNet3D-18, R(2+1)D-18, and R(2+1)D-34 on ActivityNet and compare the features on the ActivityNet TAL task using G-TAD as the TAL al- gorithm (refer to Table 3). Observations: (I) We observe similar performance trends among the different pretraining strategies regardless of the encoder type. In particular, our TSP features successfully outperform the baselines for ev- ery encoder. This indicates the generalization capability of the TSP pretraining to different backbones. (II) Aligned with observations made by previous works [77], R(2+1)D- 18 exhibits better performance compared to ResNet3D-18 (average mAP of 34.71% vs. 34.10%). (III) Not only does the deeper R(2+1)D-34 pretrained with the TSP strat- egy achieve better performance compared to R(2+1)D-18, but interestingly, the performance gap between TSP and TAC on Kinetics widens with the deeper encoder (+2.25% for R(2+1)D-18 vs. +2.49% for R(2+1)D-34). Similarly, TSP performance gap with TAC on ActivityNet increases from +0.84% for R(2+1)D-18 to +1.73% for R(2+1)D-34. This suggests that our pretraining can potentially show even larger gains for more sophisticated and deeper encoders. Study 3: TSP with other localization algorithms. We investigate here whether TSP features can consistently im- prove performance on the target task, regardless of the lo- calization algorithm used. To that end, we conduct the same TAL on ActivityNet experiment from Study 1 (cf. Table 1) but with the BMN algorithm instead of G-TAD. Table 4 summarizes the results using BMN. Observations: (I) Our TSP features used with BMN show similar performance gains as when they are used with G-TAD, with at least a 0.92% gap in average mAP with the TAC-based pretrain- ings. This demonstrates that our features are more discrim- inative for the task and that they can benefit different algo- rithms. (II)Study 4: TSP on different datasets. Here, we study two aspects of TSP: its applicability to other pretraining datasets (i.e. TSP pretrained on THUMOS14 and tested for TAL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>TSP with other localization algorithms. We conduct the same TAL on ActivityNet experiment fromTable 1but with the BMN algorithm instead of G-TAD. Our TSP features achieve the best performance when used with BMN as well.</figDesc><table><row><cell>Feature Pretraining</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg.</cell></row><row><cell>TAC on Kinetics</cell><cell cols="4">49.95 35.31 8.61 34.46</cell></row><row><cell cols="5">TAC on ActivityNet 50.78 35.40 7.96 34.75</cell></row><row><cell>TSP on ActivityNet</cell><cell cols="4">51.23 36.78 9.50 35.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Feature Pretraining</cell><cell>0.3</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.7</cell></row><row><cell>TAC on Kinetics</cell><cell cols="5">52.4 / 65.9 37.8 / 49.0 15.6 / 22.9</cell></row><row><cell>TSP on ActivityNet</cell><cell cols="5">54.2 / 65.4 39.4 / 51.0 14.7 / 22.2</cell></row><row><cell cols="6">TAC on THUMOS14 54.4 / 66.4 38.7 / 50.0 16.1 / 23.3</cell></row><row><cell cols="6">TSP on THUMOS14 58.0 / 69.1 44.2 / 53.5 18.5 / 26.0</cell></row><row><cell></cell><cell cols="2">(b) G-TAD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Pretraining</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>TAC on Kinetics</cell><cell cols="5">50.6 43.2 34.5 24.1 15.5</cell></row><row><cell>TSP on ActivityNet</cell><cell cols="5">53.4 45.9 37.0 26.7 16.1</cell></row><row><cell cols="6">TAC on THUMOS14 52.6 45.5 35.8 26.2 15.6</cell></row><row><cell>TSP on THUMOS14</cell><cell cols="5">59.6 52.0 43.2 32.2 21.1</cell></row></table><note>TSP on different datasets. We pretrain R(2+1)D-34 on THUMOS14 and on ActivityNet, and use P-GCN [92] and G- TAD [88] for the TAL task on THUMOS14. TSP features are applicable to and transferable across different datasets.(a) P-GCN. Results are reported for the RGB model / RGB+Flow models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>SOTA comparison for TAL and Dense-Captioning. We compare TSP with SOTA methods for (a) TAL on ActivityNet, (b) TAL on THUMOS14, and (c) Dense-Captioning on ActivityNet Captions. We use G-TAD<ref type="bibr" target="#b87">[88]</ref>, P-GCN<ref type="bibr" target="#b91">[92]</ref>, and BMT<ref type="bibr" target="#b31">[32]</ref> as the algorithms trained atop our features for each task, respectively. TSP achieves SOTA performance on (a) and (b) and is competitive on (c).</figDesc><table><row><cell cols="3">(a) TAL on ActivityNet</cell><cell cols="2">(b) TAL on THUMOS14</cell><cell></cell><cell cols="2">(c) Dense-Captioning</cell></row><row><cell>Method</cell><cell>0.5</cell><cell>0.75 0.95 Avg.</cell><cell>Method</cell><cell cols="2">0.3 0.4 0.5 0.6 0.7</cell><cell>Method</cell><cell>B@3 B@4 M</cell></row><row><cell cols="3">C-TCN [43] 47.60 31.90 6.20 31.10</cell><cell>G-TAD [88]</cell><cell cols="2">54.5 47.6 40.2 30.8 23.4</cell><cell cols="2">Bi-SST [79] 2.27 1.13 6.10</cell></row><row><cell cols="3">P-GCN [92] 48.26 33.16 3.27 31.11</cell><cell>TAL-Net [11]</cell><cell cols="2">53.2 48.5 42.8 33.8 20.8</cell><cell>DVC [44]</cell><cell>2.27 0.73 6.93</cell></row><row><cell>BMN [47]</cell><cell cols="2">50.07 34.78 8.29 33.85</cell><cell cols="3">Zhao et al. [93] 53.9 50.7 45.4 38.0 28.5</cell><cell>MFT [84]</cell><cell>2.82 1.24 7.08</cell></row><row><cell>GTAN [52]</cell><cell cols="2">52.61 34.14 8.91 34.31</cell><cell>PBRNet [49]</cell><cell cols="2">58.5 54.6 51.3 41.8 29.5</cell><cell cols="2">MDVC [33] 2.60 1.07 7.31</cell></row><row><cell cols="3">PBRNet [49] 53.96 34.97 8.98 35.01</cell><cell>TSA-Net [26]</cell><cell cols="2">65.6 61.4 53.0 42.4 28.8</cell><cell cols="2">SDVC [56] 2.94 0.93 8.82</cell></row><row><cell cols="3">G-TAD [88] 50.36 34.60 9.02 34.09</cell><cell>P-GCN [92]</cell><cell>63.6 57.8 49.1 -</cell><cell>-</cell><cell>BMT [32]</cell><cell>3.84 1.88 8.44</cell></row><row><cell cols="3">TSP (ours) 51.26 37.12 9.29 35.81</cell><cell>TSP (ours)</cell><cell cols="2">69.1 63.3 53.5 40.4 26.0</cell><cell cols="2">TSP (ours) 4.16 2.02 8.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>SOTA comparison for Proposals on ActivityNet. We use BMN atop our features. TSP significantly improves over BMN original performance and is competitive with SOTA.</figDesc><table><row><cell cols="4">Method [50] [93] [5] [45] [20] BMN [47] TSP</cell></row><row><cell cols="2">AR@100 74.54 75.27 76.73 76.65 78.63</cell><cell>75.01</cell><cell>76.63</cell></row><row><cell>AUC</cell><cell>66.43 66.51 68.05 68.23 69.93</cell><cell>67.10</cell><cell>69.04</cell></row></table><note>the RGB model results by at least 5.5% in mAP@0.5. Moreover, combining the predictions of our newly-trained RGB model with that of the original (unchanged) Flow modality boosts the overall performance by at least 3.5% in mAP@0.5. (II) Using TSP features pretrained on Activ- ityNet (TSP on ActivityNet) outperforms both TAC on Ki- netics and TAC on THUMOS14 in mAP@0.5. This shows that TSP features are transferable across TAL datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>SOTA SSL comparison. We compare TSP with XDC for TAL on THUMOS14. Both use R(2+1)D-18 and G-TAD.</figDesc><table><row><cell>Feature Pretraining</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>TAC on Kinetics</cell><cell cols="5">45.4 38.9 30.5 19.7 11.5</cell></row><row><cell>TAC on THUMOS14</cell><cell cols="5">48.0 41.6 33.3 23.7 14.6</cell></row><row><cell cols="6">XDC on IG-Kinetics [3] 51.5 44.9 37.2 28.7 20.0</cell></row><row><cell>TSP on THUMOS14</cell><cell cols="5">57.1 50.2 41.0 30.4 19.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance as a function of action length. We report the performance of TAL on ActivityNet for different action lengths. TSP performs significantly better on Extra Short (XS) and Short (S) actions. XS and S make up about 70% of all actions.</figDesc><table><row><cell>Instance Length</cell><cell>XS</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell></row><row><cell>% of the Dataset</cell><cell cols="3">53.7 16.2 16.8</cell><cell>9.7</cell><cell>4.0</cell></row><row><cell>TAC on Kinetics</cell><cell cols="5">16.4 41.3 53.2 68.4 72.3</cell></row><row><cell cols="6">TAC on ActivityNet 17.5 42.0 53.1 67.5 72.5</cell></row><row><cell>TSP on ActivityNet</cell><cell cols="5">19.3 44.2 53.9 67.8 71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>):</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>? 0.232 (34.241) 34.780 ? 0.154 (34.865) 36.816 ? 0.069 (36.865) 36.845 ? 0.164 (37.123) 0.80 30.191 ? 0.158 (30.434) 30.856 ? 0.197 (30.874) 32.756 ? 0.068 (32.865) 32.678 ? 0.118 (32.772) 0.85 25.119 ? 0.157 (25.299) 25.820 ? 0.155 (25.849) 27.478 ? 0.092 (27.620) 27.690 ? 0.126 (27.712) 0.90 19.152 ? 0.164 (19.157) 19.569 ? 0.232 (19.617) 20.998 ? 0.095 (21.181) 21.375 ? 0.152 (21.487) 0.95 08.028 ? 0.290 (07.847) 08.274 ? 0.429 (08.647) 09.429 ? 0.241 (09.109) 09.440 ? 0.243 (09.286) Average 33.077 ? 0.186 (33.315) 33.837 ? 0.189 (34.080) 35.671 ? 0.066 (35.748) 35.712 ? 0.062 (35.808) ? 0.251 (34.185) 34.452 ? 0.152 (34.667) 34.961 ? 0.475 (34.971) 35.011 ? 0.089 (34.991) AR@10 57.194 ? 0.501 (57.520) 57.772 ? 0.149 (57.892) 58.831 ? 0.640 (59.346) 59.126 ? 0.101 (58.961) AR@100 75.415 ? 0.305 (75.561) 75.654 ? 0.067 (75.648) 76.212 ? 0.490 (76.469) 76.539 ? 0.108 (76.627) AUC 67.637 ? 0.277 (67.912) 67.959 ? 0.087 (68.075) 68.572 ? 0.532 (68.875) 68.906 ? 0.119 (69.035)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell cols="2">TAC on Kinetics</cell><cell cols="2">TAC on ActivityNet</cell><cell cols="2">TSP w/o GVF</cell><cell>TSP on ActivityNet</cell></row><row><cell>0.50</cell><cell cols="7">48.269 ? 0.241 (48.538) 49.223 ? 0.349 (49.761) 51.389 ? 0.145 (51.445) 51.206 ? 0.162 (51.263)</cell></row><row><cell>0.55</cell><cell cols="7">45.535 ? 0.239 (45.900) 46.588 ? 0.346 (46.919) 48.532 ? 0.107 (48.641) 48.472 ? 0.124 (48.551)</cell></row><row><cell>0.60</cell><cell cols="7">42.824 ? 0.284 (43.262) 43.790 ? 0.316 (44.131) 45.683 ? 0.154 (45.872) 45.637 ? 0.109 (45.723)</cell></row><row><cell>0.65</cell><cell cols="7">40.185 ? 0.309 (40.617) 41.111 ? 0.297 (41.510) 43.182 ? 0.112 (43.212) 43.239 ? 0.108 (43.327)</cell></row><row><cell>0.70</cell><cell cols="7">37.487 ? 0.285 (37.853) 38.366 ? 0.245 (38.631) 40.451 ? 0.135 (40.673) 40.534 ? 0.169 (40.834)</cell></row><row><cell>0.75</cell><cell cols="5">33.977 (b) Proposals on ActivityNet using BMN with R(2+1)D-34.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Feature Pretraining</cell><cell></cell></row><row><cell>Metric</cell><cell>TAC on Kinetics</cell><cell></cell><cell cols="2">TAC on ActivityNet</cell><cell>TSP w/o GVF</cell><cell></cell><cell>TSP on ActivityNet</cell></row><row><cell>AR@1</cell><cell cols="6">34.002 (c) Dense-Captioning on ActivityNet Captions using BMT with R(2+1)D-34.</cell></row><row><cell></cell><cell></cell><cell cols="3">Ground Truth Proposals</cell><cell cols="3">Learned Proposals</cell></row><row><cell cols="2">Feature Pretraining</cell><cell cols="6">BLEU@3 BLEU@4 METEOR BLEU@3 BLEU@4 METEOR</cell></row><row><cell cols="2">TAC on Kinetics</cell><cell>4.32</cell><cell>1.76</cell><cell>10.93</cell><cell>3.42</cell><cell>1.58</cell><cell>8.17</cell></row><row><cell cols="2">TAC on ActivityNet</cell><cell>4.64</cell><cell>1.94</cell><cell>10.99</cell><cell>3.63</cell><cell>1.74</cell><cell>8.21</cell></row><row><cell cols="2">TSP w/o GVF</cell><cell>4.88</cell><cell>2.09</cell><cell>11.29</cell><cell>3.75</cell><cell>1.83</cell><cell>8.42</cell></row><row><cell cols="2">TSP on ActivityNet</cell><cell>4.76</cell><cell>1.99</cell><cell>11.31</cell><cell>4.16</cell><cell>2.02</cell><cell>8.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>TSP for different video encoders (extended results). Each experiment in Study 2 is repeated five times, and we report the the mean, standard deviation (std), and max values over those five runs. Each table entry is given by mean ? std (max). ? 0.297(47.573) 48.701 ? 0.170 (49.003) 49.883 ? 0.187 (50.069) 0.55 44.445 ? 0.289 (44.827) 45.846 ? 0.164 (46.157) 47.079 ? 0.203 (47.226) 0.60 41.715 ? 0.297 (42.055) 43.275 ? 0.133 (43.475) 44.483 ? 0.179 (44.433) 0.65 38.992 ? 0.273 (39.367) 40.816 ? 0.141 (40.977) 41.983 ? 0.164 (41.909) 0.70 36.233 ? 0.270 (36.653) 38.037 ? 0.126 (38.202) 39.245 ? 0.101 (39.243) 0.75 32.762 ? 0.216 (33.113) 34.273 ? 0.205 (34.562) 35.568 ? 0.091 (35.608) 0.80 28.919 ? 0.242 (29.301) 30.609 ? 0.224 (30.743) 31.595 ? 0.191 (31.987) 0.85 24.481 ? 0.163 (24.745) 25.837 ? 0.157 (25.993) 26.677 ? 0.146 (26.885) 0.90 18.691 ? 0.170 (18.839) 19.920 ? 0.198 (20.119) 20.464 ? 0.167 (20.773) 0.95 08.111 ? 0.574 (08.099) 08.971 ? 0.447 (09.424) 09.072 ? 0.434 (08.958) Average 32.157 ? 0.220 (32.457) 33.629 ? 0.161 (33.865) 34.605 ? 0.101 (34.709) ? 0.241 (48.538) 49.223 ? 0.349 (49.761) 51.206 ? 0.162 (51.263) 0.55 45.535 ? 0.239 (45.900) 46.588 ? 0.346 (46.919) 48.472 ? 0.124 (48.551) 0.60 42.824 ? 0.284 (43.262) 43.790 ? 0.316 (44.131) 45.637 ? 0.109 (45.723) 0.65 40.185 ? 0.309 (40.617) 41.111 ? 0.297 (41.510) 43.239 ? 0.108 (43.327) 0.70 37.487 ? 0.285 (37.853) 38.366 ? 0.245 (38.631) 40.534 ? 0.169 (40.834) 0.75 33.977 ? 0.232 (34.241) 34.780 ? 0.154 (34.865) 36.845 ? 0.164 (37.123) 0.80 30.191 ? 0.158 (30.434) 30.856 ? 0.197 (30.874) 32.678 ? 0.118 (32.772) 0.85 25.119 ? 0.157 (25.299) 25.820 ? 0.155 (25.849) 27.690 ? 0.126 (27.712) 0.90 19.152 ? 0.164 (19.157) 19.569 ? 0.232 (19.617) 21.375 ? 0.152 (21.487) 0.95 08.028 ? 0.290 (07.847) 08.274 ? 0.429 (08.647) 09.440 ? 0.243 (09.286) Average 33.077 ? 0.186 (33.315) 33.837 ? 0.189 (34.080) 35.712 ? 0.062 (35.808)</figDesc><table><row><cell></cell><cell cols="3">(a) TAL on ActivityNet using G-TAD with ResNet3D-18.</cell></row><row><cell></cell><cell></cell><cell>Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TAC on ActivityNet</cell><cell>TSP on ActivityNet</cell></row><row><cell>0.50</cell><cell cols="3">47.514 ? 0.310 (47.970) 48.351 ? 0.188 (48.708) 49.182 ? 0.305 (49.806)</cell></row><row><cell>0.55</cell><cell cols="3">44.629 ? 0.323 (45.207) 45.598 ? 0.170 (45.926) 46.278 ? 0.251 (46.683)</cell></row><row><cell>0.60</cell><cell cols="3">42.005 ? 0.377 (42.582) 42.977 ? 0.146 (43.178) 43.724 ? 0.209 (44.069)</cell></row><row><cell>0.65</cell><cell cols="3">39.312 ? 0.359 (39.895) 40.306 ? 0.153 (40.339) 41.094 ? 0.203 (41.374)</cell></row><row><cell>0.70</cell><cell cols="3">36.515 ? 0.329 (36.990) 37.587 ? 0.110 (37.627) 38.298 ? 0.213 (38.541)</cell></row><row><cell>0.75</cell><cell cols="3">32.878 ? 0.253 (33.206) 34.085 ? 0.129 (34.217) 34.654 ? 0.166 (34.814)</cell></row><row><cell>0.80</cell><cell cols="3">29.014 ? 0.207 (29.314) 30.121 ? 0.116 (30.205) 30.697 ? 0.076 (30.702)</cell></row><row><cell>0.85</cell><cell cols="3">24.459 ? 0.111 (24.517) 25.507 ? 0.189 (25.602) 26.176 ? 0.075 (26.182)</cell></row><row><cell>0.90</cell><cell cols="3">18.808 ? 0.229 (19.197) 19.402 ? 0.123 (19.427) 20.268 ? 0.121 (20.156)</cell></row><row><cell>0.95</cell><cell cols="3">08.306 ? 0.516 (08.955) 08.424 ? 0.270 (08.816) 08.661 ? 0.435 (08.625)</cell></row><row><cell>Average</cell><cell cols="3">32.344 ? 0.273 (32.783) 33.235 ? 0.089 (33.404) 33.903 ? 0.147 (34.095)</cell></row><row><cell></cell><cell cols="3">(b) TAL on ActivityNet using G-TAD with R(2+1)D-18.</cell></row><row><cell></cell><cell></cell><cell>Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TAC on ActivityNet</cell><cell>TSP on ActivityNet</cell></row><row><cell>0.50</cell><cell cols="3">47.218 (c) TAL on ActivityNet using G-TAD with R(2+1)D-34.</cell></row><row><cell></cell><cell></cell><cell>Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TAC on ActivityNet</cell><cell>TSP on ActivityNet</cell></row><row><cell>0.50</cell><cell>48.269</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>TSP with other localization algorithms (extended results). Each experiment in Study 3 is repeated five times, and we report the the mean, standard deviation (std), and max values over those five runs. Each table entry is given by mean ? std (max). TAL on ActivityNet using BMN with R(2+1)D-18. ? 0.253(49.951) 50.339 ? 0.270 (50.775) 51.283 ? 0.206 (51.228) 0.55 47.127 ? 0.253 (47.391) 47.731 ? 0.289 (48.239) 48.665 ? 0.209 (48.712) 0.60 44.230 ? 0.341 (44.621) 44.760 ? 0.252 (45.173) 45.759 ? 0.176 (45.741) 0.65 41.588 ? 0.280 (41.905) 42.027 ? 0.252 (42.471) 43.310 ? 0.166 (43.319) 0.70 38.727 ? 0.343 (39.078) 39.016 ? 0.232 (39.427) 40.346 ? 0.122 (40.442) 0.75 35.020 ? 0.351 (35.306) 35.201 ? 0.178 (35.397) 36.577 ? 0.165 (36.782) 0.80 31.149 ? 0.313 (31.521) 31.494 ? 0.190 (31.542) 32.609 ? 0.186 (32.803) 0.85 25.893 ? 0.227 (26.186) 26.315 ? 0.217 (26.394) 27.398 ? 0.108 (27.333) 0.90 19.568 ? 0.208 (19.980) 19.991 ? 0.276 (20.070) 20.825 ? 0.196 (20.813) 0.95 07.651 ? 1.041 (08.613) 08.614 ? 0.531 (07.963) 08.420 ? 0.557 (09.504) Average 34.075 ? 0.304 (34.455) 34.549 ? 0.169 (34.745) 35.519 ? 0.129 (35.668)</figDesc><table><row><cell></cell><cell cols="2">(a) Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TAC on ActivityNet</cell><cell>TSP on ActivityNet</cell></row><row><cell>0.50</cell><cell>49.798</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>TSP on different pretraining datasets (extended results). Each experiment in Study 4 is repeated five times, and we report the the mean, standard deviation (std), and max values over those five runs. Each table entry is given by mean ? std (max). ? 0.157 (71.215) 72.202 ? 0.115(72.193) 71.713 ? 0.090 (71.611) 73.889 ? 0.116 (74.023) 0.2 68.720 ? 0.115 (68.640) 69.836 ? 0.139 (69.739) 69.416 ? 0.071 (69.362) 72.172 ? 0.139 (72.286) 0.3 65.876 ? 0.087 (65.867) 65.412 ? 0.178 (65.403) 66.289 ? 0.080 (66.418) 68.840 ? 0.165 (69.057) 0.4 60.043 ? 0.114 (60.048) 59.844 ? 0.165 (59.979) 60.228 ? 0.165 (60.302) 63.290 ? 0.175 (63.314) 0.5 48.763 ? 0.348 (49.007) 50.331 ? 0.394 (51.038) 49.879 ? 0.421 (50.028) 52.901 ? 0.337 (53.545) 0.6 36.313 ? 0.519 (37.048) 36.339 ? 0.298 (36.732) 36.744 ? 0.309 (36.559) 40.092 ? 0.295 (40.445) 0.7 22.380 ? 0.386 (22.892) 22.191 ? 0.255 (22.221) 22.769 ? 0.302 (23.327) 25.691 ? 0.210 (26.009) 0.8 09.325 ? 0.199 (09.126) 09.270 ? 0.203 (09.285) 09.508 ? 0.157 (09.713) 10.619 ? 0.229 (10.469) 0.9 01.413 ? 0.071 (01.409) 01.399 ? 0.067 (01.393) 01.435 ? 0.103 (01.484) 01.615 ? 0.071 (01.674) (b) TAL on THUMOS14 using G-TAD with R(2+1)D-34. ? 0.553 (58.934) 60.747 ? 1.114 (62.106) 59.747 ? 0.655 (60.546) 67.605 ? 1.096 (68.498) 0.2 54.909 ? 0.383 (55.446) 57.173 ? 1.144 (58.991) 56.756 ? 0.662 (57.738) 64.542 ? 1.106 (65.279) 0.3 49.728 ? 0.685 (50.590) 51.622 ? 1.136 (53.449) 51.202 ? 0.717 (52.608) 58.205 ? 1.236 (59.628) 0.4 42.405 ? 0.591 (43.232) 43.945 ? 1.163 (45.924) 43.999 ? 0.708 (45.538) 50.853 ? 1.243 (51.987) 0.5 33.255 ? 0.760 (34.521) 35.089 ? 1.066 (37.034) 34.797 ? 0.558 (35.823) 41.500 ? 1.118 (43.232) 0.6 23.618 ? 0.615 (24.080) 24.865 ? 1.027 (26.734) 25.024 ? 0.747 (26.194) 30.196 ? 1.422 (32.201) 0.7 14.467 ? 0.918 (15.467) 14.771 ? 0.789 (16.128) 15.536 ? 0.557 (15.565) 18.446 ? 1.318 (21.052) 0.8 06.763 ? 0.694 (07.254) 06.479 ? 0.498 (07.403) 07.264 ? 0.401 (07.231) 08.836 ? 0.873 (10.592) 0.9 01.224 ? 0.157 (01.313) 01.086 ? 0.155 (01.351) 01.271 ? 0.113 (01.355) 01.491 ? 0.133 (01.721)</figDesc><table><row><cell></cell><cell cols="3">(a) TAL on THUMOS14 using P-GCN with R(2+1)D-34.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TSP on ActivityNet</cell><cell>TAC on THUMOS14</cell><cell>TSP on THUMOS14</cell></row><row><cell>0.1</cell><cell cols="3">70.978 Feature Pretraining</cell><cell></cell></row><row><cell>mAP@tIoU</cell><cell>TAC on Kinetics</cell><cell>TSP on ActivityNet</cell><cell>TAC on THUMOS14</cell><cell>TSP on THUMOS14</cell></row><row><cell>0.1</cell><cell>58.311</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>SOTA comparison for TAL on ActivityNet (extended results). We use G-TAD as the algorithms atop our features. TSP achieves SOTA performance. Net<ref type="bibr" target="#b10">[11]</ref> 38.<ref type="bibr" target="#b22">23</ref> 18.30 1.30 20.22 SCC [8] 40.00 17.90 4.70 21.70 TCN [14] 37.49 23.47 4.47 23.58 CDC [68] 45.30 26.00 0.20 23.80 BSN [48] 46.45 29.96 8.02 30.03 Zhao et al. [93] 43.47 33.91 9.21 30.12 C-TCN [43] 47.60 31.90 6.20 31.10 P-GCN [92] 48.26 33.16 3.27 31.11 BMN [47] 50.07 34.78 8.29 33.85 GTAN [52] 52.61 34.14 8.91 34.31 PBRNet [49] 53.96 34.97 8.98 35.01 G-TAD [88] 50.36 34.60 9.02 34.09 TSP (ours) 51.26 37.12 9.29 35.81</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Avg.</cell></row><row><cell>R-C3D [86]</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TAL-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>SOTA comparison for TAL on THUMOS14 (extended results). We use P-GCN as the algorithms atop our features. TSP achieves SOTA performance. 72.3 69.1 63.3 53.5 40.4 26.0 10.5 1.7</figDesc><table><row><cell>Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell>Hou et al. [30]</cell><cell>51.3</cell><cell>-</cell><cell>43.7</cell><cell>-</cell><cell>22.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SST [7]</cell><cell>-</cell><cell>-</cell><cell>37.8</cell><cell>-</cell><cell>23.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDC [68]</cell><cell>-</cell><cell>-</cell><cell cols="4">40.1 29.4 23.3 13.1</cell><cell>7.9</cell><cell>-</cell><cell>-</cell></row><row><cell>TCN [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">33.3 25.6 15.9</cell><cell>9.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TURN-TAP [21]</cell><cell cols="5">54.0 50.9 44.1 34.9 25.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R-C3D [86]</cell><cell cols="5">54.5 51.5 44.8 35.6 28.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-TAD [71]</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell><cell>-</cell><cell>9.6</cell><cell>-</cell><cell>-</cell></row><row><cell>SSN [94]</cell><cell cols="5">66.0 59.4 51.9 41.0 29.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CTAP [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Action Search [2]</cell><cell>-</cell><cell>-</cell><cell cols="5">51.8 42.4 30.8 20.2 11.1</cell><cell>-</cell><cell>-</cell></row><row><cell>CBR [22]</cell><cell cols="6">60.1 56.7 50.1 41.3 31.0 19.1</cell><cell>9.9</cell><cell>-</cell><cell>-</cell></row><row><cell>ETP [63]</cell><cell>-</cell><cell>-</cell><cell cols="5">48.2 42.4 34.2 23.4 13.9</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN [48]</cell><cell>-</cell><cell>-</cell><cell cols="5">53.5 45.0 36.9 28.4 20.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MGG[50]</cell><cell>-</cell><cell>-</cell><cell cols="5">53.9 46.8 37.4 29.5 21.3</cell><cell>-</cell><cell>-</cell></row><row><cell>GTAN [52]</cell><cell>-</cell><cell>-</cell><cell cols="3">57.8 47.2 38.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BMN [47]</cell><cell>-</cell><cell>-</cell><cell cols="5">56.0 47.4 38.8 29.7 20.5</cell><cell>-</cell><cell>-</cell></row><row><cell>DBG [45]</cell><cell>-</cell><cell>-</cell><cell cols="5">57.8 49.4 39.8 30.2 21.7</cell><cell>-</cell><cell>-</cell></row><row><cell>CMS-RC3D [6]</cell><cell cols="5">61.6 59.3 54.7 48.2 40.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>G-TAD [88]</cell><cell>-</cell><cell>-</cell><cell cols="5">54.5 47.6 40.2 30.8 23.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TAL-Net [11]</cell><cell cols="7">59.8 57.1 53.2 48.5 42.8 33.8 20.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhao et al. [93]</cell><cell>-</cell><cell>-</cell><cell cols="5">53.9 50.7 45.4 38.0 28.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PBRNet [49]</cell><cell>-</cell><cell>-</cell><cell cols="5">58.5 54.6 51.3 41.8 29.5</cell><cell>-</cell><cell>-</cell></row><row><cell>C-TCN [43]</cell><cell cols="5">72.2 71.4 68.0 62.3 52.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TSA-Net [26]</cell><cell>-</cell><cell>-</cell><cell cols="5">65.6 61.4 53.0 42.4 28.8</cell><cell>-</cell><cell>-</cell></row><row><cell>P-GCN [92]</cell><cell cols="5">69.5 67.8 63.6 57.8 49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TSP (ours)</cell><cell>74.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>SOTA comparison for Dense-Captioning on ActivityNet Captions (extended results). We use BMT as the algorithms atop our features. TSP achieves SOTA performance in terms of average BLEU and is competitive in terms of average METEOR. The best numbers are highlighted in bold and the second best is underlined.</figDesc><table><row><cell></cell><cell cols="3">Ground Truth Proposals</cell><cell></cell><cell>Learned Proposals</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">BLEU@3 BLEU@4 METEOR BLEU@3 BLEU@4 METEOR</cell></row><row><cell>Rahman et al. [64]</cell><cell>3.04</cell><cell>1.46</cell><cell>7.23</cell><cell>1.85</cell><cell>0.90</cell><cell>4.93</cell></row><row><cell>Krishna et al. [40]</cell><cell>4.09</cell><cell>1.60</cell><cell>8.88</cell><cell>1.90</cell><cell>0.71</cell><cell>5.69</cell></row><row><cell>Bi-SST [79]</cell><cell>-</cell><cell>-</cell><cell>10.89</cell><cell>2.27</cell><cell>1.13</cell><cell>6.10</cell></row><row><cell>Masked Transformer [96]</cell><cell>5.76</cell><cell>2.71</cell><cell>11.16</cell><cell>2.91</cell><cell>1.44</cell><cell>6.91</cell></row><row><cell>DVC [44]</cell><cell>4.55</cell><cell>1.62</cell><cell>10.33</cell><cell>2.27</cell><cell>0.73</cell><cell>6.93</cell></row><row><cell>MFT [84]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.82</cell><cell>1.24</cell><cell>7.08</cell></row><row><cell>MDVC [33]</cell><cell>4.52</cell><cell>1.98</cell><cell>11.07</cell><cell>2.53</cell><cell>1.01</cell><cell>7.46</cell></row><row><cell>SDVC [56]</cell><cell>4.41</cell><cell>1.28</cell><cell>13.07</cell><cell>2.94</cell><cell>0.93</cell><cell>8.82</cell></row><row><cell>BMT [32]</cell><cell>4.63</cell><cell>1.99</cell><cell>10.90</cell><cell>3.84</cell><cell>1.88</cell><cell>8.44</cell></row><row><cell>TSP (ours)</cell><cell>4.76</cell><cell>1.99</cell><cell>11.31</cell><cell>4.16</cell><cell>2.02</cell><cell>8.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>SOTA comparison for Proposals on ActivityNet (extended results). We use BMN atop our features. TSP significantly improves over BMN original performance, and is competitive with SOTA. Method [19] [48] [50] [93] [5] [45] [20] BMN [47] TSP AR@100 73.17 74.16 74.54 75.27 76.73 76.65 78.63 75.01 76.63 AUC 65.72 66.17 66.43 66.51 68.05 68.23 69.93 67.10 69.04</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action search: Spotting targets in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Contextual multi-scale region convolutional 3d network for activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09184</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Khrisna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong Duc</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03766</idno>
		<title level="m">The activitynet large-scale activity recognition challenge 2018 summary</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scale matters: Temporal scale aggregation network for precise action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangfeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint syntax representation learning and visual cue translation for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time temporal action localization in untrimmed videos by subaction discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A better use of audio-visual cues: Dense video captioning with bi-modal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Actionbytes: Learning from trimmed videos to localize actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep concept-wise temporal convolutional networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Nanning Zheng, and Gang Hua. Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SF-Net: Single-Frame Supervision for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient action localization with approximately normalized fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph for video captioning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Memory-attended recurrent network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning latent superevents to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Precise temporal action localization by evolving temporal proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Watch, listen and tell: Multi-modal weakly supervised dense event captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanzila</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dense procedure captioning in narrated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Autoloc: weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Controllable video captioning with pos sequence guidance based on gated fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Text-to-clip video retrieval with early fusion and re-captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Stat: spatial-temporal attention mechanism for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Syntax-aware action targeting for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
