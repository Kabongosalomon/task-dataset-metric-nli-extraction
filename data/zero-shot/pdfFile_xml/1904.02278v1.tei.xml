<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAGCN: Dual Attention Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
							<email>fengwen.chen@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.auhuan.huo@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of software</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAGCN: Dual Attention Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) have recently become one of the most powerful tools for graph analytics tasks in numerous applications, ranging from social networks and natural language processing to bioinformatics and chemoinformatics, thanks to their ability to capture the complex relationships between concepts. At present, the vast majority of GCNs use a neighborhood aggregation framework to learn a continuous and compact vector, then performing a pooling operation to generalize graph embedding for the classification task. These approaches have two disadvantages in the graph classification task: (1)when only the largest sub-graph structure (k-hop neighbor) is used for neighborhood aggregation, a large amount of early-stage information is lost during the graph convolution step; (2) simple average/sum pooling or max pooling utilized, which loses the characteristics of each node and the topology between nodes. In this paper, we propose a novel framework called, dual attention graph convolutional networks (DAGCN) to address these problems. DAGCN automatically learns the importance of neighbors at different hops using a novel attention graph convolution layer, and then employs a second attention component, a selfattention pooling layer, to generalize the graph representation from the various aspects of a matrix graph embedding. The dual attention network is trained in an end-to-end manner for the graph classification task. We compare our model with state-of-the-art graph kernels and other deep learning methods. The experimental results show that our framework not only outperforms other baselines but also achieves a better rate of convergence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph structured or network data are rapidly becoming ubiquitous in our daily lives, e.g., World Wide Web network, transportation networks, and protein interaction networks. Researchers have conducted extensive research on many important machine learning applications in graph with both supervised and unsupervised fashion <ref type="bibr" target="#b0">[1]</ref>, such as vertex classification <ref type="bibr" target="#b1">[2]</ref>, anomaly detection <ref type="bibr" target="#b2">[3]</ref>, link prediction <ref type="bibr" target="#b3">[4]</ref> and recommendation system <ref type="bibr" target="#b4">[5]</ref>, but the complexity of graph data imposes great challenges for many tasks including one of the central tasks in the field, graph classification (but not node classification), which aims to assign a class label to an entire graph. In a cheminformatics dataset, for instance, atoms are represented by graph nodes and chemical bonds are represented by graph edges. A graph classification model can be applied to a dataset for many applications, from detecting molecular status, such as cancer activity detection or solubility detection, molecular properties, such as toxicity detection.</p><p>To solve the problem of graph classification, the most widely used strategy consists of graph statistic-based methods which are able to represent the graph in various aspects. Graph kernel <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> is the most popular of these techniques; it employs a kernel function to measure the positive semidefinite graph similarity between pairs of graphs <ref type="bibr" target="#b7">[8]</ref>. The classification task can then be conducted on a similarity matrix by using supervised algorithms like Support Vector Machine <ref type="bibr" target="#b8">[9]</ref>. By decomposing the graph into sub-structures, the graph kernel is capable of directly processing the graph data without transforming it into feature vectors. As a result, it has achieved dramatic success in node classification, link prediction, node clustering and so on.</p><p>Graph kernel-based algorithms nevertheless still suffer from natural limitations, such as the exponential growth of computation operations and the fixed feature design, which will be discussed in more detail in Section IV. Other algorithms <ref type="bibr" target="#b9">[10]</ref> attempt to distinguish and select the sub-graph features for graph classification by recursively applying an aggregation process on each node with the attributes from local neighbors to learn the node representations. The graph feature is then generated according to all the learned node representations in the graph.</p><p>Deep learning-based approaches like graph neural network have also been applied diffusely for network representation. These approaches embed the given graph and the side information associated with it into a continuous and compact vector space. After embedding, the graphs sharing common patterns are expected to be close to each other in the vector space, therefore classical machine learning methods can be applied to the embedded vector for graph classification. However, while the graph-structured data preserves more relational information than other data formats, it also incurs more complicated noise. How to learn a good representation while screening out the interference caused by the complex noise of each node in a graph has become a significant challenge. Moreover, subgraphs which consist of multiple nodes, or even the entire graph are required in the graph classification task to achieve a more comprehensive analysis. Hence, obtaining the graph representation based on node representation is another non-negligible challenge.</p><p>Many researches have concentrated on re-factorizing neural network architectures to directly process structured graph data <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref>. However, graph data are complex in many ways; for example, the topological structure information of different sub-graphs is fickle when the size is varied. Most existing graph neural network frameworks are limited by two factors when dealing with this scattered information because: 1) these frameworks ignore the significance of different hop neighbors. Only the final aggregation output is used, i.e., only the largest sub-graph is used to learn the node representation. 2) they mainly apply average/sum pooling or max pooling which fails to leverage the valuable information of a node or sub-graph in the graph. While conducting graph classification, we attempt to pay more attention to the graph signature <ref type="bibr" target="#b15">[16]</ref> (i.e. the special node or sub-graph), which is only a small segment of the entire graph. In contrast, a simple average/sum pooling or max pooling could result in a model that is constructed on too much irrelevant information.</p><p>To address the above problems, we propose a novel framework named Dual Attention Graph Convolution Network (DGCNN). The core idea of the proposed DGCNN is to identify and maximize the importance of the nodes or subgraph when conducting graph classification. We first merge the attention technique in the graph convolution operation to capture the arbitrary local structure information in a graph. A self-attention pooling layer then generates an adaptive combination representation matrix, in which each row in the learned matrix represents one perspective of the graph. Our contributions in this paper are threefold:</p><p>? We propose a novel attention graph convolution technique which is capable of leveraging the information from different hop neighbors rather than the k-hop only; ? We propose a novel graph self-attention pooling technique which extracts a more informative embedding matrix containing multiple significant nodes or sub-graphs; ? We conduct experiments and compare our method with both deep learning-based methods and graph kernel-based algorithms. The experiment results demonstrate that the proposed Dual Attention Graph Convolutional Network (DAGCN) outperforms the deep learning benchmarks for graph classification and are highly comparable with stateof-the-art graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There have been many attempts on graph classification tasks in the literature. The earliest experiments can be traced back to 1998 when Frasconi et al. <ref type="bibr" target="#b16">[17]</ref> used a recursive neural network to process directed acyclic graphs. Subsequently, Gori et al. <ref type="bibr" target="#b17">[18]</ref> introduced Graph Neural Networks (GNNs) to extend the neural network for graph-structured data. GNNs normally consist of an aggregation process which aggregates the node features a certain number of times or until equilibrium is reached to produce an embedding for each node. This idea has been broadly adopted and improved in many tasks <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>With the great success of computer vision, there is an increasing interest in generalizing convolutions to the graph domain. Bruna et al. <ref type="bibr" target="#b21">[22]</ref> first generalized the convolution operation to the graph's spatial domain after the original data have been transformed by Graph Fourier Transform (GFT). Since the computation of eigenvectors is involved, computational complexity has become a serious issue. Many researchers have worked on optimizing the convolution filters to reduce the computational complexity <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, the learning process in all the aforementioned spectral approaches usually depends on the Laplacian eigenbasis, which handles the entire graph at one time. Thus, the issue of scalability and computational complexity still cannot be overcome.</p><p>Duvenaud et al. <ref type="bibr" target="#b24">[25]</ref> introduced a spatial GCN that directly defines the convolutions on a graph without a transform. Each node propagates the features from its 1-hop neighbors to generate a differentiable fingerprint which simulates the circular fingerprints.After Kipf et al. <ref type="bibr" target="#b25">[26]</ref> simplified the concept, Atwood et al. <ref type="bibr" target="#b26">[27]</ref> extended this idea by propagating n different hops to the center node with different weights. A common challenge of these approaches is how to define the range of neighborhoods to aggregate and the strategies for obtaining information from neighbors. More recently, Niepert et al. <ref type="bibr" target="#b27">[28]</ref> and Hamilton et al. <ref type="bibr" target="#b28">[29]</ref> addressed the challenge in another way by sampling a fixed-size neighborhood for each node and then performing the aggregation. Lately, Tran et al. <ref type="bibr" target="#b29">[30]</ref> further optimizing GCNs by extending the basic graph convolution operator. These approaches have achieved high levels of performance and have increased the scope of GCN applications. Given rapid developments in the field of GCN, we point readers to our recent, comprehensive review in <ref type="bibr" target="#b30">[31]</ref>.</p><p>An important component that usually comes with CNNs, the pooling layer, can also be generalized to graph-structured data. It is a down-sampling strategy that largely reduces the spatial size of the input while roughly retaining its location relationships. Mean pooling is the most commonly used graph pooling strategy due to its conciseness. Easily mean all node's information could also solve the issues of rotational invariance and yield better performance <ref type="bibr" target="#b18">[19]</ref>. To better preserve the relationship between nodes, Defferrard et al. <ref type="bibr" target="#b12">[13]</ref> and Zhang et al. <ref type="bibr" target="#b31">[32]</ref> proposed approaches that perform pooling after the nodes have been rearranged in a meaningful order using a different strategy. This could be viewed as selecting similar parts of different graphs so that the preserved node relation can be used effectively. Overall, the essence of pooling is to reduce the size of the input (usually the node representation) by losing some information. Deciding which information to retain is the key to the model.</p><p>Attention mechanisms have already become the standard in many fields for a number of tasks <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The most important advantage of attention mechanisms is that they are able to handle the variably sized inputs by focusing on the most relevant parts of the inputs to make decisions. When attention is implemented on the same input, it is called Self-Attention <ref type="bibr" target="#b34">[35]</ref>. There is little literature on the topic of attention mechanisms on graph-structured data. Velickovic et al. <ref type="bibr" target="#b13">[14]</ref> employed attention to dynamically compute the weight of each node's neighbors during aggregation. Attention mechanisms have mainly been used in aggregation processing. A few attempts have been made to extend attention beyond aggregation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>, but some issues have still never been studied. Inspired by recent works and the defect of them, we propose our model which uses an attention technique to maximize the use of information that underlies the original graph input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM DEFINITION AND FRAMEWORK</head><p>A graph is represented as g = (V g , E g , A g , X g ), where V g is a vertex set v i , i = 1, ...., n. E g represents the linkages between nodes, denoted as e i,j =&lt; v i , v j &gt;? E, i = j. An unweighted adjacency matrix A g ? {0, 1} Ni?Ni represents the graph's topological structure by setting A i,j = 1 if e i,j ? E g , otherwise A i,j = 0. N i is the size of the graph g i . X ? R n?c indicates the c channel content features associated with each node v i .</p><p>Given a set of graphs G = (g 1 , g 2 , ? ? ? g n ) with their labels Y = (y 1 , y 2 , ? ? ? , y n ), the goal of our paper is to learn a function f (g i ) ? y i ? L, where L = {c 1 , ? ? ? , c |L| } is the class labels for the graphs. In this paper, we will develop a novel graph convolutional network which employs dual attentions at both node level and graph level, for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Framework</head><p>Our objective is to learn a classifier which could classify the given graph G. To achieve this, we propose a novel dual attention graph convolution network (DAGCN). <ref type="figure" target="#fig_0">Figure  1</ref> demonstrates the work-flow of DAGCN which consists of two modules: the attention graph convolution module and the attention pooling module.</p><p>? Attention Graph Convolution Module The attention graph convolution module is constructed of several attention graph convolution layers. Each layer takes the features X and adjacency matrix A to extract the hierarchical local substructure features of the vertices from different hops of neighbor. ? Attention Pooling Layer The attention pooling layer uses the nodes' embedding to learn multiple graph representation from different aspect and outputs a fixed size, matrix graph embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DUAL ATTENTION GRAPH CONVOLUTION</head><p>The DAGCN consists of three parts: (1) the attention graph convolution module; (2) the self-attention pooling layer; and (3) the fully connected classifier. In this section, we first address the problem of traditional GCNs and, then propose our attention graph convolution module and self-attention pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traditional Graph Convolution</head><p>We start by describing the traditional graph convolution layer and then propose DAGCN to address the shortcomings. The most general form of graph convolution with depth of k can be expressed recursively by a broadly followed convolution structure denoted as:</p><formula xml:id="formula_0">H k+1 = ?( A D ?1 H k W ) H 0 = X,<label>(1)</label></formula><p>where A = A + I n is the adjacency matrix with selfconnection for each node, D is the diagonal node degree matrix of A, A D ?1 represents the normalized graph structure, and W is the model parameter that will be trained. After applying this operation k times, H k becomes a node properties vector that contains k-hop local structure information.</p><p>Note that, during the repetition of Equation 1, with the exception of H k , the result in every step can only be used to generate the next convolution result. During this process, a large amount of information will be lost, and only the last convolution result H k , which represents the largest sub-graph, could be used for later tasks. This kind of operation can cause a significant loss of information. Only the k-hop local structure would be captured by the convolutional layer. Our attention convolution layer aims to solve this issue by attentionally aggregating the information from each convolution step. The comparison of two graph convolution layers is shown in <ref type="figure" target="#fig_1">Figure  2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Proposed Attention Graph Convolution (AGC)</head><p>The vast majority of graph neural networks are currently driven by Equation 1 which employs k-hop message aggregation mechanism. This enables the node representation to capture the local structural information of k-hop neighbors, but as the number of layers increases, a large amount of early information is lost during each convolution step, which severely affects the final prediction output and also limits the capacity of the model. The core idea of our attention graph convolution (AGC) layer is to enhance the model to not only depend on the k-hop convolution result, but also to capture valuable information from every single hop. The convolution result will thus be a hierarchical representation containing the most valuable information from different hop convolution processes. We exhibit attention behavior and implement it on Equation 1 to form a hierarchical node representation ? vn as below:</p><formula xml:id="formula_1">? vn = k i=1 ? i H k vn<label>(2)</label></formula><p>For simplicity, we use vanilla attention to identify the importance of each hop's aggregation result, in which ? is the attention weight and H k vn represents node v n 's local structure in k ? hops. The final node representation contains the hierarchical structure information. <ref type="figure" target="#fig_1">Figure 2</ref> compares the traditional convolution layer and the attention convolution layer.</p><p>To maximize the advantages of deep learning and learn deeper latent features, we use the Residual Learning technique <ref type="bibr" target="#b36">[37]</ref> to stack m attention convolution layers and develop an attention graph convolutional module to obtain a better final node representation ? vn . The input of each AGC layer is the sum of the previous layer's output and the original X. Lastly, we use a dense layer to process the combination of outputs from each convolution layer, illustrated as the Attention Graph Convolution Module in <ref type="figure" target="#fig_0">Fig 1.</ref> ?</p><formula xml:id="formula_2">m+1 vn = k i=1 ? i H k vn H 0 vn = ? m vn + X (3) ? vn = Dense({? 0 vn , ? 1 vn , ..., ? m vn }, ?)<label>(4)</label></formula><p>where Dense() is a dense layer that combines the outputs from every attention graph convolution layer. We now have the node representation ? for all vertices v ? G. For simplicity, we denote the graph as a matrix G with size n-by-c where each row is a node's representation.</p><formula xml:id="formula_3">G = (? v1 , ? v2 , ..., ? vn )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self Attention Pooling</head><p>To perform graph classification task, we would like to generate the graph-level representation from the node's representation. Most previous works use mean/max pooling <ref type="bibr" target="#b18">[19]</ref> or sort pooling <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b31">[32]</ref> to generate a graph representation vector by aggregating all node representation vectors. We believe that simple max/mean pooling or pooling after the sort is ineffective and unnecessary, and therefore propose a self-attention pooling layer as a replacement. The goal is to encode an arbitrary graph into a fixed size embedding matrix while maximizing the information underlying the nodes' representation. <ref type="figure" target="#fig_2">Figure 3</ref> presents a sample model showing how a coefficient matrix is generated for the attention pooling layer.</p><p>We use the attention mechanism by taking the graph node representation learned from the convolution module as the input to output the weights vector ?.</p><formula xml:id="formula_4">? = softmax(u 2 tanh(u 1 G T ))<label>(5)</label></formula><p>In this equation, u 1 and u 2 are weight matrices with the shape of c-by-c and c-by-r respectively, where r is a hyperparameter that we set for the number of subspaces to learn the graph representation from the node representation. When r ? 1, ? becomes a weight matrix instead of a vector, and Equation 5 can then be written as</p><formula xml:id="formula_5">B = softmax(u 2 tanh(u 1 G T ))<label>(6)</label></formula><p>Each row of B represents one node's weight in a different sub-space. The sof tmax function is performed along the second dimension of its input. We then conduct a weighted summation according to B from Equation 6 to obtain the graph representation matrix M with shape n-by-r. M = B? Generalize final node representation ? vn for node v n ? g. Equation (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Generalizing coefficient matrix for attention pooling layer. Equation <ref type="formula" target="#formula_5">(6)</ref> 12:</p><p>Weighted sum over graph g. Equation <ref type="formula">(7)</ref> 13:</p><p>Update the all weight parameters with stochastic gradient. 14: end for 15: return Y ? R n?|C| Equation <ref type="formula" target="#formula_6">(8)</ref> We now have a graph representation matrix in which each row is a graph representation in one sub-space, and the overall matrix produces a comprehensive representation for the graph. Lastly, a fully-connected layer followed by a softmax layer takes M as the input to accomplish the graph classification.</p><formula xml:id="formula_6">Y = sof tmax(ZM + C)<label>(8)</label></formula><p>We thus obtain the final classification result Y . The step algorithm is summarized in Algorithm 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND RESULTS</head><p>We construct two sets of experiments to evaluate DAGCN with both graph kernel and GCNs methods in a graph classification task. Both experiments are based on several popular benchmark datasets. The reported result shows that DAGCN outperforms the state-of-the-art deep learning methods and yields a competitive result compared to graph kernels. Details of the code and data are available at https://github.com/dawenzi123/DAGCN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets &amp; Baselines</head><p>We use seven benchmark bioinformatics datasets to evaluate our DAGCN model according to the accuracy of the graph classification task. The datasets used are: NCI1, D&amp;D, EN-ZYMES, NCI109, PROTEINS and PTC. Brief data information is listed in <ref type="table" target="#tab_0">Table I</ref>, and a detailed dataset description can be found in <ref type="bibr" target="#b37">[38]</ref>. For the baselines, we compare our framework with major families of graph kernels in the literature and some newly deep learning approaches. For the Graph Kernel Baselines, we compare DAGCN with five state-of-the-art graph kernels: a) Random Walk (RW) <ref type="bibr" target="#b38">[39]</ref>, b) Shortest Path Kernel (SP) <ref type="bibr" target="#b39">[40]</ref>, c) Graphlet Kernel (GK) <ref type="bibr" target="#b40">[41]</ref>, d) Weisfeiler-Lehman (WL) <ref type="bibr" target="#b6">[7]</ref>, and e) Deep Graph Kernels (DGK) <ref type="bibr" target="#b37">[38]</ref>. In the same benchmark datasets, we also compare our DAGCN model with four deep learning approaches for graph classification. Because of the large amount of literature related to GCN, we could not compare every method. DCNN, PSCN, ECC and DGCNN are four recently proposed state-of-the-art GCNs which are most related to our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Kernel Configuration</head><p>For the graph kernel parameter setting, the height parameters of WL and PK are chosen from the set {0, 1, 2, 3, 4, 5}. For the Random Walk (RW) kernel, we set the decay parameter as ?, following the suggestion in <ref type="bibr" target="#b6">[7]</ref>. Results for the others were borrowed from previous works <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>. All the experimental setups were the same so that a fair comparison could be made.</p><p>For PSCN, ECC and DGCNN, we adopted the best results from the paper <ref type="bibr" target="#b41">[42]</ref>, since their experiment settings are the same as ours. For DCNN, we conducted the experiment based on the standard setting discussed below. For fairness, we also removed the edge features from all datasets, as most of the graph data were missing edge features and the methods we compared do not leverage edge features.</p><p>We attempted not to fine tune our model to improve performance. The same configuration with rough default values were shared between two sets of experiences. The hidden layer size for all dense layers and convolution layers was set to 64, k was chosen from sets {1, 5, 10}, and the chosen number of hops was k ? {3, 5, 10}. For the general setting, we adopted the same procedure as previous works <ref type="bibr" target="#b31">[32]</ref> so that a fair comparison could be made. We used the Adam <ref type="bibr" target="#b42">[43]</ref> optimization policy with L2 regularization and learning rate selected from {0.01, 0.001, 0.0001} to ensure the best play of the model. The batch size was fixed as 50, and 10-fold cross validation was implemented (9 folds for training, 1 fold for testing) to report the average classification accuracy and standard deviations. <ref type="table" target="#tab_0">Table II</ref> shows the average classification accuracy of the compared deep learning methods."?" in the table means that either the source code is not available or the previous report did not contain a related result. From the results, we can see that our proposed model consistently outperforms all other methods on six of the seven datasets, and is second best on D&amp;D. In particular, there is a 7% improvement in classification accuracy on NCI1 and more than 8% on NCI109, with a 1% -3% accuracy gain on the other four datasets (excluding D&amp;D). DAGCN outperforms DCNN and ECC in every case, proving our hypothesis that simple summing the node features is ineffective and will result in the loss of topology information. PSCN performs about the same as our model on PROTEINS and PTC but is much worse on NCI1 because it is more likely to overfit predefined node ordering. We avoid this problem by using attention pooling which dynamically learns the valuable node distributions over the graph. The improvement achieved by DAGCN can be explained as follows. 1) By using an attention mechanism to aggregate different hop neighbors, DAGCN is able to access more information underlying the graph input, thus achieving better performance. 2) By using the attention pooling layer, DAGCN is able to capture multiple graph signatures on the fly without losing any individual node or global topology information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Result</head><p>We also compare DAGCN with state-of-the-art graph kernels. The result in table III show that DAGCN is very competitive with state-of-the-art graph kernels. Our model is consistent among the top-2 in terms of performance on all datasets. This is a 1% -3% improvement in accuracy on most datasets, with a high of 9% improvement for ENZYMES, compared with graph kernels other than WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CASE STUDY</head><p>The experiment results clearly demonstrate the classification performance of DAGCN compared with other deep learning GCNs. We also compare the efficiency of DAGCN with one of the most recent deep learning models, DGCNN, on NCI1, ENZYMES and NCI109, three benchmark datasets on which the learning process is observed to be relatively stable. Since the most significant learning process occurs in the early stages of training, we set the iteration number for both models on all datasets to 200. Although DAGCN has a Residual Learning structure to enhance performance, we limit the number of attention graph convolution layers m to 1 to make this comparison fair. The learning rate has the same setting as DGCNN's default, and all other parameters have the default setting previously mentioned. <ref type="figure">Figure 4</ref> shows that DAGCN not only achieves better classification accuracy, but also has a better rate of convergence.</p><p>Compared with deep learning methods, DAGCN has obvious advantages over graph kernels. Although the overall stateof-the-art in the graph classification task is still dominated by graph kernels, DAGCN is the most practical in its ability to address efficiency and several other issues which most graph kernels suffer from.</p><p>Computational complexity. Graph kernels first need to compute the similarity between each two graphs in the training dataset to form a similarity matrix. Given a dataset of size N , then N (N ? 1)/2 computation steps are required. This number will grow exponentially when the size of the dataset is increased. In addition, calculating the similarity between a pair of graphs is also an exponential operation based on the number of nodes in the graph. This limits the power of the graph kernels only working for small data-set with small graph. By design, the computational complexity of DAGCN grows linearly for both the dataset size and graph size.</p><p>Static graph features. Graph kernels can also broadly be divided into two parts. First, a similarity matrix is constructed by the pre-defined kernel function, and a deep learning model then learns the classification rules. The two steps are independent of each other. The first step can be envisaged as human feature engineering, after which, the features are fixed and are not optimized during the training process. Similar datasets might share some common features as a result of common natural properties (i.e., two bio-informatics datasets). But datasets from different fields must have different properties (e.g., social network and protein network). Although our model is also created from two modules, it still an end-to-end model. All parameters will be optimized during the training process giving DAGCN more advantage on generality.</p><p>Single structure. Due to their nature, graph kernels can only focus on a certain scope of graph according to their kernel function. As a result, either global structure or local properties are lost. Our attention pooling layer enables us to learn hierarchical structure information that includes both local and global properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have proposed a novel Dual Attention Graph Convolutional Network (DAGCN) model with the core idea of maximally exploiting the original information underlying the graph input. We used an attention mechanism to address the weakness of traditional GCN models, in which information is largely lost in every convolution step. Our attention convolution layer design is capable of capturing more hierarchical structure information than other models and provides a much more informative representation of both individual nodes and the whole graph. The attention pooling layer generates a fixed size, comprehensive graph representation matrix by using a self-attention mechanism to focus on the different aspects of graph. The experimental results show that our model outperforms other deep learning methods and most graph kernels in a range of datasets. In future work, We intend to implement and validate our model on more complex graphs such as EHRs data and social networks. We will also analyze graph convolution in greater depth to discover how information is distributed at different convolution level. Lastly, we observe that it would be better to test a larger number of attention architectures to mimic the nature of the dataset, since our model only employs one basic attention architecture for all datasets.</p><p>ACKNOWLEDGMENT This research was funded by the Australian Government through the Australian Research Council (ARC) under grants 1) LP160100630 partnership with Australia Government Department of Health and 2) LP150100671 partnership with Australia Research Alliance for Children and Youth (ARACY) and Global Business College Australia (GBCA). We acknowledge the support of NVIDIA Corporation and MakeMagic Australia with the donation of GPU used for this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of the dual attention graph convolution network (DAGCN). The model consists of three parts: (1) The left tier is the attention graph convolution module with three AGC layers (m = 3) which learns the hierarchical local substructure features by aggregating the hops of its neighbors. (2) The middle part is the attention pooling layer, the matrix B is the attention coefficient matrix. (3) The final graph embedding matrix M is then sent to a dense layer for final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Traditional Graph Convolution Layer (up): Only the final output which contains the largest sub-structure (k-hop neighbor substructure) is used. Attention Graph Convolution Layer (down): valuable information is extracted from every convolution step to generate a hierarchical node representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Process of generating Self-Attention Pooling coefficient matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 7 ) 1</head><label>71</label><figDesc>Algorithm Procedure of DAGCN Input: T : Iterations for updating. A: Unweighted adjacency matrix; v n : Feature vector of node V n K: The number of hops for convolution operation; M : The number of attention graph convolution layers Output: Y : Prediction outcome. 1: Model initialization. k, m ? 0, H 0 vn ? v n 2: for iterator = 1, 2, 3, ..., T do 3: for m = 1 to M do 4:for k = 1 to K do and prepare input for next layer. ? m vn ? H vn , H 0 vn ? H vn + X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>NCI1</cell><cell>D&amp;D</cell><cell cols="4">ENZYMES MUTAG NCI109 PROTEINS</cell><cell>PTC</cell></row><row><cell>Nodes (max)</cell><cell>111</cell><cell>5748</cell><cell>126</cell><cell>28</cell><cell>111</cell><cell>620</cell><cell>109</cell></row><row><cell>Nodes (avg.)</cell><cell cols="2">29.80 284.32</cell><cell>32.60</cell><cell>17.93</cell><cell>29.60</cell><cell>39.06</cell><cell>25.56</cell></row><row><cell>Graphs</cell><cell>4110</cell><cell>1178</cell><cell>600</cell><cell>188</cell><cell>4127</cell><cell>1113</cell><cell>344</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>61?1.04 42.44?1.76 -57.47?1.22 61.29?1.60 56.60?2.89 44?0.47 51.00?7.29 85.83?1.66 75.03?1.72 75.54?0.94 58.59?2.47 DAGCN 81.68?1.69 58.17?8.76 87.22?6.1 81.46?1.51 76.33?4.3 62.88?9.61 00?0.24 75.07?0.54 58.24?2.44 GK 62.28?0.29 26.61?0.99 81.39?1.74 62.60?0.19 71.67?0.55 57.26?1.41 WL 82.19?0.18 52.22?1.26 84.11?1.91 82.46?0.24 74.68?0.49 57.97?0.49 DGK 80.31?0.46 53.43?0.91 -80.32?0.33 75.68?0.54 60.08?2.55 DAGCN 81.68?1.69</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">WITH DEEP LEARNING METHODS</cell><cell></cell></row><row><cell>Dataset</cell><cell>NCI1</cell><cell>ENZYMES</cell><cell>MUTAG</cell><cell>NCI109</cell><cell>PROTEINS</cell><cell>PTC</cell></row><row><cell cols="2">DCNN 56.PSCN 76.34?1.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">75.00?2.51 62.29?5.68</cell></row><row><cell>ECC</cell><cell>76.82</cell><cell>45.67</cell><cell>-</cell><cell>75.03</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">DGCNN 74.TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">COMPARISON WITH GRAPH KERNELS</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>NCI1</cell><cell>ENZYMES</cell><cell>MUTAG</cell><cell>NCI109</cell><cell>PROTEINS</cell><cell>PTC</cell></row><row><cell>RW</cell><cell>-</cell><cell cols="2">24.16?1.64 79.17?2.07</cell><cell>&gt;1 Day</cell><cell cols="2">74.22?0.42 57.85?1.30</cell></row><row><cell>SP</cell><cell cols="4">73.00?0.24 40.10?1.50 73.58.17?8.76 -87.22?6.1 81.46?1.51</cell><cell>76.33?4.3</cell><cell>62.88?9.61</cell></row></table><note>Fig. 4. Learning curve for DAGCN (blue) and DGCNN (orange)</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contextual graph markov model: A deep and generative approach to graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10636</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three-dimensional shape pattern recognition using vertex classification and vertex-edge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="377" to="387" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="369" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">Sep</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint structure feature exploration and regularization for multi-task graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 32nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1474" to="1475" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tri-party deep network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding the best not the most: regularized loss minimization subgraph selection for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3783" to="3796" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="3546" to="3553" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On filter size in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10435</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9197" to="9207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
