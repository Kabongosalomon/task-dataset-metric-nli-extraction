<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCORE-BASED GENERATIVE MODELING WITH CRITICALLY-DAMPED LANGEVIN DIFFUSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
							<email>tim.dockhorn@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCORE-BASED GENERATIVE MODELING WITH CRITICALLY-DAMPED LANGEVIN DIFFUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered "velocities" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler-Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Score-based generative models (SGMs) and denoising diffusion probabilistic models have emerged as a promising class of generative models <ref type="bibr" target="#b90">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b94">Song et al., 2021c;</ref><ref type="bibr" target="#b81">b;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021;</ref>. SGMs offer high quality synthesis and sample diversity, do not require adversarial objectives, and have found applications in image <ref type="bibr" target="#b33">(Ho et al., 2020;</ref><ref type="bibr" target="#b77">Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr" target="#b77">Dhariwal &amp; Nichol, 2021;</ref>, speech <ref type="bibr" target="#b43">Jeong et al., 2021)</ref>, and music synthesis <ref type="bibr" target="#b69">(Mittal et al., 2021)</ref>, image editing <ref type="bibr" target="#b88">Sinha et al., 2021;</ref><ref type="bibr" target="#b25">Furusawa et al., 2021)</ref>, super-resolution , image-to-image translation <ref type="bibr" target="#b86">(Sasaki et al., 2021)</ref>, and 3D shape generation <ref type="bibr">(Luo &amp; Hu, 2021;</ref><ref type="bibr">Zhou et al., 2021)</ref>. SGMs use a diffusion process to gradually add noise to the data, transforming a complex data distribution into an analytically tractable prior distribution. A neural network is then utilized to learn the score function-the gradient of the log probability density-of the perturbed data. The learnt scores can be used to solve a stochastic differential equation (SDE) to synthesize new samples. This corresponds to an iterative denoising process, inverting the forward diffusion.</p><p>In the seminal work by <ref type="bibr" target="#b94">Song et al. (2021c)</ref>, it has been shown that the score function that needs to be learnt by the neural network is uniquely determined by the forward diffusion process. Consequently, the complexity of the learning problem depends, other than on the data itself, only on the diffusion. Hence, the diffusion process is the key component of SGMs that needs to be revisited to further improve SGMs, for example, in terms of synthesis quality or sampling speed. <ref type="figure">Figure 1</ref>: In critically-damped Langevin diffusion, the data xt is augmented with a velocity vt. A diffusion coupling xt and vt is run in the joint data-velocity space (probabilities in red). Noise is injected only into vt. This leads to smooth diffusion trajectories (green) for the data xt. Denoising only requires ?v t log p(vt|xt).</p><p>Inspired by statistical mechanics <ref type="bibr" target="#b98">(Tuckerman, 2010)</ref>, we propose a novel forward diffusion process, the critically-damped Langevin diffusion (CLD). In CLD, the data variable, x t (time t along the diffusion), is augmented with an additional "velocity" variable v t and a diffusion process is run in the joint data-velocity space. Data and velocity are coupled to each other as in Hamiltonian dynamics, and noise is injected only into the velocity variable. As in Hamiltonian Monte Carlo <ref type="bibr" target="#b24">(Duane et al., 1987;</ref><ref type="bibr" target="#b75">Neal, 2011)</ref>, the Hamiltonian component helps to efficiently traverse the joint data-velocity space and to transform the data distribution into the prior distribution more smoothly. We derive the corresponding score matching objective and show that for CLD the neural network is tasked with learning only the score of the conditional distribution of velocity given data ? vt log p t (v t |x t ), which is arguably easier than learning the score of diffused data directly. Using techniques from molecular dynamics <ref type="bibr" target="#b98">Tuckerman, 2010;</ref><ref type="bibr" target="#b57">Leimkuhler &amp; Matthews, 2013)</ref>, we also derive a new SDE integrator tailored to CLD's reverse-time synthesis SDE.</p><p>We extensively validate CLD and the novel SDE solver: (i) We show that the neural networks learnt in CLD-based SGMs are smoother than those of previous SGMs. (ii) On the CIFAR-10 image modeling benchmark, we demonstrate that CLD-based models outperform previous diffusion models in synthesis quality for similar network architectures and sampling compute budgets. We attribute these positive results to the Hamiltonian component in the diffusion and to CLD's easier score function target, the score of the velocity-data conditional distribution ? vt log p t (v t |x t ). (iii) We show that our novel sampling scheme for CLD significantly outperforms the popular Euler-Maruyama method. (iv) We perform ablations on various aspects of CLD and find that CLD does not have difficult-to-tune hyperparameters.</p><p>In summary, we make the following technical contributions: (i) We propose CLD, a novel diffusion process for SGMs. (ii) We derive a score matching objective for CLD, which requires only the conditional distribution of velocity given data. (iii) We propose a new type of denoising score matching ideally suited for scalable training of CLD-based SGMs. (iv) We derive a tailored SDE integrator that enables efficient sampling from CLD-based models. (v) Overall, we provide novel insights into SGMs and point out important new connections to statistical mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Consider a diffusion process u t ? R d defined by the It? SDE</p><formula xml:id="formula_0">du t = f (u t , t) dt + G(u t , t) dw t , t ? [0, T ],<label>(1)</label></formula><p>with continuous time variable t ? [0, T ], standard Wiener process w t , drift coefficient f :</p><formula xml:id="formula_1">R d ? [0, T ] ? R d and diffusion coefficient G : R d ?[0, T ] ? R d?d .</formula><p>Defining? t := u T ?t , a corresponding reverse-time diffusion process that inverts the above forward diffusion can be derived <ref type="bibr" target="#b2">(Anderson, 1982;</ref><ref type="bibr" target="#b31">Haussmann &amp; Pardoux, 1986;</ref><ref type="bibr" target="#b94">Song et al., 2021c</ref>) (with positive dt and t ? [0, T ]):</p><formula xml:id="formula_2">d?t = ?f (?t, T ? t) + G(?t, T ? t)G(?t, T ? t) ?? t log pT ?t(?t) dt + G(?t, T ? t)dwt,<label>(2)</label></formula><p>where ?? t log p T ?t (? t ) is the score function of the marginal distribution over? t at time T ? t.</p><p>The reverse-time process can be used as a generative model. In particular, <ref type="bibr" target="#b94">Song et al. (2021c)</ref> model data x, setting p(u 0 )=p data (x). Currently used SDEs <ref type="bibr" target="#b94">(Song et al., 2021c;</ref> have drift and diffusion coefficients of the simple form f (x t , t)=f (t)x t and G(x t , t)=g(t)I d . Generally, f and G are chosen such that the SDE's marginal, equilibrium density is approximately Normal at time T , i.e., p(u T )?N (0, I d ). We can then initialize x 0 based on a sample drawn from a complex data distribution, corresponding to a far-from-equilibrium state. While the state x 0 relaxes towards equilibrium via the forward diffusion, we can learn a model s ? (x t , t) for the score ? xt log p t (x t ), which can be used for synthesis via the reverse-time SDE in Eq.</p><p>(2). If f and G take the simple form from above, the denoising score matching <ref type="bibr" target="#b102">(Vincent, 2011)</ref> objective for this task is:</p><formula xml:id="formula_3">min ? E t?U [0,T ] E x0?p(x0) E xt?pt(xt|x0) ?(t) s ? (x t , t) ? ? xt log p t (x t |x 0 ) 2 2<label>(3)</label></formula><p>If f and G are affine, the conditional distribution p t (x t |x 0 ) is Normal and available analytically <ref type="bibr" target="#b85">(S?rkk? &amp; Solin, 2019)</ref>. Different ?(t) result in different trade-offs between synthesis quality and likelihood in the generative model defined by s ? (x t , t) <ref type="bibr" target="#b93">(Song et al., 2021b;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CRITICALLY-DAMPED LANGEVIN DIFFUSION</head><p>We propose to augment the data x t ? R d with auxiliary velocity 1 variables v t ? R d and utilize a diffusion process that is run in the joint</p><formula xml:id="formula_4">x t -v t -space. With u t = (x t , v t ) ? R 2d , we set f (u t , t) := 0 ?M ?1 ?? ???M ?1 ? I d u t , G(u t , t) := 0 0 0 ? 2?? ? I d ,<label>(4)</label></formula><p>where ? denotes the Kronecker product. The coupled SDE that describes the diffusion process is</p><formula xml:id="formula_5">dx t dv t = M ?1 v t ?x t ?dt Hamiltonian component=:H + 0 d ??M ?1 v t ?dt + 0 ? 2?? dw t Ornstein-Uhlenbeck process=:O ,<label>(5)</label></formula><p>which corresponds to Langevin dynamics in each dimension. That is, each x i is independently coupled to a velocity v i , which explains the blockwise structure of f and G. The mass M ? R + is a hyperparameter that determines the coupling between the x t and v t variables; ? ? R + is a constant time rescaling chosen such that the diffusion converges to its equilibrium distribution within t ? [0, T ] (in practice, we set T =1) when initialized from a data-defined non-equilibrium state and is analogous to ?(t) in previous diffusions (we could also use time-dependent ?(t), but found constant ?'s to work well, and therefore opted for simplicity); ? ? R + is a friction coefficient that determines the strength of the noise injection into the velocities. Notice that the SDE in Eq. (5) consists of two components. The H term represents a Hamiltonian component. Hamiltonian dynamics are frequently used in Markov chain Monte Carlo methods to accelerate sampling and efficiently explore complex probability distributions <ref type="bibr" target="#b75">(Neal, 2011)</ref>. The Hamiltonian component in our diffusion process plays a similar role and helps to quickly and smoothly converge the initial joint data-velocity distribution to the equilibrium, or prior (see <ref type="figure">Fig. 1</ref>). Furthermore, Hamiltonian dynamics on their own are trivially invertible <ref type="bibr" target="#b98">(Tuckerman, 2010)</ref>, which intuitively is also beneficial in our situation when using this diffusion for training SGMs. The O term corresponds to an Ornstein-Uhlenbeck process <ref type="bibr" target="#b85">(S?rkk? &amp; Solin, 2019)</ref> in the velocity component, which injects noise such that the diffusion dynamics properly converge to equilibrium for any ?&gt;0. It can be shown that the equilibrium distribution of this diffusion is p EQ (u) = N (x; 0 d ,</p><formula xml:id="formula_6">I d ) N (v; 0 d , M I d ) (see App. B.2).</formula><p>There is a crucial balance between M and ? (McCall, 2010): For ? 2 &lt;4M (underdamped Langevin dynamics) the Hamiltonian component dominates, which implies oscillatory dynamics of x t and v t that slow down convergence to equilibrium. For ? 2 &gt;4M (overdamped Langevin dynamics) the Oterm dominates which also slows down convergence, since the accelerating effect by the Hamiltonian component is suppressed due to the strong noise injection. For ? 2 =4M (critical damping), an ideal balance is achieved and convergence to p EQ (u) occurs as fast as possible in a smooth manner without oscillations (also see discussion in App. A.1) <ref type="bibr" target="#b66">(McCall, 2010)</ref>. Hence, we propose to set ? 2 =4M and call the resulting diffusion critically-damped Langevin diffusion (CLD) (see <ref type="figure">Fig. 1</ref>).</p><p>Diffusions such as the VPSDE <ref type="bibr" target="#b94">(Song et al., 2021c)</ref> correspond to overdamped Langevin dynamics with high friction coefficients ? (see App. A.2). Furthermore, in previous works noise is injected directly into the data variables (pixels, for images). In CLD, only the velocity variables are subject to direct noise and the data is perturbed only indirectly due to the coupling between x t and v t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SCORE MATCHING OBJECTIVE</head><p>Considering the appealing convergence properties of CLD, we propose to utilize CLD as forward diffusion process in SGMs.</p><p>To this end, we initialize the joint</p><formula xml:id="formula_7">p(u 0 )=p(x 0 ) p(v 0 )=p data (x 0 )N (v 0 ; 0 d , ?M I d )</formula><p>with hyperparameter ?&lt;1 and let the distribution diffuse towards the tractable equilibrium-or prior-distribution p EQ (u). We can then learn the corresponding score functions and define CLD-based SGMs. Following a similar derivation as <ref type="bibr" target="#b93">Song et al. (2021b)</ref>, we obtain the score matching (SM) objective (see App. B.3):</p><formula xml:id="formula_8">min ? E t?U [0,T ] E ut?pt(ut) ?(t) s ? (u t , t) ? ? vt log p t (u t ) 2 2 (6)</formula><p>Notice that this objective requires only the velocity gradient of the log-density of the joint distribution, i.e., ? vt log p t (u t ). This is a direct consequence of injecting noise into the velocity variables only. Without loss of generality,</p><formula xml:id="formula_9">p t (u t )=p t (x t , v t )=p t (v t |x t )p t (x t ). Hence, ? vt log p t (u t ) = ? vt [log p t (v t |x t ) + log p t (x t )] = ? vt log p t (v t |x t )<label>(7)</label></formula><p>Figure 2: Top: Difference ?(t) (via L2 norm) between score of diffused data and score of Normal distribution. Bottom: Frobenius norm of Jacobian JF (t) of the neural network defining the score function for different t. The underlying data distribution is a mixture of Normals. Insets: Different axes (see App. E.1 for detailed definitions of ?(t) and JF (t)).</p><p>This means that in CLD the neural network-defined score model s ? (u t , t) only needs to learn the score of the conditional distribution p t (v t |x t ), an arguably easier task than learning the score of p t (x t ), as in previous works, or of the joint p t (u t ). This is the case, because our velocity distribution is initialized from a simple Normal distribution, such that p t (v t |x t ) is closer to a Normal distribution for all t?0 (and for any x t ) than p t (x t ) itself. This is most evident at t=0: The data and velocity distributions are independent at t=0 and the score of p 0 (v 0 |x 0 )=p 0 (v 0 ) simply corresponds to the score of the Normal distribution p 0 (v 0 ) from which the velocities are initialized, whereas the score of the data distribution p 0 (x 0 ) is highly complex and can even be unbounded . We empirically verify the reduced complexity of the score of p t (v t |x t ) in <ref type="figure">Fig. 2</ref>. We find that the score that needs to be learnt by the model is more similar to a score corresponding to a Normal distribution for CLD than for the VPSDE. We also measure the complexity of the neural networks that were learnt to model this score via the squared Frobenius norm of their Jacobians. We find that the CLD-based SGMs have significantly simpler and smoother neural networks than VPSDE-based SGMs for most t, in particular when leveraging a mixed score formulation (see next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCALABLE TRAINING</head><p>A Practical Objective. We cannot train directly with Eq. (6), since we do not have access to the marginal distribution p t (u t ). As presented in Sec. 2, we could employ denoising score matching (DSM) and instead sample u 0 , and diffuse those samples, which would lead to a tractable objective. However, recall that in CLD the distribution at t=0 is the product of a complex data distribution and a Normal distribution over the initial velocity. Therefore, we propose a hybrid version of score matching <ref type="bibr" target="#b39">(Hyv?rinen, 2005)</ref> and denoising score matching <ref type="bibr" target="#b102">(Vincent, 2011)</ref>, which we call hybrid score matching (HSM). In HSM, we draw samples from p 0 (x 0 )=p data (x 0 ) as in DSM, but then diffuse those samples while marginalizing over the full initial velocity distribution p 0 (v 0 )=N (v; 0 d , ?M I d ) as in regular SM (HSM is discussed in detail in App. C). Since p 0 (v 0 ) is Normal (and f and G affine), p(u t |x 0 ) is also Normal and this remains tractable. We can write this HSM objective as:</p><formula xml:id="formula_10">min ? E t?[0,T ] E x0?p0(x0) E ut?pt(ut|x0) ?(t) s ? (u t , t) ? ? vt log p t (u t |x 0 ) 2 2 .<label>(8)</label></formula><p>In HSM, the expectation over p 0 (v 0 ) is essentially solved analytically, while DSM would use a sample-based estimate. Hence, HSM reduces the variance of training objective gradients compared to pure DSM, which we validate in App. C.1. Furthermore, when drawing a sample u 0 to diffuse in DSM, we are essentially placing an infinitely sharp Normal with unbounded score  at u 0 , which requires undesirable modifications or truncation tricks for stable training <ref type="bibr" target="#b94">(Song et al., 2021c;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021)</ref>. Hence, with DSM we could lose some benefits of the CLD framework discussed in Sec. 3.1, whereas HSM is tailored to CLD and fundamentally avoids such unbounded scores. Closed form expressions for the perturbation kernel p t (u t |x 0 ) are provided in App. B.1.</p><p>Score Model Parametrization. (i) <ref type="bibr" target="#b33">Ho et al. (2020)</ref> found that it can be beneficial to parameterize the score model to predict the noise that was used in the reparametrized sampling to generate perturbed samples u t . For CLD,</p><formula xml:id="formula_11">u t = ? t (x 0 ) + L t 2d , where ? t = L t L t is the Cholesky de- composition of p t (u t |x 0 )'s covariance matrix, 2d ? N ( 2d ; 0 2d , I 2d ), and ? t (x 0 ) is p t (u t |x 0 )'s mean. Furthermore, ? vt log p t (u t |x 0 ) = ? t d:2d</formula><p>, where d:2d denotes those d components of 2d that actually affect ? vt log p t (u t |x 0 ) (since we take velocity gradients only, not all are relevant).</p><formula xml:id="formula_12">With ? t = ? xx t ? xv t ? xv t ? vv t "per-dimension" covariance matrix ? I d , we have t := ? xx</formula><p>which corresponds to training the model to predict the noise only injected into the velocity during reparametrized sampling of u t , similar to noise prediction in <ref type="bibr" target="#b33">Ho et al. (2020)</ref>; <ref type="bibr" target="#b94">Song et al. (2021c)</ref>.</p><p>Objective Weightings. For ?(t) = ??, the objective corresponds to maximum likelihood learning <ref type="bibr" target="#b93">(Song et al., 2021b</ref>) (see App. B.3). Analogously to prior work <ref type="bibr" target="#b33">(Ho et al., 2020;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021;</ref><ref type="bibr" target="#b93">Song et al., 2021b)</ref>, an objective better suited for high quality image synthesis can be obtained by setting ?(t) = ?2 t , which corresponds to "dropping the variance prefactor" 2 t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SAMPLING FROM CLD-BASED SGMS</head><p>To sample from the CLD-based SGM we can either directly simulate the reverse-time diffusion process (Eq. (2)) or, alternatively, solve the corresponding probability flow ODE <ref type="bibr" target="#b94">(Song et al., 2021c;</ref><ref type="bibr" target="#b81">b)</ref> (see App. B.5). To simulate the SDE of the reverse-time diffusion process, previous works often relied on Euler-Maruyama (EM) <ref type="bibr" target="#b53">(Kloeden &amp; Platen, 1992)</ref> and related methods <ref type="bibr" target="#b33">(Ho et al., 2020;</ref><ref type="bibr" target="#b94">Song et al., 2021c;</ref><ref type="bibr" target="#b45">Jolicoeur-Martineau et al., 2021a)</ref>. We derive a new solver, tailored to CLD-based models. Here, we provide the high-level ideas and derivations (see App. D for details).</p><p>Our generative SDE can be written as</p><formula xml:id="formula_13">(with? t = u T ?t ,x t = x T ?t ,v t = v T ?t ): dxt dvt = ?M ?1v t xt ?dt A H + 0 d ??M ?1v t ?dt + 0 d ? 2??dwt A O + 0 d 2? s(?t, T ? t) + M ?1v t ?dt S</formula><p>It consists of a Hamiltonian component A H , an Ornstein-Uhlenbeck process A O , and the score model term S. We could use EM to integrate this SDE; however, standard Euler methods are not well-suited for Hamiltonian dynamics <ref type="bibr" target="#b59">(Leimkuhler &amp; Reich, 2005;</ref><ref type="bibr" target="#b75">Neal, 2011)</ref>. Furthermore, if S was 0, we could solve the SDE in closed form. This suggests the construction of a novel integrator.</p><p>We use the Fokker-Planck operator 2 formalism <ref type="bibr" target="#b98">(Tuckerman, 2010;</ref><ref type="bibr" target="#b57">Leimkuhler &amp; Matthews, 2013;</ref>. Using a similar notation as <ref type="bibr" target="#b57">Leimkuhler &amp; Matthews (2013)</ref>, the Fokker-Planck equation corresponding to the generative SDE is ?p t (? t )/?t=(L * A +L * S )p t (? t ), whereL * A andL * S are the non-commuting Fokker-Planck operators corresponding to the A:=A H +A O and S terms, respectively. Expressions forL * A andL * S can be found in App. D. We can construct a formal, but intractable solution of the generative SDE as? t = e t(L * A +L * S )? 0 , where the operator e t(L * A +L * S ) (known as the classical propagator in statistical physics) propagates states? 0 for time t according to the dynamics defined by the combined operatorsL * A +L * S . Although this operation is not analytically tractable, it can serve as starting point to derive a practical integrator. Using the symmetric Trotter theorem or Strang splitting formula as well as the Baker-Campbell-Hausdorff formula <ref type="bibr" target="#b96">(Trotter, 1959;</ref><ref type="bibr" target="#b95">Strang, 1968;</ref><ref type="bibr" target="#b98">Tuckerman, 2010)</ref>, it can be shown that:</p><formula xml:id="formula_14">e t(L * A +L * S ) = lim N ?? e ?t 2L * A e ?tL * S e ?t 2L * A N ? e ?t 2L * A e ?tL * S e ?t 2L * A N + O(N ?t 3 ),<label>(10)</label></formula><p>for large N ? N + and time step ?t := t/N . The expression suggests that instead of directly evaluating the intractable e t(L * A +L * S ) , we can discretize the dynamics over t into N pieces of step size ?t, such that we only need to apply the individual e ?t 2L * A and e ?tL * S many times one after another for small steps ?t. A finer discretization results in a smaller error (since N =t/?t, the error effectively scales as O(?t 2 ) for fixed t). Hence, this implies an integration method. Indeed, e ?t 2L * A? t is available in closed form, as mentioned before; however, e ?tL * S? t is not. Therefore, we approximate this latter component of the integrator via a standard Euler step. Thus, the integrator formally has an error of the same order as standard EM methods. Nevertheless, as long as the dynamics is not dominated by the S component, our proposed integration scheme is expected to be more accurate than EM, since we split off the analytically tractable part and only use an Euler approximation for the S term. Recall that the model only needs to learn the score of the conditional distribution p t (v t |x t ), which is close to Normal for much of the diffusion, in which case the S term will indeed be small. This suggests that the generative SDE dynamics are in fact dominated by A H and A O in practice. Note that only the propagator e ?tL * S is computationally expensive, as it involves evaluating the neural network. We coin our novel SDE integrator for CLD-based SGMs Symmetric Splitting CLD Sampler (SSCS). A detailed derivation, analyses, and a formal algorithm are presented in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Relations to Statistical Mechanics and Molecular Dynamics. Learning a mapping between a simple, tractable and a complex distribution as in SGMs is inspired by annealed importance sampling <ref type="bibr" target="#b74">(Neal, 2001)</ref> and the Jarzynski equality from non-equilibrium statistical mechanics <ref type="bibr" target="#b40">(Jarzynski, 1997a;</ref><ref type="bibr" target="#b81">b;</ref><ref type="bibr" target="#b6">Bahri et al., 2020)</ref>. However, after <ref type="bibr" target="#b90">Sohl-Dickstein et al. (2015)</ref>, little attention has been given to the origins of SGMs in statistical mechanics. Intuitively, in SGMs the diffusion process is initialized in a non-equilibrium state u 0 and we would like to bring the system to equilibrium, i.e., the tractable prior distribution, as quickly and as smoothly as possible to enable efficient denoising. This "equilibration problem" is a much-studied problem in statistical mechanics, particularly in molecular dynamics, where a molecular system is often simulated in thermodynamic equilibrium. Algorithms to quickly and smoothly bring a system to and maintain at equilibrium are known as thermostats. In fact, CLD is inspired by the Langevin thermostat . In molecular dynamics, advanced thermostats are required in particular for "multiscale" systems that show complex behaviors over multiple time-and length-scales. Similar challenges also arise when modeling complex data, such as natural images. Hence, the vast literature on thermostats <ref type="bibr" target="#b1">(Andersen, 1980;</ref><ref type="bibr" target="#b78">Nos?, 1984;</ref><ref type="bibr" target="#b35">Hoover, 1985;</ref><ref type="bibr" target="#b38">H?nenberger, 2005;</ref><ref type="bibr" target="#b13">Ceriotti et al., 2009;</ref><ref type="bibr" target="#b98">2010;</ref><ref type="bibr" target="#b98">Tuckerman, 2010</ref>) may be valuable for the development of future SGMs. Also the framework for developing SSCS is borrowed from statistical mechanics. The same techniques have been used to derive molecular dynamics algorithms <ref type="bibr" target="#b14">Ceriotti et al., 2010;</ref><ref type="bibr" target="#b57">Leimkuhler &amp; Matthews, 2013;</ref><ref type="bibr" target="#b56">Kreis et al., 2017)</ref>.</p><p>Further Related Work. Generative modeling by learning stochastic processes has a long history <ref type="bibr" target="#b72">(Movellan, 2008;</ref><ref type="bibr" target="#b63">Lyu, 2009;</ref><ref type="bibr" target="#b89">Sohl-Dickstein et al., 2011;</ref><ref type="bibr" target="#b0">Alain et al., 2016;</ref><ref type="bibr" target="#b29">Goyal et al., 2017;</ref><ref type="bibr" target="#b9">Bordes et al., 2017;</ref><ref type="bibr" target="#b92">Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b33">Ho et al., 2020)</ref>. We build on <ref type="bibr" target="#b94">Song et al. (2021c)</ref>, which introduced the SDE framework for modern SGMs.  recently introduced non-Gaussian diffusion processes with different noise distributions. However, the noise is still injected directly into the data, and no improved sampling schemes or training objectives are introduced. <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref> proposed LSGM, which is complementary to CLD: we improve the diffusion process itself, whereas LSGM "simplifies the data" by first embedding it into a smooth latent space. LSGM is an overall more complicated framework, as it is trained in two stages and relies on additional encoder and decoder networks. Recently, techniques to accelerate sampling from pre-trained SGMs have been proposed <ref type="bibr" target="#b103">Watson et al., 2021;</ref><ref type="bibr" target="#b91">Song et al., 2021a)</ref>. Importantly, these methods usually do not permit straightforward loglikelihood estimation. Furthermore, they are originally not based on the continuous time framework, which we use, and have been developed primarily for discrete-step diffusion models.</p><p>A complementary work to CLD is "Gotta Go Fast" (GGF) <ref type="bibr" target="#b45">(Jolicoeur-Martineau et al., 2021a)</ref>, which introduces an adaptive SDE solver for SGMs, tuned towards image synthesis. GGF uses standard Euler-based methods under the hood <ref type="bibr" target="#b53">(Kloeden &amp; Platen, 1992;</ref><ref type="bibr" target="#b82">Roberts, 2012)</ref>, in contrast to our SSCS that is derived from first principles. Furthermore, our SDE integrator for CLD does not make any data-specific assumptions and performs extremely well even without adaptive step sizes.</p><p>Some works study SGMs for maximum likelihood training <ref type="bibr" target="#b93">(Song et al., 2021b;</ref><ref type="bibr" target="#b55">Huang et al., 2021)</ref>. Note that we did not focus on training our models towards high likelihood. Furthermore,  and <ref type="bibr" target="#b36">Huang et al. (2020)</ref> recently trained augmented Normalizing Flows, which have conceptual similarities with our velocity augmentation. Methods leveraging auxiliary variables similar to our velocities are also used in statistics-such as Hamiltonian Monte Carlo (Neal, 2011)-and have found applications, for instance, in Bayesian machine learning <ref type="bibr" target="#b21">Ding et al., 2014;</ref><ref type="bibr" target="#b87">Shang et al., 2015)</ref>. As shown in <ref type="bibr" target="#b64">Ma et al. (2019)</ref>, our velocity is equivalent to momentum in gradient descent and related methods <ref type="bibr" target="#b81">(Polyak, 1964;</ref><ref type="bibr" target="#b50">Kingma &amp; Ba, 2015)</ref>. Momentum accelerates optimization; the velocity in CLD accelerates mixing in the diffusion process. Lastly, our CLD method can be considered as a second-order Langevin algorithm, but even higher-order schemes are possible <ref type="bibr" target="#b71">(Mou et al., 2021)</ref> and could potentially further improve SGMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Architectures. We focus on image synthesis and implement CLD-based SGMs using NCSN++ and DDPM++ <ref type="bibr" target="#b94">(Song et al., 2021c)</ref> with 6 input channels (for velocity and data) instead of 3.</p><p>Relevant Hyperparameters. CLD's hyperparameters are chosen as ?=4, ?=1 (or equivalently M ?1 =4) in all experiments. We set the variance scaling of the inital velocity distribution to ?=0.04 and use the proposed HSM objective with the weighting ?(t)= ?2 t , which promotes image quality. Sampling. We generate model samples via: (i) Probability flow using a Runge-Kutta 4(5) method; reverse-time generative SDE sampling using either (ii) EM or (iii) our SSCS. For methods without adaptive stepsize (EM and SSCS), we use evaluation times chosen according to a quadratic function, like previous work <ref type="bibr" target="#b91">(Song et al., 2021a;</ref><ref type="bibr" target="#b103">Watson et al., 2021)</ref> (indicated by QS).</p><p>Evaluation. We measure image sample quality for CIFAR-10 via Fr?chet inception distance (FID) with 50k samples <ref type="bibr" target="#b32">(Heusel et al., 2017)</ref>. We also evaluate an upper bound on the negative log-</p><formula xml:id="formula_15">likelihood (NLL): ? log p(x 0 )??E v0?p(v0) log p ? (x 0 , v 0 )?H, where H is the entropy of p(v 0 ) and log p ? (x 0 , v 0 )</formula><p>is an unbiased estimate of log p(x 0 , v 0 ) from the probability flow ODE <ref type="bibr" target="#b30">(Grathwohl et al., 2019;</ref><ref type="bibr" target="#b94">Song et al., 2021c)</ref>. As in <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref>, the stochasticity of log p ? (x, v) prevents us from performing importance weighted NLL estimation over the velocity distribution <ref type="bibr" target="#b10">(Burda et al., 2015)</ref>. We also record the number of function-neural network-evaluations (NFEs) during synthesis when comparing sampling methods. All implementation details in App. B.5 and E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">IMAGE GENERATION</head><p>Following <ref type="bibr" target="#b94">Song et al. (2021c)</ref>, we focus on the widely used CIFAR-10 unconditional image generation benchmark. Our CLD-based SGM achieves an FID of 2.25 based on the probability flow ODE and an FID of 2.23 via simulating the generative SDE (Tab. 1). The only models marginally outperforming CLD are LSGM <ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref> and NSCN++/VESDE with 2,000 step predictorcorrector (PC) sampling <ref type="bibr" target="#b94">(Song et al., 2021c)</ref>. However, LSGM uses a model with ?475M parameters to achieve its high performance, while we obtain our numbers with a model of ?100M parameters. For a fairer comparison, we trained a smaller LSGM also with ?100M parameters, which is reported as "LSGM-100M" in Tab. 1 (details in App. E.2.7). Our model has a significantly better FID score than "LSGM-100M". In contrast to NSCN++/VESDE, we achieve extremely strong results with much fewer NFEs (for example, see n?{150, 275, 500} in Tab. 3 and also Tab. 2)-the VESDE performs poorly in this regime. We conclude that when comparing models with similar  <ref type="bibr" target="#b94">(Song et al., 2021c)</ref> 2.99 2.92 DDPM++, sub-VP (SDE) <ref type="bibr" target="#b94">(Song et al., 2021c)</ref> -2.41 NCSN++, VESDE (SDE) <ref type="bibr" target="#b94">(Song et al., 2021c)</ref> -2.20 LSGM <ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref> ?3.43 2.10 LSGM-100M <ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref> ?2.96 4.60 DDPM <ref type="bibr" target="#b33">(Ho et al., 2020)</ref> ?3.75 3.17 NCSN <ref type="bibr" target="#b92">(Song &amp; Ermon, 2019)</ref> -25.3 Adversarial DSM (Jolicoeur-Martineau et al., 2021b) -6.10 Likelihood SDE <ref type="bibr" target="#b93">(Song et al., 2021b)</ref> 2.84 2.87 DDIM (100 steps) <ref type="bibr" target="#b91">(Song et al., 2021a)</ref> -4.16 FastDDPM (100 steps)  -2.86 Improved DDPM <ref type="bibr" target="#b77">(Nichol &amp; Dhariwal, 2021)</ref> 3.37 2.90 VDM  ?2.49 7.41 (4.00) UDM  3.04 2.33 D3PM <ref type="bibr">(Austin et al., 2021)</ref> ?3   network capacity and under NFE budgets ?500, our CLD-SGM outperforms all published results in terms of FID. We attribute these positive results to our easier score matching task. Furthermore, our model reaches an NLL bound of 3.31, which is on par with recent works such as Nichol &amp; Dhariwal (2021); <ref type="bibr">Austin et al. (2021)</ref>; <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref> and indicates that our model is not dropping modes. However, our bound is potentially quite loose (see discussion in App. B.5) and the true NLL might be significantly lower. We did not focus on training our models towards high likelihood.</p><p>To demonstrate that CLD is also suitable for high-resolution image synthesis, we additionally trained a CLD-SGM on CelebA-HQ-256, but without careful hyperparameter tuning due to limited compute resources. Model samples in <ref type="figure" target="#fig_1">Fig. 4</ref> appear diverse and high-quality (additional samples in App. F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SAMPLING SPEED AND SYNTHESIS QUALITY TRADE-OFFS</head><p>We analyze the sampling speed vs. synthesis quality trade-off for CLD-SGMs and study SSCS's performance under different NFE budgets (Tabs. 2 and 3). We compare to <ref type="bibr" target="#b94">Song et al. (2021c)</ref> and use EM to solve the generative SDE for their VPSDE and PC (reverse-diffusion + Langevin sampler) for the VESDE model. We also compare to the GGF (Jolicoeur-Martineau et al., 2021a) solver for the generative SDE as well as probability flow ODE sampling with a higher-order adaptive step size solver. Further, we compare to LSGM <ref type="bibr" target="#b104">(Vahdat et al., 2021</ref>) (using our LSGM-100M), which also uses probability flow sampling. With one exception (VESDE with 2,000 NFE) our CLD-SGM outperforms all baselines, both for adaptive and fixed-step size methods. More results in App. F.2.</p><p>Several observations stand out: (i) As expected (Sec. 3.3), for CLD, SSCS significantly outperforms EM under limited NFE budgets. When using a fine discretization of the SDE (high NFE), the two perform similarly, which is also expected, as the errors of both methods will become negligible.</p><p>(ii) In the adaptive solver setting, using a simpler ODE solver, we even outperform GGF, which is tuned towards image synthesis. (iii) Our CLD-SGM also outperforms the LSGM-100M model in terms of FID. It is worth noting, however, that LSGM was designed primarily for faster synthesis, which it achieves by modeling a smooth distribution in latent space instead of the more complex data distribution directly. This suggests that it would be promising to combine LSGM with CLD and train a CLD-based LSGM, combining the strengths of the two approaches. It would also be interesting to develop a more advanced, adaptive SDE solver that leverages SSCS as the backbone  and, for example, potentially test our method within a framework like GGF. Our current SSCS only allows for fixed step sizes-nevertheless, it achieves excellent performance.  We perform ablation studies to study CLD's new hyperparameters (run with a smaller version of our CLD-SGM used above; App. E for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ABLATION STUDIES</head><p>Mass Parameter: Tab. 4 shows results for a CLD-SGM trained with different M ?1 (also recall that M ?1 and ? are tied together via ? 2 = 4M ; we are always in the critical-damping regime). Different mass values perform mostly similarly. Intuitively, training with smaller M ?1 means that noise flows from the velocity variables v t into the data x t more slowly, which necessitates a larger time rescaling ?. We found that simply tying M ?1 and ? together via ?=8 ? M works well and did not further fine-tune.</p><p>Initial Velocity Distribution: Tab. 5 shows results for a CLD-SGM trained with different initial velocity variance scalings ?. Varying ? similarly has only a small effect, but small ? seems slightly beneficial for FID, while the NLL bound suffers a bit. Due to our focus on synthesis quality as measued by FID, we opted for small ?. Intuitively, this means that the data at t=0 is "at rest", and noise flows from the velocity into the data variables only slowly.</p><p>Mixed Score: Similar to previous work <ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref>, we find training with the mixed score (MS) parametrization (Sec. 3.2) beneficial. With MS, we achieve an FID of 3.14, without only 3.56.</p><p>Hybrid Score Matching: We also tried training with regular DSM, instead of HSM. However, training often became unstable. As discussed in Sec. 3.2, this is likely because when using standard DSM our CLD would suffer from unbounded scores close to t=0, similar to previous SDEs . Consequently, we consider our novel HSM a crucial element for training CLD-SGMs.</p><p>We conclude that CLD does not come with difficult-to-tune hyperparameters. We expect our chosen hyperparameters to immediately translate to new tasks and models. In fact, we used the same M ?1 , ?, MS and HSM settings for CIFAR-10 and CelebA-HQ-256 experiments without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We presented critically-damped Langevin diffusion, a novel diffusion process for training SGMs. CLD diffuses the data in a smoother, easier-to-denoise manner compared to previous SGMs, which results in smoother neural network-parametrized score functions, fast synthesis, and improved expressivity. Our experiments show that CLD outperforms previous SGMs on image synthesis for similar-capacity models and sampling compute budgets, while our novel SSCS is superior to EM in CLD-based SGMs. From a technical perspective, in addition to proposing CLD, we derive CLD's score matching objective termed as HSM, a variant of denoising score matching suited for CLD, and we derive a tailored SDE integrator for CLD. Inspired by methods used in statistical mechanics, our work provides new insights into SGMs and implies promising directions for future research.</p><p>We believe that CLD can potentially serve as the backbone diffusion process of next generation SGMs. Future work includes using CLD-based SGMs for generative modeling tasks beyond images, combining CLD with techniques for accelerated sampling from SGMs, adapting CLD-based SGMs towards maximum likelihood, and utilizing other thermostating methods from statistical mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ETHICS AND REPRODUCIBILITY</head><p>Our paper focuses on fundamental algorithmic advances to improve the generative modeling performance of SGMs. As such, the proposed CLD does not imply immediate ethical concerns. However, we validate CLD on image synthesis benchmarks. Generative modeling of images has promising applications, for example for digital content creation and artistic expression <ref type="bibr" target="#b7">(Bailey, 2020)</ref>, but can also be used for nefarious purposes <ref type="bibr" target="#b99">(Vaccari &amp; Chadwick, 2020;</ref><ref type="bibr">Mirsky &amp; Lee, 2021;</ref><ref type="bibr" target="#b76">Nguyen et al., 2021)</ref>. It is worth mentioning that compared to generative adversarial networks <ref type="bibr" target="#b28">(Goodfellow et al., 2014)</ref>, a very popular class of generative models, SGMs have the promise to model the data more faithfully, without dropping modes and introducing problematic biases. Generally, the ethical impact of our work depends on its application domain and the task at hand.</p><p>To aid reproducibility of the results and methods presented in our paper, we made source code to reproduce the main results of the paper publicly available, including detailed instructions; see our project page https://nv-tlabs.github.io/CLD-SGM and the code repository https: //github.com/nv-tlabs/CLD-SGM. Furthermore, all training details and hyperparameters are already in detail described in the Appendix, in particular in App. E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTENTS</head><formula xml:id="formula_16">dx t dv t = M ?1 v t ?x t ?dt Hamiltonian component=:H + 0 d ??M ?1 v t ?dt + 0 ? 2?? dw t Ornstein-Uhlenbeck process=:O .<label>(11)</label></formula><p>A.1 DIFFERENT DAMPING RATIOS As discussed in Sec. 3, Langevin dynamics can be run with different ratios between mass M and squared friction ? 2 . To recap from the main paper:</p><p>(i) For ? 2 &lt; 4M (underdamped Langevin dynamics), the Hamiltonian component dominates, which implies oscillatory dynamics of x t and v t that slow down convergence to equilibrium.</p><p>(ii) For ? 2 &gt; 4M (overdamped Langevin dynamics), the O-term dominates which also slows down convergence, since the accelerating effect by the Hamiltonian component is suppressed due to the strong noise injection.</p><p>(iii) For ? 2 = 4M (critical-damping), an ideal balance is achieved and convergence to p EQ (u)occurs quickly in a smooth manner without oscillations.</p><p>In <ref type="figure" target="#fig_2">Fig. 5</ref>, we visualize diffusion trajectories according to Langevin dynamics run in the different damping regimes. We observe that underdamped Langevin dynamics show undesired oscillatory behavior, while overdamped Langevin dynamics perform very inefficiently, too. Critical-damping achieves a good balance between the two and mixes and converges quickly. In fact, it can be shown to be optimal in terms of convergence; see, for example, <ref type="bibr" target="#b66">McCall (2010)</ref>.</p><p>Consequently, we propose to set ? 2 = ? 2 critical := 4M in CLD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 VERY HIGH FRICTION LIMIT AND CONNECTIONS TO PREVIOUS SDES IN SGMS</head><p>Let us re-write the above Langevin dynamics and consider the more general case with timedependent ?(t):</p><formula xml:id="formula_17">dx t = M ?1 v t ?(t)dt,<label>(12)</label></formula><formula xml:id="formula_18">dv t = ? x t ?(t)dt (ii): potential term ? ?M ?1 v t ?(t)dt (iii): friction term + 2??(t)dw t (iv): noise term .<label>(13)</label></formula><p>To solve this SDE, let us assume a simple Euler-based integration scheme, with the update equation for a single step at time t (this integration scheme would not be optimal, as discussed in Sec. 3.3., however, it would be accurate for sufficiently small time steps and we just need this to make the connection to previous works like the VPSDE):</p><formula xml:id="formula_19">x n+1 = x n + ?(t)M ?1 v n+1 ?t,<label>(14)</label></formula><formula xml:id="formula_20">v n+1 = v n (i): current step velocity ? ?(t)x n ?t (ii): potential term ? ?(t)?M ?1 v n ?t (iii): friction term + 2?(t)?N (0 d , ?tI d ) (iv): noise term ,<label>(15)</label></formula><p>Now, let us assume a friction coefficient ? = ? max := M ?(t)?t . Since the time step ?t is usually very small, this correspond to a very high friction. In fact, it can be considered the maximum friction limit, at which the friction is so large that the current step velocity (i) is completely cancelled out by the friction term (iii). We obtain: A.2). The trajectories correspond to different Langevin trajectories run in the different regimes with indicated friction coefficients ?. We see in (b), that for critical damping the xt trajectories quickly explore the space and converge according to the distribution indicated by the underlying probability. In the under-damped regime (a), even though the trajectories mix quickly we observe undesired oscillatory behavior. For over-damped Langevin dynamics, (c) and (d), the xt trajectories mix and converge only very slowly. Note that the visualized diffusion uses different hyperparameters compared to the diffusion shown in <ref type="figure">Fig. 1</ref> in the main text: Here, we have chosen a much larger ?, such that also the slow overdamped Langevin dynamics trajectories shown here mix a little bit over the visualized diffusion time (while the probability distribution and the trajectories for critical damping converge almost instantly). Now the velocity update, Eq. <ref type="formula" target="#formula_0">(17)</ref>, does not depend on the current step velocity on the right-handside anymore. Hence, we can insert Eq. (17) directly into Eq. (16) and obtain:</p><formula xml:id="formula_21">x n+1 = x n + ?(t)M ?1 v n+1 ?t (16) v n+1 = ??(t)x t ?t + 2 M ?t N (0 d , ?tI d ).<label>(17)</label></formula><formula xml:id="formula_22">x n+1 = x n ? ?(t) 2 M ?1 x n ?t 2 + 2?(t) 2 ?tM ?1 N (0 d , ?tI d ) = x n ? ?(t) 2 M ?1 x n ?t 2 + 2?(t) 2 ?t 2 M ?1 N (0 d , I d ).<label>(18)</label></formula><p>Re-defining ?t := ?t 2 and ? (t) := ?(t) 2 , we obtain</p><formula xml:id="formula_23">x n+1 = x n ? ? (t)M ?1 x n ?t + 2? (t)?t M ?1 N (0 d , I d ),<label>(19)</label></formula><p>which corresponds to the high-friction overdamped Langevin dynamics that are frequently run, for example, to train energy-based generative models <ref type="bibr" target="#b23">(Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b104">Xiao et al., 2021)</ref>. Let's further absorb the mass M ?1 and the time step ?t into the time rescaling, defining?(t) := 2? (t)M ?1 ?t . We obtain:</p><formula xml:id="formula_24">x n+1 = x n ? 1 2? (t)x n + ? (t)N (0 d , I d ) = (1 ? 1 2? (t))x n + ? (t)N (0 d , I d ) ? 1 ??(t)x n + ? (t)N (0 d , I d ),<label>(20)</label></formula><p>where the last approximation is true for sufficiently small?(t). However, this expression corresponds to</p><formula xml:id="formula_25">x n+1 ? N (x n+1 ; 1 ??(t)x n ,?(t)I d )<label>(21)</label></formula><p>which is exactly the transition kernel of the VPSDE's Markov chain <ref type="bibr" target="#b33">(Ho et al., 2020;</ref><ref type="bibr" target="#b94">Song et al., 2021c)</ref>. We see that the VPSDE corresponds to the high-friction limit of a more general Langevin dynamics-based diffusion process of the form of Eq. (11).</p><p>If we assume a diffusion as above but with the potential term (ii) set to 0, we can similarly derive the VESDE Song et al. (2021c) as a high-friction limit of the corresponding diffusion. Generally, all previously used diffusions that inject noise directly into the data variables correspond to such high-friction diffusions.</p><p>In conclusion, we see that previous high-friction diffusions require an excessive amount of noise to be injected to bring the dynamics to the prior, which intuitively makes denoising harder. For our CLD in the critical damping regime we can run the diffusion for a much shorter time or, equivalently, can inject less noise to converge to the equilibrium, i.e., the prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CRITICALLY-DAMPED LANGEVIN DIFFUSION</head><p>Here, we present further details about our proposed critically-damped Langevin diffusion (CLD). We provide the derivations and formulas that were not presented in the main paper in the interest of brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 PERTURBATION KERNEL</head><p>To recap from the main text, in this work we propose to augment the data x t ? R d with auxiliary velocity variables v t ? R d . We then run the following diffusion process in the joint x t -v t -space</p><formula xml:id="formula_26">du t := dx t dv t = f (u t , t)dt + G(u t , t)dw t (22) f (u t , t) = (f (t) ? I d )u t , f (t) := 0 ?(t)M ?1 ??(t) ??(t)?M ?1 ,<label>(23)</label></formula><formula xml:id="formula_27">G(u t , t) = G(t) ? I d , G(t) := 0 0 0 2??(t) ,<label>(24)</label></formula><p>where w t is a standard Wiener process in R 2d and ? : [0, T ] ? R + 0 is a time rescaling. <ref type="bibr">3</ref> In particular, we consider the critically-damped Langevin diffusion which can be obtained by setting M = ? 2 /4, resulting in the following drift kernel</p><formula xml:id="formula_28">f CLD (u t , t) = (f CLD (t) ? I d )u t , f CLD (t) := 0 4?(t)? ?2 ??(t) ?4?(t)? ?1 .<label>(25)</label></formula><p>Since we only consider the critically-damped case in this work, we redefine f := f CLD and f := f CLD for simplicity. Since our drift f and diffusion G coefficients are affine, u t is Normally distributed for all t ? [0, T ] if u 0 is Normally distributed at t = 0 <ref type="bibr" target="#b85">(S?rkk? &amp; Solin, 2019)</ref>. In particular, given that</p><formula xml:id="formula_29">u 0 ? N (u 0 ; ? 0 , ? 0 = ? 0 ? I d ), where ? 0 = diag(? xx 0 , ? vv 0 )</formula><p>is a positive semi-definite diagonal 2-by-2 matrix (we restrict our derivation to diagonal covariance matrices at t = 0 for simplicity, since in our situation velocity and data are generally independent at t = 0), we derive expressions for ? t and ? t , the mean and the covariance matrix of u t , respectively.</p><p>Following S?rkk? &amp; Solin (2019) (Section 6.1), the mean and covariance matrix of u t obey the following respective ordinary differential equations (ODEs)</p><formula xml:id="formula_30">d? t dt = (f (t) ? I d )? t ,<label>(26)</label></formula><formula xml:id="formula_31">d? t dt = (f (t) ? I d )? t + [(f (t) ? I d )? t ] + G(t)G(t) ? I d .<label>(27)</label></formula><p>Notating ? 0 = (x 0 , v 0 ) , the solutions to the above ODEs are</p><formula xml:id="formula_32">? t = 2B(t)? ?1 x 0 + 4B(t)? ?2 v 0 + x 0 ?B(t)x 0 ? 2B(t)? ?1 v 0 + v 0 e ?2B(t)? ?1 ,<label>(28)</label></formula><p>and</p><formula xml:id="formula_33">? t = ? t ? I d ,<label>(29)</label></formula><formula xml:id="formula_34">? t = ? xx t ? xv t ? xv t ? vv t e ?4B(t)? ?1 ,<label>(30)</label></formula><formula xml:id="formula_35">? xx t = ? xx 0 + e 4B(t)? ?1 ? 1 + 4B(t)? ?1 (? xx 0 ? 1) + 4B 2 (t)? ?2 (? xx 0 ? 2) + 16B(t) 2 ? ?4 ? vv 0 , (31) ? xv t = ?B(t)? xx 0 + 4B(t)? ?2 ? vv 0 ? 2B 2 (t)? ?1 (? xx 0 ? 2) ? 8B 2 (t)? ?3 ? vv 0 ,<label>(32)</label></formula><formula xml:id="formula_36">? vv t = ? 2 4 e 4B(t)? ?1 ? 1 + B(t)? + ? vv 0 1 + 4B(t) 2 ? ?2 ? 4B(t)? ?1 + B(t) 2 (? xx 0 ? 2) ,<label>(33)</label></formula><p>where B(t) = t 0 ?(t) dt. For constant ?(t) = ? (as is used in all our experiments), we simply have B(t) = t?. The correctness of the proposed mean and covariance matrix can be verified by simply plugging them back into their respective ODEs; see App. G.1.</p><p>With the above derivations, we can find analytical expressions for the perturbation kernel p(u t |?). For example, when conditioning on initial data and velocity samples x 0 and v 0 (as in denoising score matching (DSM)), the mean and covariance matrix of the perturbation kernel p(u t |u 0 ) can be obtained by setting ? 0 = (x 0 , v 0 ) , ? xx 0 = 0, and ? vv 0 = 0. In our experiments, the initial velocity distribution is set to N (0 d , ?M I d ). Conditioning only on initial data samples x 0 and marginalizing over the full initial velocity distribution (as in our hybrid score matching (HSM), see Sec. C), the mean and covariance matrix of the perturbation kernel p(u t |x 0 ) can be obtained by setting ? 0 = (x 0 , 0 d ) , ? xx 0 = 0, and ? vv 0 = ?M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 CONVERGENCE AND EQUILIBRIUM</head><p>Our CLD-based training of SGMs-as well as denoising diffusion models more generally-relies on the fact that the diffusion converges towards an analytically tractable equilibrium distribution for sufficiently large t. In fact, from the above equations we can easily see that,</p><formula xml:id="formula_37">lim t?? ? xx t = 1,<label>(34)</label></formula><formula xml:id="formula_38">lim t?? ? xv t = 0,<label>(35)</label></formula><formula xml:id="formula_39">lim t?? ? vv t = ? 2 4 = M,<label>(36)</label></formula><formula xml:id="formula_40">lim t?? ? t = 0 2d ,<label>(37)</label></formula><p>which establishes p EQ (u) = N (</p><formula xml:id="formula_41">x; 0 d , I d ) N (v; 0 d , M I d ).</formula><p>Notice that our CLD is an instantiation of the more general Langevin dynamics defined by</p><formula xml:id="formula_42">dx t dv t = M ?1 v t ? xt log p pot (x t ) ?dt + 0 d ??M ?1 v t ?dt + 0 ? 2?? dw t .<label>(38)</label></formula><p>which has the equilibrium distributionp EQ (u) = p pot (x) N (v; 0 d , M I d ) <ref type="bibr" target="#b58">(Leimkuhler &amp; Matthews, 2015;</ref><ref type="bibr" target="#b98">Tuckerman, 2010)</ref>. However, the perturbation kernel of this Langevin dynamics is not available analytically anymore for arbitrary p pot (x). In our case, however, we have the analytically tractable p pot (x) = N (x; 0 d , I d ). Note that this corresponds to the classical "harmonic oscillator" problem from physics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 CLD OBJECTIVE</head><p>To derive the objective for training CLD-based SGMs, we start with a derivation that targets maximum likelihood training in a similar fashion to <ref type="bibr" target="#b93">Song et al. (2021b)</ref>. Let p 0 and q 0 be two densities, then</p><formula xml:id="formula_43">D KL (p 0 q 0 ) = D KL (p 0 q 0 ) ? D KL (p T q T ) + D KL (p T q T ) = ? T 0 ?D KL (p t q t ) ?t dt + D KL (p T q T ),<label>(39)</label></formula><p>where p t and q t are the marginal densities of p 0 and q 0 , respectively, diffused by our criticallydamped Langevin diffusion. As has been shown in <ref type="bibr" target="#b93">Song et al. (2021b)</ref>, Eq. (39) can be written as a mixture (over t) of score matching losses. To this end, let us consider the Fokker-Planck equation associated with the critically-damped Langevin diffusion:</p><formula xml:id="formula_44">?p t (u t ) ?t = ? ut ? 1 2 G(t)G(t) ? I d ? ut p t (u t ) ? p t (u t )(f (t) ? I d )u t = ? ut ? [h p (u t , t)p t (u t )] , h p (u t , t) := 1 2 G(t)G(t) ? I d ? ut log p t (u t ) ? (f (t) ? I d )u t .<label>(40)</label></formula><p>Similarly, we have ?qt(ut)</p><formula xml:id="formula_45">?t = ? ut ? [h q (u t , t)q t (u t )]</formula><p>. Assuming log p t (u t ) and log q t (u t ) are smooth functions with at most polynomial growth at infinity, we have</p><formula xml:id="formula_46">lim ut?? h p (u t , t)p t (u t ) = lim ut?? h q (u t , t)q t (u t ) = 0.<label>(41)</label></formula><p>Using the above fact, we can compute the time-derivative of the Kullback-Leibler divergence between p t and q t as</p><formula xml:id="formula_47">?D KL (p t q t ) ?t = ? ?t p t (u t ) log p t (u t ) q t (u t ) du t = ? p t (u t ) [h p (u t , t) ? h q (u t , t)] [? ut log p t (u t ) ? ? ut log q t (u t )] du t = ? 1 2 p t (u t ) [? ut log p t (u t ) ? ? ut log q t (u t )] G(t)G(t) ? I d [? ut log p t (u t ) ? ? ut log q t (u t )] du t = ??(t)? p t (u t ) ? vt log p t (u t ) ? ? vt log q t (u t ) 2 2 du t .<label>(42)</label></formula><p>Notice that due to the form of G(t), we now have only gradients with respect to the velocity component v t . Combining the above with Eq. (39), we have</p><formula xml:id="formula_48">D KL (p 0 q 0 ) = E t?U [0,T ],ut?pt(u) ??(t) ? vt log p t (u t ) ? ? vt log q t (u t ) 2 2 + D KL (p T q T ) ? E t?U [0,T ],ut?pt(u) ??(t) ? vt log p t (u t ) ? ? vt log q t (u t ) 2 2 ,<label>(43)</label></formula><p>Note that the approximation holds if p T is sufficiently "close" to q T . We obtain a more general objective function by replacing ??(t) with an arbitrary function ?(t), i.e.,</p><formula xml:id="formula_49">E t?U [0,T ],ut?pt(u) ?(t) ? vt log p t (u t ) ? ? vt log q t (u t ) 2 2<label>(44)</label></formula><p>As shown in App. C, the above can be rewritten, up to irrelevant constant terms, as either of the following two objectives:</p><formula xml:id="formula_50">HSM(?(t)) := E t?U [0,T ],x0?p0(x0),ut?pt(ut|x0) ?(t) ? vt log p t (u t | x 0 ) ? ? vt log q t (u t ) 2 2 ,<label>(45)</label></formula><formula xml:id="formula_51">DSM(?(t)) := E t?U [0,T ],u0?p0(u0),ut?pt(ut|u0) ?(t) ? vt log p t (u t | u 0 ) ? ? vt log q t (u t ) 2 2 .</formula><p>(46) For both HSM and DSM, we have shown in App. B.1 that the perturbation kernels p t (u t | x 0 ) and p t (u t | u 0 ) are Normal distributions with the following structure of the covariance matrix:</p><formula xml:id="formula_52">? t = ? t ? I d , ? t = ? xx t ? xv t ? xv t ? vv t .<label>(47)</label></formula><p>We can use this fact to compute the gradient</p><formula xml:id="formula_53">? ut log p t (u t | ?) ? ut log p t (u t | ?) = ?? ut 1 2 (u t ? ? t )? ?1 t (u t ? ? t ) = ?? ?1 t (u t ? ? t ) = ?L ? t L ?1 t (u t ? ? t ) = ?L ? t 2d ,<label>(48)</label></formula><p>where 2d ? N (0, I 2d ) and ? t = L t L t is the Cholesky factorization of the covariance matrix ? t . Note that the structure of ? t implies that</p><formula xml:id="formula_54">L t = L t ? I d , where L t L t is the Cholesky factorization of ? t , i.e, L t = L xx t L xv t L xv t L vv t = ? ? ? xx t 0 ? xv t ? ? xx t ? xx t ? vv t ?(? xv t ) 2 ? xx t ? ? .<label>(49)</label></formula><p>Furthermore, we have</p><formula xml:id="formula_55">L ? t = L ? t ? I d = ? ? ? xx t ? xv t ? ? xx t 0 ? xx t ? vv t ?(? xv t ) 2 ? xx t ? ? ?1 ? I d = ? ? 1 ? ? xx t ?? xz t ? ? xx t ? ? xx t ? zz t ?(? xv t ) 2 0 ? xx t ? xx t ? vv t ?(? xv t ) 2 ? ? ? I d .<label>(50)</label></formula><p>Using the above, we can compute</p><formula xml:id="formula_56">? vt log p t (u t | ?) = [? ut log p t (u t | ?)] d:2d = ?L ? t 2d d:2d = ? t d:2d ,<label>(51)</label></formula><p>where</p><formula xml:id="formula_57">t := ? xx t ? xx t ? vv t ? (? xv t ) 2 ,<label>(52)</label></formula><p>and d:2d denotes those (latter) d components of 2d that actually affect ? vt log p t (u t |?) .</p><p>Note that t depends on the conditioning in the perturbation kernel, and therefore t is different for DSM, which is based on p(u t | u 0 ), and HSM, which is based on p(u t | x 0 ). Therefore, we will henceforth refer to HSM t and DSM t if distinction of the two cases is necessary (otherwise we will simply refer to t for both).</p><p>As discussed in Section 3.2, we model ? vt log q t (u t ) as s ? (u t , t) = ? t ? ? (u t , t). Plugging everything back into our objective functions, Eq. (45) and Eq. <ref type="formula" target="#formula_4">(46)</ref>, we obtain where u t is sampled via reparameterization:</p><formula xml:id="formula_58">HSM(?(t)) = E t?U [0,T ],x0?p0(x0),ut?pt(ut|x0) ?(t) HSM t 2 d:2d ? ? ? (u t , t) 2 2 ,<label>(53)</label></formula><formula xml:id="formula_59">DSM(?(t)) = E t?U [0,T ],u0?p0(u0),ut?pt(ut|u0) ?(t) DSM t 2 d:2d ? ? ? (u t , t) 2 2 ,<label>(54)</label></formula><formula xml:id="formula_60">u t = ? t + L t = ? t + L xx t 0:d L xv t 0:d + L vv t d:2d .<label>(55)</label></formula><p>Note again that L t is different for HSM and DSM.</p><p>Analogously to prior work <ref type="bibr" target="#b33">(Ho et al., 2020;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021;</ref><ref type="bibr" target="#b93">Song et al., 2021b)</ref> an objective better suited for high quality image synthesis can be obtained by "dropping the variance prefactor": is diverging for t ? 0. In practice, however, we found that computation of HSM t can also be numerically unstable, even when using double precision. As is common practice for computing Cholesky decompositions, we add a numerical stabilization matrix num I 2d to ? t before computing t . In <ref type="figure" target="#fig_3">Fig. 6</ref>, we visualize HSM t and DSM t for different values of num using our main experimental setup of M = 0.25 and ? = 0.04 (also, recall that in practice we have T = 1). Note that a very small numerical stabilization of num = 10 ?9 in combination with the use of double precision makes HSM work in practice.</p><formula xml:id="formula_61">HSM ?(t) = HSM t ?2 = E t?U [0,T ],x0?p0(x0),ut?pt(ut|x0) d:2d ? ? ? (u t , t) 2 2 , (56) DSM ?(t) = DSM t ?2 = E t?U [0,T ],u0?p0(u0),ut?pt(ut|u0) d:2d ? ? ? (u t , t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 LOWER BOUNDS AND PROBABILITY FLOW ODE</head><p>Given the score model s ? (u t , t), we can synthesize novel samples via simulating the reverse-time diffusion SDE, Eq. (2) in the main text. This can be achieved, for example, via our novel SSCS, Published as a conference paper at ICLR 2022 Euler-Maruyama, or methods such as GGF <ref type="bibr" target="#b45">(Jolicoeur-Martineau et al., 2021a)</ref>. However, <ref type="bibr" target="#b93">Song et al. (2021b;</ref><ref type="bibr" target="#b1">c)</ref> have shown that a corresponding ordinary differential equation can be defined that generates samples from the same distribution, in case s ? (u t , t) models the ground truth scores perfectly. This ODE is:</p><formula xml:id="formula_62">d? t = ?f (? t , T ? t) + 1 2 G(? t , T ? t)G(? t , T ? t) ?? t log p T ?t (? t ) dt<label>(58)</label></formula><p>This ODE is often referred to as the probability flow ODE. We can use it to generate novel data by sampling the prior and solving this ODE, like previous works <ref type="bibr" target="#b94">(Song et al., 2021c)</ref>. Note that in practice s ? (u t , t) won't be a perfect model, though, such that the generative models defined by simulating the reverse-time SDE and the probability flow ODE are not exactly equivalent <ref type="bibr" target="#b93">(Song et al., 2021b)</ref>. Nevertheless, they are very closely connected and it has been shown that their performance is usually very similar or almost the same, when we have learnt a good s ? (u t , t). In addition to sampling the generative SDE in our paper, we also sample from our CLD-based SGMs via this probability flow approach.</p><p>With the definition of our CLD, the ODE becomes:</p><formula xml:id="formula_63">dx t dv t = ?M ?1v t x t ?dt A H + 0 d ? s(? t , T ? t) + M ?1v t ?dt S<label>(59)</label></formula><p>Notice the interesting form of this probability flow ODE for CLD: It corresponds to Hamiltonian dynamics (A H ) plus the score function term S . Compared to the generative SDE (Sec. 3.3), the Ornstein-Uhlenbeck term disappears. Generally, symplectic integrators are best suited for integrating Hamiltonian systems <ref type="bibr" target="#b75">(Neal, 2011;</ref><ref type="bibr" target="#b98">Tuckerman, 2010;</ref><ref type="bibr" target="#b59">Leimkuhler &amp; Reich, 2005)</ref>. However, our ODE is not perfectly Hamiltonian, due to the score term, and modern non-symplectic methods, such as the higher-order adaptive-step size Runge-Kutta 4(5) ODE integrator <ref type="bibr" target="#b22">(Dormand &amp; Prince, 1980)</ref>, which we use in practice to solve the probability flow ODE, can also accurately simulate Hamiltonian systems over limited time horizons.</p><p>Importantly, the ODE formulation also allows us to estimate the log-likelihood of given test data, as it essentially defines a continuous Normalizing flow <ref type="bibr" target="#b17">(Chen et al., 2018;</ref><ref type="bibr" target="#b30">Grathwohl et al., 2019)</ref>, that we can easily run in either direction. However, in CLD the input into this ODE is not just the data x 0 , but also the velocity variable v 0 . In this case, we can still calculate a lower bound on the log-likelihood:</p><formula xml:id="formula_64">log p(x 0 ) = log p(x 0 , v 0 )dv 0 = log p(v 0 ) p(x 0 , v 0 ) p(v 0 ) dv 0 ? E v0?p(v0) [log p(x 0 , v 0 ) ? log p(v 0 )] = E v0?p(v0) [log p(x 0 , v 0 )] + H(p(v 0 ))<label>(60)</label></formula><p>where H(p(v 0 )) denotes the entropy of p(v 0 ) (we have H(p(v 0 )) = 1 2 log (2?e?M )). We can obtain a stochastic, but unbiased estimate of log p(x 0 , v 0 ) ? log p ? (x 0 , v 0 ) via solving the probability flow ODE with initial conditions (x 0 , v 0 ) and calculating a stochastic estimate of the logdeterminant of the Jacobian via Hutchinson's trace estimator (and also calculating the probability of the output under the prior), as done in Normalizing flows <ref type="bibr" target="#b17">(Chen et al., 2018;</ref><ref type="bibr" target="#b30">Grathwohl et al., 2019)</ref> and previous works on SGMs <ref type="bibr" target="#b94">(Song et al., 2021c;</ref><ref type="bibr" target="#b81">b)</ref>. In the main paper, we report the negative of Eq. (60) as our upper bound on the negative log-likelihood (NLL).</p><p>Note that this bound can be potentially quite loose. In principle, it would be desirable to perform an importance-weighted estimation of the log-likelihood, as in importance-weighted autoencoders <ref type="bibr" target="#b10">(Burda et al., 2015)</ref>, using multiple samples from the velocity distribution. However, this isn't possible, as we only have access to a stochastic estimate log p ? (x 0 , v 0 ). The problems arising from this are discussed in detail in Appendix F of <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref>. We could consider training a velocity encoder network, somewhat similar to , to improve our bound, but we leave this for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 ON INTRODUCING A HAMILTONIAN COMPONENT INTO THE DIFFUSION</head><p>Here, we provide additional high-level intuitions and motivations about adding the Hamiltonian component to the diffusion process, as is done in our CLD.</p><p>Let us recall how the data distribution evolves in the forward diffusion process of SGMs: The role of the diffusion is to bring the initial non-equilibrium state quickly towards the equilibrium or prior distribution. Suppose for a moment, we could do so with "pure" Hamiltonian dynamics (no noise injection). In that case, we could generate data from the backward model without learning a score or neural network at all, because Hamiltonian dynamics is analytically invertible (flipping the sign of the velocity, we can just integrate backwards in reverse time direction). Now, this is not possible in practice, since Hamiltonian dynamics alone usually cannot convert the non-equilibrium distribution to the prior distribution. Nevertheless, Hamiltonian dynamics essentially achieves a certain amount of mixing on its own; moreover, since it is deterministic and analytically invertible, this mixing comes at no cost in the sense that we do not have to learn a complex score function to invert the Hamiltonian dynamics. Our thought experiment shows that we should strive for a diffusion process that behaves as deterministically (meaning that deterministic implies easily invertible) as possible with as little noise injection as possible. And this is exactly what is achieved by adding the Hamiltonian component in the overall diffusion process. In fact, recall that it is the diffusion coefficient G of the forward SDE that ultimately scales the score function term of the backward generative SDE (and it is the score function that is hard to approximate with complex neural nets). Therefore, in other words, relying more on a deterministic Hamiltonian component for enhanced mixing (mixing just like in MCMC in that it brings us quickly towards the target distribution, in our case the prior) and less on pure noise injection will lead to a nicer generative SDE that relies less on a score function that requires complex and approximate neural network-based modeling, but more on a simple and analytical Hamiltonian component. Such an SDE could then be solved easier with an appropriate integrator (like our SSCS). In the end, we believe that this is the reason why our networks are "smoother" and why given the same network capacity and limited compute budgets we essentially outperform all previous results in the literature (on CIFAR-10).</p><p>We would also like to offer a second perspective, inspired by the Markov chain Monte Carlo (MCMC) literature. In MCMC, "mixing" helps to quickly traverse the high probability parts of the target distribution and, if an MCMC chain is initialized far from the high probability manifold, to quickly converge to this manifold. However, this is precisely the situation we are in with the forward diffusion process of SGMs: The system is initialized in a far-from-equilibrium state (the data distribution) and we need to traverse the space as efficiently as possible to converge to the equilibrium distribution, this is, the prior. Without efficient mixing, it takes longer to converge to the prior, which also implies a longer generation path in the reverse direction-which intuitively corresponds to a harder problem. Therefore, we believe that ideas from the MCMC literature that accelerate mixing and traversal of state space may be beneficial also for the diffusions in SGMs. In fact, leveraging Hamiltonian dynamics to accelerate sampling is popular in the MCMC field <ref type="bibr" target="#b75">(Neal, 2011)</ref>. Note that this line of reasoning extends to thermostating techniques from statistical mechanics and molecular dynamics, which essentially tackle similar problems like MCMC methods from the statistics literature (see discussion in Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HSM: HYBRID SCORE MATCHING</head><p>We begin by recalling our objective function from <ref type="bibr">App. B.3 (Eq. (44)</ref>):</p><formula xml:id="formula_65">E t?U [0,T ] ?(t)E ut?pt(u) [ ? vt log p t (u t ) ? s ? (u t , t) 2 2 ] ,<label>(61)</label></formula><p>where s ? (u, t) is our score model. In the following, we dissect the "score matching" part of the above objective:</p><formula xml:id="formula_66">L SM := E ut?pt(ut) ? vt log p t (u t ) ? s ? (u t , t) 2 2 = E ut?pt(ut) s ? (u t , t) 2 2 ? 2S(?) + C 2 (t).<label>(62)</label></formula><p>where C 2 (t) := E ut?pt(ut) ? vt log p t (u t ) 2 2 and S(?) is the cross term discussed below. Following <ref type="bibr" target="#b102">Vincent (2011)</ref>, we can rewrite L SM as an equivalent (up to addition of a time-dependent constant) denoising score matching objective L DSM :</p><formula xml:id="formula_67">L DSM := E u0?p(u0),ut?pt(ut|u0) ? vt log p t (u t | u 0 ) ? s ? (u t , t) 2 2 = L SM + C 3 (t) ? C 2 (t),<label>(63)</label></formula><p>where C 3 (t) := E u0?p(u0),ut?pt(ut|u0) ? vt log p t (u t | u 0 ) 2 2 . Something that might not necessarily be quite obvious is that there is no fundamental need to "denoise" with the distribution p(u 0 ) (this is, use samples from the joint x 0 -v 0 distribution p(u 0 ), perturb them, and learn the score for denoising).</p><p>Instead, we can "denoise" only with the data distribution p(x 0 ) and marginalize over the entire initial velocity distribution p(v 0 ), which results in</p><formula xml:id="formula_68">L HSM := E x0?p(x0),ut?pt(ut|x0) ? vt log p t (u t | x 0 ) ? s ? (u t , t) 2 2 = E ut?pt(ut) s ? (u t , t) 2 2 ? 2E x0?p(x0),ut?pt(ut|x0) [ ? vt log p t (u t | x 0 ), s ? (u t , t) ] + C 4 (t),<label>(64)</label></formula><p>where C 4 (t) := E x0?p(x0),ut?pt(ut|x0) ? vt log p t (u t | x 0 ) 2 2 and ?, ? donates the inner product (notation chosen to be consistent with <ref type="bibr" target="#b102">Vincent (2011)</ref>). In our case, this makes sense since p(v 0 ) is Normal, and therefore (as shown in App B.1), the perturbation kernel p t (u t | x 0 ) is still Normal.</p><p>In the following, for completeness, we redo the derivation of <ref type="bibr" target="#b102">Vincent (2011)</ref> and show that L SM is equivalent to L HSM (up to addition of a constant). Starting from S(?), we have</p><formula xml:id="formula_69">S(?) = E ut?pt(ut) ? vt log p t (u t ), s ? (u t , t) = ut p t (u t ) ? vt log p t (u t ), s ? (u t , t) du t = ut ? vt p t (u t ), s ? (u t , t) du t = ut ? vt x0 p t (u t | x 0 )p 0 (x 0 ) dx 0 , s ? (u t , t) du t = ut x0 p t (u t | x 0 )p 0 (x 0 )? vt log p t (u t | x 0 ) dx 0 , s ? (u t , t) du t = ut x0 p t (u t | x 0 )p 0 (x 0 ) ? vt log p t (u t | x 0 ), s ? (u t , t) dx 0 du t = E x0?p0(x0),ut?p(ut|x0) [ ? vt log p t (u t | x 0 ), s ? (u t , t) ] .<label>(65)</label></formula><p>Hence, we have that</p><formula xml:id="formula_70">L HSM = L SM + C 4 (t) ? C 2 (t).<label>(66)</label></formula><p>This further implies that</p><formula xml:id="formula_71">L HSM = L DSM + C 4 (t) ? C 3 (t).<label>(67)</label></formula><p>Using the analysis from App B.1, we realize that C 3 and C 4 can be simplified to d DSM </p><formula xml:id="formula_72">L HSM = L DSM + d DSM t 2 ? HSM t 2 .<label>(68)</label></formula><p>Using this relation, we can also find a connection between our CLD objective functions from App. B.3. In particular, we have</p><formula xml:id="formula_73">HSM(?(t)) = E t?U [0,T ] [?(t)L HSM ] = E t?U [0,T ] [?(t)L DSM ] + d E t?U [0,T ] ?(t) DSM t 2 ? HSM t 2 , = DSM(?(t)) + d E t?U [0,T ] ?(t) DSM t 2 ? HSM t 2 .<label>(69)</label></formula><p>C.1 GRADIENT VARIANCE REDUCTION VIA HSM Above, we derived that L HSM = L DSM + const, so one might wonder why we advocate for HSM over DSM. As discussed in Sec. 3.2, one advantage of HSM is that it avoids unbounded scores at t ? 0. However, there is a second advantage: In practice, we never solve expectations analytically but rather approximate them using Monte Carlo estimates. In the remainder of this section, we will show that in practice (Monte Carlo) gradients based on HSM have lower variance than those based on DSM.</p><p>From Eq. (69), we have</p><formula xml:id="formula_74">? ? HSM(?(t)) = E t?U [0,T ] [?(t)? ? L HSM ] = E t?U [0,T ] [?(t)? ? L DSM ] , = ? ? DSM(?(t)),<label>(70)</label></formula><p>where ? are the learnable parameters of the neural network. Instead of comparing the above expectations directly, we instead compare ?(t)? ? L HSM with ?(t)? ? L DSM for t ? [0, 1] (we use T = 1 in all experiments) at discretized time values (as is done in practice). Replacing L HSM and L DSM with a single Monte Carlo estimate (as is used in practice), we have</p><formula xml:id="formula_75">?(t)? ? L HSM ? ?(t)? ? s ? (u t , t)? s ? (ut,t) ? vt log p t (u t | x 0 ) ? s ? (u t , t) 2 2 , (71) x 0 ? p(x 0 ), u t ? p t (u t | x 0 ), ?(t)? ? L DSM ? ?(t)? ? s ? (u t , t)? s ? (ut,t) ? vt log p t (u t | u 0 ) ? s ? (u t , t) 2 2 , (72) u 0 ? p(u 0 ), u t ? p t (u t | u 0 ),</formula><p>where we applied the chain-rule. Note that in Eq. (71) and Eq. (72), u t is sampled from the same distribution. Hence, ?(t)? ? s ? (u t , t) acts as a common scaling factor, with the variance difference between HSM and DSM originating from the squared norm term. Hence, we ignore ?(t)? ? s ? (u t , t) and only focus our analysis on the gradient of the norm terms, which we can further simplify:</p><formula xml:id="formula_76">1 2 ? s ? (ut,t) ? vt log p t (u t | x 0 ) ? s ? (u t , t) 2 2 = s ? (u t , t) ? ? vt log p t (u t | x 0 ) =: K HSM ,<label>(73)</label></formula><p>and</p><formula xml:id="formula_77">1 2 ? s ? (ut,t) ? vt log p t (u t | u 0 ) ? s ? (u t , t) 2 2 = s ? (u t , t) ? ? vt log p t (u t | u 0 ) =: K DSM . (74)</formula><p>We explore this difference in a realistic setup; in particular, we evaluate K HSM and K DSM for all data points in the CIFAR-10 training set. We choose s ? to be our trained ablation CLD model (with the standard setup of M ?1 = ? = 4, see Sec. E.2.1 for model details). We then use these samples to compute the empirical covariance matrices Cov HSM and Cov DSM of the random variables K HSM and K DSM , respectively.</p><p>As is common practice in statistics, we consider only the trace of the estimated covariance matrices. <ref type="bibr">4</ref> The trace of the covariance matrix (of a random variable) is also commonly referred to as the total variation (of a random variable).</p><p>We visualize our results in <ref type="figure" target="#fig_6">Fig. 7</ref>. For HSM, there is barely any visual difference in Tr(Cov) for ? = 0.04 and ? = 1. For DSM, both ? = 0.04 and ? = 1 result in very large Tr(Cov) values for small t. For large t, Tr(Cov) is considerably smaller for ? = 0.04 than for ? = 1. However, in practice, we found that DSM is even unstable for small ?. Given this analysis, we believe this is due to the large gradient variance for small t. In conclusion, these results demonstrate a clear variance reduction by the HSM objective, in particular for large ?. Ultimately, this is expected: In HSM, we are effectively integrating out the initial velocity distribution when estimating gradients, while in DSM we use noisy samples for the initial velocity.</p><p>Note that re-introducing ?(t) weightings would allow us to scale the Tr(Cov) curves according to the "reweighted" objective or the maximum likelihood objective. However, we believe it is most instructive to directly analyze the gradient of the relevant norm term itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SYMMETRIC SPLITTING CLD SAMPLER (SSCS)</head><p>In this section, we present a more complete derivation and analysis of our novel Symmetric Splitting CLD Sampler (SSCS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 BACKGROUND</head><p>Our derivation is inspired by methods from the statistical mechanics and molecular dynamics literature. In particular, we are leveraging symmetric splitting techniques as well as (Fokker-Planck) operator concepts. The high-level idea of symmetric splitting as well as the operator formalism are well-explained in <ref type="bibr" target="#b98">Tuckerman (2010)</ref>, in particular in their Section 3.10, which includes simple examples. Symmetric splitting methods for stochastic dynamics in particular are discussed in detail in <ref type="bibr" target="#b58">Leimkuhler &amp; Matthews (2015)</ref>. We also recommend <ref type="bibr" target="#b57">Leimkuhler &amp; Matthews (2013)</ref>, which discusses splitting methods for Langevin dynamics in a concise but insightful manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 DERIVATION AND ANALYSIS</head><p>Generative SDE. From Sec. 3.3, recall that our generative SDE can be written as</p><formula xml:id="formula_78">(with? t = u T ?t , x t = x T ?t ,v t = v T ?t ): dxt dvt = ?M ?1v t xt ?dt A H + 0 d ??M ?1v t ?dt + 0 d ? 2??dwt A O + 0 d 2? s(?t, T ? t) + M ?1v t ?dt S . (75)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fokker-Planck Equation and</head><p>Fokker-Planck Operators. The evolution of the probability distribution p T ?t (? t ) is described by the general Fokker-Planck equation <ref type="bibr" target="#b85">(S?rkk? &amp; Solin, 2019)</ref>:</p><formula xml:id="formula_79">?p T ?t (? t ) ?t = ? 2d i=1 ? ?? i [? i (? t , T ? t)p T ?t (? t )] + 2d i=1 2d j=1 ? 2 ?? i ?? j [D ij (? t , T ? t)p T ?t (? t )] ,<label>(76)</label></formula><formula xml:id="formula_80">with ?(? t , T ? t) = ?M ?1v t x t ? + 0 d ??M ?1v t ? + 0 d 2? s(? t , T ? t) + M ?1v t ?, (77) D(? t , T ? t) = 0 0 0 ?? ? I d .<label>(78)</label></formula><p>For our SDE, we can write the Fokker-Planck equation in short form as</p><formula xml:id="formula_81">?p T ?t (? t ) ?t = (L * A +L * S )p T ?t (? t ),<label>(79)</label></formula><p>with the Fokker-Planck operators (defined via their action on functions of the variables ?(? t )):</p><formula xml:id="formula_82">L * A ?(? t ) := ?M ?1v t ?x t ?(? t ) ? ?x t ?v t ?(? t ) + ??M ?1 ?v t [v t ?(? t )] + ???v t ?(? t ),<label>(80)</label></formula><formula xml:id="formula_83">L * S ?(? t ) := ?2???v t s(? t , T ? t) + M ?1v t ?(? t ) ,<label>(81)</label></formula><formula xml:id="formula_84">?v t := d i=1 ? 2 ?x 2 i + ? 2 ?v 2 i .<label>(82)</label></formula><p>We are providing these formulas for transparency and completeness. We do not directly leverage them. However, working with these operators can be convenient. In particular, the operators describe the time evolution of states? t under the stochastic dynamics defined by the SDE. Given an initial state? 0 , we can construct a formal solution to the generative SDE via <ref type="bibr" target="#b98">(Tuckerman, 2010;</ref><ref type="bibr" target="#b58">Leimkuhler &amp; Matthews, 2015)</ref>:?</p><formula xml:id="formula_85">t = e t(L * A +L * S )? 0 ,<label>(83)</label></formula><p>where the operator e t(L * A +L * S ) is known as the classical propagator that propagates states? 0 for time t according to the dynamics defined by the combined Fokker-Planck operatorsL * A +L * S (to avoid confusion, note that in Eq. (83) the operator e t(L * A +L * S ) is applied on? 0 in an element-wise or "vectorized" fashion on all elements of? 0 in parallel). The problem with that expression is that we cannot analytically evaluate it. However, we can leverage it to design an integration method.</p><p>Symmetric Splitting Integration. Using the symmetric Trotter theorem or Strang splitting formula as well as the Baker-Campbell-Hausdorff formula <ref type="bibr" target="#b96">(Trotter, 1959;</ref><ref type="bibr" target="#b95">Strang, 1968;</ref><ref type="bibr" target="#b98">Tuckerman, 2010)</ref>, it can be shown that:</p><formula xml:id="formula_86">e t(L * A +L * S ) = lim N ?? e ?t 2L * A e ?tL * S e ?t 2L * A N ? e ?t 2L * A e ?tL * S e ?t 2L * A N + O(N ?t 3 ),<label>(84)</label></formula><p>for large N ? N + and time step ?t := t/N . The expression suggests that instead of directly evaluating the intractable e t(L * A +L * S ) , we can discretize the dynamics over t into N pieces of step size ?t, such that we only need to apply the individual e ?t 2L * A and e ?tL * S many times one after another for small time steps ?t. A finer discretization implies a smaller error (since N =t/?t the error effectively scales as O(?t 2 ) for fixed t). Hence, this implies an integration method. The general idea of such splitting schemes is to split an initially intractable propagator into separate terms, each of which is analytically tractable. In that case, the overall integration error for many steps is only due to the splitting scheme error, 5 but not due to the evaluation of the individual updates. Such techniques are, for example, popular in molecular dynamics to develop symplectic integrators as well as accurate samplers <ref type="bibr" target="#b98">Tuckerman, 2010;</ref><ref type="bibr" target="#b57">Leimkuhler &amp; Matthews, 2013;</ref>.</p><p>Analyzing the Splitting Terms. Next, we need to analyze the two individual terms:</p><p>(i) Let us first analyze e ?t 2L * A? t : This term describes the stochastic evolution of? t under the dynamics of an SDE like Eq. (75), but with S set to zero. However, if S is set to zero, the remaining SDE has affine drift and diffusion coefficients. In that case, if the input is Normal (or a discrete state corresponding to a Normal with 0 variance) then the distribution is Normal at all times and we can calculate the evolution analytically. In particular, we can solve the differential equations for the mean? ?t 2 and covariance? ?t 2 of the Normal (see Sec. B.1), and obtain</p><formula xml:id="formula_87">? ?t 2 (? t ) = 2? ?t 2 ? ?1x t ? 4? ?t 2 ? ?2v t +x t ? ?t 2x t ? 2? ?t 2 ? ?1v t +v t e ?2? ?t 2 ? ?1 ,<label>(85)</label></formula><p>as well as?</p><formula xml:id="formula_88">?t 2 =? ?t 2 ? I d ,<label>(86)</label></formula><formula xml:id="formula_89">? ?t 2 = ? xx ?t 2? xv ?t 2 ? xv ?t 2? vv ?t 2 e ?4? ?t 2 ? ?1 ,<label>(87)</label></formula><formula xml:id="formula_90">? xx ?t 2 = e 4? ?t 2 ? ?1 ? 1 ? 4? ?t 2 ? ?1 ? 8 ? ?t 2 2 ? ?2 ,<label>(88)</label></formula><formula xml:id="formula_91">? xv ?t 2 = ?4 ? ?t 2 2 ? ?1 ,<label>(89)</label></formula><formula xml:id="formula_92">? vv ?t 2 = ? 2 4 e 4? ?t 2 ? ?1 ? 1 + ? ?t 2 ? ? 2 ? ?t 2 2 .<label>(90)</label></formula><p>The correctness of the proposed mean and covariance matrix can be verified by simply plugging them back in their respective ODEs; see App. G.2. Now, we can write the action of the the propagator e ?t 2L *</p><p>A on a state? t as:</p><formula xml:id="formula_93">e ?t 2L * A? t ? N (? t+ ?t 2 ;? ?t 2 (? t ),? ?t 2 ).<label>(91)</label></formula><p>(ii): Next, we need to analyze e ?tL * S? t . Unfortunately, we cannot calculate the action of the propagator e ?tL * S on? t analytically and we need to make an approximation. From Eq. (75), we can easily see that the propagator e ?tL * S describes the evolution of the velocity componentv t for time step ?t under the ODE (this can be easily seen by noticing that the S term in Eq. (75) only acts on the velocity component of the joint state? t ):</p><formula xml:id="formula_94">dv t = 2?? s(? t , T ? t) + M ?1v t dt.<label>(92)</label></formula><p>We propose to simply solve this ODE for the step ?t via a simple step of Euler's method, resulting in:</p><formula xml:id="formula_95">e ?tL * S? t ?? t + ?t 0 d 2?? s(? t , T ? t) + M ?1v t + O(?t 2 ) = e ?tL * Euler S? t + O(?t 2 ),<label>(93)</label></formula><p>with the informal definition e ?tL * Euler</p><formula xml:id="formula_96">S? t :=? t + ?t 0 d 2?? s(? t , T ? t) + M ?1v t .<label>(94)</label></formula><p>Error Analysis. It is now instructive to study the overall error of our proposed integrator. With the additional Euler integration in one of the splitting terms, we have </p><p>where we used N = t ?t and only kept the dominating error terms of lowest order in ?t. We see that, just like Euler's method, also our SSCS is a first-order integrator with local error ??t 2 and global A (blue) and the score model term exp ?tL * S (brown). Only the latter requires numerical approximation, which results in an overall more accurate integration scheme.</p><p>error ??t, which can be also seen from the last two lines of Eq. (95). This is expected, considering that we used an Euler step for the S term. Nevertheless, as long as the dynamics is not dominated by the S component, our proposed integration scheme is still expected to be more accurate than EM, since we split off the analytically tractable part and only use an Euler approximation for the S term.</p><p>To this end, recall that the model only needs to learn the score of the conditional distribution p t (v t |x t ), which is close to Normal for much of the diffusion, in which case the S term will indeed be small. This suggests that the generative SDE dynamics are in fact dominated by A H and A O in practice. From another perspective, note that (recalling that s ? (u t , t)</p><formula xml:id="formula_98">= ? t ? ? (u t , t) with ? ? (u t , t) = ?1 t v t /? vv t + ? ? (u t , t) from Sec. 3.2): s(? t , T ? t) + M ?1v t = ? t ? ? (u t , t) ? v t ? vv t + M ?1v t = ? t ? ? (u t , t) +v t 1 M ? 1 ? vv t .<label>(96)</label></formula><p>For large parts of the diffusion, ? vv t is indeed close to M , such that thev t term is very small (this cancellation is the reason why we pulled the M ?1v t term into the S component). In Sec. 3, we have also seen that our neural network component ? ? (u t , t) can be much smoother than that of previous SGMs. Overall, this suggests that the error of SSCS indeed might be smaller than the error we would obtain when applying a naive Euler-Maruyama integrator to the full generative SDE. Our positive experimental results in Sec. 5.2 validate that. Only in the limit for very small steps, both our SSCS and EM make only very small errors and are expected to perform equally well, which is exactly what we observe in our experiments. Our SSCS turns out to be well-suited for integrating the generative SDE of CLD-SGMs with relatively few synthesis steps.</p><p>Note that error analysis of stochastic differential equation solvers is usually performed in terms of weak and strong convergence <ref type="bibr" target="#b53">(Kloeden &amp; Platen, 1992)</ref>. Due to the use of Euler's method for the S component, as argued above, we expect our SSCS to formally have the same weak and strong convergence properties like EM, this is, weak convergence of order 1 and strong convergence of order 1 as well, since the noise is additive in our case (and assuming appropriate smoothness conditions for the drift and diffusion coefficients; furthermore, without additive noise, we would have strong convergence of order 0.5). We leave a more detailed analysis to future work.</p><p>In practice, we do not use SSCS to integrate all the way from t=0 to t=T , but only up to t=T ? , and perform a denoising step, similar to previous works <ref type="bibr" target="#b45">(Jolicoeur-Martineau et al., 2021a;</ref><ref type="bibr" target="#b94">Song et al., 2021c)</ref>. It is worth noting that our SSCS scheme would also be applicable when we used time-dependent ?(t), as in our more general derivation of the CLD perturbation kernel in App. B. However, since we only used constant ? in the main paper, we also presented SSCS in that way.</p><p>A promising direction for future work would be to extend SSCS to adaptive step sizes and to use techniques to facilitate higher-order integration, while still leveraging the advantages of SSCS. Second half-step: apply exp{ ?tn 2L *</p><formula xml:id="formula_99">A } on? n+ 1 2 t ? t + ?tn Update time end for u N ??N ? 0 ?M ?1 ?? ???M ?1 ? I d ? N + 0 d 2??s(?t, )</formula><p>Denoising (x N ,v N ) =? N Extract output data and velocity samples Note that the algorithm uses the expressions in Eqs. (85) and (86) for? t and? t . Furthermore, in practice in the denoising step at the end, we usually only update thex N component of? N , since we are only interested in the data sample. This saves us the final neural network call during denoising, which only affects thev N component (also see App. E.2.4). However, we wrote the algorithm in the general way, which also allows to correctly generate the velocity samplev N . In <ref type="figure" target="#fig_8">Fig. 8</ref>, we show a conceptual visualization of our SSCS and contrast it to EM.</p><p>Also note that we could combine the second half-step from one iteration of SSCS with the first halfstep from the next iteration of SSCS. This is commonly done in the Leapfrog integrator <ref type="bibr" target="#b59">(Leimkuhler &amp; Reich, 2005;</ref><ref type="bibr" target="#b98">Tuckerman, 2010;</ref><ref type="bibr" target="#b75">Neal, 2011;</ref><ref type="bibr" target="#b58">Leimkuhler &amp; Matthews, 2015)</ref>, 6 which follows a similar structure as our SSCS. However, it is not important in our case, as the only computationally costly operation is in the center full step, which involves the neural network evaluation. The first and last half-steps come at virtually no computational cost.</p><p>(a) Difference ?(t) (via L2 norm) between score of diffused data and score of Normal distribution.</p><p>(b) Frobenius norm of Jacobian JF (t) of the neural network defining the score function for different t. </p><formula xml:id="formula_100">p data (x) = 9 k=1 1 9 p (k) (x),<label>(97)</label></formula><p>where p (k) (x) = N (x; ? k ; 0.04 2 I 2 ) and ? 1 = ?a 0 , ? 2 = ?a/2 a/2 , ? 3 = 0 a , ? 4 = ?a/2 ?a/2 , ? 5 = 0 0 , ? 6 = a/2 a/2 ,</p><formula xml:id="formula_101">? 7 = 0 ?a , ? 8 = a/2 ?a/2 , ? 9 = a 0 ,</formula><p>and a = 2 ? 1 2 . The choice of this data distribution is not arbitrary. In fact, mixture of Normal distributions are diffused by simply diffusing the components, i.e., setting p 0 (x 0 ) = p data (x), we have</p><formula xml:id="formula_102">p t (x t ) = 9 k=1 1 9 p (k) t (x t ),<label>(98)</label></formula><p>where p (k) t are the diffused components (analogously for CLD with velocity augmentation). This means that for both CLD as well as <ref type="bibr">VPSDE Song et al. (2021c)</ref> we can diffuse p data (x) with analytical access to the diffused marginal p t (x t ) or p t (u t ). This allows us to perform interesting analyses that would be impossible when working, for example, with image data. We visualize the data distribution in <ref type="figure">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score experiment:</head><p>We empirically verify the reduced complexity of the score of p t (v t |x t ), which is learned in CLD, compared to the score of p t (x t ), which is learned in VPSDE. To avoid scaling issues between VPSDE and CLD, we chose M = ? = 1 for CLD in this experiment; this results in an equilibrium distribution of N (0 2 , I 2 ) (for both data and velocity components, which are independent at equilibrium), which is the same as the equilibrium distribution of the VPSDE. We then </p><formula xml:id="formula_103">(x) = N (0 2 , I 2 ) is simply ? x log p(x) = ?x), ? VPSDE (t) := E xt?p(xt) ? xt log p t (x t ) + x t 2 2 ,<label>(99)</label></formula><formula xml:id="formula_104">? CLD (t) := E ut?p(ut) ? vt log p t (v t | x t ) + v t 2 2 .<label>(100)</label></formula><p>The expectations are approximated using 10 5 samples from p(x t ) and p(u t ) for VPSDE and CLD, respectively. As can be seen in <ref type="figure" target="#fig_10">Fig. 9a</ref>, ? CLD (t) is smaller than ? VPSDE (t) for all t ? [0, T ]. The difference is particularly striking for small time values t. Other previous SDEs, such as the VESDE, sub-VPSDE, etc., are expected to behave similarly. This result implies that the ground truth scores that need to be learnt in CLD are closer to Normal scores than the ground truth scores in previous SDEs like the VPSDE. Since the score of a Normal is very simple-and indeed directly leveraged in our mixed score formulation-we would intuitively expect that the CLD training task is easier.</p><p>Complexity experiment: Therefore, to understand the above observations in terms of learning neural networks, we train a small ResNet architecture (less than 100k parameters) for each of the following four setups: both CLD and VPSDE each with and without a mixed score parameterization. The mixed score of the VPSDE simply assumes a standard Normal data distribution (which is also the equilibrium distribution of VPSDE) resulting in adding ?x t to the score function. Formally, ?x t is the score of a Normal distribution with unit variance.</p><p>We train the models for 1M iterations using fresh data synthesized from p data at a batch size of 512. The model and data distributions are visualized in <ref type="figure">Fig. 10</ref>. We see that all models have learnt good representations of the data. We measure the complexity of the trained neural networks using the squared Frobenius norm of the networks' Jacobians. For CLD, we have</p><formula xml:id="formula_105">J CLD F (t) := E ut?p(ut) ? ut ? ? (u t ) 2 F .<label>(101)</label></formula><p>Similarly, for the VPSDE we compute</p><formula xml:id="formula_106">J VPSDE F (t) := E xt?p(xt) ? xt ? ? (x t ) 2 F .<label>(102)</label></formula><p>For both CLD and VPSDE, expectations are again approximated using 10 5 samples. As can be seen in <ref type="figure" target="#fig_10">Fig. 9b</ref> the neural network complexity is significantly lower for CLD compared to VPSDE. A mixed score formulation further helps decreasing the neural network complexity for both CLD and VPSDE. This result implies that the arguably simpler training task in CLD indeed also translates to reduced model complexity in that the neural network is smoother as measured by J F (t). In largescale experiments, this would mean that, given similar model capacity, a CLD-based SGM could potentially have a higher expressivity. Or, on the other hand, similar performance could be achieved with a smoother and potentially smaller model. Indeed these findings are in line with our strong results on the CIFAR-10 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 IMAGE MODELING EXPERIMENTS</head><p>We perform image modeling experiments on CIFAR-10 as well as CelebA-HQ-256. We report FID scores on CIFAR-10 for our main model for various different solvers; see Tab. 3 and Tab. 2. We further present generated samples for both models in Sec. 5 using Euler-Maruyama with 2000  quadratic striding steps and Runge-Kutta 4(5) for CIFAR10 and CelebA-HQ-256, respectively. We present additional samples for various solver settings in App. F. All (average) NFEs for the Runge-Kutta solver are computed using a batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 TRAINING DETAILS AND MODEL ARCHITECTURES</head><p>Our models are based on the NCSN++ and the DDPM++ architectures from <ref type="bibr" target="#b94">Song et al. (2021c)</ref>. Importantly, we changed the number of input channels from three to six to facilitate the additional velocity variables. Note that the number of additional neural network parameters due to this change is negligible.</p><p>For fair a comparison, we train our models using the same t-sampling cutoff during training as is used for VESDE and VPSDE in <ref type="bibr" target="#b94">Song et al. (2021c)</ref>. Note, however, that this is not strictly necessary for CLD as we do not have any "blow-up" of the SDE due to unbounded scores as t ? 0 (also see <ref type="figure" target="#fig_8">Fig. 18</ref> and <ref type="figure" target="#fig_10">Fig. 19</ref>).</p><p>We summarize our three model architectures as well as our SDE and training setups in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 CIFAR-10 RESULTS FOR VESDE AND VPSDE</head><p>The results reported for VESDE and VPSDE using the GGF sampler are taken from <ref type="bibr" target="#b45">Jolicoeur-Martineau et al. (2021a)</ref>. All other results for VESDE and VPSDE are generated using the provided PyTorch code as well as the provided checkpoints from <ref type="bibr" target="#b94">Song et al. (2021c)</ref>. <ref type="bibr">7</ref> We used EM and PC to sample from the VPSDE and VESDE models, respectively (see Sec. 5.2), since these choices correspond to their recommended settings. 8</p><p>Furthermore, in App. F.2 we also used DDIM <ref type="bibr" target="#b91">(Song et al., 2021a)</ref> to sample the VPSDE. DDIM's update rule is </p><formula xml:id="formula_107">x t?1 = ? t?1 ? t x t + ? 2 t s ? (x t , t) ? ? t?1 ? t s ? (x t , t),<label>(103)</label></formula><formula xml:id="formula_108">where ? t = exp ?0.5 t 0 ?(t) dt , ? 2 t = 1 ? exp ? t 0 ?(t) dt ,</formula><formula xml:id="formula_109">with ?t i = 1 NNFE ? i ? [0, N NFE ? 1].</formula><p>However, prior work <ref type="bibr" target="#b91">(Song et al., 2021a;</ref><ref type="bibr" target="#b103">Watson et al., 2021)</ref> has shown that it can be beneficial to focus function evaluations (neural network calls) on times t "close to the data". This is because the diffusion process distribution is most complex close to the data and almost perfectly Normal close to the prior. Among other techniques, these works used a useful heuristic, denoted as quadratic striding (QS), which discretizes the integration interval such that the evaluation times follow a quadratic schedule and the individual time steps follow a linear schedule. We also used this QS approach in our experiments.</p><p>We can formally define it as follows (assuming a time interval [0.0, 1.0] here for simplicity): Denote the evaluation times as ? i (including 0.0 and 1.0) and define:</p><formula xml:id="formula_110">? i = c ? i 2 ?i ? [0, N NFE ].<label>(104)</label></formula><p>Hence, This describes the time steps as going from t = 0 to t = 1. During synthesis, however, we are going backwards. Hence, we can define our time steps as</p><formula xml:id="formula_111">?t i = ? i ? ? i?1 = c ? (2i ? 1) ?i ? [1, N NFE ],<label>(105)</label></formula><formula xml:id="formula_112">?t j = c ? [2N NFE ? 2j + 1] ?j ? [1, N NFE ],<label>(106)</label></formula><p>where j now counts time steps in the other direction. Note that this can be easily adapted to general integration intervals [ , T ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.4 DENOISING</head><p>As has been pointed out in <ref type="bibr" target="#b46">Jolicoeur-Martineau et al. (2021b)</ref>, samples that are generated with models similar to ours can contain noise that is hard to detect visually but worsens FID scores significantly.</p><p>Denoising Formulas. For a fair comparison we use the same denoising setup for all experiments we conducted (including VESDE (PC/ODE) and VPSDE (EM/ODE)) except for LSGM. <ref type="bibr">9</ref> We simulate the underlying generative ODE/SDE until the time cutoff ? = 10 ?3 and then take a single denoising step of the form</p><formula xml:id="formula_113">u 0 = u ? ? ?f (u ? , ?) + ?G(u ? , ?)G(u ? , ?) 0 d s ? (u ? , ?) .<label>(107)</label></formula><p>This denoising step can be considered as an Euler-Maruyama step without noise injection. For SDEs acting on data directly (VESDE, VPSDE, etc.) the corresponding denoising formula is  <ref type="bibr" target="#b94">Song et al. (2021c)</ref>. When we simulate the generative probability flow ODE we found that denoising is important in order for the Runge-Kutta solver not to "blow-up" as t ? 0. On the other hand, when simulating CLD using our new SSCS solver, we found that denoising only slightly influences FID (see Tab. 7). We believe that this might be because the neural network does not have any influence on the denoising step for CLD. More specifically, the neural network only denoises the velocity component. However, we are primarily interested in the data component. Putting the drift and diffusion coefficients of CLD in the denoising formula in Eq. <ref type="formula" target="#formula_0">(107)</ref>, we obtain</p><formula xml:id="formula_114">x 0 = x ? ? ?f (x ? , ?) + ?G(x ? , ?)G(x ? , ?) s ? (x ? , ?).<label>(108)</label></formula><formula xml:id="formula_115">u 0 = u ? ? ? 0 ?M ?1 ?? ???M ?1 ? I d u ? + ? 0 d 2??s ? (u ? , ?) =? x 0 = x ? ? ??M ?1 v ? .<label>(109)</label></formula><p>E.2.5 SOLVER ERROR TOLERANCES FOR RUNGE-KUTTA 4(5)</p><p>In Tab. 2, we report FID scores for a Runge-Kutta 4(5) solver <ref type="bibr" target="#b22">(Dormand &amp; Prince, 1980)</ref> as well as the "Gotta Go Fast" solver from Jolicoeur-Martineau et al. (2021a) (see their <ref type="table" target="#tab_0">Table 1</ref>). For simulating CLD with Runge-Kutta 4(5) we chose the solver error tolerances to hit certain regimes of NFEs to facilitate comparisons with VPSDE and VESDE. We obtain a mean number of function evaluations of 312 and 137 using Runge-Kutta 4(5) solver error tolerances of 10 ?5 and 10 ?3 , respectively. For VESDE, VPSDE and LSGM we used 10 ?5 as the ODE solver error tolerance, following the recommended default setups <ref type="bibr" target="#b94">(Song et al., 2021c;</ref><ref type="bibr" target="#b104">Vahdat et al., 2021)</ref>. These values are used for both relative and absolute error tolerances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.6 ABLATION EXPERIMENTS</head><p>The model architecture used for all ablation experiments can be found in Tab. 6. As pointed out in Sec. 5 we found that the hyperparameters ? and M only have small effects on CIFAR-10 FID scores. On the other hand, we found that the mixed score parameterization helps significantly in obtaining competitive FIDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.7 LSGM-100M MODEL</head><p>Our CLD-based SGM has ?108M parameters, while the original CIFAR-10 Latent SGM from <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref>, to which we compare in Tab. 1, uses ?476M parameters. To establish a fairer comparison between our CLD-based SGMs and LSGM <ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref>, we train another smaller LSGM model with ?109M parameters. To do this, we followed the exact setup of the "CIFAR-10 (balanced)" model from LSGM (see <ref type="table" target="#tab_10">Table 7</ref> in <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref>), with a few minor modifications: We used a VAE backbone model with only 10 groups instead of 20, which corresponds to a reduction in parameters by a factor of 2 in the encoder and decoder networks. We also reduced the convolutional channels in the latent space SGM from 512 to 256 and reduced the number of the residual cells per scale from 8 to 4. With these modifications the resulting "LSGM-100M" uses only ?109M parameters overall with approximately half of them in the encoder and decoder networks and the other half in the latent SGM. Other than these architecture modifications, our model is trained in exactly the same way as the bigger, original models in <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref>. For evaluation, we follow the recommended setting by <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref> and use the same Runge-Kutta 4(5) ODE solver with an error tolerance of 10 ?5 to solve the probability flow ODE in LSGM's latent space. LSGM-100M achieves an FID of 4.60, an NLL bound of 2.96 bpd, and requires on average 131 NFE for sampling new images. We report these results in Tabs. 1 and 2 in the main text.</p><p>Note that we also tried training a model following the "CIFAR-10 (best FID)" setup, but found training to be unstable (however, the orignal "CIFAR-10 (best FID)" model from <ref type="bibr" target="#b104">Vahdat et al. (2021)</ref> only performs marginally better in FID than their "CIFAR-10 (balanced)" model anyway). Furthermore, we also tried training another small LSGM with a similar number of parameters but with more parameters in the latent SGM and less in the encoder and decoder networks, compared to the reported LSGM-100M. However, this model performed significantly worse.</p><formula xml:id="formula_116">F ADDITIONAL EXPERIMENTS F.1 TOY EXPERIMENTS F.1.1 ANALYTICAL SAMPLING</formula><p>In order to test combinations of diffusions and numerical samplers in isolation, we consider a dataset for which we know the ground truth score function (for all t) analytically. In particular, we use the mixture of Normals introduced in App. E.1; see <ref type="figure">Fig. 10a</ref> for a visualization of the data distribution. In <ref type="figure" target="#fig_13">Fig. 11</ref>, we show samples for VPSDE (Euler-Maruyama (EM) sampler) and CLD (EM and SSCS samplers). For quantitative comparison, we also compute negative log-likelihoods for the three combinations (which can be done easily due to our access to the ground truth distribution): as can be seen in Tab. 8, for each number of steps n ? {20, 50, 100, 200} CLD with SCSS outperforms both VPSDE and CLD with EM. As discussed in Sec. 3.3, we can see in Tab. 8 that EM is not wellsuited for CLD. This is true, in particular, when using a small number of synthesis steps n (function evaluations). In <ref type="figure" target="#fig_13">Fig. 11</ref>, we see that CLD with EM leads to sampling distributions which are too broad. These results are exactly in line with the "diverging" dynamics that is observed when solving Hamiltonian dynamics with a non-symplectic integrator, such as the standard Euler method <ref type="bibr" target="#b75">(Neal, 2011)</ref>. This problematic behavior of Euler-based techniques is more pronounced when using fewer steps with larger stepsizes, which is also what we observe in our experiments. These results further motivate the use of our novel SSCS, which addresses these challenges, for sampling from our CLDbased SGMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 MAXIMUM LIKELIHOOD TRAINING</head><p>For maximum likelihood training, models based on overdamped Langevin dynamics such as VPSDE need to learn an unbounded score for t ? 0. Our model, on the other hand, only ever needs to learn a bounded score even for t = 0. For our image data experiments, we use a reweighted objective function to improve visual quality of samples (as is general practice).</p><p>Here, we also study training towards maximum likelihood on toy dataset tasks. To explore this, we repeat the neural network complexity experiment from App. E.1 with maximum likelihood training (instead of the reweighted objective). Furthermore, we also train VPSDE-based and CLD-based SGMs on a challenging toy dataset and find that CLD significantly outperforms VPSDE. We leave the study of CLD with maximum likelihood training for high-dimensional (image) datasets to future work.  Complexity Experiment. The setup of this experiment is equivalent to the setup in App. E.1 up to the training objective: in this experiment we do maximum likelihood learning, i.e., we train CLD models with the objective from Eq. (8) with ?(t) = ??. 10 Furthermore, we test CLD in this setup for three different values of ?. The results of this experiment can be found in <ref type="figure" target="#fig_14">Fig. 12</ref>. For CLD, we find that larger values of ? generally lead to less complex networks, in particular for smaller times t. However, even for ? = 0.04 the learned neural network is still significantly smoother than the network learned for the VPSDE when a mixed score parameterization is used. 11</p><p>Challenging Toy Dataset. Using the same simple ResNet architecture (less than 100k parameters) from the above experiment, we trained a VPSDE-based as well as a CLD-based SGM to maximize the likelihood of a more challenging toy dataset (the dataset is essentially "multi-scale", as it involves both large scale-the placement of the swiss rolls-and fine scale-the swiss rolls themselvesstructure). Similar to the other toy datasets, the models are trained for 1M iterations using fresh data synthesized from the data distribution in each batch at a batch size of 512.</p><p>In <ref type="figure" target="#fig_0">Fig. 13</ref>, we compare samples of the models to the data distribution. Even with our simple model architecture, CLD is able to capture the multi-scale structure of the dataset: the five rolls are adequately resembled and only a few samples are in between modes. VPSDE, on the other hand, only captures the main modes, but not the fine structure. Furthermore, VPSDE has the undesired behavior of "connecting" the modes.</p><p>Overall, we conclude that also in the maximum likelihood training setting CLD is a promising diffusion showing superior behavior compared to the VPSDE in our toy experiments.  In this section, we provide additional results on the CIFAR-10 image modeling benchmark.</p><p>An extended version of Tab. 3 (sampling the generative SDE with different fixed-step size solvers for different compute budgets) including additional baselines can be found in Tab. 9. Note that time stepping with quadratic striding (QS) improves sampling from VPSDE-and CLD-based models for all settings except for the combination of VPSDE and EM sampling in the setting n = {1000, 2000}.</p><p>For the VESDE (using PC sampling), QS significantly worsens FID scores. The reason for this could be that the variance of the VESDE already follows an exponential schedule (see <ref type="figure" target="#fig_2">Fig. 5</ref> in <ref type="bibr" target="#b94">Song et al. (2021c)</ref>). We additionally present results for the VPSDE using the DDIM (Denoising Diffusion Implicit Models) sampler <ref type="bibr" target="#b91">(Song et al., 2021a)</ref>. As was observed by <ref type="bibr" target="#b91">Song et al. (2021a)</ref>, QS also helps for DDIM. Importantly, for any n ? 150, our CLD with our novel SSCS (and QS) even outperforms DDIM. Only for n = 50, DDIM performs better. It needs to be mentioned, however, that the DDIM sampler was specifically designed for few-step sampling, whereas our CLD with SSCS is derived in a general fashion without this particular regime in mind. In particular, DDIM sampling can be interpreted as a non-Markovian sampling method and it is not clear how to calculate the loglikelihood of hold-out validation data under this non-Markovian synthesis approach. Nevertheless, it would be interesting to also explore non-Markovian DDIM-inspired techniques for CLD-SGMs to further improve sampling speed in CLD-SGMs.</p><p>Note that our DDIM results shown in Tab. 9 are better than those presented in <ref type="bibr" target="#b91">Song et al. (2021a)</ref> itself, because we are relying on the DDPM++ model trained in <ref type="bibr" target="#b94">Song et al. (2021c)</ref>, whereas <ref type="bibr" target="#b91">Song et al. (2021a)</ref> uses the DDPM model from <ref type="bibr" target="#b33">Ho et al. (2020)</ref>.</p><p>Finally, we present additional generated samples from our CLD-SGM model: see <ref type="figure" target="#fig_1">Fig. 14 and Fig. 15</ref> for samples from EM-QS with 2000 evaluations and SSCS-QS with 150 evaluations, respectively.  In this section, we provide additional qualitative results on CelebA-HQ-256. For high quality samples using our new SSCS solver see <ref type="figure" target="#fig_3">Fig. 16</ref>.</p><p>Samples generated with an adaptive step size Runge-Kutta 4(5) solver at different solver tolerances can be found in <ref type="figure" target="#fig_6">Fig. 17</ref>. We found that our model still generates very good samples even for a solver error tolerance of 10 ?3 using an average of 129 neural network evaluations.</p><p>Lastly, we show "generation paths" of samples from our CelebA-HQ-256 model: see <ref type="figure" target="#fig_8">Fig. 18</ref> and <ref type="figure" target="#fig_10">Fig. 19</ref> for samples from the probability flow ODE and the generative SDE, respectively. We visualize the continuous generation paths via snapshots of data and velocity variables at eight different time steps. Interestingly, we can see that the velocity variables "encode" the data at intermediate t.</p><p>On the other hand, at time t = 1.0, by construction, both data and velocity are distributed according to the "equilibrium distribution" of the diffusion, namely, p EQ (u) = N (x; 0 d , I d ) N (v; 0 d , M I d ). Furthermore, as t ? 0 the data variables approximately converge to the data distribution, while the velocity variables approximately converge to another Normal distribution N (v; 0 d , ?M I d ) (with ? = 0.04 in our experiments).</p><p>Recall that for CLD, the neural network approximates the score ? vt log p t (v t |x t ). We believe that the generation paths are further evidence that CLD-SGMs need to learn simpler models: for fixed t the velocity variable v t appears to be a "noisy" version of the data x t , and therefore we believe p t (v t |x t ) to be relatively smooth and simple when compared to the marginal p t (x t ).</p><p>Finally, note that in <ref type="figure" target="#fig_8">Figs. 18 and 19</ref>, when visualizing the velocity variables, we used a colorization scheme that corresponds exactly to the inverse of the color scheme used for visualizing the images themselves. Alternatively, we could also interpret this in such a way that we are not actually visualizing velocities, but negative velocities with flipped signs. When using this inverse colorization scheme for the velocities, we see that at intermediate t, where the velocities encode the data, the color values visualizing image data and velocities are, apart from the additional noise in the velocities, similar (i.e. the velocities appear as noisy versions of the actual images). This implies that image pixel values x t translate into corresponding negative velocities v t that pull the pixel values back towards the mean of the equilibrium distribution. This is a consequence of the Hamiltonian coupling between the data and velocity variables. In other words, it is a result of the negative sign in front of x t in the H term in Eq. (5) (and analogously for the reverse-time generative SDE). Also see the visualizations on our project page (https://nv-tlabs.github.io/CLD-SGM). (c) ODE solver error tolerance 10 ?3 ; 129 average NFE.</p><p>(d) ODE solver error tolerance 10 ?2 ; 99.4 average NFE. <ref type="figure" target="#fig_6">Figure 17</ref>: Samples generated by our model on the CelebA-HQ-256 dataset using a Runge-Kutta 4(5) adaptive ODE solver to solve the probability flow ODE. We show the effect of the ODE solver error tolerance on the quality of samples ((a), (b), (c) and (d) were generated using the same prior samples). Little visual differences can be seen between 10 ?5 and 10 ?4 . Low frequency artifacts can be observed at 10 ?3 . Deterioration starts to set in at 10 ?2 . <ref type="figure" target="#fig_8">Figure 18</ref>: Generation paths of samples from our CelebA-HQ-256 model (Runge-Kutta 4(5) solver; mean NFE: 288). Odd and even rows visualize data and velocity variables, respectively. The eight columns correspond to times t ? {1.0, 0.5, 0.3, 0.2, 0.1, 10 ?2 , 10 ?3 , 10 ?5 } (from left to right). The velocity distribution converges to a Normal (different variances) for both t ? 0 and t ? 1. See App. F.3 for visualization details and discussion. <ref type="figure" target="#fig_10">Figure 19</ref>: Generation paths of samples from our CelebA-HQ-256 model (SSCS-QS using only 150 steps). Odd and even rows visualize data and velocity variables, respectively. The eight columns correspond to times t ? {1.0, 0.5, 0.3, 0.2, 0.1, 10 ?2 , 10 ?3 , 10 ?5 } (from left to right). The velocity distribution converges to a Normal (different variances) for both t ? 0 and t ? 1. See App. F.3 for visualization details and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PROOFS OF PERTURBATION KERNELS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR-10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>CelebA-HQ-256 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Langevin dynamics in different damping regimes. Each pair of visualizations corresponds to the (coupled) evolution of data xt and velocities vt. We show the marginal (red) probabilities and the projections of the (green) trajectories. The probabilities always correspond to the same optimal setting ? = ?critical (recall that ?critical = 2 ? M and ?max = M/(?(t)?t); see Sec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>num = 1e ? 12 HSM w/ num = 1e ? 9 HSM w/ num = 1e ? 6 DSM w/ num = 1e ? 12 DSM w/ num = 1e ? 9 DSM w/ num = 1e ? 6 Comparison of HSM t (in green) and DSM t (in orange) for our main hyperparameter setting with M = 0.25 and ? = 0.04. In contrast to DSM t , HSM tis analytically bounded. Nevertheless, numerical computation can be unstable (even when using double precision) in which case adding a numerical stabilization of num = 10 ?9 to the covariance matrix before computing t suffices to make HSM work (see App. B.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. Here, we used the fact that the expected squared norm of a multivariate standard Normal random variable is equal to its dimension, i.e., E ??N (0 d ,I d ) ? 2 2 = d. This analysis then simplifies Eq. (67) to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Traces of the estimated covariance matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Conceptual visualization of our new SSCS sampler and comparison to Euler-Maruyama (for image synthesis): (a) In EM-based sampling, in each integration step the entire SDE is integrated using an Euler-based approximation. This can be formally expressed as solving the full-step propagator exp ?t(L * A +L * S ) via Euler-based approximation for N small steps of size ?t (see red steps; for simplicity, this visualization assumes constant ?t). (b): In contrast, in our SSCS the propagator is partitioned into an analytically tractable component exp ?t 2L *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>SSCS Algorithm. Finally, we summarize SSCS in terms of a concise algorithm:Algorithm 1 Symmetric Splitting CLD Sampler (SSCS) Input: Score function s ? (?t, T ? t), CLD parameters ?, ?, M = ? 2 /4, number of sampling steps N , step sizes {?tn ? 0} N ?1n=0 chosen such that := T ? N ?1 n=0 ?tn ? 0 (stepsizes can vary, for example in QS). Output: Synthesized model samplex N , along with a velocity samplev N .x0 ? N (x0; 0 d , I d ),v0 ? N (v0; 0 d , M I d ),?0 = (x0,v0)Draw initial prior samples from pEQ(u) t = 0Initialize time for n = 0 to N ? 1 d? u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Toy experiments for mixture of Normals dataset.E IMPLEMENTATION AND EXPERIMENT DETAILS E.1 SCORE AND JACOBIAN EXPERIMENTSIn this section, we provide details for the experiments presented in Sec. 3.1. For both experiments, we consider a two-dimensional simple mixture of Normals of the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Data p data (b) CLD w/ MS (c) CLD w/o MS (d) VPSDE w/ MS (e) VPSDE w/o MS Figure 10: Mixture of Normals: data and trained models (samples). measure the difference of the respective scores at time t and the equilibrium (or prior) scores, i.e. (recall that the score of a Normal distribution p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>and c ? = 1 N 2 NFE to ensure that ? NFE = 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>(a) VPSDE + EM, n = 20 (b) CLD + EM, n = 20 (c) CLD + SSCS, n = 20 (d) VPSDE + EM, n = 50 (e) CLD + EM, n = 50 (f) CLD + SSCS, n = 50 (g) VPSDE + EM, n = 100 (h) CLD + EM, n = 100 (i) CLD + SSCS, n = 100 (j) VPSDE + EM, n = 200 (k) CLD + EM, n = 200 (l) CLD + SSCS, n = 200 Mixture of Normals: numerical simulation with analytical score function for different diffusions (VPSDE with EM vs. CLD with EM/SSCS) and number of synthesis steps n. A visualization of the data distribution can be found in Fig. 10a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Frobenius norm J F (t) of the neural network defining the score function for different t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Data distribution and model samples for multi-scale toy experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Additional samples using EM-QS with 2000 function evaluations. This setup gave us our best FID score of 2.23.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Additional samples using SSCS-QS. This setup resulted in an FID score of 3.07 using only 150 function evaluations. F.3 CELEBA-HQ-256 -EXTENDED RESULTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Samples generated by our model on the CelebA-HQ-256 dataset using our SSCS solver.(b) ODE solver error tolerance 10 ?4 ; 190 average NFE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Unconditional CIFAR-10 generative performance.</figDesc><table><row><cell>Class</cell><cell>Model</cell><cell>NLL?</cell><cell>FID?</cell></row><row><cell>Score</cell><cell>CLD-SGM (Prob. Flow) (ours) CLD-SGM (SDE) (ours)</cell><cell>?3.31 -</cell><cell>2.25 2.23</cell></row><row><cell></cell><cell>DDPM++, VPSDE (Prob. Flow) (Song et al., 2021c)</cell><cell>3.13</cell><cell>3.08</cell></row><row><cell></cell><cell>DDPM++, VPSDE (SDE) (Song et al., 2021c)</cell><cell>-</cell><cell>2.41</cell></row><row><cell></cell><cell>DDPM++, sub-VP (Prob. Flow)</cell><cell></cell><cell></cell></row><row><cell>Score</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>(right) Performance using adaptive stepsize solvers (ODE is based on probability flow, GGF simulates generative SDE). ?: taken from Jolicoeur-Martineau et al. (2021a). LSGM corresponds to the small LSGM-100M model for fair comparison (details in App. E.2.7). Error tolerances were chosen to obtain similar NFEs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>(bottom) Performance using non-adaptive stepsize solvers (for PC, QS performed poorly). ?: 2.23 FID is our evaluation,<ref type="bibr" target="#b94">Song et al. (2021c)</ref> reports 2.20 FID. See Tab. 9 in App. F.2 for extended results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="3">Solver NFEs? FID?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLD</cell><cell>ODE</cell><cell>312</cell><cell>2.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VPSDE GGF</cell><cell>330</cell><cell>2.56  ?</cell></row><row><cell>Model</cell><cell>Sampler</cell><cell cols="6">FID at n function evaluations ? n=50 n=150 n=275 n=500 n=1000 n=2000</cell><cell cols="2">VESDE GGF CLD ODE</cell><cell>488 147</cell><cell>2.99  ? 2.71</cell></row><row><cell>CLD CLD</cell><cell>EM-QS SSCS-QS</cell><cell>52.7 20.5</cell><cell>7.00 3.07</cell><cell>3.24 2.38</cell><cell>2.41 2.25</cell><cell>2.27 2.30</cell><cell>2.23 2.29</cell><cell cols="2">VPSDE ODE VPSDE GGF VESDE ODE</cell><cell>141 151 182</cell><cell>2.76 2.73  ? 7.63</cell></row><row><cell cols="2">VPSDE EM-QS</cell><cell>28.2</cell><cell>4.06</cell><cell>2.65</cell><cell>2.47</cell><cell>2.66</cell><cell>2.60</cell><cell cols="2">VESDE GGF</cell><cell>170</cell><cell>10.15  ?</cell></row><row><cell cols="2">VESDE PC</cell><cell>460</cell><cell>216</cell><cell>11.2</cell><cell>3.75</cell><cell>2.43</cell><cell>2.23  ?</cell><cell>LSGM</cell><cell>ODE</cell><cell>131</cell><cell>4.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Mass hyperpa-</cell></row><row><cell>rameter.</cell><cell></cell></row><row><cell cols="2">M ?1 NLL? FID?</cell></row><row><cell>1 4 16</cell><cell>?3.30 3.23 ?3.37 3.14 ?3.26 3.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>: Initial velocity</cell></row><row><cell cols="2">distribution width.</cell></row><row><cell>?</cell><cell>NLL? FID?</cell></row><row><cell cols="2">0.04 ?3.37 3.14 0.4 ?3.15 3.21 1 ?3.15 3.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Score Matching Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Scalable Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.3 Sampling from CLD-based SGMs . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Sampling Speed and Synthesis Quality Trade-Offs . . . . . . . . . . . . . . . . . . 8 5.3 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Different Damping Ratios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Very High Friction Limit and Connections to previous SDEs in SGMs . . . . . . . 18 Perturbation Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Convergence and Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 CLD Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.4 CLD-specific Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . 24 B.5 Lower Bounds and Probability Flow ODE . . . . . . . . . . . . . . . . . . . . . . 24 B.6 On Introducing a Hamiltonian Component into the Diffusion . . . . . . . . . . . . 26 Gradient Variance Reduction via HSM . . . . . . . . . . . . . . . . . . . . . . . . 28 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.2 Derivation and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Score and Jacobian Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.2 Image Modeling Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.2.1 Training Details and Model Architectures . . . . . . . . . . . . . . . . . . 36 E.2.2 CIFAR-10 Results for VESDE and VPSDE . . . . . . . . . . . . . . . . . 36 E.2.3 Quadratic Striding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 E.2.4 Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 E.2.5 Solver Error Tolerances for Runge-Kutta 4(5) . . . . . . . . . . . . . . . . 38 E.2.6 Ablation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 E.2.7 LSGM-100M Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Toy Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 F.1.1 Analytical Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 F.1.2 Maximum Likelihood Training . . . . . . . . . . . . . . . . . . . . . . . . 39 F.2 CIFAR-10 -Extended Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 F.3 CelebA-HQ-256 -Extended Results . . . . . . . . . . . . . . . . . . . . . . . . 45 Forward Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 G.1.1 Proof of Correctness of the Mean . . . . . . . . . . . . . . . . . . . . . . 50 G.1.2 Proof of Correctness of the Covariance . . . . . . . . . . . . . . . . . . . 51 G.2 Analytical Splitting Term of SSCS . . . . . . . . . . . . . . . . . . . . . . . . . . 52 G.2.1 Proof of Correctness of the Mean . . . . . . . . . . . . . . . . . . . . . . 53 G.2.2 Proof of Correctness of the Covariance . . . . . . . . . . . . . . . . . . . 53 A LANGEVIN DYNAMICS Here, we discuss different aspects of Langevin dynamics. Recall the Langevin dynamics, Eq. (5), from the main paper:</figDesc><table><row><cell>1 Introduction 2 Background 3 Critically-Damped Langevin Diffusion 3.1 4 Related Work E.1 F Additional Experiments</cell><cell>1 2 3 6 39</cell></row><row><cell>5 Experiments 5.1 6 Conclusions F.1 G Proofs of Perturbation Kernels</cell><cell>7 9 50</cell></row><row><cell>7 Ethics and Reproducibility G.1</cell><cell>10</cell></row><row><cell>References</cell><cell>10</cell></row><row><cell>A Langevin Dynamics</cell><cell>18</cell></row><row><cell>A.1 B Critically-Damped Langevin Diffusion</cell><cell>20</cell></row><row><cell>B.1 C HSM: Hybrid Score Matching</cell><cell>26</cell></row><row><cell>C.1 D Symmetric Splitting CLD Sampler (SSCS)</cell><cell>29</cell></row><row><cell>D.1 E Implementation and Experiment Details</cell><cell>34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Model architectures as well as SDE and training setups for our experiments on CIFAR-10 and CelebA-HQ-256.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Influence of denoising step on FID scores (using our main CIFAR-10 model).Influence of Denoising on Results. For SDEs acting in the data space directly, it has been reported that this denoising step is crucial to obtain good FID scores Jolicoeur-Martineau et al. (2021b);</figDesc><table><row><cell>Sampler</cell><cell cols="2">FID at n function evaluations ? Denoising n=50 n=500</cell></row><row><cell>SSCS</cell><cell>81.1</cell><cell>2.30</cell></row><row><cell>SSCS-QS</cell><cell>20.5</cell><cell>2.25</cell></row><row><cell>SSCS</cell><cell>78.9</cell><cell>2.32</cell></row><row><cell>SSCS-QS</cell><cell>28.5</cell><cell>2.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performance (measured in negative log-likelihood) using analytical scores for non-adaptive stepsize solvers for varying numbers of synthesis steps n (function evaluations).</figDesc><table><row><cell>Model</cell><cell cols="5">? log p(x) at n function evaluations ? Sampler n=20 n=50 n=100 n=200</cell></row><row><cell>CLD</cell><cell>EM</cell><cell>60.6</cell><cell>9.71</cell><cell>0.72</cell><cell>-1.04</cell></row><row><cell>CLD</cell><cell>SSCS</cell><cell>10.5</cell><cell>1.55</cell><cell>-1.25</cell><cell>-1.54</cell></row><row><cell cols="2">VPSDE EM</cell><cell>14.2</cell><cell>4.68</cell><cell>-0.35</cell><cell>-1.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Performance using non-adaptive step size solvers. Extended version of Tab. 3.</figDesc><table><row><cell>Model</cell><cell>Sampler</cell><cell cols="6">FID at n function evaluations ? n=50 n=150 n=275 n=500 n=1000 n=2000</cell></row><row><cell>CLD</cell><cell>EM</cell><cell>143</cell><cell>31.5</cell><cell>10.9</cell><cell>3.96</cell><cell>2.50</cell><cell>2.27</cell></row><row><cell>CLD</cell><cell>EM-QS</cell><cell>52.7</cell><cell>7.00</cell><cell>3.24</cell><cell>2.41</cell><cell>2.27</cell><cell>2.23</cell></row><row><cell>CLD</cell><cell>SSCS</cell><cell>81.1</cell><cell>10.5</cell><cell>2.86</cell><cell>2.30</cell><cell>2.32</cell><cell>2.29</cell></row><row><cell>CLD</cell><cell>SSCS-QS</cell><cell>20.5</cell><cell>3.07</cell><cell>2.38</cell><cell>2.25</cell><cell>2.30</cell><cell>2.29</cell></row><row><cell cols="2">VPSDE EM</cell><cell>92.0</cell><cell>30.3</cell><cell>13.1</cell><cell>4.42</cell><cell>2.46</cell><cell>2.43</cell></row><row><cell cols="2">VPSDE EM-QS</cell><cell>28.2</cell><cell>4.06</cell><cell>2.65</cell><cell>2.47</cell><cell>2.66</cell><cell>2.60</cell></row><row><cell cols="2">VPSDE DDIM</cell><cell>6.04</cell><cell>4.04</cell><cell>3.53</cell><cell>3.26</cell><cell>3.09</cell><cell>3.01</cell></row><row><cell cols="2">VPSDE DDIM-QS</cell><cell>3.78</cell><cell>3.15</cell><cell>3.05</cell><cell>2.99</cell><cell>2.96</cell><cell>2.95</cell></row><row><cell cols="2">VESDE PC</cell><cell>460</cell><cell>216</cell><cell>11.2</cell><cell>3.75</cell><cell>2.43</cell><cell>2.23</cell></row><row><cell cols="2">VESDE PC-QS</cell><cell>461</cell><cell>388</cell><cell>155</cell><cell>5.47</cell><cell>11.4</cell><cell>11.2</cell></row><row><cell cols="4">F.2 CIFAR-10 -EXTENDED RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We call the auxiliary variables velocities, as they play a similar role as velocities in physical systems. Formally, our velocity variables would rather correspond to physical momenta, but the term momentum is already widely used in machine learning and our mass M is unitless anyway.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The Fokker-Planck operator is also known as Kolmogorov operator. If the underlying dynamics is fully Hamiltonian, it corresponds to the Liouville operator<ref type="bibr" target="#b58">(Leimkuhler &amp; Matthews, 2015;</ref><ref type="bibr" target="#b98">Tuckerman, 2010)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For our experiments, we only used constant ?; however, for generality, we present all derivations for time-dependent ?(t).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Arguably, the most prominent algorithm that follows this practice is principal component analysis (PCA).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In principle, the error of the splitting scheme is defined more specifically by the commutator of the noncommuting Fokker-Planck operators. See, for example<ref type="bibr" target="#b57">Leimkuhler &amp; Matthews (2013;</ref>;<ref type="bibr" target="#b98">Tuckerman (2010)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The Leapfrog integrator corresponds to the velocity Verlet integrator in molecular dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/yang-song/score_sde_pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55 9 Denoising has not been used in the original LSGM work<ref type="bibr" target="#b104">(Vahdat et al., 2021)</ref> and is not needed in their case, since the output of the latent SGM lives in a smooth latent space and is further processed by a decoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">For the ML objective of the VPSDE, we refer the reader to<ref type="bibr" target="#b93">Song et al. (2021b)</ref>.11  The VPSDE-based model with mixed score parameterization did not converge to the target distribution, and therefore is not included inFig. 12.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ? 4?(t)? ?1 ? v t ,(128)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this section, we prove the correctness of the perturbation kernels of the forward diffusion (App. B.1) as well as for the analytical splitting term in our SSCS (App. D.2). All derivations are presented for general time-dependent ?(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 FORWARD DIFFUSION</head><p>We have the following ODEs describing the evolution of the mean and the covariance matrix</p><p>where</p><p>In App. B.1, we claim the following solutions:</p><p>and</p><p>where B(t) = t 0 ?(t) dt and ? 0 = [x 0 , v 0 ] as well as ? xx 0 and ? vv 0 are initial conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1.1 PROOF OF CORRECTNESS OF THE MEAN</head><p>Plugging the claimed solution (Eqs. (114)-(118)) back into the ODE (Eq. 110), we obtain</p><p>The above can be decomposed into two equations:</p><p>Eq. (127): Plugging the claimed solution into Eq. (127), we obtain:</p><p>Eq. (128): After simplification, plugging in the claimed solution into Eq. (128), we obtain:</p><p>This completes the proof of the correctness of the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1.2 PROOF OF CORRECTNESS OF THE COVARIANCE</head><p>Plugging the claimed solution (Eqs. (119)-(125)) back in the ODE (Eq. <ref type="formula">(111)</ref>), we obtain</p><p>Noting that</p><p>and</p><p>as well as the fact that dDt dt = ?4?(t)? ?1 D t , we can decompose Eq. (131) into three equations:</p><p>Eq. <ref type="formula">(134)</ref>: Plugging the claimed solution into Eq. (134), we obtain</p><p>Eq. (135): After simplification, plugging the claimed solution into Eq. (135), we obtain</p><p>Eq. <ref type="formula">(136)</ref>: After simplification, plugging the claimed solution into Eq. <ref type="formula">(136)</ref>, we obtain</p><p>This completes the proof of the correctness of the covariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 ANALYTICAL SPLITTING TERM OF SSCS</head><p>We have the following ODEs describing the evolution of the mean and the covariance matrix</p><p>These ODEs are very similar to the ODEs of the forward diffusion in App. G.1, the only difference being flipped signs in the off-diagonal terms of f (T ? t) (highlighted in red).</p><p>In App. D.2, we claim the following solutions</p><p>and?</p><p>where</p><p>is an initial condition. Differences of the above solution to the solutions of the forward diffusion are again highlighted in red. Note that by construction the initial covariance for the analytical splitting term of SSCS is the zero matrix, i.e., ? xx t =? xv t =? vv t = 0, since we always initialize from an "updated sample", which itself does not have any uncertainty. Also note that in this derivation we use general initial t (whereas in App. G.1 we set t = 0 for simplicity).</p><p>Plugging the claimed solution (Eqs. (144)-(148)) into the ODE (Eq. (140)), we obtai?</p><p>The above can be decomposed into two equations:</p><p>where we used the fact that dCt</p><p>Eq. <ref type="formula">(157)</ref>: Plugging the claimed solution into Eq. (157), we obtain:</p><p>Eq. <ref type="formula">(128)</ref>: After simplification, plugging the claimed solution into Eq. (128), we obtain:</p><p>This completes the proof of the correctness of the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 PROOF OF CORRECTNESS OF THE COVARIANCE</head><p>Plugging the claimed solution (Eqs. (149)-(155)) into the ODE (Eq. (141)), we obtain <ref type="bibr">161)</ref> with f = f (T ? t) and G = G(T ? t).</p><p>Noting that</p><p>and</p><p>as well as the fact dDt dt = ?4?(T ? t)? ?1 D t , we can decompose Eq. (161) into three equations:</p><p>Eq. (164): Plugging the claimed solution into Eq. (164), we obtain</p><p>Eq. (166): After simplification, plugging the claimed solution into Eq. <ref type="formula">(166)</ref>, we obtain</p><p>@ @ @ @ @ @ @ @ ??(T ? t)e 4B(t)? ?1 @ @ @ @ @ +?(T ? t)? @ @ @ @ @ @ @ ?4?(T ? t)B(t) = @ @ @ @ @ @ @ @ @ @ 2?(T ? t) ?4B 2 (t)? ?1 @ @ @ @ @ @ @ @ @ +2??(T ? t)e 4B(t)? ?1 .</p><p>This completes the proof of the correctness of the covariance.</p><p>To connect back to the SSCS as presented in App. D.2, recall that in practice we use constant ? (and T = 1) and that we solve for small time steps of size ?t 2 , such that B(t) = ? ?t 2 , which leads to the expressions presented in App. D.2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GSNs: generative stochastic networks. Information and Inference: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno>2049-8764</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="249" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Molecular dynamics simulations at constant pressure and/or temperature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andersen</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/abs/10.1063/1.439486?casa_token=gHLpgunpqikAAAAA:NCq8f3NXxPmAtB2UcwiTHgyAKuf1N7hYlse-CRwd-0TrmV4Ryepni12JTJD_gN1nrfkBmdcnGGbOXg</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2384" to="2393" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Contrastive Learning Approach for Training Variational Autoencoder Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Refining Deep Generative Models via Discriminator Gradient Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Abdul Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Liang Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured Denoising Diffusion Models in Discrete State-Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kadmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno type="DOI">https:/www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745</idno>
	</analytic>
	<monogr>
		<title level="j">Jascha Sohl-Dickstein, and Surya Ganguli. Statistical Mechanics of Deep Learning. Annual Review of Condensed Matter Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="501" to="528" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The tools of generative art, from flash to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art in America</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Generative Stochastic Networks Trainable by Backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Generate Samples from Noise through Infusion Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Importance Weighted Autoencoders.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate sampling using Langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Bussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
		<idno type="DOI">https:/journals.aps.org/pre/abstract/10.1103/PhysRevE.75.056707</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">56707</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Canonical sampling through velocity rescaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Bussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Donadio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/full/10.1063/1.2408420?casa_token=r7dJcjelUI4AAAAA%3AwY02eEQqoEkKUl5giwbKjyD3DWpgPDrmolbb2a37SIfbFvymGKu5-8D_cyzbsUlHzlzKWaCNOROK4Q</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14101</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Langevin Equation with Colored Noise for Constant-Temperature Molecular Dynamics Simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Ceriotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Bussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
		<idno type="DOI">https:/journals.aps.org/prl/abstract/10.1103/PhysRevLett.102.020601</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">20601</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient stochastic thermostatting of path integral molecular dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Ceriotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Markland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Manolopoulos</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/full/10.1063/1.3489925?casa_token=zlE_DVhY8wEAAAAA%3Ajy5DTUfsR4M_hROG9gXNaeN0-z0vOMkZok5_DkXM69_Ewl68BiSnbgs6ZsbhH1xcNzCslKmJI6604A</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124104</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VFlow: More Expressive Generative Flows with Variational Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wave-Grad: Estimating Gradients for Waveform Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual Flows for Invertible Generative Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic Gradient Hamiltonian Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian Sampling Using Stochastic Gradient Thermostats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Skeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A family of embedded Runge-Kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Implicit Generation and Modeling with Energy Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3608" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hybrid Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Simon Duane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chie</forename><surname>Furusawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinya</forename><surname>Kitaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Odagiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14518</idno>
		<title level="m">Generative Probabilistic Image Colorization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Energy-Based Models by Diffusion Recovery Likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AutoGAN: Neural Architecture Search for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3224" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Time Reversal of Diffusions. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1188" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<title level="m">Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Canonical dynamics: Equilibrium phase-space distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoover</surname></persName>
		</author>
		<idno type="DOI">https:/journals.aps.org/pra/abstract/10.1103/PhysRevA.31.1695</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1695" to="1697" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07101</idno>
		<title level="m">Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Variational Perspective on Diffusion-Based Generative Models and Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Thermostat Algorithms for Molecular Dynamics Simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">H</forename><surname>H?nenberger</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/b99427</idno>
	</analytic>
	<monogr>
		<title level="j">Advanced Computer Simulation. Advances in Polymer Science</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Estimation of Non-Normalized Statistical Models by Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jarzynski</surname></persName>
		</author>
		<idno type="DOI">https:/journals.aps.org/pre/abstract/10.1103/PhysRevE.56.5018</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="5018" to="5035" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonequilibrium Equality for Free Energy Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jarzynski</surname></persName>
		</author>
		<idno type="DOI">https:/journals.aps.org/prl/abstract/10.1103/PhysRevLett.78.2690</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="2690" to="2693" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Equalities and Inequalities: Irreversibility and the Second Law of Thermodynamics at the Nanoscale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jarzynski</surname></persName>
		</author>
		<idno type="DOI">https:/www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-062910-140506?casa_token=_6kz1zfmVlUAAAAA%3AlephG2iwVxBpn6FPrRqLhiPPExfW0yHYNqZWeVnOtKJBP9tDP_xlH82Bbty8lXSVtIYYx53exvSpJA</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Condensed Matter Physics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="329" to="351" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Byoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01409</idno>
		<title level="m">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<title level="m">Tal Kachman, and Ioannis Mitliagkas. Gotta Go Fast When Generating Data with Score-Based Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Remi Tachet des Combes</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distribution Augmentation for Generative Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05527</idno>
		<title level="m">Score Matching Model for Unbounded Data Score</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Glow: Generative Flow with Invertible 1x1 Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Numerical Solution of Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On Fast Sampling of Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">From classical to quantum and back: Hamiltonian adaptive resolution path integral, ring polymer, and centroid molecular dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaello</forename><surname>Potestio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Tuckerman</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/full/10.1063/1.5000701?casa_token=LklzDV2S3KAAAAAA%3AU6Vj2gJBDInu3VmuXLomKXlQb8GRI64y_p0kzkF-YnXveQvb4WmxVRaELXuLWes8jXMo0vpGxYgr2w</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">244104</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rational Construction of Stochastic Numerical Methods for Molecular Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedict</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics Research eXpress</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="56" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedict</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Dynamics: With Deterministic and Stochastic Numerical Methods. Interdisciplinary Applied Mathematics</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedict</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<title level="m">Simulating Hamiltonian Dynamics. Cambridge Monographs on Applied and Computational Mathematics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srdiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14951</idno>
		<title level="m">Single Image Super-Resolution with Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02388</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Diffusion Probabilistic Models for 3D Point Cloud Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Interpretation and Generalization of Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.5555/1795114.1795156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-An</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niladri</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00996</idno>
		<title level="m">Is There an Analog of Nesterov Acceleration for MCMC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Nos?-Hoover chains: The canonical ensemble via continuous dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">J</forename><surname>Martyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tuckerman</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/abs/10.1063/1.463940?casa_token=l48jpMUJAoAAAAAA:_NYtqr8KZl3osQR455RPx87yFSah8FSKjH3G9bNQErXYVTinvxJ0ka2HnfSbZEZPDpvNMyBTIOJo1Q</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2635" to="2643" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Classical Mechanics: From Newton to Einstein: A Modern Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken, N.J.</pubPlace>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">SDEdit: Image Synthesis and Editing with Stochastic Differential Equations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The Creation and Detection of Deepfakes: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3425780?casa_token=dNM1WT1Tk5cAAAAA:EY6oH11-RXcmnv683wpcTq_LnG_M6CpJzlCYiJYVcIle8DQM6fuZGMuVyrKNw6-nEy9gsgRO9Atibw</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2021/paper/000058.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Society for Music Information Retrieval Conference</title>
		<meeting>the 22nd International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-An</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Contrastive Divergence in Gaussian Diffusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2238" to="2252" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07582</idno>
		<title level="m">Non Gaussian Denoising Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1023/A:1008923215028</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">MCMC Using Hamiltonian Dynamics. Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="113" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11573</idno>
		<title level="m">Quoc Viet Hung Nguyen, Cuong M. Nguyen, Dung Nguyen, Duc Thanh Nguyen, and Saeid Nahavandi. Deep Learning for Deepfakes Creation and Detection: A Survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improved Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A unified formulation of the constant temperature molecular dynamics methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuichi</forename><surname>Nos?</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/abs/10.1063/1.447334?casa_token=qwfnwtVbU_wAAAAA:Q4eno4quZFIqV3J5nprUTUl88IiAb64BX1yIu4J13vdVF0DBs_gviYlLVI1Wuahg73n_Kgiv5z-6Bw</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="511" to="519" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel Recurrent Neural Networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dual Contradistinctive Generative Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="823" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<idno>0041-5553</idno>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Modify the Improved Euler scheme to integrate stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.0933</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Image Super-Resolution via Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<title level="m">Noise Estimation for Generative Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Applied Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>S?rkk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05358</idno>
		<title level="m">UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedict</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06819</idno>
		<title level="m">D2C: Diffusion-Denoising Models for Few-shot Conditional Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Minimum Probability Flow Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Deweese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Implicit Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Maximum Likelihood Training of Score-Based Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">On the Construction and Comparison of Difference Schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Strang</surname></persName>
		</author>
		<idno type="DOI">https:/epubs.siam.org/doi/abs/10.1137/0705041?casa_token=cIRVUWnT6RAAAAAA:PGihxI3dblGhEJmdxZIjycMefwGSrLqsyVXTqEbj2bQegcUI5wCZTMAGMccN8LxhAnCS5-wRVDc</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="517" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">On the Product of Semi-Groups of Operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Trotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="545" to="551" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Reversible multiple time scale molecular dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tuckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Berne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Martyna</surname></persName>
		</author>
		<idno type="DOI">https:/aip.scitation.org/doi/abs/10.1063/1.463137?casa_token=ny5RBZegCksAAAAA:c9PLT8Lw750OYq3x9LN3uSJbYNO-dSEinhmIJ8N5sYpN5egrEl_bs0-D2biWwSPEezxbidMbpzYCRw</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Tuckerman</surname></persName>
		</author>
		<title level="m">Statistical Mechanics: Theory and Molecular Simulation</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chadwick</surname></persName>
		</author>
		<idno type="DOI">https:/journals.sagepub.com/doi/full/10.1177/2056305120903408</idno>
	</analytic>
	<monogr>
		<title level="j">Social Media + Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2056305120903408</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Score-based Generative Modeling in Latent Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A Connection Between Score Matching and Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Learning to Efficiently Sample from Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Making Convolutional Networks Shift-Invariant Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Differentiable Augmentation for Data-Efficient GAN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">3D Shape Generation and Completion through Point-Voxel Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. where we used the fact that dCt dt = ?2?(t)? ?1 C t</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. where we used the fact that dCt dt = ?2?(t)? ?1 C t</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
