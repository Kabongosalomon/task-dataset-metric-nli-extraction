<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Variational Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Likelihood-based generative modeling is a central task in machine learning that is the basis for a wide range of applications ranging from speech synthesis <ref type="bibr">[Oord et al., 2016]</ref>, to translation <ref type="bibr">[Sutskever et al., 2014]</ref>, to compression <ref type="bibr">[MacKay, 2003]</ref>, to many others. Autoregressive models have long been the dominant model class on this task due to their tractable likelihood and expressivity, as shown in <ref type="figure">Figure 1</ref>. Diffusion models have recently shown impressive results in image <ref type="bibr" target="#b9">[Ho et al., 2020</ref><ref type="bibr">, Song et al., 2021b</ref><ref type="bibr">, Nichol and Dhariwal, 2021</ref> and audio generation <ref type="bibr" target="#b17">[Kong et al., 2020</ref> in terms of perceptual quality, but have yet to match autoregressive models on density estimation benchmarks. In this paper we make several technical contributions that allow diffusion models to challenge the dominance of autoregressive models in this domain. Our main contributions are as follows:</p><p>? We introduce a flexible family of diffusion-based generative models that achieve new stateof-the-art log-likelihoods on standard image density estimation benchmarks (CIFAR-10 and ImageNet). This is enabled by incorporating Fourier features into the diffusion model and using a learnable specification of the diffusion process, among other modeling innovations. ? We improve our theoretical understanding of density modeling using diffusion models by analyzing their variational lower bound (VLB), deriving a remarkably simple expression in * Equal contribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet 64x64</head><p>State-of-the-art models in each of the 5 past years (lower is better) (b) ImageNet 64x64 <ref type="figure">Figure 1</ref>: Autoregressive generative models were long dominant in standard image density estimation benchmarks. In contrast, we propose a family of diffusion-based generative models, Variational Diffusion Models (VDMs), that outperforms contemporary autoregressive models in these benchmarks. See <ref type="table" target="#tab_2">Table 1</ref> for more results and comparisons.</p><p>terms of the signal-to-noise ratio of the diffusion process. This result delivers new insight into the model class: for the continuous-time (infinite-depth) setting we prove a novel invariance of the generative model and its VLB to the specification of the diffusion process, and we show that various diffusion models from the literature are equivalent up to a trivial time-dependent rescaling of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work builds on diffusion probabilistic models (DPMs) <ref type="bibr">[Sohl-Dickstein et al., 2015]</ref>, or diffusion models in short. DPMs can be viewed as a type of variational autoencoder (VAE) <ref type="bibr">Welling, 2013, Rezende et al., 2014]</ref>, whose structure and loss function allows for efficient training of arbitrarily deep models. Interest in diffusion models has recently reignited due to their impressive image generation results <ref type="bibr">[Ho et al., 2020, Song and</ref><ref type="bibr">Ermon, 2020]</ref>. <ref type="bibr" target="#b9">Ho et al. [2020]</ref> introduced a number of model innovations to the original DPM, with impressive results on image generation quality benchmarks. They showed that the VLB objective, for a diffusion model with discrete time and diffusion variances shared across input dimensions, is equivalent to multiscale denoising score matching, up to particular weightings per noise scale. Further improvements were proposed by Nichol and Dhariwal <ref type="bibr">[2021]</ref>, resulting in better log-likelihood scores. <ref type="bibr" target="#b5">Gao et al. [2020]</ref> show how diffusion can also be used to efficiently optimize energy-based models (EBMs) towards a close approximation of the log-likelihood objective, resulting in high-fidelity samples even after long MCMC chains.</p><p>Song and Ermon <ref type="bibr">[2019]</ref> first proposed learning generative models through a multi-scale denoising score matching objective, with improved methods in Song and Ermon <ref type="bibr">[2020]</ref>. This was later extended to continuous-time diffusion with novel sampling algorithms based on reversing the diffusion process <ref type="bibr">[Song et al., 2021b]</ref>.</p><p>Concurrent to our work, <ref type="bibr">Song et al. [2021a]</ref>, <ref type="bibr" target="#b10">Huang et al. [2021], and</ref><ref type="bibr">Vahdat et al. [2021]</ref> also derived variational lower bounds to the data likelihood under a continuous-time diffusion model. Where we consider the infinitely deep limit of a standard <ref type="bibr">VAE, Song et al. [2021a]</ref> and <ref type="bibr">Vahdat et al. [2021]</ref> present different derivations based on stochastic differential equations. <ref type="bibr" target="#b10">Huang et al. [2021]</ref> considers both perspectives and discusses the similarities between the two approaches. An advantage of our analysis compared to these other works is that we present an intuitive expression of the VLB in terms of the signal-to-noise ratio of the diffused data, leading to much simplified expressions of the discrete-time and continuous-time loss, allowing for simple and numerically stable implementation. This also leads to new results on the invariance of the generative model and its VLB to the specification of the diffusion process. We empirically compare to these works, as well as others, in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Previous approaches to diffusion probabilistic models fixed the diffusion process; in contrast optimize the diffusion process parameters jointly with the rest of the model. This turns the model into a type of VAE <ref type="bibr">Welling, 2013, Rezende et al., 2014]</ref>. This is enabled by directly parameterizing the mean and variance of the marginal q(z t |z 0 ), where previous approaches instead parameterized the individual diffusion steps q(z t+ |z t ). In addition, our denoising models include several architecture changes, the most important of which is the use of Fourier features, which enable us to reach much better likelihoods than previous diffusion probabilistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We will focus on the most basic case of generative modeling, where we have a dataset of observations of x, and the task is to estimate the marginal distribution p(x). As with most generative models, the described methods can be extended to the case of multiple observed variables, and/or the task of estimating conditional densities p(x|y). The proposed latent-variable model consists of a diffusion process (Section 3.1) that we invert to obtain a hierarchical generative model (Section 3.3). As we will show, the model choices below result in a surprisingly simple variational lower bound (VLB) of the marginal likelihood, which we use for optimization of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forward time diffusion process</head><p>Our starting point is a Gaussian diffusion process that begins with the data x, and defines a sequence of increasingly noisy versions of x which we call the latent variables z t , where t runs from t = 0 (least noisy) to t = 1 (most noisy). The distribution of latent variable z t conditioned on x, for any t ? [0, 1] is given by:</p><formula xml:id="formula_0">q(z t |x) = N ? t x, ? 2 t I ,<label>(1)</label></formula><p>where ? t and ? 2 t are strictly positive scalar-valued functions of t. Furthermore, let us define the signal-to-noise ratio (SNR):</p><formula xml:id="formula_1">SNR(t) = ? 2 t /? 2 t .</formula><p>(2) We assume that the SNR(t) is strictly monotonically decreasing in time, i.e. that SNR(t) &lt; SNR(s) for any t &gt; s. This formalizes the notion that the z t is increasingly noisy as we go forward in time. We also assume that both ? t and ? 2 t are smooth, such that their derivatives with respect to time t are finite. This diffusion process specification includes the variance-preserving diffusion process as used by <ref type="bibr">[Sohl-Dickstein et al., 2015</ref><ref type="bibr" target="#b9">, Ho et al., 2020</ref> as a special case, where ? t = 1 ? ? 2 t . Another special case is the variance-exploding diffusion process as used by <ref type="bibr" target="#b5">[Song and</ref><ref type="bibr">Ermon, 2019, Song et al., 2021b]</ref>, where ? 2 t = 1. In experiments, we use the variance-preserving version. The distributions q(z t |z s ) for any t &gt; s are also Gaussian, and given in Appendix A. The joint distribution of latent variables (z s , z t , z u ) at any subsequent timesteps 0 ? s &lt; t &lt; u ? 1 is Markov: q(z u |z t , z s ) = q(z u |z t ). Given the distributions above, it is relatively straightforward to verify through Bayes rule that q(z s |z t , x), for any 0 ? s &lt; t ? 1, is also Gaussian. This distribution is also given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noise schedule</head><p>In previous work, the noise schedule has a fixed form (see Appendix H, <ref type="figure">Fig. 4a</ref>). In contrast, we learn this schedule through the parameterization ? 2 t = sigmoid(? ? (t)) (3) where ? ? (t) is a monotonic neural network with parameters ?, as detailed in Appendix H.</p><p>Motivated by the equivalence discussed in Section 5.1, we use ? t = 1 ? ? 2 t in our experiments for both the discrete-time and continuous-time models, i.e. variance-preserving diffusion processes. It is straightforward to verify that ? 2 t and SNR(t), as a function of ? ? (t), then simplify to:</p><formula xml:id="formula_2">? 2 t = sigmoid(?? ? (t)) (4) SNR(t) = exp(?? ? (t))<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reverse time generative model</head><p>We define our generative model by inverting the diffusion process of Section 3.1, yielding a hierarchical generative model that samples a sequence of latents z t , with time running backward from t = 1 to t = 0. We consider both the case where this sequence consists of a finite number of steps T , as well as a continuous time model corresponding to T ? ?. We start by presenting the discrete-time case.</p><p>Given finite T , we discretize time uniformly into T timesteps (segments) of width ? = 1/T . Defining s(i) = (i ? 1)/T and t(i) = i/T , our hierarchical generative model for data x is then given by:</p><formula xml:id="formula_3">p(x) = z p(z 1 )p(x|z 0 ) T i=1 p(z s(i) |z t(i) ).<label>(6)</label></formula><p>With the variance preserving diffusion specification and sufficiently small SNR(1), we have that q(z 1 |x) ? N (z 1 ; 0, I). We therefore model the marginal distribution of z 1 as a spherical Gaussian:</p><formula xml:id="formula_4">p(z 1 ) = N (z 1 ; 0, I).<label>(7)</label></formula><p>We wish to choose a model p(x|z 0 ) that is close to the unknown q(x|z 0 ). Let x i and z 0,i be the i-th elements of x, z 0 , respectively. We then use a factorized distribution of the form:</p><formula xml:id="formula_5">p(x|z 0 ) = i p(x i |z 0,i ),<label>(8)</label></formula><p>where we choose p(x i |z 0,i ) ? q(z 0,i |x i ), which is normalized by summing over all possible discrete values of x i (256 in the case of 8-bit image data). With sufficiently large SNR(0), this becomes a very close approximation to the true q(x|z 0 ), as the influence of the unknown data distribution q(x) is overwhelmed by the likelihood q(z 0 |x). Finally, we choose the conditional model distributions as</p><formula xml:id="formula_6">p(z s |z t ) = q(z s |z t , x =x ? (z t ; t)),<label>(9)</label></formula><p>i.e. the same as q(z s |z t , x), but with the original data x replaced by the output of a denoising model x ? (z t ; t) that predicts x from its noisy version z t . Note that in practice we parameterize the denoising model as a function of a noise prediction model (Section 3.4), bridging the gap with previous work on diffusion models <ref type="bibr" target="#b9">[Ho et al., 2020]</ref>. The means and variances of p(z s |z t ) simplify to a remarkable degree; see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Noise prediction model and Fourier features</head><p>We parameterize the denoising model in terms of a noise prediction model? ? (z t ; t):</p><formula xml:id="formula_7">x ? (z t ; t) = (z t ? ? t? ? (z t ; t))/? t ,<label>(10)</label></formula><p>where? ? (z t ; t) is parameterized as a neural network. The noise prediction models we use in experiments closely follow <ref type="bibr" target="#b9">Ho et al. [2020]</ref>, except that they process the data solely at the original resolution. The exact parameterization of the noise prediction model and noise schedule is discussed in Appendix B.</p><p>Prior work on diffusion models has mainly focused on the perceptual quality of generated samples, which emphasizes coarse scale patterns and global consistency of generated images. Here, we optimize for likelihood, which is sensitive to fine scale details and exact values of individual pixels.</p><p>To capture the fine scale details of the data, we propose adding a set of Fourier features to the input of our noise prediction model. Let x be the original data, scaled to the range [?1, 1], and let z be the resulting latent variable, with similar magnitudes. We then append channels sin(2 n ?z) and cos(2 n ?z), where n runs over a range of integers {n min , ..., n max }. These features are high frequency periodic functions that amplify small changes in the input data z t ; see Appendix C for further details. Including these features in the input of our denoising model leads to large improvements in likelihood as demonstrated in Section 6 and <ref type="figure" target="#fig_2">Figure 5</ref>, especially when combined with a learnable SNR function. We did not observe such improvements when incorporating Fourier features into autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Variational lower bound</head><p>We optimize the parameters towards the variational lower bound (VLB) of the marginal likelihood, which is given by</p><formula xml:id="formula_8">? log p(x) ? ?VLB(x) = D KL (q(z 1 |x)||p(z 1 )) Prior loss + E q(z0|x) [? log p(x|z 0 )] Reconstruction loss + L T (x).</formula><p>Diffusion loss <ref type="formula" target="#formula_0">(11)</ref> The prior loss and reconstruction loss can be (stochastically and differentiably) estimated using standard techniques; see <ref type="bibr" target="#b14">[Kingma and Welling, 2013]</ref>. The diffusion loss, L T (x), is more complicated, and depends on the hyperparameter T that determines the depth of the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discrete-time model</head><p>In the case of finite T , using s(i) = (i ? 1)/T , t(i) = i/T , the diffusion loss is:</p><formula xml:id="formula_9">L T (x) = T i=1 E q(z t(i) |x) D KL [q(z s(i) |z t(i) , x)||p(z s(i) |z t(i) )].<label>(12)</label></formula><p>In appendix E we show that this expression simplifies considerably, yielding:</p><formula xml:id="formula_10">L T (x) = T 2 E ?N (0,I),i?U {1,T } (SNR(s) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2 ,<label>(13)</label></formula><p>where U {1, T } is the uniform distribution on the integers {1, . . . , T }, and z t = ? t x + ? t . This is the general discrete-time loss for any choice of forward diffusion parameters (? t , ? t ). When plugging in the specifications of ? t , ? t andx ? (z t ; t) that we use in experiments, given in Sections 3.2 and 3.4, the loss simplifies to:</p><formula xml:id="formula_11">L T (x) = T 2 E ?N (0,I),i?U {1,T } (exp(? ? (t) ? ? ? (s)) ? 1) ?? ? (z t ; t) 2 2<label>(14)</label></formula><p>where z t = sigmoid(?? ? (t))x + sigmoid(? ? (t)) . In the discrete-time case, we simply jointly optimize ? and ? by maximizing the VLB through a Monte Carlo estimator of Equation 14.</p><p>Note that exp(.) ? 1 has a numerically stable primitive expm1(.) in common numerical computing packages; see figure 6. Equation 14 allows for numerically stable implementation in 32-bit or lower-precision floating point, in contrast with previous implementations of discrete-time diffusion models (e.g. <ref type="bibr" target="#b9">[Ho et al., 2020]</ref>), which had to resort to 64-bit floating point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">More steps leads to a lower loss</head><p>A natural question to ask is what the number of timesteps T should be, and whether more timesteps is always better in terms of the VLB. In Appendix F we analyze the difference between the diffusion loss with T timesteps, L T (x), and the diffusion loss with double the timesteps, L 2T (x), while keeping the SNR function fixed. We then find that if our trained denoising modelx ? is sufficiently good, we have that L 2T (x) &lt; L T (x), i.e. that our VLB will be better for a larger number of timesteps. Intuitively, the discrete time diffusion loss is an upper Riemann sum approximation of an integral of a strictly decreasing function, meaning that a finer approximation yields a lower diffusion loss. This result is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>5 Continuous-time model: T ? ? Since taking more time steps leads to a better VLB, we now take T ? ?, effectively treating time t as continuous rather than discrete. The model for p(z t ) can in this case be described as a continuous time diffusion process <ref type="bibr">[Song et al., 2021b]</ref> governed by a stochastic differential equation; see Appendix D. In Appendix E we show that in this limit the diffusion loss L T (x) simplifies further. Letting SNR (t) = dSNR(t)/dt, we have, with z t = ? t x + ? t : This is the general continuous-time loss for any choice of forward diffusion parameters (? t , ? t ).</p><formula xml:id="formula_12">L ? (x) = ? 1 2 E ?N (0,I) 1 0 SNR (t) x ?x ? (z t ; t) 2 2 dt,<label>(15)</label></formula><formula xml:id="formula_13">= ? 1 2 E ?N (0,I),t?U (0,1) SNR (t) x ?x ? (z t ; t) 2 2 .<label>(16)</label></formula><p>When plugging in the specifications of ? t , ? t andx ? (z t ; t) that we use in experiments, given in Sections 3.2 and 3.4, the loss simplifies to:</p><formula xml:id="formula_14">L ? (x) = 1 2 E ?N (0,I),t?U (0,1) ? ? (t) ?? ? (z t ; t) 2 2 ,<label>(17)</label></formula><p>where ? ? (t) = d? ? (t)/dt. We use the Monte Carlo estimator of this loss for evaluation and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Equivalence of diffusion models in continuous time</head><p>The signal-to-noise function SNR(t) is invertible due to the monotonicity assumption in Section 3.1. Due to this invertibility, we can perform a change of variables, and make everything a function</p><formula xml:id="formula_15">of v ? SNR(t) instead of t, such that t = SNR ?1 (v). Let ? v and ? v be the functions ? t and ? t evaluated at t = SNR ?1 (v), and correspondingly let z v = ? v x + ? v . Similarly, we rewrite our noise prediction model asx ? (z, v) ?x ? (z, SNR ?1 (v))</formula><p>. With this change of variables, our continuous-time loss in Equation 15 can equivalently be written as:</p><formula xml:id="formula_16">L ? (x) = 1 2 E ?N (0,I) SNRmax SNRmin x ?x ? (z v , v) 2 2 dv,<label>(18)</label></formula><p>where instead of integrating w.r.t. time t we now integrate w.r.t. the signal-to-noise ratio v, and where SNR min = SNR(1) and SNR max = SNR(0).</p><p>What this equation shows us is that the only effect the functions ?(t) and ?(t) have on the diffusion loss is through the values SNR(t) = ? 2 t /? 2 t at endpoints t = 0 and t = 1. Given these values SNR max and SNR min , the diffusion loss is invariant to the shape of function SNR(t) between t = 0 and t = 1. The VLB is thus only impacted by the function SNR(t) through its endpoints SNR min and SNR max .</p><p>Furthermore, we find that the distribution p(x) defined by our generative model is also invariant to the specification of the diffusion process. Specifically, let p A (x) denote the distribution defined by the combination of a diffusion specification and denoising function {?</p><formula xml:id="formula_17">A v , ? A v ,x A ? }, and similarly let p B (x) be the distribution defined through a different specification {? B v , ? B v ,x B ? }, where both specifications have equal SNR min , SNR max ; as shown in Appendix G, we then have that p A (x) = p B (x) ifx B ? (z, v) ?x A ? ((? A v /? B v )z, v).</formula><p>The distribution on all latents z v is then also the same under both specifications, up to a trivial rescaling. This means that any two diffusion models satisfying the mild constraints set in 3.1 (which includes e.g. the variance exploding and variance preserving specifications considered by <ref type="bibr">Song et al. [2021b]</ref>), can thus be seen as equivalent in continuous time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Weighted diffusion loss</head><p>This equivalence between diffusion specifications continues to hold even if, instead of the VLB, these models optimize a weighted diffusion loss of the form:  Here, w(v) is a weighting function that generally puts increased emphasis on the noisier data compared to the VLB, and which thereby can sometimes improve perceptual generation quality as measured by certain metrics like FID and Inception Score. For the models presented in this paper, we further use w(v) = 1 as corresponding to the (unweighted) VLB.</p><formula xml:id="formula_18">L ? (x, w) = 1 2 E ?N (0,I) SNRmax SNRmin w(v) x ?x ? (z v , v) 2 2 dv,<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Variance minimization</head><p>Lowering the variance of the Monte Carlo estimator of the continuous-time loss generally improves the efficiency of optimization. We found that using a low-discrepancy sampler for t, as explained in Appendix I.1, leads to a significant reduction in variance. In addition, due to the invariance shown in Section 5.1 for the continous-time case, we can optimize the schedule between its endpoints w.r.t. to minimize the variance of our estimator of loss, as detailed in Appendix I. The endpoints of the noise schedule are simply optimized w.r.t. the VLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We demonstrate our proposed class of diffusion models, which we call Variational Diffusion Models (VDMs), on the CIFAR-10 [ <ref type="bibr" target="#b18">Krizhevsky et al., 2009]</ref>   6.1 Likelihood and samples <ref type="table" target="#tab_2">Table 1</ref> shows our results on modeling the CIFAR-10 dataset, and the downsampled ImageNet dataset. We establish a new state-of-the-art in terms of test set likelihood on all considered benchmarks, by a significant margin. Our model for CIFAR-10 without data augmentation surpasses the previous best result of 2.80 about 10x faster than it takes the Sparse Transformer to reach this, in wall clock time on equivalent hardware. Our CIFAR-10 model, whose hyper-parameters were tuned for likelihood, results in a FID (perceptual quality) score of 7.41. This would have been state-of-the-art until recently, but is worse than recent diffusion models that specifically target FID scores <ref type="bibr">[Nichol and Dhariwal, 2021</ref><ref type="bibr">, Song et al., 2021b</ref><ref type="bibr" target="#b9">, Ho et al., 2020</ref>. By instead using a weighted diffusion loss, with the weighting function w(SNR) used by <ref type="bibr" target="#b9">Ho et al. [2020]</ref> and described in Appendix K, our FID score improves to 4.0. We did not pursue further tuning of the model to improve FID instead of likelihood. A random sample of generated images from our model is provided in <ref type="figure" target="#fig_1">Figure 3</ref>. We provide additional samples from this model, as well as our other models for the other datasets, in Appendix M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablations</head><p>Next, we investigate the relative importance of our contributions. In <ref type="table" target="#tab_4">Table 2</ref> we compare our discretetime and continuous-time specifications of the diffusion model: When evaluating our model with a small number of steps, our discretely trained models perform better by learning the diffusion schedule to optimize the VLB. However, as argued theoretically in Section 4.1, we find experimentally that more steps T indeed gives better likelihood. When T grows large, our continuously trained model performs best, helped by training its diffusion schedule to minimize variance instead.</p><p>Minimizing the variance also helps the continuous time model to train faster, as shown in <ref type="figure" target="#fig_2">Figure 5</ref>. This effect is further examined in <ref type="table">Table 4b</ref>, where we find dramatic variance reductions compared to our baselines in continuous time. <ref type="figure">Figure 4a</ref> shows how this effect is achieved: Compared to the other schedules, our learned schedule spends much more time in the high SNR(t) / low ? 2 t range. In <ref type="figure" target="#fig_2">Figure 5</ref> we further show training curves for our model including and excluding the Fourier features proposed in Appendix C: with Fourier features enabled our model achieves much better likelihood. For comparison we also implemented Fourier features in a PixelCNN++ model <ref type="bibr">[Salimans et al., 2017]</ref>, where we do not see a benefit. In addition, we find that learning the SNR is necessary to get the most out of including Fourier features: if we fix the SNR schedule to that used by <ref type="bibr" target="#b9">Ho et al. [2020]</ref>, the maximum log-SNR is fixed to approximately 8 (see <ref type="figure">figure 7)</ref>, and test set negative likelihood stays above 4 bits per dim. When learning the SNR endpoints, our maximum log-SNR ends up at 13.3, which, combined with the inclusion of Fourier features, leads to the SOTA test set likelihoods reported in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Lossless compression</head><p>For a fixed number of evaluation timesteps T eval , our diffusion model in discrete time is a hierarchical latent variable model that can be turned into a lossless compression algorithm using bits-back coding [Hinton and <ref type="bibr" target="#b6">Van Camp, 1993]</ref>. As a proof of concept of practical lossless compression,   ANS <ref type="bibr">[Townsend et al., 2018]</ref>, an implementation of bits-back coding based on asymmetric numeral systems <ref type="bibr" target="#b4">[Duda, 2009]</ref>. Details of our implementation are given in Appendix N. We achieve state-ofthe-art net codelengths, proving our model can be used as the basis of a lossless compression algorithm. However, for large T eval a gap remains with the theoretically optimal codelength corresponding to the negative VLB, and compression becomes computationally expensive due to the large number of neural network forward passes required. Closing this gap with more efficient implementations of bits-back coding suitable for very deep models is an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented state-of-the-art results on modeling the density of natural images using a new class of diffusion models that incorporates a learnable diffusion specification, Fourier features for fine-scale modeling, as well as other architectural innovations. In addition, we obtained new theoretical insight into likelihood-based generative modeling with diffusion models, showing a surprising invariance of the VLB to the forward time diffusion process in continuous time, as well as an equivalence between various diffusion processes from the literature previously thought to be different. A Distribution details</p><formula xml:id="formula_19">A.1 q(z t |z s )</formula><p>The distribution of z t given z s , for any 0 ? s &lt; t ? 1, is given by:</p><formula xml:id="formula_20">q(z t |z s ) = N ? t|s z s , ? 2 t|s I ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_21">? t|s = ? t /? s ,<label>(21)</label></formula><p>and</p><formula xml:id="formula_22">? 2 t|s = ? 2 t ? ? 2 t|s ? 2 s .<label>(22)</label></formula><p>A.2 q(z s |z t , x)</p><p>Due to the Markov property of the forward process, for t &gt; s, we have that q(z s , z t |x) = q(z s |x)q(z t |z s ). The term q(z s |z t , x) can be viewed as a Bayesian posterior resulting from a prior q(z s |x), updated with a likelihood term q(z t |z s ).</p><p>Generally, when we have a Gaussian prior of the form p(x) = N (? A , ? 2 A ) and a linear-Gaussian likelihood of the form p(y|x) = N (ax, ? 2 B ), then the general solution for the posterior is p </p><formula xml:id="formula_23">(x|y) = N (?,? 2 ), where? ?2 = ? ?2 A + a 2 ? ?2 B , and? =? ?2 (? ?2 A ? A + a? ?2 B y).</formula><formula xml:id="formula_24">q(z s |z t , x) = N (? Q (z t , x; s, t), ? 2 Q (s, t)I) (23) where ? ?2 Q (s, t) = ? ?2 s + ? 2 t|s ? ?2 t|s = ? 2 t|s ? 2 s /? 2 t<label>(24)</label></formula><p>and ? Q (z t , x; s, t) = ? t|s ? 2 s ? 2</p><formula xml:id="formula_25">t z t + ? s ? 2 t|s ? 2 t x.<label>(25)</label></formula><p>A.3 p(z s |z t )</p><p>Finally, we choose the conditional model distributions as</p><formula xml:id="formula_26">p(z s |z t ) = q(z s |z t , x =x ? (z t ; t)),<label>(26)</label></formula><p>i.e. the same as q(z s |z t , x), but with the original data x replaced by the output of a denoising model x ? (z t ; t) that predicts x from its noisy version z t . We then have p(z s |z t ) = N (z s ; ? ? (z t ; s, t), ? 2 Q (s, t)I)</p><p>with variance ? 2 Q (s, t) the same as in Equation 24, and</p><formula xml:id="formula_28">? ? (z t ; s, t) = ? t|s ? 2 s ? 2 t z t + ? s ? 2 t|s ? 2 tx ? (z t ; t) = 1 ? t|s z t ? ? 2 t|s ? t|s ? t? ? (z t ; t) = 1 ? t|s z t + ? 2 t|s ? t|s s ? (z t ; t),<label>(28)</label></formula><p>where?</p><formula xml:id="formula_29">? (z t ; t) = (z t ? ? tx? (z t ; t))/? t<label>(29)</label></formula><p>and</p><formula xml:id="formula_30">s ? (z t ; t) = (? tx? (z t ; t) ? z t )/? 2 t .<label>(30)</label></formula><p>Equation <ref type="formula" target="#formula_5">28</ref> shows that we can interpret our model in three different ways: 1. In terms of the denoising modelx ? (z t ; t) that recovers x from its corrupted version z t . 2. In terms of a noise prediction model? ? (z t ; t) that directly infers the noise that was used to generate z t . 3. In terms of a score model s ? (z t ; t), that at its optimum equals the scores of the marginal density: s * (z t ; t) = ? z log q(z t ); see Appendix L.</p><p>These are three equally valid views on the same model class, that have been used interchangeably in the literature. We find the denoising interpretation the most intuitive, and will therefore mostly usex ? (z t ; t) in the theoretical part of this paper, although in practice we parameterize our model vi? ? (z t ; t) following Ho et al. <ref type="bibr">[2020]</ref>. The parameterization of our model is discussed in Appendix B.</p><p>A.4 Further simplification of p(z s |z t )</p><p>After plugging in the specifications of ? t , ? t andx ? (z t ; t) that we use in experiments, given in Sections 3.2 and 3.4, it can be verified that the distribution p(z s |z t ) = N (? ? (z t ; s, t), ? 2 Q (s, t)I) simplifies to:</p><formula xml:id="formula_31">? ? (z t ; s, t) = ? s ? t (z t + ? t expm1(? ? (s) ? ? ? (t))? ? (z t ; t)) (31) ? 2 Q (s, t) = ? 2 s ? (?expm1(? ? (s) ? ? ? (t)))<label>(32)</label></formula><p>where ? s = sigmoid(?? ? (s)), ? t = sigmoid(?? ? (t)), ? 2 s = sigmoid(? ? (s)), ? s = sigmoid(? ? (s)), and where expm1(.) = exp(.) ? 1; see <ref type="figure" target="#fig_4">Figure 6</ref>. Ancestral sampling from this distribution can be performed through simply doing:</p><formula xml:id="formula_32">z s = ? 2 s /? 2 t (z t ? ? t c? ? (z t ; t)) + (1 ? ? 2 s )c<label>(33)</label></formula><p>where ? 2 s = sigmoid(?? ? (s)), ? 2 t = sigmoid(?? ? (t)), c = ?expm1(? ? (s) ? ? ? (t)), and ? N (0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters, architecture, and implementation details</head><p>In this section we provide details on the exact setup for each of our experiments. In Sections B.1 we describe the choices in common to each of our experiments. Hyperparameters specific to the individual experiments are given in Section B.2. We are currently working towards open sourcing our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Model and implementation</head><p>Our denoising models are parameterized in terms of noise prediction models? ? (z t ; ? t ), as explained in Section 3.4. Our noise prediction models? ? closely follow the architecture used by <ref type="bibr" target="#b9">Ho et al. [2020]</ref>, which is based on a U-Net type neural net <ref type="bibr">[Ronneberger et al., 2015]</ref> that maps from the input z ? R d to output of the same dimension. As compared to their publically available code at https://github.com/hojonathanho/diffusion, our implementation differs in the following ways:</p><p>? Our networks don't perform any internal downsampling or upsampling: we process all the data at the original input resolution. ? Our models are deeper than those used by <ref type="bibr" target="#b9">Ho et al. [2020]</ref>. Specific numbers are given in Section B.2. ? Instead of taking time t as input to the noise prediction model, we use ? t , which we rescale to have approximately the same range as t of [0, 1] before using it to form 'time' embeddings in the same way as <ref type="bibr" target="#b9">Ho et al. [2020]</ref>. ? Our models calculate Fourier features on the input data z t as discussed in Appendix C, which are then concatenated to z t before being fed to the U-Net. ? Apart from the middle attention block that connects the upward and downward branches of the U-Net, we remove all other attention blocks from the model. We found that these attention blocks made it more likely for the model to overfit to the training set. ? All of our models use dropout at a rate of 0.1 in the intermediate layers, as did <ref type="bibr" target="#b9">Ho et al. [2020]</ref>. In addition we regularize the model by using decoupled weight decay <ref type="bibr" target="#b19">[Loshchilov and Hutter, 2017]</ref> with coefficient 0.01. ? We use the Adam optimizer with a learning rate of 2e ?4 and exponential decay rates of ? 1 = 0.9, ? 2 = 0.99. We found that higher values for ? 2 resulted in training instabilities.</p><p>? For evaluation, we use an exponential moving average of our parameters, calculated with an exponential decay rate of 0.9999.</p><p>We regularly evaluate the variational bound on the likelihood on the validation set and find that our models do not overfit during training, using the current settings. We therefore do not use early stopping and instead allow the network to be optimized for 10 million parameter updates for CIFAR-10, and for 2 million updates for ImageNet, before obtaining the test set numbers reported in this paper. It looks like our models keep improving even after this number of updates, in terms of likelihood, but we did not explore this systematically due to resource constraints.</p><p>All of our models are trained on TPUv3 hardware (see https://cloud.google.com/tpu) using data parallelism. We also evaluated our trained models using CPU and GPU to check for robustness of our reported numbers to possible rounding errors. We found only very small differences when evaluating on these other hardware platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Settings for each dataset</head><p>Our model for CIFAR-10 with no data augmentation uses a U-Net of depth 32, consisting of 32 ResNet blocks in the forward direction and 32 ResNet blocks in the reverse direction, with a single attention layer and two additional ResNet blocks in the middle. We keep the number of channels constant throughout at 128. This model was trained on 8 TPUv3 chips, with a total batch size of 128 examples. Reaching a test-set BPD of 2.65 after 10 million updates takes 9 days, although our model already surpasses sparse transformers (the previous state-of-the-art) of 2.80 BPD after only 2.5 hours of training.</p><p>For CIFAR-10 with data augmentation we used random flips, 90-degree rotations, and color channel swapping, which were previously shown to help for density estimation by <ref type="bibr" target="#b11">Jun et al. [2020]</ref>. Each of the three augmentations independently were given a 50% probability of being applied to each example, which means that 1 in 8 training examples was not augmented at all. For this experiment, we doubled the number of channels in our model to 256, and decreased the dropout rate from 10% to 5%. Since overfitting was less of a problem with data augmentation, we add back the attention blocks after each ResNet block, following <ref type="bibr" target="#b9">Ho et al. [2020]</ref>. We also experimented with conditioning our model on an additional binary feature that indicates whether or not the example was augmented, which can be seen as a simplified version of the augmentation conditioning proposed by <ref type="bibr" target="#b11">Jun et al. [2020]</ref>. Conditioning made almost no difference to our results, which may be explained by the relatively large fraction (12.5%) of clean data fed to our model during training. We trained our model for slightly over a week on 128 TPUv3 chips to obtain the reported result.</p><p>Our model for 32x32 ImageNet looks similar to that for CIFAR-10 without data augmentation, with a U-Net depth of 32, but uses double the number of channels at 256. It is trained using data parallelism on 32 TPUv3 chips, with a total batch size of 512.</p><p>Our model for 64x64 ImageNet uses double the depth at 64 ResNet layers in both the forward and backward direction in the U-Net. It also uses a constant number of channels of 256. This model is trained on 128 TPUv3 chips at a total batch size of 512 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Fourier features for improved fine scale prediction</head><p>Prior work on diffusion models has mainly focused on the perceptual quality of generated samples, which emphasizes coarse scale patterns and global consistency of generated images. Here, we optimize for likelihood, which is sensitive to fine scale details and exact values of individual pixels. Since our reconstruction model p(x|z 0 ) given in Equation 8 is weak, the burden of modeling these fine scale details falls on our denoising diffusion modelx ? . In initial experiments, we found that the denoising model had a hard time accurately modeling these details. At larger noise levels, the latents z t follow a smooth distribution due to the added Gaussian noise, but at the smallest noise levels the discrete nature of 8-bit image data leads to sharply peaked marginal distributions q(z t ).</p><p>To capture the fine scale details of the data, we propose adding a set of Fourier features to the input of our denoising modelx ? (z t ; t). Such Fourier features consist of a linear projection of the original data onto a set of periodic basis functions with high frequency, which allows the network to more easily model high frequency details of the data. Previous work <ref type="bibr">[Tancik et al., 2020]</ref> has used these features for input coordinates to model high frequency details across the spatial dimension, and for time embeddings to condition denoising networks over the temporal dimension <ref type="bibr">[Song et al., 2021b]</ref>.</p><p>Here we apply it to color channels for single pixels, in order to model fine distributional details at the level of each scalar input.</p><p>Concretely, let z i,j,k be the scalar value in the k-th channel in the (i, j) spatial position of network input z t . We then add additional channels to the input of the denoising model of the form f n i,j,k = sin(z i,j,k 2 n ?), and g n i,j,k = cos(z i,j,k 2 n ?),</p><p>where n runs over a range of integers {n min , ..., n max }. These additional channels are then concatenated to z t before being used as input in a standard convolutional denoising model similar to that used by <ref type="bibr" target="#b9">Ho et al. [2020]</ref>. We find that the presence of these high frequency features allows our network to learn with much higher values of SNR max , or conversely lower noise levels ? 2 0 , than is otherwise optimal. This leads to large improvements in likelihood as demonstrated in Section 6 and <ref type="figure" target="#fig_2">Figure  5</ref>. We did not observe such improvements when incorporating Fourier features into autoregressive models.</p><p>In our expreriments, we got best results with n min = 7 and n max = 8, probably since Fourier features with these frequencies are most relevant; features with lower frequencies can be learned by the network from z, and higher frequencies are not present in the data thus irrelevant for likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D As a SDE</head><p>When we take the number of steps T ? ?, our model for p(z t ) can best be described as a continuous time diffusion process <ref type="bibr">[Song et al., 2021b]</ref>, governed by the stochastic differential equation</p><formula xml:id="formula_34">dz t = [f (t)z t ? g 2 (t)s ? (z t ; t)]dt + g(t)dW t ,<label>(35)</label></formula><p>with time running backwards from t = 1 to t = 0, where W denotes a Wiener process (standard Brownian motion), and  <ref type="bibr">[Sohl-Dickstein et al., 2015]</ref>, we decompose the negative variational lower bound (VLB) as:</p><formula xml:id="formula_35">f (t) = d log ? t dt , g 2 (t) = d? 2 (t) dt ? 2 d log ? t dt ? 2 (t).<label>(36</label></formula><formula xml:id="formula_36">? log p(x) ? ?VLB(x) = D KL (q(z 1 |x)||p(z 1 )) Prior loss + E q(z0|x) [? log p(x|z 0 )] Reconstruction loss + L T (x).</formula><p>Diffusion loss <ref type="formula" target="#formula_4">(37)</ref> The prior loss and reconstruction loss can be (stochastically and differentiably) estimated using standard techniques. We will now derive an estimator for the diffusion loss L T (x), the remaining and more challenging term. In the case of finite T , using s(i) = (i ? 1)/T , t(i) = i/T , the diffusion loss is:</p><formula xml:id="formula_37">L T (x) = T i=1 E q(z t(i) |x) D KL [q(z s(i) |z t(i) , x)||p(z s(i) |z t(i) )].<label>(38)</label></formula><p>We will use s and t as shorthands for s(i) and t(i). We will first derive an expression of D KL (q(z s |z t , x)||p(z s |z t )).</p><formula xml:id="formula_38">Recall that p(z s |z t ) = q(z s |z t , x =x ? (z t ; t)), and thus q(z s |z t , x) = N (z s ; ? Q (z t , x; s, t), ? 2 Q (s, t)I) and p(z s |z t ) = N (z s ; ? ? (z t ; s, t), ? 2 Q (s, t)I), with ? Q (z t , x; s, t) = ? t|s ? 2 s ? 2 t z t + ? s ? 2 t|s ? 2 t x (39) ? ? (z t ; s, t) = ? t|s ? 2 s ? 2 t z t + ? s ? 2 t|s ? 2 tx ? (z t ; t),<label>(40)</label></formula><p>and ? 2 Q (s, t) = ? 2 t|s ? 2 s /? 2 t .</p><p>Since q(z s |z t , x) and p(z s |z t ) are Gaussians, their KL divergence is available in closed form as a function of their means and variances, which due to their with equal variances simplifies as:</p><formula xml:id="formula_40">D KL (q(z s |z t , x)||p(z s |z t )) = 1 2? 2 Q (s, t) ||? Q ? ? ? || 2 2 (42) = ? 2 t 2? 2 t|s ? 2 s ? 2 s ? 4 t|s ? 4 t ||x ?x ? (z t ; t)|| 2 2 (43) = 1 2? 2 s ? 2 s ? 2 t|s ? 2 t ||x ?x ? (z t ; t)|| 2 2 (44) = 1 2? 2 s ? 2 s (? 2 t ? ? 2 t|s ? 2 s ) ? 2 t ||x ?x ? (z t ; t)|| 2 2 (45) = 1 2 ? 2 s ? 2 t /? 2 s ? ? 2 t ? 2 t ||x ?x ? (z t ; t)|| 2 2 (46) = 1 2 ? 2 s ? 2 s ? ? 2 t ? 2 t ||x ?x ? (z t ; t)|| 2 2 (47) = 1 2 (SNR(s) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2<label>(48)</label></formula><p>Reparameterizing z t ? q(z t |x) as z t = ? t x + ? t , where ? N (0, I), our diffusion loss becomes:</p><formula xml:id="formula_41">L T (x) = T i=1 E q(zt|x) [D KL (q(z s |z t , x)||p(z s |z t ))] (49) = 1 2 E ?N (0,I) [ T i=1 (SNR(s) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2 ] (50) E.2 Estimator of L T (x)</formula><p>To avoid having to compute all T terms when calculating the diffusion loss, we construct an unbiased estimator of L T (x) using</p><formula xml:id="formula_42">L T (x) = T 2 E ?N (0,I),i?U {1,T } (SNR(s) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2<label>(51)</label></formula><p>where U {1, T } is the discrete uniform distribution from 1 to (and including) T , s = (i ? 1)/T , t = i/T and z t = ? t x + ? t . This trivially yields an unbiased Monte Carlo estimator, by drawing random samples i ? U {1, T } and ? N (0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Infinite depth (T ? ?)</head><p>To calculate the limit of the diffusion loss as T ? ?, we express L T (x) as a function of ? = 1/T :</p><formula xml:id="formula_43">L T (x) = 1 2 E ?N (0,I),i?U {1,T } SNR(t ? ? ) ? SNR(t) ? ||x ?x ? (z t ; t)|| 2 2 ,<label>(52)</label></formula><p>where again t = i/T and z t = ? t x + ? t .</p><p>As ? ? 0, T ? ?, and letting SNR (t) denote the derivative of the SNR function, this then gives</p><formula xml:id="formula_44">L ? (x) = ? 1 2 E ?N (0,I),t?U [0,1] SNR (t)||x ?x ? (z t ; t)|| 2 2 (53) = ? 1 2 E ?N (0,I) 1 0 SNR (t) x ?x ? (z t ; t) 2 2 dt.<label>(54)</label></formula><p>F Influence of the number of steps T on the VLB Recall that the diffusion loss for our choice of model p, q, when using T timesteps, is given by</p><formula xml:id="formula_45">L T (x) = 1 2 E ?N (0,I) T i=1 (SNR(s(i)) ? SNR(t(i))) ||x ?x ? (z t(i) ; t(i))|| 2 2 ,<label>(55)</label></formula><p>with s(i) = (i ? 1)/T , t(i) = i/T . This can then be written equivalently as</p><formula xml:id="formula_46">L T (x) = 1 2 E ?N (0,I) T i=1 (SNR(s) ? SNR(t ) + SNR(t ) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2 ,<label>(56)</label></formula><p>with t = t ? 0.5/T .</p><p>In contrast, the diffusion loss with 2T timesteps can be written as</p><formula xml:id="formula_47">L 2T (x) = 1 2 E ?N (0,I) T i=1 (SNR(s) ? SNR(t )) ||x ?x ? (z t ; t )|| 2 2 + T i=1 (SNR(t ) ? SNR(t)) ||x ?x ? (z t ; t)|| 2 2 .<label>(57)</label></formula><p>Subtracting the two results, we get</p><formula xml:id="formula_48">L 2T (x) ? L T (x) = 1 2 E ?N (0,I) T i=1 (SNR(s) ? SNR(t )) ||x ?x ? (z t ; t )|| 2 2 ? ||x ?x ? (z t ; t)|| 2 2 .<label>(58)</label></formula><p>Since t &lt; t, z t is a less noisy version of the data from earlier in the diffusion process compared to z t . Predicting the original data x from z t is thus strictly easier than from z t , leading to lower mean squared error if our modelx ? is good enough. We thus have that L 2T (x) ? L T (x) &lt; 0, which means that doubling the number of timesteps always improves our diffusion loss. For this reason we argue for using the continuous-time VLB corresponding to T ? ? in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Equivalence of diffusion specifications</head><formula xml:id="formula_49">(x), z B v be defined through a different specification {? B v , ? B v ,x B ? }. Since v ? ? 2 v /? 2 v , we have that ? v = ? v / ? v, which means that z v (x, ) = ? v x + ? v = ? v (x + / ? v)</formula><p>. This holds for any diffusion specification by definition, and therefore we have</p><formula xml:id="formula_50">z A v (x, ) = (? A v /? B v )z B v (x, )</formula><p>. The latents z v for different diffusion specifications are thus identical, up to a trivial rescaling, and their information content only depends on the signal-to-noise ratio v, not on ? v , ? v separately.</p><p>For the purpose of denoising from a latent z B v , this means we can simply define the denoising model</p><formula xml:id="formula_51">asx B ? (z B v , v) ?x A ? ((? A v /? B v )z B v , v)</formula><p>, and we will then get the same reconstruction loss as when denoising from z A v using modelx A ? . Assuming endpoints SNR max and SNR min are equal for both specifications, Equation 18 then tells us that L A ? (x) = L B ? (x), i.e. they both produce the same diffusion loss in continuous time.</p><p>Similarly, the conditional model distributions over the latents z v in our generative model are functions of the denoising modelx ? (z v , v) (see Equation 26), and we therefore have that the specification H Implementation of monotonic neural net noise schedule ? ? (t)</p><formula xml:id="formula_52">{? A v , ? A v ,x A ? }</formula><p>To learn the signal-to-noise ratio SNR(t), we parameterize it as SNR(t) = exp(?? ? (t)) with ? ? (t) a monotonic neural network. This network consists of 3 linear layers with weights that are restricted to be positive, l 1 , l 2 , l 3 , which are composed as? ? (t) = l 1 (t) + l 3 (?(l 2 (l 1 (t)))), with ? the sigmoid function. Here, the l 2 layer has 1024 outputs, and the other layers have a single output.</p><p>In case of the continuous-time model, for the purpose of variance minimization, we postprocess the noise schedule as detailed in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Variance minimization</head><p>Reduction of the variance diffusion loss estimator can lead to faster optimization.</p><p>For the continuous-time model, we reduce the variance of the diffusion loss estimator through two methods: (1) optimizing the noise schedule w.r.t. the variance of the diffusion loss, and (2) using a low-discrepency sampler. Note that these methods can be omitted if one aims for a simple implementation of our methods, at the expense of slower optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Low-discrepency sampler</head><p>When processing a minibatch of k examples x i , i ? {1, . . . , k}, we require k timesteps t i sampled from a uniform distribution. Instead of sampling these timesteps independently, we sample a single uniform random number u 0 ? U [0, 1] and then set t i = mod(u 0 + i/k, 1). Each t i now has the correct uniform marginal distribution, but the minibatch of timesteps covers the space in [0, 1] more equally than when sampling independently, which we find to reduce the variance in our VLB estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Optimizing the noise schedule w.r.t. the variance of the diffusion loss</head><p>In case of the continuous-time model, for the purpose of variance minimization, we postprocess the noise schedule as follows. At this point, the range of? ? (t) is unbounded and so the resulting SNR is not yet restricted to [SNR min , SNR max ]. We therefore postprocess the monotonic neural network as</p><formula xml:id="formula_53">? ? (t) = ? 0 + (? 1 ? ? 0 )? ? (t) ?? ? (0) ? ? (1) ?? ? (0) ,<label>(59)</label></formula><p>with ? 0 = ? log(SNR max ), ? 1 = ? log(SNR min ). Now SNR(t) = exp(?? ? (t)) has the correct range and interpolates exactly between SNR min and SNR max . We treat ? 0 , ? 1 as free parameters that we optimize directly w.r.t. the VLB. The remaining parameters ? are instead learned by minimizing the variance of the stochastic estimate of the VLB.</p><p>We minimize the variance by performing stochastic gradient descent on our squared diffusion loss</p><formula xml:id="formula_54">L M C ? (x, w, ?) 2 . We have that E t, [L M C ? (x, w, ?) 2 ] = L ? (x, w) 2 + Var t, [L M C ? (x, w, ?)],</formula><p>where the first part is independent of ? ? (t), and hence that</p><formula xml:id="formula_55">E t, [? ? L M C ? (x, w, ? ? ) 2 ] = ? ? Var t, [L M C ? (x, w, ? ? )].<label>(60)</label></formula><p>We can calculate this gradient with negligible computational overhead as a by-product of calculating the gradient of the VLB, details of which are given in Appendix H.</p><p>We wish to calculate ? ? [L M C ? (x, ? ? ) 2 ] without performing a second backpropagation pass through the denoising model due to this objective being different than for the other parameters. To do this, we decompose the gradient as</p><formula xml:id="formula_56">d d? [L M C ? (x, ? ? ) 2 ] = d dSNR L M C ? (x, SNR) 2 d d? [SNR(?)] ,<label>(61)</label></formula><p>and</p><formula xml:id="formula_57">d dSNR L M C ? (x, SNR) 2 = 2 d dSNR L M C ? (x, SNR) L M C ? (x, SNR),<label>(62)</label></formula><p>where denotes elementwise multiplication. Here d dSNR L M C ? (x, SNR) is computed along with the other gradients when performing the single backpropagation pass for calculating ? ? [L M C ? ]. The remaining operations required to get ? ? [L M C ? (x, ? ? ) 2 ] have negligible computational cost. This strategy of minimizing the variance of our diffusion loss estimate remains valid for weighted diffusion losses, w(v) = 1, not corresponding to the VLB, and we therefore expect it to be useful beyond the goal of optimizing for likelihood that we consider in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Numerical stability</head><p>Floating point numbers are much worse at representing numbers close to 1, than at representing numbers close to 0. Since a na?ve implementation of our model and its discrete-time loss function requires computing intermediate values that are close to 1, those numbers are erroneously rounded to 1, leading to numerical issues and incorrect results. Note that previous implementations of discretetime diffusion models (e.g. <ref type="bibr" target="#b9">[Ho et al., 2020]</ref>) used 64-bit floating point numbers to avoid numerical problems. We found this unnecessary in our model. A numerically problematic term, for example, is ? 2 t|s which is used for sampling. It is straightforward to verify that: ? 2 t|s = ?expm1(softplus(?(s)) ? softplus(?(t))),</p><p>where expm1(x) ? exp(x) ? 1 and softplus(x) ? log(1 + exp(x)) are functions with numerically stable primitives in common numerical computing packages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Comparison to DDPM and NCSN objectives</head><p>Previous works using denoising diffusion models <ref type="bibr" target="#b9">[Ho et al., 2020</ref><ref type="bibr">, Song and Ermon, 2019</ref><ref type="bibr">, Nichol and Dhariwal, 2021</ref> used a training objective that can be understood as a weighted diffusion loss of the form given in Equation <ref type="formula" target="#formula_0">19</ref>:</p><formula xml:id="formula_59">L ? (x, w) = 1 2 E ?N (0,I) SNRmax SNRmin w(v) x ?x ? (z v , v) 2 2 dv (64) = ? 1 2 E ?N (0,I) 1 0 SNR (t)w(SNR(t)) x ?x ? (z t ; t) 2 2 dt (65) = 1 2 E ?N (0,I) 1 0 ? (t)w(exp(??(t))) ?? ? (z t ; t) 2 2 dt,<label>(66)</label></formula><p>where ?(t) = ? log SNR(t).</p><p>When using the loss in Equation 64, we set w(v) = 1, corresponding to optimization of a variational bound on the likelihood of the data. <ref type="bibr" target="#b9">Ho et al. [2020]</ref>, Song and Ermon <ref type="bibr">[2019]</ref>, <ref type="bibr">Nichol and Dhariwal [2021]</ref> instead choose to minimize the simple objective defined as  <ref type="bibr" target="#b9">[Ho et al., 2020]</ref> and improved DDPM <ref type="bibr">[Nichol and Dhariwal, 2021]</ref> instead use implied weighting functions that put relatively more weight on the noisy data with low to medium signal-to-noise ratio. The latter two works report better FID and Inception Score than Song et al.</p><formula xml:id="formula_60">L simple (x) ? 1 0 ?? ? (z t , t) 2 2 dt,<label>(67)</label></formula><p>[2020] and the current paper, which we hypothesize is due to their loss emphasizing the global consistence and coarse level patterns more than the fine scale features of the data.</p><p>For DDPM, <ref type="bibr" target="#b9">Ho et al. [2020]</ref> use a diffusion process in discrete time with ? i = i j=1 (1 ? ? j ), ? 2 i = 1 ? ? 2 i , where ? i linearly interpolates between ? 1 = 1e ?4 and ? T = 0.02 in T = 1000 discrete steps. When defining time t = i/T , this can be closely approximated as ? 2 t = exp(?1e ?4 ? 10t 2 ), and correspondingly with SNR(t) = 1/expm1(1e ?4 + 10t 2 ) or ?(t) = log[expm1(1e ?4 + 10t 2 )], where expm1(x) = exp(x) ? 1. This approximation is shown in <ref type="figure" target="#fig_5">Figure 8</ref>. For NCSNv2, Song and Ermon [2020] instead use ? t = 1 and let ? t be a geometric series interpolating between 0.01 and 50, i.e. ? 2 t = exp(?(t)) with ?(t) = 2 log[0.01] + 2 log[5000]t. This means that ? (t) = 2 log[5000] and thus that w(v) is a constant. The procedure proposed by Song and Ermon [2020] is thus consistent with maximization of the VLB like we propose here. The same holds for <ref type="bibr">[Song and Ermon, 2019]</ref>.</p><p>For IDDPM, Nichol and Dhariwal [2021] use? t = cos( t+0.008 1.008 ? 2 )/ cos( 0.008 1.008 ? 2 ). The values for? t are then translated into value for ? t , which are then clipped to 0.999. Subsequently we can then derive the ? t , ? t , ?(t) corresponding to those ? t . Due to the clipping these expressions do not simplify, but we include their numerical results in <ref type="figure">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N Lossless compression</head><p>For a fixed number of evaluation timesteps T eval , our diffusion model in discrete time is a hierarchical latent variable model that can be turned into a lossless compression algorithm using bits-back coding [Hinton and <ref type="bibr" target="#b6">Van Camp, 1993]</ref>. Assuming a source of auxiliary random bits is available alongside the data, bits-back coding encodes a latent and data together, with the latent sampled from the approximate posterior using the auxiliary random bits. The net coding cost of bits-back coding is given by subtracting the number of bits needed to sample the latent from the number of bits needed to encode the latent and data using the reverse process, so the negative VLB of our discrete time model is the theoretical expected coding cost for bits-back coding.</p><p>As a proof of concept for lossless compression using our model, <ref type="table" target="#tab_4">Table 2</ref> reports net codelengths on the CIFAR10 test set for various settings of T eval using BB-ANS <ref type="bibr">[Townsend et al., 2018]</ref>, a practical implementation of bits-back coding based on asymmetric numeral systems <ref type="bibr" target="#b4">[Duda, 2009]</ref>. Since diffusion models have Markov forward and reverse processes, we use the Bit-Swap implementation of BB-ANS <ref type="bibr" target="#b16">[Kingma et al., 2019]</ref>. Practical implementations of bits-back coding must discretize continuous latent variables and their associated continuous probability distributions; for simplicity, our implementation uses a uniform discretization of the continuous latents and their associated Gaussian conditionals from the forward and reverse processes. Additionally, we found it crucial to encrypt the ANS bitstream before each decoding operation to ensure clean bits for sampling from the approximate posterior; we did so by applying the XOR operation to the ANS bitstream with pseudorandom bits from a fixed sequence of seeds. For example, without cleaning the bitstream using encryption, compressing a batch of 100 examples using T eval = 250 costs 2.74 bits per byte, but with encryption, the cost improves to 2.68 bits per dimension.</p><p>For a small number of timesteps T eval , our bits-back implementation attains net codelengths that agree closely with the negative VLB, but there is some discrepancy for large T eval . This is due to inaccuracies in the compression algorithm to represent discretized Gaussians with small standard deviations, and small discrepancies in codelength compound into a gap of up to 0.05 bits per dimension when T is large. (In prior work, e.g. <ref type="bibr" target="#b16">[Kingma et al., 2019</ref><ref type="bibr" target="#b8">, Ho et al., 2019b</ref><ref type="bibr">, Townsend et al., 2020</ref>, practical implementations of bits-back coding have been tested on latent variable models with only tens of layers, not hundreds.) In addition, a large number of timesteps makes compression computationally expensive, because a neural network forward pass must be run for each timestep. Closing the codelength gap with an efficient implementation of bits-back coding for a large number of timesteps is an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O Density estimation on additional data sets</head><p>At the request of one of the reviewers we also ran our model on additional data sets of higher resolution, less diverse, images. Specifically, we obtain a test set likelihood of 2.14 bits per dim on CelebA-HQ <ref type="bibr" target="#b12">[Karras et al., 2017]</ref>, and 1.44 on LSUN bedrooms <ref type="bibr">[Yu et al., 2015]</ref>, both at 128 ? 128 resolution. Since these are not established benchmarks in density estimation, and since downsampling methods in the literature are not consistent, we don't compare against previous methods for these data sets. Our results are provided purely to give a ballpark estimate of how well our proposed method scales to higher resolution images.</p><p>The model used for these data sets is based on that used for Imagenet 64 ? 64, with an additional level in the UNet at resolution 128 ? 128, consisting of 16 residual layers using 128 channels. Our model downsamples between the 128 ? 128 and 64 ? 64 resolutions, similar to e.g. <ref type="bibr" target="#b9">Ho et al. [2020]</ref>, but unlike the models we used for the other data sets we considered.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the diffusion loss with few segments T (left), more segments T (middle), and infinite segments T (continuous time, right). The continuous-time loss(Equation 18) is an integral of mean squared error (MSE) over SNR, here visualized as a black curve. The black curve is strictly decreasing when the model is sufficiently well trained, so the discrete-time loss(Equation 13) is an upper bound (an upper Riemann sum approximation) of this integral that becomes better when segments are added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Non cherry-picked unconditional samples from our Imagenet 64x64 model, trained in continuous time and generated using T = 1000. The model's hyper-parameters and parameters are optimized w.r.t. the likelihood bound, so the model is not optimized for synthesis quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Test set likelihoods during training, with/without Fourier features, and with/without learning the noise schedule to minimize variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Plugging our prior term q(z s |x) = N ? s x, ? 2 s I (Equation 1) and linear-Gaussian likelihood term q(z t |z s ) = N (? t|s z s , ? 2 t|s ) (Equation 20) into this general posterior equation, yields a posterior:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>For optimization of the discrete-time diffusion loss and sampling from either the continuoustime or discrete-time model, an operation of the form exp(x)?1 needs to be performed. This operation can result in large numerical errors when performed with 32-bit (float32) or 16-bit (e.g. bfloat16) floating point numbers. For this reason, many numerical packages implement the numerically more stable version expm1(x) = exp(x) ? 1. We here plot the absolute numerical errors for each of these versions for float32 and bfloat16. For this plot we used expm1(.) with 64-bit floating point (float64) as ground truth. The plotted numerical error is the absolute value of the difference between the ground truth and the computed float32 or bfloat16 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Log signal-to-noise ratio for the discrete-time diffusion process in<ref type="bibr" target="#b9">Ho et al. [2020]</ref> and our continous-time approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Random samples from an unconditional diffusion model trained on CIFAR-10 for 2 million parameter updates. The model was trained in continuous-time, and sampled using T = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Random samples from an unconditional diffusion model trained on 32x32 ImageNet for 3.7 million parameter updates. The model was trained in continuous-time, and sampled using T = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Random samples from an unconditional diffusion model trained on 64x64 ImageNet for 2 million parameter updates. The model was trained in continuous-time, and sampled using T = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>State-of-the-art models in each of the 5 past years (lower is better)</figDesc><table><row><cell></cell><cell></cell><cell cols="5">CIFAR 10 without data augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bits per dimension (test set)</cell><cell>2.7 2.8 2.9 3.0 3.1 3.2</cell><cell>PixelRNN</cell><cell cols="2">Image Transformer Autoregressive models Sparse Transformer</cell><cell>Sparse Transformer</cell><cell>VDM (ours)</cell><cell>Bits per dimension (test set)</cell><cell>3.40 3.45 3.50 3.55 3.60 3.65</cell><cell>Gated PixelCNN</cell><cell cols="3">SPN Autoregressive models Sparse Transformer Routing Transformer</cell><cell>VDM (ours)</cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.35</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5</cell><cell>2016-2017</cell><cell>2018</cell><cell>2019 Year</cell><cell>2020</cell><cell>2021</cell><cell></cell><cell>3.30</cell><cell>2016-2017</cell><cell>2018</cell><cell>2019 Year</cell><cell>2020</cell><cell>2021</cell></row><row><cell></cell><cell></cell><cell cols="5">(a) CIFAR-10 without data augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2107.00630v4 [cs.LG] 13 Jun 2022</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Summary of our findings for density modeling tasks, in terms of bits per dimension (BPD) on the test set. Model types are autoregressive (AR), normalizing flows (Flow), variational autoencoders (VAE), or diffusion models (Diff). Our results were obtained using the continuous-time formulation of our model. CIFAR-10 data augmentations are: (A) extensive, (B) small translations, or (C) horizontal flips. The numbers for VDM are variational bounds, and can likely be improved by estimating the marginal likelihood through importance sampling, or through evaluation of the corresponding continuous normalizing flow as done by Song et al. [2021a].which e.g. captures all the different objectives discussed by Song et al. [2021b], see Appendix K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>reports net codelengths on the CIFAR10 test set for various settings of T eval using BB-(a) log SNR vs time t SNR(t) schedule Var(BPD) The ?-Cosine schedule from Nichol and Dhariwal[2021]. All schedules were scaled and shifted on the log scale such that the resulting SNR min , SNR max were the equal to our learned endpoints, resulting in the same VLB estimate of 2.66. We report the variance of our VLB estimate per data point, computed on the test set, and conditional on the data: This does not include the noise due to sampling minibatches of data.</figDesc><table><row><cell>Learned (ours)</cell><cell>0.53</cell></row><row><cell>log SNR-linear</cell><cell>6.35</cell></row><row><cell>?-Linear [1]</cell><cell>31.6</cell></row><row><cell>?-Cosine [2]</cell><cell>31.1</cell></row><row><cell cols="2">(b) Variance of VLB estimate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Discrete versus continuous-time train- ing and evaluation with CIFAR-10, in terms of bits per dimension (BPD).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018. Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055-4064. PMLR, 2018. Townsend, Thomas Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. In International Conference on Learning Representations, 2018. James Townsend, Thomas Bird, Julius Kunze, and David Barber. HiLLoC: lossless image compression with hierarchical latent variable models. In International Conference on Learning Representations, 2020.</figDesc><table><row><cell>James Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint</cell></row><row><cell>arXiv:2007.03898, 2020.</cell></row><row><cell>Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. arXiv</cell></row><row><cell>preprint arXiv:2106.05931, 2021.</cell></row><row><cell>Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In</cell></row><row><cell>International Conference on Machine Learning, pages 1747-1756, 2016.</cell></row><row><cell>Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-</cell></row><row><cell>tion, 23(7):1661-1674, 2011.</cell></row></table><note>Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pages 1278-1286, 2014. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer- assisted intervention, pages 234-241. Springer, 2015. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. Samarth Sinha and Adji B Dieng. Consistency regularization for variational auto-encoders. arXiv preprint arXiv:2105.14859, 2021. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 11895-11907, 2019. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. arXiv e-prints, pages arXiv-2101, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling Through Stochastic Differential Equations. In International Conference on Learning Representations, 2021b. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014. Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. arXiv preprint arXiv:2006.10739, 2020.Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>defines the same generative model over z v , x as the specification {? B</figDesc><table><row><cell>up to a rescaling of the latents by ? A v /? B v .</cell><cell>, ? B v ,x B ? }, again</cell></row></table><note>v</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Below, we derive the ?(t), and thus the weighting function w(v), corresponding to the diffusion processes used by Ho et al. [2020], Song and Ermon [2019], Nichol and Dhariwal [2021]. We visualize these weighting functions in Figure 7.Figure 7: Implied weighting functions corresponding to the losses used by Ho et al. [2020], Song et al. [2020], and Nichol and Dhariwal [2021], as well as our proposed loss. NCSN [Song et al., 2020] uses a constant implied weighting function, and is thus consistent with maximization of the variational bound like we propose in this paper. However, unlike Song and Ermon[2019]  we also learn the endpoints SNR min , SNR max , which results in a better optimized VLB value. DDPM</figDesc><table><row><cell>or a discrete-time version of this.</cell></row><row><cell>Comparing Equation 67 with Equation 66, we can see that the loss used by Ho et al. [2020], Song</cell></row><row><cell>and Ermon [2019], Nichol and Dhariwal [2021] corresponds to a weighting function w(SNR(t)) =</cell></row><row><cell>1/? (t).</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yang Song, Kevin Murphy, Mohammad Norouzi and Chin-Yun Yu for helpful feedback on the paper, and Ruiqi Gao for helping with writing an open source version of the code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:2011.10650</idno>
		<title level="m">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Asymmetric numeral systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Duda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.0271</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08125</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual Conference on Computational Learning Theory</title>
		<meeting>the Sixth Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compression with flows via local bits-back coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Lohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3874" to="3883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion probabilistic models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A variational perspective on diffusion-based generative models and score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02808</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bit-swap: Recursive bits-back coding for lossless compression with hierarchical latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friso</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3408" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
