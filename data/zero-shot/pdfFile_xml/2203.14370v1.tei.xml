<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CaCo: Both Positive and Negative Samples are Directly Learnable via Cooperative-adversarial Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
						</author>
						<title level="a" type="main">CaCo: Both Positive and Negative Samples are Directly Learnable via Cooperative-adversarial Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Contrastive Learning</term>
					<term>Cooperative-Adversarial Learning</term>
					<term>Self-Supervised Learning</term>
					<term>Positive+Negative Samples</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a representative self-supervised method, contrastive learning has achieved great successes in unsupervised training of representations. It trains an encoder by distinguishing positive samples from negative ones given query anchors. These positive and negative samples play critical roles in defining the objective to learn the discriminative encoder, avoiding it from learning trivial features. While existing methods heuristically choose these samples, we present a principled method where both positive and negative samples are directly learnable end-to-end with the encoder. We show that the positive and negative samples can be cooperatively and adversarially learned by minimizing and maximizing the contrastive loss, respectively. This yields cooperative positives and adversarial negatives with respect to the encoder, which are updated to continuously track the learned representation of the query anchors over mini-batches. The proposed method achieves 71.3% and 75.3% in top-1 accuracy respectively over 200 and 800 epochs of pre-training ResNet-50 backbone on ImageNet1K without tricks such as multi-crop or stronger augmentations. With Multi-Crop, it can be further boosted into 75.7%. The source code and pre-trained model are released in https://github.com/maple-research-lab/caco.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Unsupervised representation learning <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> has attracted tremendous attentions recently as model pre-training becomes an essential step for deep neural networks <ref type="bibr" target="#b3">[4]</ref>. Among them are a family of instance discrimination-based methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, which train the network by distinguishing positive samples from their negative counterparts given query anchors from mini-batches during the learning process. Contrastive learning <ref type="bibr" target="#b0">[1]</ref> and many variants <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> are one of the most popular directions that achieved great success in last 3 years by pulling the embeddings of positive pairs together and pushing that of negative pairs away. In these methods, a critical question arises regarding how to find the most critical positives and negatives that can provide discriminative knowledge to self-supervise the network training.</p><p>SimCLR <ref type="bibr" target="#b5">[6]</ref> and MoCo <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> are two representative contrastive learning methods in this category. Given a query anchor, SimCLR uses the other samples from the same mini-batch as negative examples while an augmented view of the anchor is used as the positive sample. On the contrary, the MoCo maintains a memory bank as in <ref type="bibr" target="#b4">[5]</ref>, and uses the negative samples from the memory bank. The memory bank is built by queueing the learned representations over past minibatches in a FIFO (First-In-First-Out) fashion, and a momentum update is applied to the key network whose weights are an exponential moving average of the query networks over time. SimCLR <ref type="bibr" target="#b5">[6]</ref> usually needs a very large batch size to involve sufficient negative samples in each iteration, making it very computationally and memory demanding. Although the MoCo also has a very large memory bank, it can be maintained efficiently as the negative samples in it are collected on-the-fly from the past mini-batches.</p><p>Both methods show that successful training strongly relies on the construction of negative samples. Usually, harder negatives that are closer and mixed with the query anchors contain more discriminative information, making it harder to learn shallow features providing shortcut solutions to distinguish between positives and negatives. Along this direction, recent works <ref type="bibr" target="#b11">[12]</ref> use heuristic sampling by concentrating on the hard negatives around queries.</p><p>On the contrary, a recent breakthrough demonstrates that negative samples are directly learnable by treating them as a part of network weights through adversarial contrastive learning <ref type="bibr" target="#b7">[8]</ref>, naturally resulting in hard negatives pushed towards query anchors. It provides a more principled way to learn rather than heuristically construct discriminative negatives. Specifically, by treating the negatives and the encoder as two adversarial players, the AdCo outperforms the SimCLR and MoCo in terms of both accuracy and efficiency. However, a question still remains -can we learn positive samples as well just like those negative samples? In this paper, we strive to answer it by revealing the relation between positive and negative samples via a cooperative-adversarial principle.</p><p>Unlike negative samples, we will show that given a query anchor, its positive sample ought to be learned cooperatively rather than adversarially with the encoder network. This is based on the observation that the representation of a positive sample that cooperatively minimizes the contrastive loss with the encoder will be pulled towards the given anchor, instead of being pushed away if it were adversarially learned by maximizing the loss. The cooperative positive sample will also be chosen from the memory bank shared with the negative samples. This could mitigate false negative problem as memory bank samples in close proximity of a query anchor can be trained as positives, instead of being blindly treated as negatives. Both positive and negative samples will be end-to-end trained together with the encoder through such a Cooperativeadversarial Contrastive (CaCo) learning approach. Moreover, we propose to use the most probable positive samples that are stable to update for an end-to-end training of the CaCo model. Experiment results demonstrate the proposed method achieves 71.3% and 75.3% in top-1 accuracy over 200 epochs and 800 epochs of pre-training on Imagenet1K without multicrop augmentations. It can achieve 75.7% in top-1 accuracy after multi-crop augmentations with 800 epochs pre-training.</p><p>The remainder of the paper is organized as follows. We will review the related works in Section II, and revisit the preliminary works on contrastive learning in Section III. The proposed CaCo method is presented in Section IV, followed by the experiments in Section V. We will conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will review the related works on contrastive learning from three perspectives -instance discrimination-based contrastive learning, hard negative samples, and trainable memory bank. Instance Discrimination. Most of recent works on contrastive learning focus on instance discrimination-based methods <ref type="bibr" target="#b4">[5]</ref>. Given a query anchor from a training minibatch, it explicitly constructs the positive and negative samples, and self-trains the representation network (i.e., an encoder) by distinguishing positives from negatives. It minimizes the InfoNCE loss (a form of the contrastive loss) that can be implemented as a cross entropy by treating each instance as a positive or a negative class. The positive sample of a query anchor is often given as the augmented view of the same anchor. Depending on different ways to form negatives, the contrastive learning has many variants. Among them are the SimCLR <ref type="bibr" target="#b5">[6]</ref> where negatives come from the other samples of the same mini-batch, as well as the MoCo <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> where exists a memory bank storing the representations over past mini-batches as negatives. The former usually relies on a large size of mini-batches to perform reliable training of the encoder, which is demanding for computing and memory. MoCo addresses this problem by reusing the obtained representation of past samples without having to re-computing them for each mini-batches. While some negative samples may belong to the same class as a query anchor in instance discrimination methods (i.e, false negatives), a nearest neighbor contrastive learning extends the SimCLR and the MoCo by instead assigning the negative most similar to the anchor as its positive <ref type="bibr" target="#b12">[13]</ref>, which somehow mitigates the false negative problem. Hard Negative Samples. It has been observed that the harder negative samples that are difficult to distinguish from their positive counterparts play a more critical role in contrastive learning. They prevent the encoder from learning trivial features to shortcut the aforementioned instance discrimination task. There exist methods based on heuristically sampling the samples in proximity of positive anchors. For example, hard negative sampling methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> apply importance sampling to find harder negatives with a concentration parameter. Recent works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> on adversarial pre-training also shed some light on learning representations resilient against adversarial attacks. However, they do not directly train negative samples end-to-end by treating memory bank as learnable network weights as in <ref type="bibr" target="#b7">[8]</ref>. Instead, they apply the adversarial training to input images or augmented views that may adversely affect positive samples that ought to be constructed cooperatively as we find in this paper. Trainable Memory Bank. The recent work on Adversarial Contrastive learning (AdCo) <ref type="bibr" target="#b7">[8]</ref> made a significant progress to demonstrate that the hard negatives are directly learnable as a part of network weights that are adversaries to the encoder network in the other part. Instead of minimizing the InfoNCE loss as the encoder, it maximizes it that pushes the learned negatives towards their query anchors. It provides a principled way to finding more discriminative negatives for an effective training of the encoder. Along this line, an implicit feature modification (IFM) <ref type="bibr" target="#b16">[17]</ref> is proposed to iteratively update the hard samples. The IFM also pushes the samples towards the representation of anchors like the AdCo, but its step size is set by a given level of budget. Instead, the iterative update direction of these samples in the AdCo is derived from the positive gradient of the InfoNCE loss as an adversary learner, and its step size is proportional to the probability of being a positive to a query anchor. In this way, the AdCo can produce more discriminative negatives by making it harder to distinguish them from the positive anchors.</p><p>In this paper, along the road outlined from instance discrimination and trainable memory bank, we will unify the joint training of both positive and negative samples given query anchors. Both positives and negatives will share a memory bank that is trained end-to-end. For this purpose, it needs to answer two critical questions in contrastive learning:</p><p>? Can we end-to-end train positive and negative samples jointly through a shared memory bank? ? If so, will adversarial criterion still work for training positives? The answer to these questions leads to the proposed Cooperative-adversarial Contrastive (CaCo) learning presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES AND NOTATIONS</head><p>In this section, we revisit the contrastive learning and its adversarial training of memory bank samples. We will also define several notations that will be used later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contrastive Learning</head><p>Consider an unlabeled example x called a query anchor in the current training mini-batch and its representation z output from an encoder network z ? ? (x) parameterized by ?. To train the encoder ? ? , a positive sample x + of x is obtained by applying an augmentation (e.g., random crop, color jitting and Gaussian noises) to x. The resultant x + provides an alternate view of the original anchor x, along with its representation z + . Usually, in a typical contrastive learning approach, the anchor x is also given by applying a random augmentation to the original image.</p><p>In instance discrimination-based contrastive learning such as MoCo <ref type="bibr" target="#b6">[7]</ref> and AdCo <ref type="bibr" target="#b7">[8]</ref>, together with mini-batches over iterations, a memory bank M {b j |j = 1, ? ? ? , K} of K representations are also provided as negatives, in contrast to the positive representation z + for an anchor. The memory bank is shared across all unlabeled anchors in a minibatch. Such a bank can be constructed by queueing the obtained representations from the past minibatches in a FIFO (First-In-First-Out) fashion such as in MoCo <ref type="bibr" target="#b6">[7]</ref>. Alternatively, a more principled way is to directly learn the memory bank by maximizing the contrastive loss (i.e., InfoNCE loss), which has demonstrated more efficient to update the encoder <ref type="bibr" target="#b7">[8]</ref>. In both cases, the encoder parameters ? are trained by</p><formula xml:id="formula_0">? = arg min ? x?B (x)</formula><p>over a mini-batch B with the following InfoNCE loss <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> </p><formula xml:id="formula_1">(x) = ? log exp(z ? z + /? ) exp(z ? z + /? ) + K j=1 exp(z ? b j /? )<label>(1)</label></formula><p>where ? denotes the inner product between two feature vectors and ? is the temperature controlling the sharpness of the loss. SimCLR <ref type="bibr" target="#b10">[11]</ref> also symmetrizes the loss by switching z and z + with the latter as a query anchor instead. Since the representations are normalized to a unit norm, the inner product gives the cosine similarity between samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial Contrastive Learning</head><p>Constructing the memory bank <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref> plays a critical role in contrastive learning. The negative samples in it gives the important specification about how the underlying representation network ? ? ought to be learned by discriminating between positives and negatives. Previous results show that hard negatives could provide more useful information, since hard negative samples can prevent the encoder from learning trivial representations focusing on low-level features such as local details rather than high-level semantic structures such as object shapes and categories.</p><p>Furthermore, it has been shown that hard negative samples naturally result from an adversarial contrastive learning (AdCo) approach <ref type="bibr" target="#b7">[8]</ref>, with a novel idea of directly learning the negatives to form a memory bank. This is contrary to the MoCo where the negative samples are collected by queueing the representations from past minibatches heuristically. Formally, the AdCo considers two adversary players -the memory bank M and the representation encoder ? ? . They are trained in a mutually adversarial manner by</p><formula xml:id="formula_2">M , ? = arg max bj ?M min ? x?B (x)<label>(2)</label></formula><p>The previous results <ref type="bibr" target="#b7">[8]</ref> show that the memory bank and the encoder can be updated alternately, resulting in more efficient representation learning since the negative samples can be updated more efficiently to provide critical information to specify the contrastive learning. In contrast, given a query anchor z, CaCo directly learns the cooperative positive b j + and adversarial negatives b j , j = j + from the shared memory bank through backward gradients (grad).</p><p>Although the AdCo <ref type="bibr" target="#b7">[8]</ref> has made a significant progress to directly learn negative samples, the question still remains regarding if the positive sample x + and its representation z + for an unlabeled anchor can also be learned directly. This will give rise to a more elegant solution by unifying the end-toend training of both positives and negatives. Meanwhile, it could also mitigate the false negative problem in contrastive learning, since it is known that some negative samples in the learned memory bank would be positive to an anchor if their representations are sufficiently close. This inspires us to develop the following Cooperative-adversarial Contrastive (CaCo) learning method. <ref type="figure" target="#fig_0">Figure 1</ref> gives a glance at the differences between MoCo [7], <ref type="bibr" target="#b10">[11]</ref>, AdCo <ref type="bibr" target="#b7">[8]</ref> and the proposed CaCo. Both MoCo and AdCo only has negative samples from the memory bank, and MoCo queues these samples from the past mini-batches without training them directly. In contrast, both positive and negative samples are obtained from the shared memory bank given a query anchor z, and they are directly trained through backward gradients by minimizing and maximizing the loss, respectively. The proposed CaCo will be presented in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CACO: COOPERATIVE-ADVERSARIAL CONTRASTIVE LEARNING</head><p>In this section, we will present the proposed Cooperativeadversarial Contrastive (CaCo) Learning. First we will explicitly integrate positive samples shared with negatives from the memory bank into the contrastive learning in Section IV-A. Then in Section IV-B we will show that positives and negatives ought to play cooperative and adversarial role in the network training, respectively, followed by the elaboration on cooperative-adversarial training over minibatches in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Positives from Shared Memory Bank</head><p>Following the definition defined in the last section, consider the representation z of an unlabeled anchor x. In the AdCo, we assume its negative samples all come from the trainable memory bank M. Now let us make a bold step by assuming the corresponding positive representation also comes from M. We denote the index of the positive sample in M by j + ? K 1, ? ? ? , K. Thus, for x with embedding z, its positive representation is given by b j + ? M. Accordingly, the contrastive InfoNCE loss becomes</p><formula xml:id="formula_3">(x) ? log exp(z ? b j + /? ) exp(z ? b j + /? ) + j =j + exp(z ? b j /? ) = ? log exp(z ? b j + /? ) K j=1 exp(z ? b j /? )<label>(3)</label></formula><p>We note that since both the positive and negative samples come from the shared memory bank, all terms in the denominator can be written uniformly with the same set of the memory bank representations, yielding a more elegant contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cooperative Positives vs. Adversarial Negatives</head><p>An unlabeled anchor x has both positive and negative samples, which, as aforementioned, we assume are all directly learnable. Following the gradient decent method, we can derive the gradient of (3) over the positive sample b j + and its negative counterparts b j for j = j + to update them.</p><p>It is not hard to obtain the following gradients</p><formula xml:id="formula_4">? (x) ?b j + = ? 1 ? 1 ? p(b j + |z) z<label>(4)</label></formula><p>and</p><formula xml:id="formula_5">? (x) ?b j = 1 ? p(b j |z)z, j = j +</formula><p>which are for the positive and negative samples, respectively. Here, p(b j |z) denotes the probability of a memory bank sample b j being positive to the input representation z,</p><formula xml:id="formula_6">p(b j |z) = exp(z ? b j /? ) K k=1 exp(z ? b k /? )<label>(5)</label></formula><p>Based on the idea behind the adversarial training, the negative samples can thus be updated by maximizing (x), resulting in</p><formula xml:id="formula_7">b j ? b j + ? ? p(b j |z)z, j = j + ,</formula><p>where ? is the learning rate for the gradient update. This shows that each negative representation b j will be updated towards the anchor representation z, which will make it harder to distinguish the negative sample from the anchor z. The previous results show that such harder negatives can provide more useful information to train the encoder. In this sense, the negative samples are adversarial players to the encoder. For the adversarial negative example training, AdCo <ref type="bibr" target="#b7">[8]</ref> has carefully discussed it and utilized it for contrastive learning successfully. Now the question is: should we also make the positive sample an adversary to the encoder? Let us take a look at the gradient of the positive representation b + in (4). If it were assume to be an adversary, it would be updated by</p><formula xml:id="formula_8">b j + ? b j + ? ? ? 1 ? p(b j + |z) z</formula><p>In this case, the positive b j + would be pushed away from the anchor z, instead of being pushed towards it. This causes a dilemma -what we chose as a positive sample would eventually end up being pushed away from the anchor, making it more likely to be a negative rather than a positive sample. Obviously, this is not our intention to learn a positive sample. This shows that the positive sample should be a cooperative player rather than an adversary to the encoder by minimizing the contrastive loss. This results in the following update to the positive sample</p><formula xml:id="formula_9">b j + ? b j + + ? ? 1 ? p(b j + |z) z</formula><p>which in turn pushes the positive representation towards the anchor, in line with our intuition to learn the positive sample close to the anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cooperative-Adversarial Training</head><p>The above analysis yields the following Cooperative-Adversarial Contrastive (CaCo) objective</p><formula xml:id="formula_10">z?B min b j + ,? max bj ,j =j + ? log exp(z ? b j + /? ) K j=1 exp(z ? b j /? )<label>(6)</label></formula><p>where both the positive sample b j + and the encoder with the parameters ? minimizes the loss, thus being cooperative players, while the negative samples b j , j = j + are adversary instead to maximize it.</p><p>The above CaCo objective is defined over a mini-batch B. One can assign a distinct positive sample from the memory bank to each anchor z in B. In other words, the positive index For a encoded query q, we used its counterpart key k by the key encoder to identify its most probable positive out of memory bank as its positive pair, while all other embeddings in the memory bank are its negative pairs. Then the network is optimized by minimizing the contrastive loss, the memory bank is coopertive-adversarial optimized as discussed in the paper.</p><p>j + is a function of z in <ref type="bibr" target="#b5">(6)</ref>. Thus, for a memory bank sample b j , we use P + j (N ? j resp.) ? B to denote all anchors z's, for which b j is a positive (negative resp.) sample. For each b j , we have P + j N + j = ? and P + j N + j = B. Then, according to <ref type="bibr" target="#b5">(6)</ref>, all memory bank samples can be updated by</p><formula xml:id="formula_11">b j ? b j ? z?P + j ? ? (x) ?b + j + z?N ? j ? ? (x) ?b j<label>(7)</label></formula><p>with a positive learning rate ?, where the minus "?" and the plus "+" in front of the last two terms reflect the cooperative and the adversarial updates to b j . Effect of l 2 -normalization on memory bank updates. It is worth noting that the gradient in (4) was derived by assuming both z and all memory bank samples b j 's have already been normalized to a unit length. Even though they are normalized, the gradient update will result in new vectors not normalized to a unit length anymore. Thus, an l 2 -normalization is often required after the gradient update. Such a normalization can be built into <ref type="formula" target="#formula_3">(3)</ref>   b j / b j 2 , respectively. This results in the following gradient</p><formula xml:id="formula_12">? (x) ?b j + = ? 1 ? 1 ? p(b j + |z) b j + 2 2 z ? z ? b j + b j + z 2 b j + 3 2</formula><p>This shows how to update the positive sample end-to-end by viewing its representation b j + as free network weights. For simplicity, one can assume every time after a mini-batch update, b j + and z are normalized back to a unit length such that we always keep the current samples b j + 2 = 1 and z 2 = 1. In this case, the above gradient can be simplified to</p><formula xml:id="formula_13">? (x) ?b j + = ? 1 ? 1 ? p(b j + |z) (z ? z ? b j + b j + ) = ? 1 ? 1 ? p(b j + |z) (I ? b j + b T j + )z<label>(8)</label></formula><p>Similarly, we can derive the gradient over negative samples by assuming all b j 's are normalized to a unit norm after each gradient update</p><formula xml:id="formula_14">? (x) ?b j = 1 ? p(b j |z)(I ? b j b T j )z, j = j +<label>(9)</label></formula><p>From <ref type="formula" target="#formula_13">(8)</ref> and <ref type="formula" target="#formula_14">(9)</ref>, we can see that both positive and negative samples will be updated towards the transformed direction</p><formula xml:id="formula_15">T j z with T j = I ? b j b T j .</formula><p>It is easy to verify that ? The gradient T j z is orthogonal to b j , i.e., b T j T j z = 0. It suggests that the gradient is tangent to the unithypersphere such that it tends to keep the unit norm of b j unchanged as expected. ? Along the gradient, the sample b j also tends to be pushed closer towards z since their cosine similarity Now, we can rewrite the gradient update <ref type="bibr" target="#b6">(7)</ref> to all samples j = 1, ? ? ? , K in the memory bank as</p><formula xml:id="formula_16">z T T j z = 1 ? (z ? b j ) 2 ? 0 is nonnegative.</formula><formula xml:id="formula_17">b j ? b j + z?P + j ? ? [1 ? p(b j |z)] T j z cooperative positives + z?N ? j ? ? p(b j |z)T j z adversarial negatives .<label>(10)</label></formula><p>From this update, we observe that ? For cooperative positives, the more probable b j is positive to z with a higher p(b j |z), the less it will be pushed towards z. This is intuitive as a positive sample already closer to its anchor z does not need to be updated too much; ? For adversarial negatives, we have an opposite observation -the more probable b j is positive to z, the more it should be pushed towards the anchor. In this way, it will become a harder negative to be distinguished from the positive counterparts, thereby providing more discriminative information to update the encoder. The Most Probable Positive. The last question we need to answer is how we choose the positive sample given an anchor. Here, we propose to use the Most Probable Positive (MPP) sample resulting in the minimum change that favors a stable update for the training purpose.</p><p>Specifically, once a positive sample b j + is chosen for an anchor z, its representation will be updated along the (negative) gradient based on <ref type="bibr" target="#b7">(8)</ref>. Ideally, we expect that the updated sample will still be positive to the anchor. This ensures the positive assignments in P + j (and thus N ? j as well) be as stable as possible so that they can be updated stably based on <ref type="bibr" target="#b9">(10)</ref>.</p><p>For this reason, we note that [1 ? p(b j |z)] is the step size (up to a constant coefficient 1 ? ) to update the positive sample b j + in <ref type="bibr" target="#b7">(8)</ref>. To minimize the change to the positive sample, we choose the positive sample that minimizes this step size,  to the anchor z. Intuitively, the chosen positive b j + will be the one that minimizes the error in assigning a false positive sample to z based on p(b j |z) in <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_18">j + = arg min j 1 ? p(b j |z) = arg max j p(b j |z),<label>(11)</label></formula><p>To address the most probable positive clearly, the diagram of the training process is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. For a fair comparison with the SOTA methods such as MoCo <ref type="bibr" target="#b6">[7]</ref> and AdCo <ref type="bibr" target="#b7">[8]</ref>, the CaCo also has two streams composed of a momentum encoder and a query encoder, where the momentum encoder is used to identify the positives in the memory bank, and the contrastive loss will be propagated to query encoder and memory bank to train them. All positive and negative samples are treated as free parameters like the network weights, and they are directly learned through gradient descent/ascent by backpropagating the contrastive loss in the proposed cooperativeadversarialfashion, as shown in Eqn. <ref type="bibr" target="#b9">(10)</ref>. Avoidance of collapse. To explain the avoidance of collapse, it is worth noting that both positive and negative samples share the memory bank across query anchors in a mini-batch, and they are dynamically updated over iterations in the proposed cooperative-adversarial training. In other words, a memory bank sample b can be positive for one query anchor, while being negative for another one, which keeps the memory bank from collapsing to a single trivial point. Memory bank samples will also be updated over iterations, and different positives will be assigned to the feature representation z of a sample, keeping the latter from collapsing to a prefixed point b + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we evaluate the proposed CaCo and compare the results with the other unsupervised models including the state-of-the-art contrastive learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>For a fair comparison with the existing models <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we adopt the ResNet-50 as the backbone for unsupervised   pre-training on ImageNet. The output feature map from the top ResNet-50 block is average-pooled and projects to a 256-D feature vector through three MLP layers with two 2048-D hidden layers and the ReLU <ref type="bibr" target="#b5">[6]</ref>. The resultant vector is 2 normalized to calculate the cosine similarity. We adopt the singlecrop protocol used in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> to augment data in singlecrop experiments and we used multi-crop augmentations used in literature <ref type="bibr" target="#b22">[23]</ref> to apply multi-crop in final experiments that can further boost performances. We adopt up to 4, 096 images in a mini-batch for pretraining across up to four 8-Nvidia Tesla V100 GPU servers for single-crop. For the multi-crop setting, based on the same number of four 8-Nvidia Tesla V100 GPU servers, we adopt a batch size of 2, 048 images for pre-training. We will show that the proposed CaCo could also achieve competitive results with smaller batch sizes. By convention, the number of negative adversaries is set to 65, 536 to make a fair comparison with the other methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>For the network pre-training, we adopt an initial learning rate of 0.03 * batch size/256 and 0.3 * batch size/256 in the SGD <ref type="bibr" target="#b28">[29]</ref> and LARS optimizer <ref type="bibr" target="#b29">[30]</ref> for updating the backbone network for smaller (&lt; 1024) and larger (? 1024) batch sizes, respectively, along with a weight decay of 10 ?4 and a momentum of 0.9. The memory bank is always directly optimized by SGD optimizer with learning rate 3.0 without weight decay and a momentum of 0.9. The cosine scheduler <ref type="bibr" target="#b30">[31]</ref> is used to gradually decay the learning rates. We set the same temperate ? to 0.08 when updating the memory bank and network backbone. After the backbone network is initialized, its output feature vectors over randomly drawn training images are used to initialize the memory bank. After that, the encoder and the shared memory bank with both positives and negatives are alternately updated.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Imagenet1K</head><p>First, we perform a linear evaluation by fine-tuning a fully connected classifier for 100 epochs on top of the frozen 2048-D feature vector from the ResNet-50 backbone. <ref type="table" target="#tab_1">Table I</ref> reports the results on Imagenet1K after 200 epochs of pretraining. With different batch sizes of 256, 1024 and 4, 096 for network pre-training, CaCo even outperforms the stateof-the-art SWAV, BYOL and NNCLR with a much larger batch size of 4096 when we used small batch-size. It is well known that the performances of these SOTA models improve with larger batch sizes <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, larger batch sizes require multiple GPU servers with at least 32 Nvidia V100 GPU cards to accommodate 4096 images. This makes the network pre-training unaffordable and inconvenient to set up across multiple servers for most of research teams. The proposed CaCo not only outperforms SOTA methods with much smaller batch sizes, but also allows affordable network pre-training on a single GPU server. Our training time is on par with AdCo <ref type="bibr" target="#b7">[8]</ref>, MoCo <ref type="bibr" target="#b17">[18]</ref> as shown in <ref type="table" target="#tab_1">Table IV</ref> For single-crop settings, we also report the results with more than 200 pre-training epochs in <ref type="table" target="#tab_1">Table II</ref>. Here we run the CaCo with a batch size of 1, 024 and 4, 096 images to compare with the other methods. Here 1, 024 is the largest size that can fit the GPU memory of a single 8 ? V 100 server. It still outperforms the SOTA models with various batch sizes and epochs. This makes the CaCo affordable and competitive for network pretraining. It is worth noting that here we do not apply any tricks such as multi-crop augmentations <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref> and stronger augmentations <ref type="bibr" target="#b32">[33]</ref> that have shown effective in improving the accuracy in unsupervised learning literature. This shows the CaCo is an elegant method easier to implement without relying on those tricks.</p><p>To compare with the methods using multi-crop augmentations <ref type="bibr" target="#b22">[23]</ref>, we have conducted the experiments following the same protocol used in SwAV <ref type="bibr" target="#b22">[23]</ref>. The results are shown in <ref type="table">Table.</ref> III. We outperformed the other methods in this setting even with a smaller batch size of 2, 048 based on the same number of four 8-Nvidia Tesla V100 GPU servers. We believe the performance 75.7% can be further boosted if we can increase the batch size to 4, 096 as used in other methods. However, this requires doubling computing resources, which makes it unaffordable for many research groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transfer Learning Results</head><p>We also perform transfer learning tasks on various datasets 1 . The ResNet50 backbone was pretrained on Imagenet1K over 800 epochs with a batch size of 4096. By following the linear evaluation protocol in <ref type="bibr" target="#b23">[24]</ref>, a fully connected layer of classifier is trained upon the frozen backbone. <ref type="table" target="#tab_8">Table V</ref> reports the results under the linear evaluation compared with the other models. The results show the CaCo performs the best on 8 out of these 11 datasets among the compared self-supervised methods. This demonstrates the CaCo representation pre-trained on the Imagenet1K dataset can well generalize to downstream tasks on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>In this section, we present a thorough analysis of CaCo.</p><p>Here we focused on two most important factors of CaCo: the size of the memory bank (the sum of the positives and negatives) and the contribution of positives and negatives to the performance.</p><p>Size of the memory banks. We set up the experiments with batch-size 4096 with 200 epoch pre-training, which indicates the number of positive is always 4096. We only change the memory bank size across different experiments. As shown in <ref type="table" target="#tab_1">Table VI</ref>, we changed the memory size from 4M to 12M and observed the clear improvement of performance from 71.7% to 72.1%. That's to say, CaCo can be benefited by the size increase of memory bank. This is reasonable since bigger memory bank size has two benefits: 1) more accurate positives by checking the most probable positive from a bigger memory bank; 2) the task become more challenge since encoder needs to select the correct positive out of a bigger pool including more negative pairs.</p><p>Contribution of positives and negatives. We conducted an ablation study to compare the contribution of positives and negatives. Here all experiments are based on a batch size of 1024 with 200 epoch pre-training. Here None setting is without memory bank, while both positive and negatives directly coming from the concurrent mini-batch. Here Positive setting is similar to that NNCLR <ref type="bibr" target="#b12">[13]</ref>, where we keep a queue to save the embeddings of previous batches and we select the nearest neighbor of query in memory bank as positive while negatives are from the concurrent mini-batch. Here Negative setting is similar to AdCo <ref type="bibr" target="#b7">[8]</ref>, where the memory bank is adversarially trained and serves as the negatives, and the positives are from the current mini-batch. Here P+N setting is our CaCo setting, where both positives and negatives are from the memory bank and memory bank is collaboratively and adversarially trained by the contrastive loss in an end-toend fashion. We keep all other settings to the same to have <ref type="bibr" target="#b0">1</ref> The link to Birdsnap dataset is broken or refers to a dataset with missing data, making it difficult to make a direct comparison. a fair comparison. The results are shown in <ref type="table" target="#tab_1">Table VII</ref>. It's clear that CaCo clearly improved compared to using positive or negative alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Result Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Quality of Learned Positives and Negatives:</head><p>The learned positives and negatives in memory bank are "virtual" samples that do not necessarily correspond to the features of some known images. To understand a virtual sample, one can find its most similar image from Imagenet in the feature space and use it as the surrogate.</p><p>We randomly choose some images as queries, and show the surrogates of the most probable positive samples (cf. Eq. (11)) as well as the negative samples from the memory bank in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that the positives (negatives) belong to the same (different) class of query images. This demonstrates that the queries successfully learn the true positives and true negatives from the memory bank.</p><p>Moreover, given a query, the maximum probability corresponding to the most probable positive in (11) measures the likelihood of obtaining a true positive by the model. We plot the curve of the mean maximum positive probability over epochs in <ref type="figure" target="#fig_3">Figure 4</ref>, along with the nearest neighbor (NN) accuracy based on 20% Imagenet1K training set. We plot the NN accuracy to measure the performance of the pre-trained representation as it can be computed in a few seconds.</p><p>We can see that at the beginning, the probability is close to zero as randomly choosing a positive sample from the memory bank of 65, 536 samples is as low as 1.5 ? 10 ?5 . After 200 epochs of pretraining, the probability increase significantly by 10 3 times to above 1.2 ? 10 ?2 . The increase in the mean maximum positive probability is consistent with the increase in the NN accuracy. It suggests that high-quality positives and negatives are eventually learned, leading to better representations learned over epochs, and this partly explains the success of the CaCo method.</p><p>2) Visualization of Representations: In <ref type="figure" target="#fig_4">Figure 5</ref>, we also use t-SNE to visualize the feature embeddings on CIFAR10, PETS and VOC2017 datasets. The encoder is learned by CaCo over 800 epochs on Imagenet1k dataset without labels. It shows that the feature embeddings obtained via t-SNE exhibit distributional patterns well aligned with the classes of various colors on these datasets. This suggests the learned unsupervised features on one dataset well generalize to downstream datasets without access to their labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This paper presents Cooperative-adversarial Contrastive (CaCo) learning for network pre-training. It directly learns both positives and negatives over a shared memory bank as cooperators and adversaries to an encoder by minimizing and maximizing the underlying contrastive loss, respectively. This results in an end-to-end training of positives and negatives iteratively updated towards query anchors along the directions tangent to the unit hypersphere. Jointly trained with the encoder, unsupervised features are learned to distinguish the  Here None setting denotes both positive and negative are from current minibatch; Positive setting denotes the positive embedding is from memory bank and the negative embedding is from the current mini-batch; Negative setting suggests the positive embedding is from current mini-batch and the negative embedding is from the memory bank; P+N is our CaCo setting, where both positive and negative are from the memory bank. Here the #Positive Pool and #Negative Pool suggests the total number of embeddings that we selected from to serve as positive and negatives, respectively.</p><p>learned adversarial negatives from the most probable cooperative positive given each query anchor. Experiments demonstrate that the CaCo outperforms the SOTA self-supervised methods on multiple downstream tasks without relying on multi-crop augmentations.</p><p>ACKNOWLEDGMENT</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>This figure compares existing contrastive learning methods -MoCo and AdCo -with the proposed CaCo. Both MoCo and AdCo only has negative samples from the memory bank, and MoCo queues these samples from the encoder output over past minibatches without training them directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The diagram of CaCo. CaCo trains a visual representation encoder and a cooperative-adversarial memory bank at the same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The surrogate images of the Most Probable Positive (MPP) and random negatives from the learned memory bank for some query examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Plot of Mean Maximum Positive Probability and NN accuracy (based on 20% training set) over 200 epochs of pretraining on Imagenet1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>t-SNE visualization of learned representations on CIFAR10, PETS and VOC2017 datasets, where different colors represent different classes on these datasets. The encoder is pre-trained by CaCo over 800 epochs on Imagenet1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: Top-1 accuracy under the linear evaluation on Ima-</cell></row><row><cell>genet1K dataset with the ResNet-50 backbone. All compared methods</cell></row><row><cell>use single-crop augmentations pre-trained over 200 epochs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>: Top-1 accuracy under the linear evaluation on Im-</cell></row><row><cell>agenet1K with the ResNet-50 backbone. The table compares the</cell></row><row><cell>methods using a single crop augmentation pre-trained with more</cell></row><row><cell>epochs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Top-1 accuracy under the linear evaluation on Im-agenet1K with the ResNet-50 backbone. The table compares the methods using same multi-crop augmentation in SwAV<ref type="bibr" target="#b22">[23]</ref> pretrained with more epochs.</figDesc><table><row><cell>Method</cell><cell cols="3">Epoch Time(h) GPU</cell><cell>GPU Time /epoch</cell></row><row><cell cols="2">MoCo v2 [11] 200</cell><cell cols="2">53.0 8?V100</cell><cell>2.12</cell></row><row><cell>BYOL [24]</cell><cell>1000</cell><cell>8.0</cell><cell>512?TPU</cell><cell>4.10</cell></row><row><cell cols="2">SWAV* [23] 800</cell><cell>50</cell><cell>64?V100</cell><cell>4.06</cell></row><row><cell>AdCo [8]</cell><cell>200</cell><cell cols="2">56.5 8?V100</cell><cell>2.26</cell></row><row><cell>CaCo</cell><cell>200</cell><cell>56</cell><cell>8?V100</cell><cell>2.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Running time comparison of different methods.</figDesc><table /><note>GPU time/Epoch means sum of time cost of all GPUs to train an epoch. Here the time are measured under the asymmetrical settings to have fair comparison with early methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Transfer learning results on various datasets with ResNet-50 pretrained with Imagenet1K over 800 epochs. The results are obtained with ? https://github.com/facebookresearch/moco under the CC-BY-NC 4.0 license and ? https://github. com/maple-research-lab/AdCo/ under MIT license. Other results were reported directly in the other papers.</figDesc><table><row><cell>Method</cell><cell cols="4">Symmetric Loss Batch Size #Negative Samples #Params. Top-1</cell><cell>(GPU ? Time) /epoch</cell></row><row><cell>SimCLR [6]</cell><cell>8192</cell><cell>-</cell><cell>-</cell><cell>67.0</cell><cell>1.92</cell></row><row><cell>MoCo v2 [11]</cell><cell>256</cell><cell>65536</cell><cell>8M</cell><cell>70.2</cell><cell>3.34</cell></row><row><cell>BYOL [24]</cell><cell>4096</cell><cell>-</cell><cell>1M</cell><cell>70.6</cell><cell>4.10</cell></row><row><cell>SimSiam [22]</cell><cell>256</cell><cell>-</cell><cell>2M</cell><cell>70.0</cell><cell>-</cell></row><row><cell>AdCo [8]</cell><cell>256</cell><cell>65536</cell><cell>8M</cell><cell>70.5</cell><cell>3.50</cell></row><row><cell>AdCo [8]</cell><cell>256</cell><cell>16384</cell><cell>2M</cell><cell>70.0</cell><cell>3.46</cell></row><row><cell>AdCo [8]</cell><cell>256</cell><cell>8192</cell><cell>1M</cell><cell>70.2</cell><cell>3.45</cell></row><row><cell>CaCo</cell><cell>4096</cell><cell>37268</cell><cell>4M</cell><cell>71.7</cell><cell>3.83</cell></row><row><cell>CaCo</cell><cell>4096</cell><cell>65536</cell><cell>8M</cell><cell>72.0</cell><cell>3.84</cell></row><row><cell>CaCo</cell><cell>4096</cell><cell>98304</cell><cell>12M</cell><cell>72.1</cell><cell>3.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Top-1 accuracy under the linear evaluation on ImageNet with the ResNet-50 backbone. The table compares the methods over 200 epochs of pretraining. # Parameters: the parameter counts of negative samples, as to BYOL and SimSiam, it means the parameter counts of the predictor.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Top-1 accuracy under the linear evaluation on ImageNet with the ResNet-50 backbone. The table compares different settings over 200 epochs of pretraining.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11260</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1074" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01028</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14548</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Contrastive learning with adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12050</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07589</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Can contrastive learning avoid shortcut solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretextinvariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An empirical study of training selfsupervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">K-shot contrastive learning of visual features with multiple instance augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07713</idno>
		<title level="m">Contrastive learning with stronger augmentations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
