<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
							<email>liunian228@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans perform co-saliency detection by first summarizing the consensus knowledge in the whole group and then searching corresponding objects in each image. Previous methods usually lack robustness, scalability, or stability for the first process and simply fuse consensus features with image features for the second process. In this paper, we propose a novel consensus-aware dynamic convolution model to explicitly and effectively perform the "summarize and search" process. To summarize consensus image features, we first summarize robust features for every single image using an effective pooling method and then aggregate crossimage consensus cues via the self-attention mechanism. By doing this, our model meets the scalability and stability requirements. Next, we generate dynamic kernels from consensus features to encode the summarized consensus knowledge. Two kinds of kernels are generated in a supplementary way to summarize fine-grained image-specific consensus object cues and the coarse group-wise common knowledge, respectively. Then, we can effectively perform object searching by employing dynamic convolution at multiple scales. Besides, a novel and effective data synthesis method is also proposed to train our network. Experimental results on four benchmark datasets verify the effectiveness of our proposed method. Our code and saliency maps are available at https://github.com/nnizhang/CADC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Co-salient object detection (Co-SOD) mimics the human visual system to distinguish common and salient objects when viewing a group of relevant images. Although various Co-SOD methods have been proposed, let us review this problem from the humans perspective. Given a group of images, humans can not segment the co-salient object in each image directly. Instead, they need to first observe all images and summarize the consensus knowledge about what kind of objects this group is focusing on. Then, they look back at each image and search the corresponding objects. We call this process "summarize and search", which is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. A similar explanation can also be found in <ref type="bibr" target="#b47">[48]</ref>. Therefore, we can model Co-SOD in such an intuitive way to summarize the consensus knowledge first and then search consensus objects in each image.</p><p>Previous models can also be explained from such a point. For consensus knowledge summarization, early traditional methods employed graph models <ref type="bibr" target="#b17">[18]</ref> or clustering methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref> to learn the common patterns. However, their models lack end-to-end learning, thus limiting the model performance. Some recent deep models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref> chose to concatenate and convolve all image features for summarizing the consensus knowledge. However, convolution can only aggregate the information at the same location among different images, while co-salient objects often show variations in scales and locations in different images. Hence, these models may easily fail in consensus summarization. Using non-local dependencies <ref type="bibr" target="#b33">[34]</ref> to summarize the consensus cues is another choice <ref type="bibr" target="#b7">[8]</ref>. However, this method lacks scalability since it is computationally prohibitive for processing a large number of images. Some other work <ref type="bibr" target="#b16">[17]</ref> adopted recurrent networks to summarize the consensus cues step by step. However, recurrent models define an input order for image sequences, thus lacking model stability since different input orders will lead to different results.</p><p>For consensus object searching, many works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref> directly fused the consensus feature with image-specific features via summation or concatenation operations. <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b4">[5]</ref> fused co-attention maps with the image-specific information via element-wise multiplication. Such simple methods conduct object searching by linear information fusion, which can not fully exploit the guidance of the summarized consensus knowledge. Besides, <ref type="bibr" target="#b47">[48]</ref> computed channel-wise weight for each single image feature based on its similarity with the consensus representation, which can be seen as an attribute-wise object searching method. We argue that direct spatial searching might be more accurate and easy to learn.</p><p>In this paper, we propose a novel consensus-aware dynamic convolution (CADC) model directly from the "summarize and search" point of view. The image features of the whole group are first summarized and then the consensus knowledge is encoded as dynamic kernels, which capture the appearance traits of common objects. Next, the searching step is performed by using the kernels to convolve the image features to obtain final results, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>However, adopting dynamic convolution for Co-SOD requires delicate model design. We propose to summarize the consensus knowledge via first summarizing the feature of every single image and then integrating cross-image consensus features. For the first step, we propose to use a multiscale max-pooling module to achieve position and scale robust features. For the second step, we leverage the selfattention mechanism <ref type="bibr" target="#b30">[31]</ref>. In this way, our model can meet the needs for scalability and stability. For consensus-aware dynamic kernel generation, we propose to simultaneously construct image adaptive kernels and a common kernel. The former is generated for each image separately to capture fine-grained image-specific cues while the latter is generated for the whole group to summarize coarse group-wise common knowledge. Theoretically, the latter can serve as a supplement and regularization for the former to avoid them focusing too much on the image-specific information. We also generate efficient large dynamic kernels to further consider spatial structures and enlarge the searching range.</p><p>Besides, considering the lack of training data in the Co-SOD field, we propose a novel and effective data synthesis method by fusing common objects with unrelated salient objects in two different ways to mimic the real-world scenarios. It can largely improve the Co-SOD performance and shows superiority when compared with previous methods.</p><p>Our main contributions can be summarized as follows.</p><p>? From the "summarize and search" perspective, we propose a novel CADC model for Co-SOD. Dynamic kernels are generated to summarize the consensus knowledge and object searching is performed using dynamic convolution. ? We propose to combine multi-scale max-pooling and self-attention models to obtain consensus features with both model scalability and stability.</p><p>? We construct two types of dynamic kernels in a supplementary way to capture image-specific cues and the group-wise common knowledge, respectively. ? We develop a novel and more effective data synthesis method to mimic the challenging scenarios in the real world for Co-SOD models training. ? Our CADC network achieves new state-of-the-art Co-SOD results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Co-Saliency Detection</head><p>Early Co-SOD methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> often devote to design hand-crafted features based on different low-level image features. Recent works have introduced deep learning techniques into Co-SOD and gained promising performance. One bunch of works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref> combines deep learning features with other traditional methods. However, such separate learning schemes do not make full use of the advantage of CNNs in a data-driven manner.</p><p>In contrast, another bunch of works adopts end-to-end deep models to learn the common patterns of relevant images. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref> concatenated and convolved all image features to generate the consensus feature, which is sensitive to the variations of object locations and scales. In contrast, we propose a multi-scale max-pooling module to extract position and scale robust features. Besides, some works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref> employed summation or concatenation operations to fuse the consensus features with single image features in a linear space. As a result, they can not explore more effective guidance from the consensus knowledge, hence performing unsatisfactorily for object searching. In contrast, we learn two types of consensusaware dynamic kernels to perform diverse and supplementary consensus summarization and perform dynamic convolution for effective object searching.</p><p>Some other existing models explore long-range dependencies to detect co-salient objects, such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. However, <ref type="bibr" target="#b7">[8]</ref> only explored the interaction between a pair of images by the non-local network <ref type="bibr" target="#b33">[34]</ref>, which is fragile to obtain common features because similar extraneous objects may also appear in both images and cause distraction. Besides, this method also lacks scalability since it is computationally prohibitive to process a large number of images. <ref type="bibr" target="#b16">[17]</ref> utilized recurrent networks to explore the interactions from all available images step by step. However, recurrent models have a sequential order issue and may cause model instability. In contrast, our adopted multi-scale maxpooling module can first decrease the feature dimensionality for each image, which further enables us to summarize the consensus knowledge from all images through the selfattention mechanism. Thus, we can effectively capture the global consensus with both model scalability and stability. ? mean dynamic convolution and our decoder module, respectively. S ? denotes the spatial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamic Convolution</head><p>As a specified method of meta-learning, dynamic convolution uses predicted kernels to perform the convolution operation, which is different from traditional convolutions with fixed filters once trained. Xu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a dynamic filter network to learn custom parameters for different input samples. This idea is widely adopted to address the few-shot learning problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>, where a learner is first trained on a large set of available training data of base categories and then utilized to generate dynamic weights for classifying novel categories. Subsequently, some works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> introduced dynamic convolution into the instance segmentation task. However, most of them only learned one dynamic kernel with the 1 ? 1 size and did not consider learning large spatial kernels due to the involved large computational costs. Pang et al. <ref type="bibr" target="#b22">[23]</ref> introduced large 3 ? 3 dynamic kernels with different dilation rates for RGB-D SOD. However, they generated a different kernel for every pixel in every image, which has significantly large computational costs. Different from previous methods, we design both group-specific and image-specific dynamic kernels to learn diverse and supplementary meta knowledge for Co-SOD. Our dynamic convolution is also computationally efficient by using the depthwise separable mechanism <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows our overall pipeline for Co-SOD. We propose the CADC model for consensus summarization and object searching, where the former is performed by consensus feature aggregation and consensus-aware kernel construction. We embed this model into a U-shaped <ref type="bibr" target="#b27">[28]</ref> model and conduct hierarchical object searching in multiple feature levels. At the same time, we propose a novel and effective data synthesis method to train the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Consensus Feature Aggregation</head><p>Given a group of N relevant images {I n } N n=1 , we first employ our encoder to extract their encoding feature maps X ? R N ?H?W ?C , where H, W , and C represent its height, width, and channel number, respectively. We follow <ref type="bibr" target="#b21">[22]</ref> to slightly modify the original VGG-16 <ref type="bibr" target="#b28">[29]</ref> backbone and insert the modified DASPP module <ref type="bibr" target="#b37">[38]</ref> after it as our encoder. Then, as shown in <ref type="figure">Figure 3</ref>, we employ the adaptive max-pooling layer on X with three target scales and obtain the output features with spatial sizes of 1 ? 1, 3 ? 3 and 6 ? 6, respectively. Then, these output features are flattened and concatenated to generate a feature F ? R N ?46?C . For each image, the obtained feature summarizes the dominated object features at multiple scales, hence is robust to both position and scale variations of co-salient objects.</p><p>The multi-scale max-pooling module dramatically decreases the feature number of each image from H ? W to 46, hence providing us the feasibility to summarize the global consensus from all images by self-attention. Following <ref type="bibr" target="#b30">[31]</ref>, we first apply linear transformations to project F into the query, key, and value spaces with C 2 channels. After that, an affinity matrix A ? R 46N ?46N is calculated by matrix multiplication between the query and the key matrices, and it indicates the pairwise similarities among the 46N features of all images. Since a feature is usually more similar to other features in the same image than those in other images, we reset the self-similarity elements in A computed within each same image into a very small value to avoid intra-image similarities dominating the affinity matrix.</p><p>After that, we obtain an attention matrix via adopting normalization along the second dimension and then perform matrix multiplication with the value to generate an aggre-</p><formula xml:id="formula_0">gated feature Y ? R 46N ? C 2 .</formula><p>Next, Y is re-projected to C channels by a linear transformation and then reshaped to the shape N ? 46 ? C. Finally, it is added onto the original <ref type="figure">Figure 3</ref>. Pipeline of our proposed CADC for consensus summarization and object searching. We generate two types of kernels, i.e., adaptive kernel, and common kernel, for each image and the whole group, respectively. 'SA' means the self-attention module. ? and ? mean the depthwise separable convolution and concatenation, respectively. feature map F for providing a residual signal to generate the consensus feature Z ? R N ?46?C .</p><formula xml:id="formula_1">? 46 ? . . . ? 46 ? SA 46 ? ? ? ? ? 1 ? ? 1 * ... * 46 ? ... ? ? 1 * 1 46 ? ... *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Consensus-aware Kernel Construction</head><p>Based on the consensus feature Z, we generate two kinds of kernels for each image group to encode the summarized consensus knowledge. Since the co-occurring salient objects may have various appearances and scales in different images, we first construct adaptive kernels for each image to encode the fine-grained image-specific consensus object information. A common kernel is also generated for the whole group to capture the coarse group-wise common object knowledge. The latter can be regarded as a supplement and regularization for the former to avoid them paying too much attention to the image-specific information and ignoring the common information. To this end, generating these two kinds of kernels disentangles the learning of imagespecific consensus object information and group-wise common object knowledge, thus better conforming to the nature of Co-SOD and facilitating potential relation exploration between them. Besides, doing so mimics the multibranch architecture widely used in CNNs, which increases the transformation complexity and model capability.</p><p>(1) Vanilla dynamic kernels with 1 ? 1 size</p><p>We first follow most traditional dynamic convolution methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> to generate dynamic kernels with 1 ? 1 size, which is straightforward and easy to implement.</p><p>Adaptive kernels with 1 ? 1 size. We utilize Z to generate adaptive kernels for different images. First, we flatten Z to R N ?46C and learn a feature attention ? ? R N ?46 via:</p><formula xml:id="formula_2">? = F C(ReLU (BN (F C(Z)))),<label>(1)</label></formula><p>where ? is further normalized by the softmax operation along the second dimension to select which one is the most discriminative among all the 46 features of each image. The intermediate FC layer has 1024 nodes. Next, Z is weighted summed by ? along the second dimension to generate the attended feature F a ? R N ?C .</p><p>Finally, the 1 ? 1 adaptive kernels are learned from F a via:</p><p>K a = F C(P ReLU (BN (F C(F a )))),</p><p>where K a ? R N ?C1C and is further reshaped to the shape N ? C 1 ? C ? 1 ? 1. Here C 1 is the desired output channel number of the dynamic convolution operation and the intermediate FC layer has C nodes. We adopt the Parametric ReLU (PReLU) <ref type="bibr" target="#b10">[11]</ref> activation function for generating kernels since they usually have both positive and negative activation.</p><p>Common Kernel with 1 ? 1 size. We aim to employ an attention weight W ? R N ?46 to aggregate all image features in Z along the first two dimensions and generate a group-wise common feature F c ? R C . As discussed in <ref type="bibr" target="#b1">[2]</ref>, the computed self-attention of different queries all tend to highlight the same set of most discriminative key elements. Thus, we can find which features are the most discriminative from the self-attention matrix sof tmax(A). Specifically, we can get the weight W by averaging this matrix along the first dimension. Then, F c can be obtained by using W to weighted sum Z along the first two dimensions. Finally, the common kernel can be learned via:</p><formula xml:id="formula_4">K c = F C(P ReLU (F C(F c ))),<label>(3)</label></formula><p>where K c ? R C1C and is further reshaped to the shape C 1 ? C ? 1 ? 1 as the group-wise kernel. The intermediate FC layer also has C nodes.</p><p>(2) Efficient large dynamic kernels</p><p>The vanilla dynamic kernels with 1 ? 1 size can only encode channel-wise consensus knowledge and ignore spatial cues. Besides, they can only enable object searching within the 1 ? 1 range, resulting in limited searching capability. To introduce spatial cues for the consensus knowledge and also enlarge the searching range, we propose to generate large spatial dynamic kernels. However, it will incur large computation costs and large amounts of the FC parameters if directly use the same method as the vanilla dynamic kernels. For instance, if we want to generate dynamic kernels with a spatial size of 3?3, K a and K c will be 9 times larger, so do the parameters of the FC layers used to generate them. This is also the reason that most dynamic convolution methods do not learn large spatial kernels. We overcome this issue by using the depthwise separable convolution <ref type="bibr" target="#b11">[12]</ref> operation. We construct both adaptive kernels and the common kernel with the 3 ? 3 size in this form as follows.</p><p>Adaptive Kernels with 3 ? 3 size. We decompose 3 ? 3 adaptive kernels K la into depthwise adaptive kernels K da ? R N ?C?3?3 and pointwise adaptive kernels K pa ? R N ?C1?C?1?1 . The latter can be constructed in the same way as K a .</p><p>To construct K da , We transform each of the 46-d features in Z to generate the 3 ? 3 kernel for each channel and each image. We first permute Z to the shape R N C?46 and adopt FC layers as follows:</p><formula xml:id="formula_5">K da = F C(P ReLU (BN (F C(Z)))),<label>(4)</label></formula><p>where the intermediate FC layer has 46 nodes, K da ? R N C?9 and it is further reshaped to the shape N ? C ? 3 ? 3.</p><p>Common Kernel with 3 ? 3 size. We also decompose the 3 ? 3 common kernel into the depthwise common kernel K dc ? R C?3?3 and the pointwise common kernel K pc ? R C1?C?1?1 . Note that the construction of K pc is also the same as K c . To construct K dc , we need to aggregate the information across N images in the consensus feature Z. To this end, we leverage an attention ? 3 ? R N to aggregate the image features with its N attention weights.</p><p>To learn ? 3 , we first flatten Z to the shape of R N ?46C and then generate two attentions ? 1 ? R N ?C and ? 2 ? R N ?46 via:</p><formula xml:id="formula_6">? 1 = F C(ReLU (BN (F C(Z)))), ? 2 = F C(ReLU (BN (F C(Z)))),<label>(5)</label></formula><p>where the intermediate FC layers for both ? 1 and ? 2 have 1024 nodes. Then, ? 1 and ? 2 are further normalized by softmax along the second dimension. Next, we can obtain ? 3 by successively using ? 1 and ? 2 to aggregate the information in Z along the "C" and the "46" dimensions. Finally, we apply softmax on ? 3 and employ it to weighted sum Z for eliminating the first dimension. As a result, we can obtain a feature F dc ? R C?46 , which is utilized to generate K dc via:</p><formula xml:id="formula_7">K dc = F C(P ReLU (BN (F C(F dc )))),<label>(6)</label></formula><p>where the intermediate FC layer has 46 nodes, K dc ? R C?9 and is further reshaped to the shape C ? 3 ? 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Searching via Dynamic Convolution</head><p>After obtaining consensus-aware dynamic kernels, we adopt dynamic convolution on the original feature maps {X n } N n=1 to perform explicit object searching. For vanilla 1 ? 1 kernels, we directly use them to convolve each X n . For efficient large kernels, following depthwise separable convolution <ref type="bibr" target="#b11">[12]</ref>, we first use the depthwise kernels to perform 3 ? 3 group convolution for each channel separately, and then adopt the pointwise kernels to perform regular 1?1 convolution.</p><p>For each image, we use both its adaptive kernel and the common kernel to perform dynamic convolution simultaneously and then fuse the two response maps to C 1 channels via concatenation and convolution, as shown in <ref type="figure">Figure 3</ref>.</p><p>We use the proposed CADC to connect the encoderdecoder pairs in our U-shaped network at multiple levels, hence performing hierarchical object searching at different scales and can effectively improve the searching accuracy. Specifically, we perform hierarchical object searching in the first four decoder modules. In each decoder module, we first perform CADC on the encoder feature map. Then, we concatenate the searching response map with the previous decoder feature map and use two 3 ? 3 Conv layers to fuse them. BN <ref type="bibr" target="#b13">[14]</ref> layers and ReLU are also used right after the Conv layers. For the last two decoder modules, we do not use CADC anymore. Instead, we simply use the previous decoder feature to generate a spatial attention map to filter the current encoder feature, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computational Costs Analysis</head><p>In this section, we discuss the computational costs of our consensus feature aggregation and consensus-aware kernel construction. In the former, our multi-scale max-pooling module dramatically decreases the feature number of each image from H ? W to 46, hence making it possible to aggregate a group of images while the original self-attention incurs large computational costs. For example, given N images, the computational complexity of using self-attention on the original feature maps and our pooled ones are O((N W H) 2 ) and O((46N ) 2 ), respectively, where 46 ? W H. For the consensus-aware kernel construction, our model enlarges the searching range without dramatically increasing computational costs by introducing the depthwiseseparable convolution. It reduces the kernel size to construct from</p><formula xml:id="formula_8">C 1 ? C ? 3 ? 3 to C ? 3 ? 3 + C 1 ? C.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">New Data Synthesis Strategy</head><p>Many previous models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> combined various datasets to train their Co-SOD models. We follow <ref type="bibr" target="#b15">[16]</ref> to use a subset of the COCO dataset <ref type="bibr" target="#b19">[20]</ref> with 9213 images of 65 groups to train our model. However, this dataset highlights all objects that belong to the same category as ground truth, without discriminating salient and non-salient ones. To this end, <ref type="bibr" target="#b15">[16]</ref> had to use an off-the-shelf SOD model <ref type="bibr" target="#b48">[49]</ref> trained on DUTS <ref type="bibr" target="#b32">[33]</ref> as a pre-computed saliency prior. Hence, we also leverage DUTS <ref type="bibr" target="#b32">[33]</ref> in our model training.</p><p>To fit the DUTS dataset to the Co-SOD task, <ref type="bibr" target="#b47">[48]</ref> divided its images into different groups based on the categories of salient objects, obtaining the DUTS class dataset, which contains 8250 images of 291 groups. However, each image in this dataset only contains target salient objects without distractions. To this end, <ref type="bibr" target="#b47">[48]</ref> synthesized images for their model training by using a jigsaw strategy. This method splices each image of the target class with an image from other classes. Although this strategy can simulate the distraction from extraneous salient objects in Co-SOD, it still has drawbacks that the splicing results are unnatural and objects will have large distortions when the synthesized images are resized to fixed shapes for network training.</p><p>Instead, we propose a copy and blend synthesis strategy based on poisson blending <ref type="bibr" target="#b24">[25]</ref>. For each image of the target class, we randomly select an image from other classes and then copy and blend its salient object on the target image background as the distraction to generate the synthesized image. However, for images synthesized in such a normal way, the target objects are usually more salient than the copied extraneous objects. As a result, the trained models easily downgrade to only learn to detect salient objects instead of co-salient objects. To tackle this issue, we also propose a reverse synthesis strategy to copy and blend target objects on the backgrounds of extraneous images using the same aforementioned synthesis method. Finally, we combine both normal and reverse strategies to train our model.</p><p>Compared with <ref type="bibr" target="#b47">[48]</ref>, our proposed method can achieve more natural synthesis results and preserve reasonable shapes for objects, hence is more suitable for training Co-SOD models. <ref type="figure" target="#fig_3">Figure 4</ref> shows some synthesized examples generated by our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>We evaluate our proposed method on four co-saliency benchmark datasets as follows. MSRC <ref type="bibr" target="#b36">[37]</ref> is collected for recognizing objects and we follow <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7]</ref> to select 233 images of 7 groups from MSRC for evaluation. CoSal2015 <ref type="bibr" target="#b41">[42]</ref> and CoSOD3k <ref type="bibr" target="#b5">[6]</ref> are two large-scale datasets containing 2015 images of 50 groups and 3316 images of 160 groups, respectively. CoCA <ref type="bibr" target="#b47">[48]</ref> is the latest and most challenging dataset which contains 1295 images of 80 groups. Different from other datasets, each image in CoCA contains at least one extraneous salient object, hence being more suitable for real-world applications and evaluating the performance of Co-SOD methods.</p><p>We adopt four widely-used evaluation metrics to compare our proposed method with state-of-the-art methods. Maximum F-measure (maxF) considers both precision and recall for co-saliency maps binarized by an optimal threshold. Structure-measure S m <ref type="bibr" target="#b2">[3]</ref> considers object-aware and region-aware structural similarities. Enhanced-alignment measure E ? <ref type="bibr" target="#b3">[4]</ref> considers both global information and local details. Mean Absolute Error (MAE) computes the average absolute per-pixel difference between the predicted co-saliency maps and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>In our data synthesis strategy, for each original image in DUTS class, we generate three synthesized images using the normal strategy and another three using the reverse strategy.</p><p>We follow <ref type="bibr" target="#b20">[21]</ref> to conduct data augmentation and use 256 ? 256 as the training and testing size. We employ the cross-entropy loss as the training loss and deploy deep supervision for each decoder module. Stochastic gradient descent is used as our optimization algorithm. We select at most 14 images from each group as each minibatch and set the total iteration step to 40,000. The initial learning rate is set to 0.01 and divided by 10 at the 20, 000 th and the 30, 000 th iterations, respectively. Our code is implemented using Pytorch <ref type="bibr" target="#b23">[24]</ref>. <ref type="table">Table 1</ref>. Quantitative results of different settings of our proposed model. "VAK" and "VCK" mean vanilla adaptive kernels and the vanilla common kernel, respectively. "LAK" and "LCK" represent large adaptive kernels and the large common kernel. "ML" means adopting CADC at multiple decoder levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>CoCA  </p><formula xml:id="formula_9">Sm ? maxF ? E ? ? MAE ? Baseline</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We conduct ablation studies on the most challenging and the latest Co-SOD dataset CoCA <ref type="bibr" target="#b47">[48]</ref>.</p><p>Effectiveness of CADC. The first row in <ref type="table">Table 1</ref> denotes our baseline model, i.e., employing UNet and DASPP with five simple decoders. This model degenerates to a pure SOD model without considering consensus information among images. Next, we separately use vanilla adaptive kernels (+VAK) and the vanilla common kernel (+VCK) in the baseline to incorporate consensus summarization. It can be seen that vanilla kernels obviously gain improvements when compared with the baseline. Furthermore, by adopting the efficient large dynamic kernels, i.e., large adaptive kernels (+LAK) and the large common kernel (+LCK), the model performance can be further improved when compared with using vanilla kernels. <ref type="figure">Figure 5</ref> also indicates that larger kernels can better search co-occurring objects while vanilla kernels can be easily interfered with by distraction objects or miss to completely segment the whole object. Combining these two kinds of kernels (+LAK+LCK) can bring more performance gains, indicating that consensus object searching can be better performed in a supplementary way. <ref type="figure">Figure 6</ref> also indicates adaptive kernels and common kernels can provide supplemental information.</p><p>Furthermore, we conduct hierarchical object searching on multiple levels (+LAK+LCK+ML), i.e., in the first four decoders. We can find that using dynamic convolution on multi-level feature maps can significantly bringing performance improvements. Hence, we use this setting as our final CADC network. Effectiveness of our data synthesis strategy. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison results of training our model on different data. We first train our model on the COCO subset, i.e., COCO-sub. Then, we respectively add the original DUTS class dataset, the synthesized images using the jigsaw strategy <ref type="bibr" target="#b47">[48]</ref>, synthesizing only using our normal synthesis strategy, synthesizing only using our reverse synthesis strategy, and using our bidirectional synthesis strategy (normal and reverse). The results show that adding original DUTS class images can bring performance gains compared to only using the COCO-sub dataset, indicating the supplementary of saliency attributes is necessary. Furthermore, only using our normal or reverse strategy can obtain slightly better results than using the original DUTS class data. However, using both of them can lead to large performance gains and outperform the jigsaw strategy. Hence, our normal and reverse synthesis strategies provide complementary cues to each other and both of them are indispensable for effective model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with State-of-the-Art Methods</head><p>We compare our proposed model with other 11 state-ofthe-art methods, i.e., CBCS <ref type="bibr" target="#b6">[7]</ref>, DIM <ref type="bibr" target="#b40">[41]</ref>, CODW <ref type="bibr" target="#b41">[42]</ref>, MIL <ref type="bibr" target="#b44">[45]</ref>, IML <ref type="bibr" target="#b26">[27]</ref>, SP-MIL <ref type="bibr" target="#b43">[44]</ref>, GONet <ref type="bibr" target="#b12">[13]</ref>, CSMG <ref type="bibr" target="#b45">[46]</ref>, GCAGC <ref type="bibr" target="#b46">[47]</ref>, GICD <ref type="bibr" target="#b47">[48]</ref> , and ICNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>We illustrate the quantitative comparison results in table 3. Generally, our model achieves the best performance on all four datasets. On the most challenging dataset CoCA, our model brings 3.8% improvement in terms of maxF compared with the second-best method. We also show quali- <ref type="table">Table 3</ref>. Quantitative comparison of our proposed model with other 11 SOTA Co-SOD methods on 4 benchmark datasets. Red and blue denote the best and the second-best results, respectively. '-' indicates the code or result is not available.  tative comparison results in <ref type="figure" target="#fig_5">Figure 7</ref>. It can be seen that our model can better search and segment the co-occurring salient objects in many challenging scenes while other methods often are disturbed by other extraneous salient objects. Specifically, for the ants class, our model can accurately search the targets which are similar to the background while other methods either lost the targets or be interfered with by other salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a consensus-aware dynamic convolution model to explicitly perform the "summarize and search" process for co-saliency detection. Two types of efficient large dynamic kernels are constructed in a sup-plementary way to capture image-specific consensus object cues and the group-wise common knowledge, respectively. We hierarchically search the co-salient objects by performing the dynamic convolution operation at multiple levels. We also present a new data synthesis method to effectively mimic the distraction of extraneous objects in the real world. Extensive experimental results demonstrate the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SearchFigure 1 .</head><label>1</label><figDesc>Main idea of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Framework of our CADC network. ? and D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of our proposed data synthesis method. The first and second columns show the original image and the normally synthesized image, and their corresponding ground truth. The last column shows the reversely synthesized image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visual comparison between "+LAK" and "+VAK". Visual comparison among "+LAK", "+LCK", and "+LAK+LCK".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons of our proposed model with other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of using different training strategies.</figDesc><table><row><cell>Train strategy</cell><cell cols="3">CoCA Sm ? maxF ? E ? ?</cell><cell>MAE ?</cell></row><row><cell>COCO-sub</cell><cell>0.628</cell><cell>0.467</cell><cell>0.707</cell><cell>0.171</cell></row><row><cell>+DUTS class [48]</cell><cell>0.645</cell><cell>0.494</cell><cell>0.720</cell><cell>0.165</cell></row><row><cell>+jigsaw strategy [48]</cell><cell>0.669</cell><cell>0.537</cell><cell>0.740</cell><cell>0.149</cell></row><row><cell>+normal strategy</cell><cell>0.653</cell><cell>0.504</cell><cell>0.725</cell><cell>0.157</cell></row><row><cell>+reverse strategy</cell><cell>0.653</cell><cell>0.510</cell><cell>0.735</cell><cell>0.155</cell></row><row><cell cols="2">+bidirectional strategy 0.681</cell><cell>0.548</cell><cell>0.744</cell><cell>0.132</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Re-thinking co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taking a deeper look at co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2919" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-saliency detection with co-attention fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangshuai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="877" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified metric learning-based framework for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based cosaliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Icnet: Intra-saliency correlation network for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wen-Da Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting robust co-saliency with recurrent co-attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page" from="818" to="825" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning intraimage and inter-images features for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning selective self-mutual attention for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13756" to="13765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic filtering network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="235" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointins: Point-based instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-saliency detection via integration of multi-layer convolutional features and inter-image propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8917" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Group-wise deep co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3041" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep group-wise fully convolutional network for co-saliency detection with graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5052" to="5063" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting co-saliency detection: A novel approach based on two-stage multi-view spectral rotation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic and pyramid attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2398" to="2408" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1163" to="1176" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Co-saliency detection via looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2994" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detection of co-salient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A self-paced multiple-instance learning framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cosaliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3095" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional network with attention graph clustering for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9050" to="9059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient-induced co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
