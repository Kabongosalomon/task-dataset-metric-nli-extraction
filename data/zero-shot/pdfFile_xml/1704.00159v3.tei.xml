<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Human Pose Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
							<email>jiaxiang.shang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
							<email>shuangliang@tongji.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>yichenw@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Human Pose Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M <ref type="bibr" target="#b19">[20]</ref> and is competitive with state-of-the-art results on MPII <ref type="bibr" target="#b2">[3]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation has been extensively studied for both 3D <ref type="bibr" target="#b19">[20]</ref> and 2D <ref type="bibr" target="#b2">[3]</ref>. Recently, deep convolutional neutral networks (CNNs) have achieved significant progresses.</p><p>Existing approaches fall into two categories: detection based and regression based. Detection based methods generate a likelihood heat map for each joint and locate the joint as the point with the maximum value in the map. These heat maps are usually noisy and multi-mode. The ambiguity is reduced by exploiting the dependence between the joints in various ways. A prevalent family of state-of-the-art methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref> adopt a multi-stage architecture, where the output of the previous stage is used as input to enhance the learning of the next stage. These methods are dominant for 2D pose estimation <ref type="bibr" target="#b0">[1]</ref>. However, they do not easily generalize to 3D pose estimation, because the 3D heat maps are too demanding for memory and computation.</p><p>Regression based methods directly map the input image to the output joints. They directly target at the task and they are general for both 3D and 2D pose estimation. Nevertheless, they are not performing as well as detection based * Corresponding author. methods. As an evidence, only one method <ref type="bibr" target="#b5">[6]</ref> in the 2D pose benchmark <ref type="bibr" target="#b0">[1]</ref> is regression based. While they are widely used for 3D pose estimation <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b32">33]</ref>, the performance is not satisfactory. A central problem is that they simply minimize the per-joint location errors independently but ignore the internal structures of the pose. In other words, joint dependence is not well exploited.</p><p>In this work, we propose a structure-aware approach, called compositional pose regression. It is based on two ideas. First, it uses bones instead of joints as pose representation, because the bones are more primitive, more stable, and easier to learn than joints. Second, it exploits the joint connection structure to define a compositional loss function that encodes long range interactions between the bones.</p><p>The approach is simple, effective and efficient. It only re-parameterizes the pose representation, which is the network output, and enhances the loss function, which relates the output to ground truth. It does not alter other algorithm design choices and is compatible with such choices, such as network architecture. It can be easily adapted into any existing regression approaches with little overhead for memory and computation, in both training and inference.</p><p>The approach is general and can be used for both 3D and 2D pose regression, indistinguishably. Moreover, 2D and 3D data can be easily mixed simultaneously in the training. For the first time, it is shown that such directly mixed learning is effective. This property makes our approach different from all existing ones that target at either 3D or 2D task.</p><p>The effectiveness of our approach is validated by comprehensive evaluation with a few new metrics, rigorous ablation study and comparison with state-of-the-art on both 3D and 2D benchmarks. Specifically, it advances the stateof-the-art on 3D Human3.6M dataset <ref type="bibr" target="#b19">[20]</ref> by a large margin and achieves a record of 59.1 mm average joint error, about 12% relatively better that state-of-the-art. On 2D MPII dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>, it achieves 86.4% (PCKh 0.5). It is the bestperforming regression based method and on bar with the state-of-the-art detection based methods. As a by-product, our approach generates high quality 3D poses for in the wild images, indicating the potential of our approach for transfer learning of 3D pose estimation in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose estimation has been extensively studied for years. A complete review is beyond the scope of this work. We refer the readers to <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> for a detailed survey.</p><p>The previous works are reviewed from two perspectives related to this work. First is how to exploit the joint dependency for 3D and 2D pose estimation. Second is how to exploit "in the wild" 2D data for 3D pose estimation.</p><p>3D Pose Estimation Some methods use two separate steps. They first perform 2D joint prediction and then reconstruct the 3D pose via optimization or search. There is no end-to-end learning. Zhou et al. <ref type="bibr" target="#b56">[56]</ref> combines uncertainty maps of the 2D joints location and a sparsity-driven 3D geometric prior to infer the 3D joint location via an EM algorithm. Chen et al. <ref type="bibr" target="#b6">[7]</ref> searches a large 3D pose library and uses the estimated 2D pose as query. Bogo et al. <ref type="bibr" target="#b3">[4]</ref> fit a recently published statistical body shape model <ref type="bibr" target="#b26">[27]</ref> to the 2D joints. Jahangiri et al. <ref type="bibr" target="#b20">[21]</ref> generates multiple hypotheses from 2D joints using a novel generative model. Some methods implicitly learn the pose structure from data. Tekin et al. <ref type="bibr" target="#b41">[42]</ref> represents the 3D pose with an over-complete dictionary. A high-dimensional latent pose representation is learned to account for joint dependencies. Pavlakos et al. <ref type="bibr" target="#b33">[34]</ref> extends the Hourglass <ref type="bibr" target="#b30">[31]</ref> framework from 2D to 3D. A coarse-to-fine approach is used to address the large dimensionality increase. Li et al. <ref type="bibr" target="#b23">[24]</ref> uses an image-pose embedding sub-network to regularize the 3D pose prediction.</p><p>Above works do not use prior knowledge in 3D model. Such prior knowledge is firstly used in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">55]</ref> by embedding a kinematic model layer into deep neutral networks and estimating model parameters instead of joints. The geometric structure is better preserved. Yet, the kinematic model parameterization is highly nonlinear and its optimization in deep networks is hard. Also, the methods are limited for a fully specified kinematic model (fixed bone length, known scale). They do not generalize to 2D pose estimation, where a good 2D kinematic model does not exist.</p><p>2D Pose Estimation Before the deep learning era, many methods use graphical models to represent the structures in the joints. Pictorial structure model <ref type="bibr" target="#b12">[13]</ref> is one of the earliest. There is a lot of extensions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref>. Pose estimation is formulated as inference problems on the graph. A common drawback is that the inference is usually complex, slow, and hard to integrate with deep networks.</p><p>Recently, the graphical models have been integrated into deep networks in various ways. Tompson et al. <ref type="bibr" target="#b45">[46]</ref> firstly combine a convolutional network with a graphical model for human pose estimation. Ouyang et al. <ref type="bibr" target="#b31">[32]</ref> joints feature extraction, part deformation handling, occlusion handling and classification all into deep learning framework. Chu et al. <ref type="bibr" target="#b9">[10]</ref> introduce a geometrical transform kernels in CNN framework that can pass informations between differ-ent joint heat maps. Both features and their relationships are jointly learned in a end-to-end learning system. Yang et al. <ref type="bibr" target="#b48">[49]</ref> combine deep CNNs with the expressive deformable mixture of parts to regularize the output.</p><p>Another category of methods use a multi-stage architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>. The results of the previous stage are used as inputs to enhance or regularize the learning of the next stage. Newell et al. <ref type="bibr" target="#b30">[31]</ref> introduce an Stacked Hourglass architecture that better capture the various spatial relationships associated with the body. Chu et al. <ref type="bibr" target="#b10">[11]</ref> further extend <ref type="bibr" target="#b30">[31]</ref> with a multi-context attention mechanism. Bulat et al. <ref type="bibr" target="#b4">[5]</ref> propose a detection-followed-by-regression CNN cascade. Wei et al. <ref type="bibr" target="#b46">[47]</ref> design a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages. Gkioxari et al. <ref type="bibr" target="#b13">[14]</ref> predict joint heat maps sequentially and conditionally according to their difficulties. All such methods learn the joint dependency from data, implicitly.</p><p>Different to all above 3D and 2D methods, our approach explicitly exploits the joint connection structure in the pose. It does not make further assumptions and does not involve complex algorithm design. It only changes the pose representation and enhances the loss function. It is simple, effective, and can be combined with existing techniques.</p><p>Leveraging in the wild 2D data for 3D pose estimation 3D pose capturing is difficult. The largest 3D human pose dataset Human3.6M <ref type="bibr" target="#b19">[20]</ref> is still limited in that the subjects, the environment, and the poses have limited complexity and variations. Models trained on such data do not generalize well to other domains, such as in the wild images.</p><p>In contrast, in the wild images and 2D pose annotation are abundant. Many works leverage the 2D data for 3D pose estimation. Most of them consist of two separate steps.</p><p>Some methods firstly generate the 2D pose results (joint locations or heat maps) and then use them as input for recovering the 3D pose. The information in the 2D images is discarded in the second step. Bogo et al. <ref type="bibr" target="#b3">[4]</ref> first use Deep-Cut <ref type="bibr" target="#b37">[38]</ref> to generate 2D joint location, then fit with a 3D body shape model. Moreno et al. <ref type="bibr" target="#b29">[30]</ref> use CPM <ref type="bibr" target="#b46">[47]</ref> to detect 2D position of human joints, and then use these observations to infer 3D pose via distance matrix regression. Zhou et al. <ref type="bibr" target="#b57">[57]</ref> use Hourglass <ref type="bibr" target="#b30">[31]</ref> to generate 2D joint heat maps and then coupled with a geometric prior and Jahangiri et al. <ref type="bibr" target="#b20">[21]</ref> also use Hourglass to predict 2D joint heat maps and then infer multiple 3D hypotheses from them. Wu et al. <ref type="bibr" target="#b47">[48]</ref> propose 3D interpreter network that sequentially estimates 2D keypoint heat maps and 3D object structure.</p><p>Some methods firstly train the deep network model on 2D data and fine-tune the model on 3D data. The information in 2D data is partially retained by the pre-training, but not fully exploited as the second fine-tuning step cannot use 2D data. Pavlakos et al. <ref type="bibr" target="#b33">[34]</ref> extends Hourglass <ref type="bibr" target="#b30">[31]</ref> model for 3D volumetric prediction. 2D heat maps are used as in-termediate supervision. Tome et al. <ref type="bibr" target="#b43">[44]</ref> extends CPM <ref type="bibr" target="#b46">[47]</ref> to 3D by adding a probabilistic 3D pose model to the CPM. Some methods train both 2D and 3D pose networks simultaneously by sharing intermediate CNN features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>. Yet, they use separate networks for 2D and 3D tasks.</p><p>Unlike the above methods, our approach treats the 2D and 3D data in the same way and combine them in a unified training framework. The abundant information in the 2D data is fully exploited during training. As a result, our method achieves strong performance on both 3D and 2D benchmarks. As a by-product, it generates plausible and convincing 3D pose results for in the wild images.</p><p>Some methods use synthetic datasets which are generated from deforming a human template model with known ground truth <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. These methods are complementary to the others as they focus on data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compositional Pose Regression</head><p>Given an image of a person, the pose estimation problem is to obtain the 2D (or 3D) position of all the K joints, J = {J k |k = 1, ..., K}. Typically, the coordinate unit is pixel for 2D and millimeter (mm) for 3D.</p><p>Without loss of generality, the joints are defined with respect to a constant origin point in the image coordinate system. For convenience, let the origin be J 0 . Specifically, for 2D pose estimation, it is the top-left point of the image. For 3D pose estimation, it is the ground truth pelvis joint <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>For regression learning, normalization is necessary to compensate for the differences in magnitude of the variables. We use the standard normalization by subtraction of mean and division of standard deviation. For a variable var, it is normalized as</p><formula xml:id="formula_0">var = N (var) = var ? mean(var gt ) std(var gt ) .<label>(1)</label></formula><p>The inverse function for unnormalization is</p><formula xml:id="formula_1">var = N ?1 (? ar) =? ar?std(var gt )+mean(var gt ). (2)</formula><p>Note that both mean( * ) and std( * ) are constants and calculated from the ground truth training samples. The predicted output from the network is assumed already normalized. Both functions N ( * ) and N ?1 ( * ) are parameter free and embedded in the network. For notation simplicity, we use? ar for N (var).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Direct Joint Regression: A Baseline</head><p>Most previous regression based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> directly minimize the squared difference of the predicted and ground truth joints. In experiments, we found that the absolute difference (L1 norm) performs better. In our direct joint regression baseline, the joint loss is</p><formula xml:id="formula_2">L(J ) = K k=1 ||J k ?J gt k || 1 .<label>(3)</label></formula><p>Note that both the prediction and ground truth are normalized.</p><p>There is a clear drawback in loss Eq.(3). The joints are independently estimated. The joint correlation, or the internal structure in the pose, is not well exploited. For example, certain geometric constraints (e.g., bone length is fixed) are not satisfied.</p><p>Previous works only evaluate the joint location accuracy. This is also limited because the internal structures in the pose are not well evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Bone Based Representation</head><p>We show that a simple reparameterization of the pose is effective to address the above issues. As shown in <ref type="figure" target="#fig_0">Figure 1(left)</ref>, a pose is structured as a tree. Without loss of generality, let pelvis be the the root joint J 1 and tree edges be directed from the root to the end joints such as wrists and ankles. Let the function parent(k) return the index of parent joint for k th joint. For notation consistency, let the parent of the root joint J 1 be the origin J 0 , i.e., parent(1) = 0. Now, for k th joint, we define its associated bone as a directed vector pointing from it to its parent,</p><formula xml:id="formula_3">B k = J parent(k) ? J k .<label>(4)</label></formula><p>The joints J are defined in the global coordinate system. In contrast, bones B = {B k |k = 1, ..., K} are more primitive and defined in the local coordinate systems. Representing the pose using bones brings several benefits.</p><p>Stability Bones are more stable than joints and easier to learn. <ref type="figure" target="#fig_0">Figure 1</ref> (middle and right) shows that the standard deviation of bones is much smaller than that of their corresponding joints, especially for parts (ankle, wrist, head) far away from the root pelvis, in both 3D (Human 3.6M <ref type="bibr" target="#b19">[20]</ref>) and 2D datasets (MPII <ref type="bibr" target="#b2">[3]</ref>).</p><p>Geometric convenience Bones can encode the geometric structure and express the geometric constraints more easily than joints. For example, constraint of "bone length is fixed" involves one bone but two joints. Constraint of "joint rotation angle is in limited range" involves two bones but three joints. Such observations motivate us to propose new evaluation metrics for geometric validity, as elaborated in Section 5. Experiments show that bone based representation is better than joint based representation on such metrics.</p><p>Application convenience Many pose-driven applications only need the local bones instead of the global joints. For example, the local and relative "elbow to wrist" motion can sufficiently represent a "pointing" gesture that would be useful for certain human computer interaction scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Compositional Loss Function</head><p>Similar to the joint loss in Eq. <ref type="formula" target="#formula_2">(3)</ref>, bones can be learnt by minimizing the bone loss function</p><formula xml:id="formula_4">L(B) = K k=1 ||B k ?B gt k || 1 .<label>(5)</label></formula><p>However, there is a clear drawback in this loss. As the bones are local and independently estimated in Eq. <ref type="formula" target="#formula_4">(5)</ref>, the errors in the individual bone predictions would propagate along the skeleton and accumulate into large errors for joints at the far end. For example, in order to predict J wrist , we need to concatenate B wrist , B elbow ,...,B pelvis . Errors in these bones will accumulate and affect the accuracy of J wrist in a random manner.</p><p>To address the problem, long range objectives should be considered in the loss. Long range errors should be balanced over the intermediate bones. In this way, bones are jointly optimized. Specifically, let J u and J v be two arbitrary joints. Suppose the path from J u to J v along the skeleton tree has M joints. Let the function I(m) return the index of the m th joint on the path, e.g., I(1) = u, I(M ) = v. Note that M and I( * ) are constants but depend on u and v. Such dependence is omitted in the notations for clarity.</p><p>The long range, relative joint position ?J u,v is the summation of the bones along the path, as  Eq. <ref type="formula">(6)</ref> is differentiable with respect to the bones. It is efficient and has no free parameters. It is implemented as a special compositional layer in the neutral networks.</p><formula xml:id="formula_5">?J u,v = M ?1 m=1 J I(m+1) ? J I(m) = M ?1</formula><p>The ground truth relative position is</p><formula xml:id="formula_6">?J gt u,v = J gt u ? J gt v .<label>(7)</label></formula><p>Then, given a joint pair set P, the compositional loss function is defined as</p><formula xml:id="formula_7">L(B, P) = (u,v)?P ||?J u,v ??J gt u,v || 1 .<label>(8)</label></formula><p>In this way, every joint pair (u, v) constrains the bones along the path from u to v. Each bone is constrained by multiple paths given a large number of joint pairs. The errors are better balanced over the bones during learning.</p><p>The joint pair set P can be arbitrary. To validate the effectiveness of Eq. <ref type="formula" target="#formula_7">(8)</ref>, we test four variants:</p><p>? P joint = {(u, 0)|u = 1, ..., K}. It only considers the global joint locations. It is similar to joint loss Eq.(3).</p><p>? P bone = {(u, parent(u))|u = 1, ..., K}. It only considers the bones. It degenerates to the bone loss Eq.(5).</p><p>? P both = P joint P bone . It combines the above two and verifies whether Eq.(8) is effective.</p><formula xml:id="formula_8">? P all = {(u, v)|u &lt; v, u, v = 1, ..., K}.</formula><p>It contains all joint pairs. The pose structure is fully exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unified 2D and 3D Pose Regression</head><p>All the notations and equations in Section 3 are applicable for both 3D and 2D pose estimation in the same way. The output pose dimension is either 3K or 2K.</p><p>Training using mixed 3D and 2D data is straightforward. All the variables, such as joint J, bone B, and relative joint position ?J u,v , are decomposed into xy part and z part.</p><p>The loss functions can be similarly decomposed. </p><p>The xy term L xy ( * , * ) is always valid for both 3D and 2D samples. The z term L z ( * , * ) is only computed for 3D samples and set to 0 for 2D samples. In the latter case, no gradient is back-propagated from L z ( * , * ).</p><p>Note that the xy part and z part variables have different dimensions. xy is in image coordinate frame and the unit is in pixel. z is in camera coordinate frame and the unit is metric (millimeters in our case). This is no problem. During training, they are appropriately normalized (Eq.(8), Eq.(1)) or unnormalized (Eq.(6), Eq. <ref type="formula">(2)</ref>). During inference, in order to recover the 3D metric coordinates, the xy part is backprojected into camera space using known camera intrinsic parameters and a perspecitive projection model.</p><p>Training We use the state-of-the-art ResNet-50 <ref type="bibr" target="#b15">[16]</ref>. The model is pre-trained on ImageNet classification dataset <ref type="bibr" target="#b11">[12]</ref>. The last fully connected layer is then modified to output 3K (or 2K) coordinates and the model is finetuned on our target task and data. The training is the same for all the tasks (3D, 2D, mixed). SGD is used for optimization. There are 25 epoches. The base learning rate is 0.03. It drops to 0.003 after 10 epoches and 0.0003 after another 10 epoches. Mini-batch size is 64. Two GPUs are used. Weight decay is 0.0002. Momentum is 0.9. Batch-normalization <ref type="bibr" target="#b18">[19]</ref> is used. Implementation is in Caffe <ref type="bibr" target="#b21">[22]</ref>.</p><p>Data Processing and Augmentation The input image is normalized to 224 ? 224. Data augmentation includes random translation(?2% of the image size), scale(?25%), rotation(?30 degrees) and flip. For MPII dataset, the training data are augmented by 20 times. For Human3.6M dataset, the training data are augmented by 4 times. For mixed 2D-3D task, each mini-batch consists of half 2D and half 3D samples, randomly sampled and shuffled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our approach is evaluated on 3D and 2D human pose benchmarks. Human3.6M <ref type="bibr" target="#b19">[20]</ref> is the largest 3D human pose benchmark. The dataset is captured in controlled environment. The image appearance of the subjects and the background is simple. Accurate 3D human joint locations are obtained from motion capture devices.</p><p>MPII <ref type="bibr" target="#b2">[3]</ref> is the benchmark dataset for 2D human pose estimation. It includes about 25k images and 40k annotated 2D poses. 25k of them are for training and another 7k of the remaining are for testing. The images were collected from YouTube videos covering daily human activities with complex poses and image appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comprehensive Evaluation Metrics</head><p>For 3D human pose estimation, previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54]</ref> use the mean per joint position error (MPJPE). We call this metric Joint Error. Some works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">57]</ref> firstly align the predicted 3D pose and ground truth 3D pose with a rigid trans-  formation using Procrustes Analysis <ref type="bibr" target="#b14">[15]</ref> and then compute MPJPE. We call this metric PA Joint Error. For 2D human pose estimation in MPII <ref type="bibr" target="#b2">[3]</ref>, Percentage of Correct Keypoints (PCK) metric is used for evaluation.</p><p>Above metrics only measures the accuracy of absolute joint location. They do not fully reflect the accuracy of internal structures in the pose. We propose three additional metrics for a comprehensive evaluation.</p><p>The first metric is the mean per bone position error, or Bone Error. It is similar to Joint Error, but measures the relative joint location accuracy. This metric is applicable for both 3D and 2D pose.</p><p>The next two are only for 3D pose as they measure the validity of 3D geometric constraints. Such metrics are important as violation of the constraints will cause physically infeasible 3D poses. Such errors are critical for certain applications such as 3D motion capture.</p><p>The second metric is the bone length standard deviation, or Bone Std. It measures the stability of bone length. For each bone, the standard deviation of its length is computed over all the testing samples of the same subject.</p><p>The third metric is the percentage of illegal joint angle, or Illegal Angle. It measures whether the rotation angles at a joint are physically feasible. We use the recent method and code in <ref type="bibr" target="#b1">[2]</ref> to evaluate the legality of each predicted joint. Note that this metric is only for joints on the limbs and does not apply to those on the torso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on 3D Pose of Human3.6M</head><p>For Human3.6M <ref type="bibr" target="#b19">[20]</ref>, there are two widely used evaluation protocols with different training and testing data split.</p><p>Protocol 1 Six subjects (S1, S5, S6, S7, S8, S9) are used in training. Evaluation is performed on every 64th frame of Subject 11's videos. It is used in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">57]</ref>. PA Joint Error is used for evaluation.</p><p>Protocol 2 Five subjects (S1, S5, S6, S7, S8) are used for training. Evaluation is performed on every 64th frame of two subjects (S9, S11). It is used in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. Joint Error is used for evaluation.</p><p>Ablation study. The direct joint regression baseline and four variants of our method are compared. They are briefly summarized in <ref type="table" target="#tab_0">Table 1</ref>. As explained in Section 4, training can use additional 2D data (from MPII), optionally.  <ref type="table">Table 2</ref>. Results of all methods under all evaluation metrics (the lower the better), with or without using MPII data in training. Note that the performance gain of all Ours methods relative to the Baseline method is shown in the subscript. The Illegal Angle metric for "Human3.6M+MPII" setting is not included because it is very good (&lt; 1%) for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Joint  <ref type="table">Table 3</ref>. Detailed results on all joints for Baseline (BL) and Ours (all) methods, only trained on Human3.6M data (top half in <ref type="table">Table 2</ref>). The relative performance gain is shown in the subscript. Note that the left most column shows the names for both the joint (and the bone).</p><p>We therefore tested two sets of training data: 1) only Hu-man3.6M; 2) Human3.6M plus MPII. <ref type="table">Table 2</ref> reports the results under Protocol 2, which is more commonly used. We observe several conclusions.</p><p>Using 2D data is effective. All metrics are significantly improved after using MPII data. For example, joint error is reduced from 102.2 to 64.2. This improvement should originate from the better learnt feature from the abundant 2D data. See the contemporary work <ref type="bibr" target="#b52">[53]</ref> for more discussions. Note that adding 2D data in this work is simple and not considered as a main contribution. Rather, it is considered as a baseline to validate our regression approach.</p><p>Bone representation is superior than joint representation. This can be observed by comparing Baseline with Ours (joint) and Ours (bone). They are comparable because they use roughly the same amount of supervision signals in the training. The two variants of ours are better on nearly all the metrics, especially the geometric constraint based ones.</p><p>Compositional loss is effective. When the loss function becomes better (Ours (both) and Ours (all)), further improvement is observed. Specifically, when trained only on Human3.6M, Ours (all) improves the Baseline by 9.8 mm (relative 9.6%) on joint error, 7.5 mm (relative 10%) on PA joint error, 7.1 mm (relative 10.8%) on bone error, 4.7 mm (relative 17.8%) on bone std, and 1.2% (relative 32.4%) on illegal angle. <ref type="table">Table 3</ref> further reports the performance improvement from Ours (all) to Baseline on all the joints (bones). It shows several conclusions. First, limb joints are harder than torso joints and upper limbs are harder than lower limbs. This is consistent as <ref type="figure" target="#fig_0">Figure 1 (middle)</ref>. It indicates that the variance is a good indicator of difficulty and a per-joint analysis is helpful in both algorithm design and evaluation. Second, our method significantly improves the accuracy for all the joints, especially the challenging ones like wrist, elbow and ankle. <ref type="figure" target="#fig_5">Figure 2</ref> shows the results on a testing video sequence with challenging arm motions. Our result is much better and more stable.</p><p>Comparison with the state-of-the-art There are abundant previous works. They have different experiment settings and fall into three categories. They are compared to our method in <ref type="table" target="#tab_3">Table 4</ref>, 5, and 6, respectively.</p><p>The comparison is not completely fair due to the differences in the training data (when extra data are used), the network architecture and implementation. Nevertheless, two common conclusions validate that our approach is effective and sets the new state-of-the-art in all settings by a large margin. First, our baseline is strong. It is simple but already improves the state-of-the-art, by 3.9 mm (relative 7%) in <ref type="table" target="#tab_3">Table 4</ref>, 2.7 mm (relative 4%) in <ref type="table">Table 5</ref>, and    <ref type="table">Table 6</ref>. Therefore, it serves as a competitive reference. Second, our method significantly improves the baseline, using exactly the same network and training. Thus, the improvement comes from the new pose representation and loss function. It improves the state-ofthe-art significantly, by 7 mm (relative 12.7%) in <ref type="table" target="#tab_3">Table 4</ref>, 7.8 mm (relative 11.7%) in <ref type="table">Table 5</ref>, and 14.9 mm (relative 13.9%) in <ref type="table">Table 6</ref>.</p><p>Example 3D pose results are illustrated in <ref type="figure" target="#fig_7">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on 2D Pose of MPII</head><p>All leading methods on MPII benchmark <ref type="bibr" target="#b0">[1]</ref> have sophisticated network architectures. As discussed in Section 2, the best-performing family of methods adopts a multi-stage architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>. Our method is novel in the pose representation and loss func-  <ref type="table">Table 7</ref>. Results of the baseline and four variants of our method (see <ref type="table" target="#tab_0">Table 1</ref>), in the two-stage IEF*.</p><p>tion. It is complementary to such sophisticated networks. In this experiment, it is integrated into the Iterative Error Feedback method (IEF) <ref type="bibr" target="#b5">[6]</ref>, which is the only regression based method in the family. We implement a two stage baseline IEF, using ResNet-50</p><p>as the basic network in each stage. For reference, the original IEF <ref type="bibr" target="#b5">[6]</ref> uses five stages with GoogLeNet for each stage. We denote our implementation as IEF*. The two stages in IEF* are then modified to use our bone based representation and compositional loss function. The training for all the settings remains the same, as specified in Section 4.</p><p>Ablation study <ref type="table">Table 7</ref> shows the results of IEF* and our four variants. We observe the same conclusions as in <ref type="table">Table 2</ref>. Both bone based representation and compositional loss function are effective under all metrics. In addition, both stages in IEF* benefit from our approach.</p><p>Comparison with the state-of-the-art <ref type="table">Table 8</ref> reports the comparison result to state-of-the-art works on MPII. PCKH0.5 metric is used. Top section of <ref type="table">Table 8</ref> is detection based methods and bottom section is regression based. Ours  <ref type="table">Table 5</ref>, trained with both 3D and 2D data. Note that the MPII 3D results are quite plausible and convincing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Head Sho. Elb. Wri. Hip Knee Ank. Mean Pishchulin <ref type="bibr" target="#b36">[37]</ref> 74. <ref type="bibr" target="#b2">3</ref>   <ref type="table">Table 8</ref>. Comparison to state-of-the-art works on MPII (top: detection based, bottom: regression based). PCKH 0.5 metric is used. Our approach significantly improves the baseline IEF and is competitive to other detection based methods.</p><p>(86.4%) produces significant improvement over the baseline (IEF*) and becomes the best regression based method. It is competitive to other detection based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We show that regression based approach is competitive to the leading detection based approaches for 2D pose estimation once pose structure is appropriately exploited. Our approach is more potential for 3D pose estimation, where more complex structure constraints are critical.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: a human pose is represented as either joints J or bones B. Middle/Right: standard deviations of bones and joints for the 3D Human3.6M dataset<ref type="bibr" target="#b19">[20]</ref> and 2D MPII dataset<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m=1 sgn(parent(I(m)), I(m + 1)) ? N ?1 (B I(m) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 6 )</head><label>6</label><figDesc>The function sgn( * , * ) indicates whether the bone B I(m) direction is along the path direction. It returns 1 when parent(I(m)) = I(m + 1) and ?1 otherwise. Note that the network predicted boneB( * ) is normalized, as in Eq.<ref type="bibr" target="#b5">(6)</ref>. It is unnormalized via Eq. (2) before summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, for compositional loss function Eq.(8), we have L(B, P) = L xy (B, P) + L z (B, P).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, P joint ), Eq.(8) Ours (bone) L(B, P bone ), Eq.(8) Ours (both) L(B, P both ), Eq.(8) Ours (all) L(B, P all ), Eq.(8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>(best viewed in color) Errors of wrist joint/bone of Baseline and Ours (all) methods on a video sequence from Human3.6M S9, action Pose. The average error over the sequence is shown in the legends. For this action, the arms have large motion and are challenging. Our method has much smaller joint and bone error. Our result is more stable over the sequence. The 3D predicted pose and ground truth pose are visualized for a few frames. More video results are at https://www.youtube.com/watch?v=c-hgHqVK90M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5. 1</head><label>1</label><figDesc>mm (relative 4.8%) in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>(best viewed in color) Examples of 3D pose estimation for Human3.6M (top row) and MPII (middle and bottom rows), using Ours (all) method in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The baseline and four variants of our method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?18.9 130.2 115.2 ?15.0 102.6 89.0 ?13.6 40.6 30.6 ?6.6 25.9 12.6 ?13.3 1.9% 0.8% ?1.1</figDesc><table><row><cell></cell><cell></cell><cell>Error</cell><cell cols="2">PA Joint Error</cell><cell cols="2">Bone Error</cell><cell cols="2">Bone Std</cell><cell cols="2">Illegal Angle</cell></row><row><cell>Method</cell><cell>BL</cell><cell>Ours (all)</cell><cell>BL</cell><cell>Ours (all)</cell><cell>BL</cell><cell>Ours (all)</cell><cell>BL</cell><cell>Ours (all)</cell><cell>BL</cell><cell>Ours (all)</cell></row><row><cell>Average</cell><cell>102.2</cell><cell>92.4 ?9.8</cell><cell>75.0</cell><cell>67.5 ?7.5</cell><cell>65.5</cell><cell>58.4 ?7.1</cell><cell>26.4</cell><cell>21.7 ?4.7</cell><cell cols="2">3.7% 2.5% ?1.2</cell></row><row><cell>Ankle(? Knee)</cell><cell>94.5</cell><cell>88.5 ?6.0</cell><cell>81.5</cell><cell>75.8 ?5.7</cell><cell>81.2</cell><cell>74.1 ?7.1</cell><cell>32.9</cell><cell>32.0 ?0.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Knee(?Hip)</cell><cell>68.6</cell><cell>63.7 ?4.9</cell><cell>69.2</cell><cell>62.9 ?6.3</cell><cell>69.1</cell><cell>63.4 ?5.7</cell><cell>21.7</cell><cell>22.8 ?1.1</cell><cell cols="2">4.8% 3.8% ?1.0</cell></row><row><cell>Hip(?Pelvis)</cell><cell>29.9</cell><cell>25.0 ?4.9</cell><cell>63.3</cell><cell>58.4 ?4.9</cell><cell>29.9</cell><cell>25.0 ?4.9</cell><cell>21.3</cell><cell>16.4 ?4.9</cell><cell cols="2">0.6% 0.6% ?0.0</cell></row><row><cell>Thorax(?Pelvis)</cell><cell>97.2</cell><cell>90.1 ?7.1</cell><cell>30.7</cell><cell>28.1 ?2.6</cell><cell>97.2</cell><cell>90.1 ?7.1</cell><cell>28.0</cell><cell>26.7 ?1.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Neck(?Thorax)</cell><cell>104.3</cell><cell>96.4 ?7.9</cell><cell>36.7</cell><cell>35.5 ?1.2</cell><cell>22.2</cell><cell>22.9 ?0.7</cell><cell>12.4</cell><cell>11.7 ?0.7</cell><cell cols="2">2.2% 1.3% ?0.9</cell></row><row><cell>Head(?Neck)</cell><cell>115.4</cell><cell>108.4 ?7.0</cell><cell>42.8</cell><cell>41.1 ?1.7</cell><cell>39.7</cell><cell>37.3 ?2.4</cell><cell>15.3</cell><cell>14.8 ?0.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Wrist(?Elbow)</cell><cell cols="8">181.9 163.0 ?10.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Elbow(?Shoulder)</cell><cell cols="3">168.8 146.9 ?21.9 115.8</cell><cell>97.6 ?18.2</cell><cell>96.9</cell><cell cols="2">81.4 ?15.5 27.6</cell><cell>21.4 ?6.2</cell><cell cols="2">8.5% 5.4% ?3.1</cell></row><row><cell cols="3">Shoulder(?Thorax) 115.6 104.4 ?11.2</cell><cell>57.7</cell><cell>52.2 ?5.5</cell><cell>55.1</cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with previous work on Human3.6M. Protocol 1 is used. Evaluation metric is averaged PA Joint Error. Extra 2D training data is used in all the methods. Baseline and Ours (all) use MPII data in the training. Ours (all) is the best and also wins in all the 15 activity categories.</figDesc><table><row><cell>Method</cell><cell cols="2">Direction Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell></row><row><cell>Yasin[52]</cell><cell>88.4</cell><cell>72.5</cell><cell cols="2">108.5 110.2</cell><cell>97.1</cell><cell>81.6</cell><cell>107.2</cell><cell>119.0</cell></row><row><cell>Rogez[40]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Chen[7]</cell><cell>71.6</cell><cell>66.6</cell><cell>74.7</cell><cell>79.1</cell><cell>70.1</cell><cell>67.6</cell><cell>89.3</cell><cell>90.7</cell></row><row><cell>Moreno[30]</cell><cell>67.4</cell><cell>63.8</cell><cell>87.2</cell><cell>73.9</cell><cell>71.5</cell><cell>69.9</cell><cell>65.1</cell><cell>71.7</cell></row><row><cell>Zhou[57]</cell><cell>47.9</cell><cell>48.8</cell><cell>52.7</cell><cell>55.0</cell><cell>56.8</cell><cell>49.0</cell><cell>45.5</cell><cell>60.8</cell></row><row><cell>Baseline</cell><cell>45.2</cell><cell>46.0</cell><cell>47.8</cell><cell>48.4</cell><cell>54.6</cell><cell>43.8</cell><cell>47.0</cell><cell>60.6</cell></row><row><cell>Ours(all)</cell><cell>42.1</cell><cell>44.3</cell><cell>45.0</cell><cell>45.4</cell><cell>51.5</cell><cell>43.2</cell><cell>41.3</cell><cell>59.3</cell></row><row><cell>Method</cell><cell>SitDown</cell><cell>Smoke</cell><cell>Photo</cell><cell>Wait</cell><cell>Walk</cell><cell cols="2">WalkDog WalkPair</cell><cell>Avg</cell></row><row><cell>Yasin[52]</cell><cell>170.8</cell><cell>108.2</cell><cell>142.5</cell><cell>86.9</cell><cell>92.1</cell><cell>165.7</cell><cell>102.0</cell><cell>108.3</cell></row><row><cell>Rogez[40]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.1</cell></row><row><cell>Chen[7]</cell><cell>195.6</cell><cell>83.5</cell><cell>93.3</cell><cell>71.2</cell><cell>55.7</cell><cell>85.9</cell><cell>62.5</cell><cell>82.7</cell></row><row><cell>Moreno[30]</cell><cell>98.6</cell><cell>81.3</cell><cell>93.3</cell><cell>74.6</cell><cell>76.5</cell><cell>77.7</cell><cell>74.6</cell><cell>76.5</cell></row><row><cell>Zhou[57]</cell><cell>81.1</cell><cell>53.7</cell><cell>65.5</cell><cell>51.6</cell><cell>50.4</cell><cell>54.8</cell><cell>55.9</cell><cell>55.3</cell></row><row><cell>Baseline</cell><cell>79.0</cell><cell>54.5</cell><cell>56.0</cell><cell>46.7</cell><cell>42.2</cell><cell>51.0</cell><cell>47.9</cell><cell>51.4</cell></row><row><cell>Ours (all)</cell><cell>73.3</cell><cell>51.0</cell><cell>53.0</cell><cell>44.0</cell><cell>38.3</cell><cell>48.0</cell><cell>44.8</cell><cell>48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with previous work on Human3.6M. Protocol 2 is used. Evaluation metric is averaged Joint Error. Extra 2D training data is used in all the methods. Baseline and Ours (all) use MPII data in the training. Ours (all) is the best and also wins in all the 15 activity categories. Comparison with previous work on Human3.6M. Protocol 2 is used. Evaluation metric is averaged Joint Error. No extra training data is used. Ours (all) is the best and wins in 12 out of 15 activity categories. Note that Tekin et al.<ref type="bibr" target="#b42">[43]</ref> report more accurate results for "Walk" and "WalkPair", but their method uses the temporal context information in the video. Our method only runs on individual frames.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="4">Direction Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell></row><row><cell></cell><cell cols="2">Chen[7]</cell><cell></cell><cell></cell><cell>89.9</cell><cell>97.6</cell><cell>90.0</cell><cell cols="2">107.9 107.3</cell><cell>93.6</cell><cell>136.1</cell><cell>133.1</cell></row><row><cell></cell><cell cols="2">Tome[44]</cell><cell></cell><cell></cell><cell>65.0</cell><cell>73.5</cell><cell>76.8</cell><cell>86.4</cell><cell>86.3</cell><cell>68.9</cell><cell>74.8</cell><cell>110.2</cell></row><row><cell></cell><cell cols="2">Moreno[30]</cell><cell></cell><cell></cell><cell>69.5</cell><cell>80.2</cell><cell>78.2</cell><cell>87.0</cell><cell>100.8</cell><cell>76.0</cell><cell>69.7</cell><cell>104.7</cell></row><row><cell></cell><cell cols="2">Zhou[57]</cell><cell></cell><cell></cell><cell>68.7</cell><cell>74.8</cell><cell>67.8</cell><cell>76.4</cell><cell>76.3</cell><cell>84.0</cell><cell>70.2</cell><cell>88.0</cell></row><row><cell></cell><cell cols="3">Jahangiri[21]</cell><cell></cell><cell>74.4</cell><cell>66.7</cell><cell>67.9</cell><cell>75.2</cell><cell>77.3</cell><cell>70.6</cell><cell>64.5</cell><cell>95.6</cell></row><row><cell></cell><cell cols="2">Mehta[28]</cell><cell></cell><cell></cell><cell>57.5</cell><cell>68.6</cell><cell>59.6</cell><cell>67.3</cell><cell>78.1</cell><cell>56.9</cell><cell>69.1</cell><cell>98.0</cell></row><row><cell></cell><cell cols="3">Pavlakos[34]</cell><cell></cell><cell>58.6</cell><cell>64.6</cell><cell>63.7</cell><cell>62.4</cell><cell>66.9</cell><cell>57.7</cell><cell>62.5</cell><cell>76.8</cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell>57.0</cell><cell>58.6</cell><cell>57.9</cell><cell>58.7</cell><cell>67.1</cell><cell>54.2</cell><cell>65.9</cell><cell>75.4</cell></row><row><cell></cell><cell cols="2">Ours(all)</cell><cell></cell><cell></cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell></row><row><cell></cell><cell cols="2">Method</cell><cell cols="3">SitDown</cell><cell>Smoke</cell><cell>Photo</cell><cell>Wait</cell><cell>Walk</cell><cell>WalkDog WalkPair</cell><cell>Avg</cell></row><row><cell></cell><cell cols="2">Chen[7]</cell><cell></cell><cell cols="2">240.1</cell><cell>106.7</cell><cell cols="2">139.2 106.2</cell><cell>87.0</cell><cell>114.1</cell><cell>90.6</cell><cell>114.2</cell></row><row><cell></cell><cell cols="2">Tome[44]</cell><cell></cell><cell cols="2">173.9</cell><cell>85.0</cell><cell>110.7</cell><cell>85.8</cell><cell>71.4</cell><cell>86.3</cell><cell>73.1</cell><cell>88.4</cell></row><row><cell></cell><cell cols="2">Moreno[30]</cell><cell></cell><cell cols="2">113.9</cell><cell>89.7</cell><cell>102.7</cell><cell>98.5</cell><cell>79.2</cell><cell>82.4</cell><cell>77.2</cell><cell>87.3</cell></row><row><cell></cell><cell cols="2">Zhou[57]</cell><cell></cell><cell cols="2">113.8</cell><cell>78.0</cell><cell>98.4</cell><cell>90.1</cell><cell>62.6</cell><cell>75.1</cell><cell>73.6</cell><cell>79.9</cell></row><row><cell></cell><cell cols="3">Jahangiri[21]</cell><cell cols="2">127.3</cell><cell>79.6</cell><cell>79.1</cell><cell>73.4</cell><cell>67.4</cell><cell>71.8</cell><cell>72.8</cell><cell>77.6</cell></row><row><cell></cell><cell cols="2">Mehta[28]</cell><cell></cell><cell cols="2">117.5</cell><cell>69.5</cell><cell>82.4</cell><cell>68.0</cell><cell>55.3</cell><cell>76.5</cell><cell>61.4</cell><cell>72.9</cell></row><row><cell></cell><cell cols="3">Pavlakos[34]</cell><cell cols="2">103.5</cell><cell>65.7</cell><cell>70.7</cell><cell>61.6</cell><cell>56.4</cell><cell>69.0</cell><cell>59.5</cell><cell>66.9</cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell>98.1</cell><cell>66.2</cell><cell>71.1</cell><cell>58.4</cell><cell>51.4</cell><cell>65.2</cell><cell>56.8</cell><cell>64.2</cell></row><row><cell></cell><cell cols="2">Ours(all)</cell><cell></cell><cell></cell><cell>86.7</cell><cell>61.5</cell><cell>67.2</cell><cell>53.4</cell><cell>47.1</cell><cell>61.6</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell cols="4">Direction Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell></row><row><cell></cell><cell></cell><cell>Zhou[56]</cell><cell></cell><cell cols="2">87.4</cell><cell>109.3</cell><cell>87.1</cell><cell cols="2">103.2 116.2</cell><cell>106.9</cell><cell>99.8</cell><cell>124.5</cell></row><row><cell></cell><cell></cell><cell>Tekin[43]</cell><cell cols="3">102.4</cell><cell>147.7</cell><cell>88.8</cell><cell cols="2">125.4 118.0</cell><cell>112.4</cell><cell>129.2</cell><cell>138.9</cell></row><row><cell></cell><cell></cell><cell>Xingyi[54]</cell><cell></cell><cell cols="2">91.8</cell><cell>102.4</cell><cell>97.0</cell><cell>98.8</cell><cell>113.4</cell><cell>90.0</cell><cell>93.8</cell><cell>132.2</cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">98.8</cell><cell>101.9</cell><cell>89.8</cell><cell>89.9</cell><cell>100.0</cell><cell>97.2</cell><cell>113.2</cell><cell>102.0</cell></row><row><cell></cell><cell></cell><cell>Ours(all)</cell><cell></cell><cell cols="2">90.2</cell><cell>95.5</cell><cell>82.3</cell><cell>85.0</cell><cell>87.1</cell><cell>87.9</cell><cell>93.4</cell><cell>100.3</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell cols="3">SitDown</cell><cell>Smoke</cell><cell>Photo</cell><cell>Wait</cell><cell>Walk</cell><cell>WalkDog WalkPair</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell>Zhou[56]</cell><cell cols="3">199.2</cell><cell>107.4</cell><cell cols="2">139.5 118.1</cell><cell>79.4</cell><cell>114.2</cell><cell>97.7</cell><cell>113.0</cell></row><row><cell></cell><cell></cell><cell>Tekin[43]</cell><cell cols="3">224.9</cell><cell>118.4</cell><cell cols="2">182.7 138.8</cell><cell>55.1</cell><cell>126.3</cell><cell>65.8</cell><cell>125.0</cell></row><row><cell></cell><cell></cell><cell>Xingyi[54]</cell><cell cols="3">159.0</cell><cell>106.9</cell><cell>125.2</cell><cell>94.4</cell><cell>79.0</cell><cell>126.0</cell><cell>99.0</cell><cell>107.3</cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell cols="3">138.9</cell><cell>101.7</cell><cell>101.1</cell><cell>95.9</cell><cell>90.8</cell><cell>108.9</cell><cell>102.7</cell><cell>102.2</cell></row><row><cell></cell><cell></cell><cell>Ours(all)</cell><cell cols="3">135.4</cell><cell>91.4</cell><cell>94.5</cell><cell>87.3</cell><cell>78.0</cell><cell>90.4</cell><cell>86.5</cell><cell>92.4</cell></row><row><cell>Stage</cell><cell>Metric</cell><cell>IEF*</cell><cell>joint</cell><cell></cell><cell>bone</cell><cell>both</cell><cell>all</cell><cell></cell></row><row><cell></cell><cell>Joint Error</cell><cell>29.7</cell><cell>27.2</cell><cell></cell><cell>27.8</cell><cell>27.5</cell><cell>27.2</cell><cell></cell></row><row><cell>0</cell><cell>Bone Error</cell><cell>24.8</cell><cell>23.1</cell><cell></cell><cell>22.1</cell><cell>22.7</cell><cell>22.5</cell><cell></cell></row><row><cell></cell><cell>PCKH 0.5</cell><cell>76.5%</cell><cell cols="2">79.3%</cell><cell>79.0%</cell><cell>79.2%</cell><cell>79.6%</cell><cell></cell></row><row><cell></cell><cell>Joint Error</cell><cell>25.0</cell><cell>23.8</cell><cell></cell><cell>25.2</cell><cell>23.0</cell><cell>22.8</cell><cell></cell></row><row><cell>1</cell><cell>Bone Error</cell><cell>21.2</cell><cell>20.5</cell><cell></cell><cell>20.9</cell><cell>19.7</cell><cell>19.5</cell><cell></cell></row><row><cell></cell><cell>PCKH 0.5</cell><cell>82.9%</cell><cell cols="2">84.1%</cell><cell>82.7%</cell><cell>84.9%</cell><cell>86.4%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>49.0 40.8 34.1 36.5 34.4 35.2 44.1 Tompson[46] 95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 Tompson[45] 96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 Hu[17] 95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4 Pishchulin[38] 94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4 Lifshitz[25] 97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0 Gkioxary[14] 96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1 Raf[39] 97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3 Insafutdinov[18] 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5 Wei[47] 97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5 Bulat[5] 97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7 Newell[31] 98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.0 Chu[11] 98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5 Carreira(IEF)[6] 95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3 IEF* 96.3 92.6 83.1 74.6 83.7 74.1 71.4 82.9 Ours (all) 97.5 94.3 87.0 81.2 86.5 78.5 75.4 86.4</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mpii Leader Board</surname></persName>
		</author>
		<ptr target="http://human-pose.mpi-inf.mpg.de.1" />
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06524</idno>
		<title level="m">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5600" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generating multiple hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02258</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust and accurate shape model matching using random forest regression-voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bromiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Ionita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1862" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09813</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A survey of computer vision-based human motion capture. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="231" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09010</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07828</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A coarse-to-fine approach for fast deformable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1844" to="1853" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lifting from the deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional 3d pose estimation from a single image</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06854</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02354</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
