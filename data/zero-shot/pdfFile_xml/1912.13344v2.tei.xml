<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning 3D Human Shape and Pose from Dense Body Parts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wanli</forename><forename type="middle">Ouyang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhenan</forename><forename type="middle">Sun</forename></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning 3D Human Shape and Pose from Dense Body Parts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing 3D human shape and pose from monocular images is challenging despite the promising results achieved by the most recent learning-based methods. The commonly occurred misalignment comes from the facts that the mapping from images to the model space is highly non-linear and the rotation-based pose representation of body models is prone to result in the drift of joint positions. In this work, we investigate learning 3D human shape and pose from dense correspondences of body parts and propose a Decompose-and-aggregate Network (DaNet) to address these issues. DaNet adopts the dense correspondence maps, which densely build a bridge between 2D pixels and 3D vertices, as intermediate representations to facilitate the learning of 2D-to-3D mapping. The prediction modules of DaNet are decomposed into one global stream and multiple local streams to enable global and fine-grained perceptions for the shape and pose predictions, respectively. Messages from local streams are further aggregated to enhance the robust prediction of the rotation-based poses, where a position-aided rotation feature refinement strategy is proposed to exploit spatial relationships between body joints. Moreover, a Part-based Dropout (PartDrop) strategy is introduced to drop out dense information from intermediate representations during training, encouraging the network to focus on more complementary body parts as well as neighboring position features. The efficacy of the proposed method is validated on both indoor and real-world datasets including Human3.6M, UP3D, COCO, and 3DPW, showing that our method could significantly improve the reconstruction performance in comparison with previous state-of-the-art methods. Our code is publicly available at https://hongwenzhang.github.io/dense2mesh. Index Terms-3D human shape and pose estimation, decompose-and-aggregate network, position-aided rotation feature refinement, part-based dropout.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECONSTRUCTING human shape and pose from a monocular image is an appealing yet challenging task, which typically involves the prediction of the camera and parameters of a statistical body model (e.g. the most commonly used SMPL [1] model). <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows an example of the reconstructed result. The challenges of this task come from the fundamental depth ambiguity, the complexity and flexibility of human bodies, and variations in clothing and viewpoint, etc. Classic optimization-based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> fit the SMPL model to 2D evidence such as 2D body joints or silhouettes in images, which involve complex non-linear optimization and iterative refinement. Recently, regressionbased approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> integrate the SMPL model within neural networks and predict model parameters directly in an end-to-end manner.</p><p>Though great progress has been made, the direct prediction of the body model from the image space is still complex and difficult even for deep neural networks. In this work, we propose to adopt IUV maps as intermediate representations to facilitate the learning of the mapping from images to models. As depicted in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, compared with other 2D representations <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, the IUV map could provide more rich information, because it encodes the dense correspondence between foreground pixels on 2D images and vertices on 3D meshes. Such a dense semantic map not only contains essential information for shape and pose estimation from RGB images, but also eliminates the interference of unrelated factors such as appearance, clothing, and illumination variations. The representation of 3D body model <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref> can be factorized into the shape and pose components, depicting the model at different scales. The body shape gives an identity-dependent description about the model, while arXiv:1912.13344v2 [cs.CV] 6 Dec 2020 the body pose provides more detailed descriptions about the rotation of each body joint. Previous regression-based methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref> typically predict them simultaneously using global information from the last layer of the neural network. We observe that the detailed pose of body joints should be captured by local visual cues instead of global information. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, we can estimate the rotations of those visible body joints only based on local visual cues, while the information from other body joints and background regions would be irrelevant.</p><p>For the rotation-based pose representation of commonly used body models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, small rotation errors accumulated along the kinematic chain could lead to large drift of position at the leaf joint. Moreover, the rotation estimation is error-prone for those occluded body joints since their perceptions are less reliable under occlusions. Hence, it is crucial to utilize information from visible body joints and the prior about the structure of human bodies. As shown in previous work <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, the structural information at the feature level is helpful for more robust and accurate pose estimation. However, it is non-trivial to apply these feature refinement methods to our case due to the weak correlation between rotation-based poses of different joints. For instance, the shoulder, elbow, and wrist are three consecutive body joints, and one can hardly infer the relative rotation of wrist w.r.t. the elbow given the relative rotation of elbow w.r.t. the shoulder. On the other hand, we observe that the 3D locations of body joints have stronger correlations than the rotation of body joints. For instance, the positions of shoulder, elbow, and wrist are strongly constrained by the length of the arm.</p><p>Based on the observations above, we propose a Decompose-and-aggregate Network (DaNet) to learn 3D human shape and pose from dense correspondences of body parts. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, DaNet utilizes IUV maps as the intermediate information for more efficient learning, and decomposes the prediction modules into multiple streams considering that the prediction of different parameters requires the receptive fields with different sizes. To robustly predict the rotations of body joints, DaNet aggregates messages from different streams and refines the rotation features via an auxiliary position feature space to exploit the spatial relationships between body joints. For better generalization, a Part-based Dropout (PartDrop) strategy is further introduced to drop out dense information from intermediate representations during training, which could effectively regularize the network and encourage it to learn features from complementary body parts and leverage information from neighboring body joints. As will be validated in our experiments, all the above new designs could contribute to better part-based learning and improve the reconstruction performance. To sum up, the main contributions in this work are listed as follows. <ref type="bibr">?</ref> We comprehensively study the effectiveness of adopting the IUV maps in both global and local scales, which contains densely semantic information of body parts, as intermediate representations for the task of 3D human pose and shape estimation. ? Our reconstruction network is designed to have decomposed streams to provide global perception for the camera and shape prediction while detailed perception for pose prediction of each body joint. ? A part-based dropout strategy is introduced to drop dense information from intermediate representations during training. Such a strategy can encourage the network to learn features from complementary body parts, which also has the potential for other structured image understanding tasks. ? A position-aided rotation feature refinement strategy is proposed to aggregate messages from different part features. It is more efficient to exploit the spatial relationship in an auxiliary position feature space since the correlations between position features are much stronger. An early version of this work appeared in <ref type="bibr" target="#b10">[11]</ref>. We have made significant extensions to our previous work in three main aspects. First, the methodology is improved to be more accurate and robust thanks to several new designs, including the part-based dropout strategy for better generalization performance and the customized graph convolutions for more efficient and better feature mapping and refinement. Second, more extensive evaluations and comparisons are included to validate the effectiveness of our method, including evaluations on additional datasets and comparisons of the reconstruction errors across different human actions and model surface areas. Third, more discussions are provided in our ablation studies, including comprehensive evaluations on the benefit of adopting IUV as intermediate representations and in-depth analyses on the refinement upon the rotation feature space and position feature space.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews previous work related to ours. Section 3 provides preliminary knowledge about the SMPL model and IUV maps. Details of the proposed network are presented in Section 4. Experimental results and analyses are included in Section 5. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Human Shape and Pose Estimation</head><p>Early pioneering work on 3D human model reconstruction mainly focuses on the optimization of the fitting process. Among them, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> fit the body model SCAPE <ref type="bibr" target="#b7">[8]</ref> with the requirement of ground truth silhouettes or manual initialization. Bogo et al. <ref type="bibr" target="#b1">[2]</ref> introduce the optimization method SMPLify and make the first attempt to automatically fit the SMPL model to 2D body joints by leveraging multiple priors. Lassner et al. <ref type="bibr" target="#b2">[3]</ref> extend this method and improve the reconstruction performance by incorporating silhouette information in the fitting procedure. These optimizationbased methods typically rely on accurate 2D observations and the prior terms imposed on the shape and pose parameters, making the procedure time-consuming and sensitive to the initialization. Alternatively, recent regression-based methods employ neural networks to predict the shape and pose parameters directly and learn the priors in a datadriven manner. These efforts mainly focus on several aspects including intermediate representation leveraging, architecture designs, structural information modeling, and reprojection loss designs, etc. Our work makes contributions to the first three aspects above and is also complementary to the work focusing on the re-projection loss designs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, reconstruction from videos or multi-view images <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and detailed or holistic body model learning <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Intermediate Representation</head><p>The recovery of the 3D human pose from a monocular image is challenging. Common strategies use intermediate estimations as the proxy representation to alleviate the difficulty. These methods can benefit from existing state-ofthe-art networks for lower-level tasks. For the recovery of 3D human pose or human model, 2D joint positions <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, silhouette <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, segmentation <ref type="bibr" target="#b6">[7]</ref>, depth maps <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, joint heatmaps <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>, volumetric representation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and 3D orientation fields <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> are adopted in literature as intermediate representations to facilitate the learning task. Though the aforementioned representations are helpful for the task, detailed information contained within body parts is missing in these coarse representations, which becomes the bottleneck for fine-grained prediction tasks. Recently, DensePose <ref type="bibr" target="#b38">[39]</ref> regresses the IUV maps directly from images, which provides the dense correspondence mapping from the image to the human body model. However, the 3D model cannot be directly retrieved from such a 2.5D projection. In our work, we propose to adopt such a densely semantic map as the intermediate representation for the task of 3D human shape and pose estimation. To the best of our knowledge, we are among the first attempts <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> to leverage IUV maps for 3D human model recovery. In comparison, the major differences between concurrent efforts and ours lie in three aspects: 1) <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> obtain IUV predictions from a pretrained network of DensePose <ref type="bibr" target="#b38">[39]</ref>, while our work augments the annotations of 3D human pose datasets with the rendered ground-truth IUV maps and imposes dense supervisions on the intermediate representations; 2) <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> only leverage global IUV maps, while our work exploits using IUV maps in both global and local scales; 3) DenseRaC <ref type="bibr" target="#b40">[41]</ref> resorts to involving more synthetic IUV maps as additional training data while our work introduces the part-based dropout upon IUV maps to improve generalization. We believe these concurrent work complement each other and enrich the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Architecture Design</head><p>Existing approaches to 3D human shape and pose estimation have designed a number of network architectures for more effective learning of the highly nonlinear imageto-model mapping. Tan et al. <ref type="bibr" target="#b41">[42]</ref> develop an encoderdecoder based framework where the decoder learns the SMPL-to-silhouette mapping from synthetic data and the encoder learns the image-to-SMPL mapping with the decoder frozen. Kanazawa et al. <ref type="bibr" target="#b4">[5]</ref> present an end-to-end framework HMR to reconstruct the SMPL model directly from images using a single CNN with an iterative regression module. Kolotouros et al. <ref type="bibr" target="#b42">[43]</ref> enhance HMR with the fitting process of SMPLify <ref type="bibr" target="#b1">[2]</ref> to incorporate regression-and optimization-based methods. Pavlakos et al. <ref type="bibr" target="#b5">[6]</ref> propose to predict the shape and pose parameters from the estimated silhouettes and joint locations respectively. Sun et al. <ref type="bibr" target="#b43">[44]</ref> also leverage joint locations and further involve deep features into the prediction process. Instead of regressing the shape and pose parameters directly, Kolotouros et al. <ref type="bibr" target="#b39">[40]</ref> employ a Graph CNN <ref type="bibr" target="#b44">[45]</ref> to regress the 3D coordinates of the human mesh vertices, while Yao et al. <ref type="bibr" target="#b45">[46]</ref> regress the 3D coordinates in the form of an unwrapped position map. All aforementioned regression-based methods predict the pose in a global manner. In contrast, our DaNet predicts joint poses from multiple streams, hence the visual cues could be captured in a fine-grained manner. Recently, G?ler et al. <ref type="bibr" target="#b13">[14]</ref> also introduce a part-based reconstruction method to predict poses from the deep features pooled around body joints. In comparison, the pooling operation of our DaNet is performed on intermediate representations, enabling detailed perception for better pose feature learning. Moreover, existing approaches for rotation-based pose estimation do not consider feature refinement, while DaNet includes an effective rotation feature refinement scheme for robust pose predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Structural Information Modeling</head><p>Leveraging the articulated structure information is crucial for human pose modeling <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Recent deep learning-based approaches to human pose estimation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> incorporate the structured feature learning in their network architecture designs. All these efforts exploit the relationship between the position features of body joints and their feature refinement strategies are only validated on the position-based pose estimation problem. Our work is complementary to them by investigating the refinement for rotation features under the context of the rotation-based pose representation, which paves a new way to impose structural constraints upon rotation features. Our solution aggregates the rotation features into a position feature space, where the aforementioned structural feature learning approaches could be easily applied.</p><p>For more geometrically reasonable pose predictions, different types of pose priors <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> are also employed as constraints in the learning procedure. For instance, Akhter and Black <ref type="bibr" target="#b52">[53]</ref> learn the pose prior in the form of joint angle constraints. Sun et al. <ref type="bibr" target="#b54">[55]</ref> design handcrafted constraints such as limb-lengths and their proportions. Similar constraints are exploited in <ref type="bibr" target="#b55">[56]</ref> under the weakly-supervised setting. For the rotation-based pose representation in the SMPL model, though it inherently satisfies structure constraints such as limb proportions, the pose prior is still essential for better reconstruction performance. SMPLify <ref type="bibr" target="#b1">[2]</ref> imposes several penalizing terms on predicted poses to prevent unnatural results. Kanazawa et al. <ref type="bibr" target="#b4">[5]</ref> introduce an adversarial prior for guiding the prediction to be realistic. All these methods consider the pose prior at the output level. In our work, we will exploit the relationship at the feature level for better 3D pose estimation in the SMPL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regularization in Neural Networks</head><p>Regularization is important to neural networks for better generalization performance. A number of regularization techniques have been proposed to remove features from neural networks at different granularity levels. Among them, dropout <ref type="bibr" target="#b57">[58]</ref> is commonly used at the fully connected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SMPL MODEL AND IUV MAPS</head><p>SMPL Model. The Skinned Multi-Person Linear model (SMPL) <ref type="bibr" target="#b0">[1]</ref> is one of the widely used statistical human body models, which represents the body mesh with two sets of parameters, i.e., the shape and pose parameters. The shape indicates the model's height, weight and limb proportions while the pose indicates how the model deforms with the rotated skeleton joints. Such decomposition of shape and pose makes it convenient for algorithms to focus on one of these two factors independently. In the SMPL model, the shape parameters ? ? R 10 denote the coefficients of the PCA basis of the body shape. The pose parameters ? ? R 3K denote the axis-angle representations of the relative rotation of K skeleton joints with respect to their parents in the kinematic tree, where K = 24 in the SMPL model. For simplicity, the root orientation is also included as the pose parameters of the root joint in our formulation. Given the pose and shape parameters, the model deforms accordingly and generates a triangulated mesh with N = 6890 vertices M(?, ?) ? R 3?N . The deformation process M(?, ?) is differentiable with respect to the pose ? and shape ?, which means that the SMPL model could be integrated within a neural network as a typical layer without any learnable weights. After obtaining the final mesh, vertices could be further mapped to sparse 3D keypoints by a pretrained linear regressor. IUV Maps. Reconstructing the 3D object model from a monocular image is ambiguous, but there are determinate correspondences between foreground pixels on 2D images and vertices on 3D surfaces. Such correspondences could be represented in the form of UV maps, where the foreground pixels contain the corresponding UV coordinate values. In this way, the pixels on the foreground could be projected back to vertices on the template mesh according to a predefined bijective mapping between the 3D surface space and the 2D UV space. For the human body model, the correspondence could have finer granularity by introducing the Index of the body parts <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b61">[62]</ref>, which results in the IUV maps H = (H i |H u |H v ) ? R (1+P )?hiuv?wiuv?3 , where P denotes the number of body parts, h iuv and w iuv denote the height and width of IUV maps. The Index channels H i indicates whether a pixel belongs to the background or a specific body part, while the U V channels H u and H v contain the corresponding U , V values of visible body parts respectively. The IUV maps H encode Index, U, and V values individually for P body parts in a one-hot manner along (1 + P ) ways. The Index values for body parts count from 1 and Index 0 is reserved for the background. For each body part, the UV space is independent so that the representation could be more fine-grained. The IUV annotation of the human body is firstly introduced in DenseReg <ref type="bibr" target="#b61">[62]</ref> and DensePose <ref type="bibr" target="#b38">[39]</ref>. Figs. 2(a)(b)(c) show the Index, U, and V values on the SMPL model as defined in DensePose <ref type="bibr" target="#b38">[39]</ref>.</p><p>Preparation of IUV Maps for 3D Human Pose Datasets. Currently, there is no 3D human pose dataset providing IUV annotations. In this work, for those datasets providing SMPL parameters with human images, we augment their annotations by rendering the corresponding ground-truth IUV maps based on the same IUV mapping protocol of DensePose <ref type="bibr" target="#b38">[39]</ref>. Specifically, we first construct a template texture map from IUV values of each vertex on the SMPL model, and then employ a renderer to generate IUV maps. As illustrated in <ref type="figure">Fig. 2(d)</ref>, for each face in the triangulated mesh, the texture values used for rendering is a triplet vector denoting the corresponding Index, U , and V values. Then, given SMPL models, the corresponding IUV maps can be generated by existing rendering algorithms such as <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Specifically, the renderer takes the template texture map and 3D model as inputs and output a rendered image with the size of h iuv ? w iuv ? 3. Afterwards, the rendered image is reorganized as the shape of (1+P )?h iuv ?w iuv ?3 by converting values into one-hot representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, our DaNet decomposes the prediction task into one global stream for the camera and shape predictions and multiple local streams for joint pose predictions. The overall pipeline involves two consecutive stages. In the first stage, the IUV maps are estimated from global and local perspectives in consideration of the different sizes of the receptive fields required by the prediction of different parameters. In the second stage, the global and local IUV maps are used for different feature extraction and prediction tasks. The global features are extracted from global IUV maps and then directly used to predict camera and body shape. The rotation features are extracted from partial IUV maps and further fed into the aggregated refinement module before the final prediction of joint poses. During training, the part-based dropout is applied to the estimated IUV maps between the above two stages. Overall, our objective function is a combination of three objectives:</p><formula xml:id="formula_0">L = L inter + L target + L ref ine ,<label>(1)</label></formula><p>where L inter is the objective for estimating the intermediate representations (Sec. 4.1), L target is the objective for predicting the camera and SMPL parameters (Sec. 4.2), L ref ine is the objective involving in the aggregated refinement module (Sec. 4.3). In the following subsections, we will present the technical details and rationale of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Global and Partial IUV Estimation</head><p>The first stage in our method aims to estimate corresponding IUV maps from input images for subsequent prediction tasks. Specifically, a fully convolutional network is employed to produce K +1 sets of IUV maps, including one set of global IUV maps and K sets of partial IUV maps for the corresponding K body joints. The global IUV maps are aligned with the original image through up-sampling, while the partial IUV maps are centered around the body joints. <ref type="figure">Fig. 4</ref> visualizes a sample of the global and partial IUV maps. The feature maps outputted from the last layer of the FCN would be shared by the estimation tasks of both global and partial IUV maps. The estimation of the global IUV maps is quite straightforward since they could be obtained by simply feeding these feature maps into a convolutional layer. For the estimation of each set of partial IUV maps, a joint-centric RoI pooling would be first performed on these feature maps to extract appropriate sub-regions, which results in K sets of partial feature maps. Then, the K sets of partial IUV maps would be estimated independently from these partial feature maps. Now, we will give details about the RoI pooling process for partial IUV estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-centric RoI Pooling.</head><p>For pose parameters in the SMPL model, they represent the relative rotation of each body joint with respect to its parent in the kinematic tree. Hence, the perception of joint poses should individually focus on corresponding body parts. In other words, globally zooming, translating the human in the image should have no effect on the pose estimation of body joints. Moreover, the ideal scale factors for the perception of joint poses should vary from one joint to another since the proportions of body parts are different. To this end, we perform joint-centric RoI pooling on feature maps for partial IUV estimation. Particularly, for each body joint, a sub-region of the feature maps is extracted and spatially transformed to a fixed resolution for subsequent partial IUV map estimation and joint pose prediction. In our implementation, the RoI pooling is accomplished by a Spatial Transformer Network (STN) <ref type="bibr" target="#b64">[65]</ref>. In comparison with the conventional STNs, the pooling process in our network is learned in an explicitly supervised manner.</p><p>As illustrated in <ref type="figure">Fig. 5</ref>(a), the joint-centric RoI pooling operations are guided by 2D joint positions so that each sub-region is centered around the target joint. Specifically, 2D joint heatmaps are estimated along with the global IUV maps in a multi-task learning manner, and 2D joint positions are retrieved from heatmaps using the soft-argmax <ref type="bibr" target="#b65">[66]</ref> operation. Without loss of generality, let j k denote the position of the k-th body joint. Then, the center and scale parameters used for spatial transformation are determined individually for each set of partial IUV maps. Specifically, for the k-th set of partial IUV maps, the center c k is the position of the k-th joint, while the scale s k is proportional to the size of the foreground region, i.e.,</p><formula xml:id="formula_1">c k = j k , s k = ? k max(w bbox , h bbox ) + ?,<label>(2)</label></formula><p>where ? k and ? are two constants, w bbox and h bbox denote the width and height of the foreground bounding box respectively. In our implementation, the foreground is obtained from the part segmentation (i.e., Index channels of estimated IUV maps). Compared with our previous work <ref type="bibr" target="#b10">[11]</ref> calculating s k from 2D joints, the s k s determined by foreground regions here are more robust to 2D joint localization. Note that the above constants ? k and ? can be handcrafted or learned in the STN by taking ground-truth IUV maps as inputs. For learned ? k s, <ref type="figure">Fig. 5</ref>(b) shows how the values of different body joints evolve over learning iterations. It can be observed that ? k s are enlarged for some joints while shrunk for others, which provides more suitable RoI sizes for each body joint.</p><p>After obtaining the transformation parameters in Eq. 2, the feature maps extracted from the last layer of fully convolutional network are spatially transformed to a fixed resolution and used to estimate the partial IUV maps, where the corresponding ground-truth ones are also extracted from the ground-truth global IUV maps using the same pooling process.</p><p>Considering that the pose of a body joint is only related to its adjacent body parts, we can further simplify partial IUV maps by discarding those irrelevant body parts. For each set of partial IUV maps, we retain specific channels corresponding to those body parts surrounding the target joint. The partial IUV maps before and after the simplification are depicted in <ref type="figure">Fig. 4</ref>(b) and <ref type="figure">Fig. 4</ref>(c) respectively.</p><p>Loss Functions. A classification loss and several regression losses are involved in the training of this stage. For both global and partial IUV maps, the loss is calculated in the same manner and denoted as L iuv . Specifically, a classification loss is imposed on the Index channels of IUV maps, where a (1 + P )-way cross-entropy loss is employed to classify a pixel belonging to either background or one among P body parts. For the U V channels of IUV maps, an L 1 based regression loss is adopted, and is only taken into account for those foreground pixels. In other words, the estimated U V channels are firstly masked by the groundtruth Index channel before applying the regression loss. For the 2D joint heatmaps and 2D joint positions estimated for RoI pooling, an L 1 based regression loss is adopted and denoted as L roi . Overall, the objective in the IUV estimation stage involves two main losses:</p><formula xml:id="formula_2">L inter = ? iuv L iuv + ? roi L roi ,<label>(3)</label></formula><p>where ? iuv and ? roi are used to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Camera, Shape and Pose Prediction</head><p>After obtaining the global and partial IUV maps, the camera and shape parameters would be predicted in the global stream, while pose parameters would be predicted in the local streams. The global stream consists of a ResNet <ref type="bibr" target="#b66">[67]</ref> as the backbone network and a fully connected layer added at the end with 13 outputs, corresponding to the camera scale s ? R, translation t ? R 2 and the shape parameters ? ? R 10 . In the local streams, a tailored ResNet acts as the backbone network shared by all body joints and is followed by K residual layers for rotation feature extraction individually. For the k-th body joint, the extracted rotation features would be refined (see Sec. 4.3) and then used to predict the rotation matrix R k ? R 3?3 via a fully connected layer. Here, we follow previous work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> to predict the rotation matrix representation of the pose parameters ? rather than the axis-angle representation defined in the SMPL model. Note that using other rotation representations such as the 6D continuous representation <ref type="bibr" target="#b67">[68]</ref> is also feasible. An L 1 loss is imposed on the predicted camera, shape, and pose parameter, and we denote it as L smpl .</p><p>Following previous work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we also add additional constraint and regression objective for better performance. For the predicted rotation matrix, we impose an orthogonal constraint loss L orth =</p><formula xml:id="formula_3">K?1 k=0 R k R T k ? I 2 upon the predicted rotation matrices {R k } K?1 k=0</formula><p>to guarantee their orthogonality. Moreover, given the predicted SMPL parameters, the performance could be further improved by adding supervision explicitly on the resulting model M(?, ?). Specifically, three L 1 based loss functions are used to measure the difference between the ground-truth positions and the predicted ones. The corresponding losses are denoted as L vert for vertices on 3D mesh, L 3Dkp for sparse 3D human keypoints, and L reproj for the reprojected 2D human keypoints, respectively. For the sparse 3D human keypoints, the predicted positions are obtained via a pretrained linear regressor by mapping the mesh vertices to the 3D keypoints defined in human pose datasets. Overall, the objective in this prediction stage is the weighted sum of multiple losses:</p><formula xml:id="formula_4">L target = ? smpl L smpl + ? orth L orth + ? point (L vert + L 3Dkp + L reproj ) ,<label>(4)</label></formula><p>where ? smpl , ? orth , and ? point are balance weights. Part-based Dropout. Our approach learn the shape and pose from the IUV intermediate representation, which contains dense correspondences of the body parts. Following previous work on data augmentation <ref type="bibr" target="#b60">[61]</ref> and model regularization <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr">[</ref> </p><p>where H[p, :, :, :] denotes IUV maps with the part index of p.</p><p>PartDrop is motivated by the observation that the estimated IUV maps on real-world images typically have errors on irregular parts in challenging cases such as heavy occlusions. <ref type="figure">Fig. 6</ref> visualizes how PartDrop, DropBlock <ref type="bibr" target="#b59">[60]</ref>, and Dropout <ref type="bibr" target="#b57">[58]</ref> drop values in part-wise, block-wise, and unit-wise manners. It can be observed that, Dropout leaves obvious pepper-like artifacts after dropping, DropBlock introduces unwanted square patterns, while PartDrop brings much less visual artifacts in the resulting IUV maps. In comparison with DropBlock and Dropout, the proposed Part-Drop can remove semantic information from foreground areas in a more structured manner, which consequently enforces the neural network to learn features from complementary body parts and improves its generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rotation Feature Refinement</head><p>In our approach, the rotation features extracted in local streams are aggregated to exploit spatial relationships among body joints. As illustrated in <ref type="figure" target="#fig_4">Fig. 7(a)</ref>, the positionaided rotation feature refinement involves three consecutive steps, namely rotation feature collection, position feature refinement, and refined feature conversion. Specifically, the rotation features are first collected into the position feature space where the feature refinement is performed. After that, the rotation feature refinement is accomplished by converting the refined position features back to the rotation feature space. All these three steps are performed by customized graph convolution layers. In particular, we consider the following graph-based convolution layer G(?) that employs one popular formulation of the Graph Convolution Networks as proposed in Kipf et al. <ref type="bibr" target="#b44">[45]</ref>.</p><formula xml:id="formula_6">Z out = G(A, Z in ) = ?(?Z in W ),<label>(6)</label></formula><p>where Z in and Z out are input and output features respectively, ?(?) is the activation function, W is the parameters of convolution kernels,? denotes the row-normalized matrix of the graph adjacency matrix A, i.e.,</p><formula xml:id="formula_7">? = D ? 1 2 AD ? 1 2 if A is a symmetric matrix, and otherwise? = D ?1 A, where D is the diagonal node degree matrix of A with D ii = j A ij .</formula><p>For simplicity, we also refer to the graph with adjacency matrix of A as graph A.</p><p>Step 1: Rotation Feature Collection. Note that the rotation of each body joint could be viewed as sequential data along the kinematic chain. This is inspired by the fact that the human could act in a recurrent manner according to the kinematic tree shown in <ref type="figure" target="#fig_4">Fig.7(b)</ref>. The position of a specific body joint can be calculated from the collection of the relative rotations and bone lengths of those joints belonging to the same kinematic chain. At the feature level, we propose to learn the mapping from rotation feature space to position feature space. To that end, one graph convolution layer is customized to gather information from body joints along the kinematic chain and learn such mapping. Formally, let X ? R K?C denote the rotation features extracted from K sets of partial IUV maps with C being the feature dimension. The position features Y ? R K?C of K joints is obtained by feeding X to the graph convolution, i.e.,</p><formula xml:id="formula_8">Y = G(A r2p , X),<label>(7)</label></formula><p>where the graph with adjacency matrix A r2p is customized as a collection graph for mapping rotation features into the position feature space, in which A r2p ij = 1 if the j-th joint is one of the ancestors of the i-th joint along the kinematic chain, and otherwise A r2p ij = 0. The adjacency matrix A r2p of the collection graph is depicted in <ref type="figure" target="#fig_4">Fig. 7(c)</ref>.</p><p>Step 2: Position Feature Refinement. Since there are strong spatial correlations among neighboring body joints, utilizing such structured constraints could effectively improve the features learned at each joint. Towards this goal, a graph-based convolution network is employed to exploit spatial relationships between joints. Specifically, the position features Y are fed into L graph convolution layers with the following layer-wise formulation:</p><formula xml:id="formula_9">Y (l) = G(A rf , Y (l?1) ),<label>(8)</label></formula><p>where Y l denotes the position features obtained from the l-th layer with Y 0 = Y , and the graph with adjacency matrix A rf = I +? rf serves as a refinement graph for feature refinement, in which? rf ij = 1 if the i-th and j-th joints are neighboring, and otherwise? rf ij = 0. After graph convolutions, the refined position features? are obtained by adding Y L with the original position features Y in a residual manner, i.e.,? = Y + Y L . <ref type="figure" target="#fig_4">Fig. 7(d)</ref> shows an example of the adjacency matrix A rf which considers both one-hop and two-hop neighbors. Note that A rf could have various forms according to the neighbor definition of body joints.</p><p>Inspired by previous work <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b68">[69]</ref>, we also add a learnable edge weighting mask on the graph convolution of this step since messages from different joints should have different contributions to the feature refinement of the target joint. In this way, we have the adjacency matrix A rf improved as  <ref type="table" target="#tab_3">0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  1  1  1  1  1  1  1  1  1</ref>   <ref type="table" target="#tab_3">0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23</ref> 1 <ref type="table" target="#tab_3">0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22</ref>  where ? denotes the element-wise product, M ? [0, 1] K?K is the learnable edge weighting matrix serving as an attention mask of the graph to balance the contributions of neighboring features to the target feature.</p><formula xml:id="formula_10">A rf = I + M ?? rf ,<label>(9)</label></formula><formula xml:id="formula_11">1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 (d) A rf</formula><p>Step 3: Refined Feature Conversion. The last step of refinement is to convert the refined features back to the original rotation feature space. Since the rotation and position of body joints are two mutual representation of 3D human pose, after the refinement of position features, the rotation features can be refined accordingly 1 . Specifically, for the k-th body joint, its rotation features can be refined by aggregating messages from the refined position features of three consecutive body joints, i.e., the joint itself and its parent and child joints. Similar to the first step, the mapping from position features to rotation features is also learned via a graph-based convolution layer, where the difference lies in the adjacency matrix of the graph. Formally, the refined position features? are fed into the graph to obtain features in the rotation space, resulting in the refined rotation feature? X for the final prediction of joint pose parameters, i.e.,</p><formula xml:id="formula_12">X = G(A p2r ,? ),<label>(10)</label></formula><p>where the graph with adjacency matrix A p2r = I +? p2r is customized as a conversion graph for mapping position features to rotation features, in which? p2r ij = 1 if the jth joint is the parent or child joint of the i-th joint, and otherwise? ij = 0. The adjacency matrix A p2r of the conversion graph is depicted in <ref type="figure" target="#fig_4">Fig. 7(e)</ref>.</p><p>Supervision in Refinement. The rotation and position feature spaces are built under corresponding supervisions during training. As illustrated in <ref type="figure" target="#fig_4">Fig. 7(a)</ref>, the rotation 1. Strictly speaking, the joint rotations can not be fully retrieved from the joint positions due to the fewer DoFs specified in position-based poses. This issue is mild at the feature level since features could be more redundant.</p><p>features X andX are used to predict joint rotations, while the position features Y and? are used to predict joint positions. L 1 based rotation and position supervisions are imposed on these predictions correspondingly, which compose the objective L ref ine involved in the refinement procedure. Note that these intermediate predictions are unnecessary during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>The FCN for IUV estimation in our framework adopts the architecture of HRNet-W48 <ref type="bibr" target="#b69">[70]</ref>, which is one of the most recent state-of-the-art networks for dense estimation tasks. The FCN receives the 224 ? 224 input and produces 56 ? 56 feature maps for estimating global and local IUV maps with the same resolution. The IUV estimation network is initialized with the model pretrained on the COCO keypoint detection dataset <ref type="bibr" target="#b70">[71]</ref>, which is helpful for robust joint-centric RoI pooling and partial IUV estimation. Two ImageNetpretrained ResNet-18 <ref type="bibr" target="#b66">[67]</ref> are employed as the backbone networks for global and rotation feature extraction respectively. During training, data augmentation techniques, including color jittering and flipping, are applied randomly to input images. Random rotation is used when in-thewild datasets are involved for training. The IUV estimation task is first trained for 5k iterations before involving the parameter prediction task. The ? k s in Eq. (2) are first learned using ground-truth IUV maps as inputs and then frozen as constants for other experiments, while ? is empirically set to 0.1. The hyper-parameters ?s are decided based on the scales of values in objectives. The dropping rate ? for PartDrop is adopted as 0.3 in our experiments. For more robust pose recovery from the estimated partial IUV, we perform random jittering on the estimated 2D joint position and the scale of partial IUV maps during training. Following previous work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref>, the predicted poses are initialized from the mean pose parameters. For faster runtime, the local streams are implemented to run in a parallel manner. Specifically, the partial IUV maps of all body joints are concatenated batch-wise and then fed into the backbone feature extractor. Moreover, individual rotation feature extraction is implemented based on group convolution. By default, we adopt the Adam <ref type="bibr" target="#b71">[72]</ref> optimizer with an initial learning rate of 1 ? 10 ?4 to train our model, and reduce the learning rate to 1 ? 10 ?5 after 30k iterations. The learning process converges after around 60k iterations and takes about 25 hours on a single TITAN Xp GPU. During testing, due to the fundamental depth-scale ambiguity, we follow previous work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref> to center the person within the image and perform scaling such that the inputs have the same setting as training. Our experiments are implemented in PyTorch <ref type="bibr" target="#b72">[73]</ref>. More implementation details could be found in the publicly available code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets and Evaluation Metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b73">[74]</ref> is a large-scale dataset which consists of 3.6 millions of video frames captured in the controlled environment, and currently the most commonly used benchmark dataset for 3D human pose estimation.</p><p>Kanazawa et al. <ref type="bibr" target="#b4">[5]</ref> generated the ground truth SMPL parameters by applying MoSH <ref type="bibr" target="#b74">[75]</ref> to the sparse 3D MoCap marker data. Following the common protocols <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we use five subjects (S1, S5, S6, S7, S8) for training and two subjects (S9, S11) for evaluation. We also down-sample the original videos from 50fps to 10fps to remove redundant frames, resulting in 312,188 frames for training and 26,859 frames for testing.</p><p>UP-3D. UP-3D <ref type="bibr" target="#b2">[3]</ref> is a collection dataset of existing 2D human pose datasets (i.e., LSP <ref type="bibr" target="#b75">[76]</ref>, LSP-extended <ref type="bibr" target="#b76">[77]</ref>, MPII HumanPose <ref type="bibr" target="#b77">[78]</ref>, and FashionPose <ref type="bibr" target="#b78">[79]</ref>), containing 5,703 images for training, 1,423 images for validation, and 1,389 images for testing. The SMPL parameter annotations of these real-world images are augmented in a semi-automatic way by using an extended version of SMPLify <ref type="bibr" target="#b2">[3]</ref>.</p><p>COCO. The COCO dataset <ref type="bibr" target="#b70">[71]</ref> contains a large scale of images and person instances labeled with 17 keypoints. Based on the COCO dataset, DensePose-COCO <ref type="bibr" target="#b38">[39]</ref> further provides the dense correspondences from 2D images to the 3D surface of the human body model for 50K humans. Different from our rendered IUV maps, the correspondence annotations in DensePose-COCO only consist of approximately 100-150 points per person, which are a sparse subset of the foreground pixels of human images. In our experiments, we discard those persons without 2D keypoint annotations, resulting in 39,210 samples for training. Since there are no ground-truth shape and pose parameters for COCO, we evaluate our method quantitatively on the keypoint localization task using its validation set, which includes 50,197 samples.</p><p>3DPW. The 3DPW dataset <ref type="bibr" target="#b79">[80]</ref> is a recent in-the-wild dataset providing accurate shape and pose ground truth annotations. This dataset captured IMU-equipped actors in challenging outdoor scenes with various activities. Following previous work <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b42">[43]</ref>, we do not use its data for training but perform evaluations on its defined test set only. There are 35,515 samples extracted from videos for testing.</p><p>Fitted SMPL labels from SPIN. Kolotouros et al. <ref type="bibr" target="#b42">[43]</ref> proposed SPIN to incorporate a fitting procedure within the training of a SMPL regressor. The regressor provided better initialization for the fitting of human models to 2D keypoints, and the resulting SMPL parameters could be more accurate than those fitted in a static manner. For evaluation on 3DPW <ref type="bibr" target="#b79">[80]</ref>, our model would be supervised with the final fitted SMPL labels from SPIN <ref type="bibr" target="#b42">[43]</ref> for inthe-wild datasets including LSP <ref type="bibr" target="#b75">[76]</ref>, LSP-Extended <ref type="bibr" target="#b76">[77]</ref>, MPII <ref type="bibr" target="#b77">[78]</ref>, COCO <ref type="bibr" target="#b70">[71]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b80">[81]</ref>.</p><p>Evaluation Metrics. Following previous work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref>, for evaluating the reconstruction performance, we adopt the mean Per-vertex Error (PVE) as the primary metric, which is defined as the average point-to-point Euclidean distance between the predicted model vertices and the ground truth model vertices. Besides the PVE metric, we further adopt PVE-S and PVE-P as secondary metrics for separately evaluate the shape and pose prediction results. The PVE-S computes the per-vertex error with the pose parameters of ground truth and predicted models set as zeros (i.e., models under the rest pose <ref type="bibr" target="#b0">[1]</ref>), while the PVE-P computes the analogous per-vertex error with the shape parameters set as zeros. For the Human3.6M dataset, the widely used Mean Per Joint Position Error (MPJPE) and the MPJPE after rigid alignment of the prediction with ground truth using Procrustes Analysis (MPJPE-PA) are also adopted to quantitatively evaluate the 3D human pose estimation performance. The above three metrics will be reported in millimeters (mm) by default.</p><p>For the keypoint localization task on COCO, the commonly-used metric is the Average Precision (AP) defined by its organizers 2 . The keypoint localization AP is calculated based on the Object Keypoint Similarity (OKS), which plays the same role as the IoU in object detection. We report results using the mean AP, and the variants of AP including AP 50 (AP at OKS = 0.50), AP 75 (AP at OKS = 0.75), AP M for medium objects, and AP L for large objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparison on the Indoor Dataset.</head><p>Evaluation on Human3.6M. We evaluate the 3D human mesh recovery as well as pose estimation performance for quantitative comparison on Human3.6M, where our model is trained on its training set. <ref type="table" target="#tab_3">Table 1</ref> reports the comparison results with previous methods that output more than sparse 3D keypoint positions. For regression-based methods in <ref type="table" target="#tab_3">Table 1</ref>, different architectures have been designed to predict the shape and pose parameters. Among them, HMR <ref type="bibr" target="#b4">[5]</ref> adopts a single CNN and an iterative regression module to produce all parameters. Pavlakos et al. <ref type="bibr" target="#b5">[6]</ref> decompose the shape and pose prediction tasks, while their pose parameters are predicted from 2D joints positions. NBF <ref type="bibr" target="#b6">[7]</ref> adopts segmentation as the intermediate representation and learns all parameters from it. CMR <ref type="bibr" target="#b39">[40]</ref> directly regresses 3D meshes with a graph-based convolutional network. All these architectures estimate pose parameters through a single stream with an exception that HoloPose <ref type="bibr" target="#b13">[14]</ref> regresses poses using a part-based model. As can be seen from <ref type="table" target="#tab_3">Table 1</ref>, our network significantly outperforms the above-mentioned architectures. It's worth noting that the methods reported in <ref type="table" target="#tab_3">Table 1</ref> are not strictly comparable since they may use 2. https://cocodataset.org/#keypoints-eval different datasets for training. Among existing state-of-theart approaches, we have a very competitive result which is only inferior to SPIN in <ref type="table" target="#tab_3">Table 1</ref>. SPIN has the same architecture as HMR except that it uses the 6D continuous representation <ref type="bibr" target="#b67">[68]</ref> for 3D rotations. SPIN aims to incorporate regression-and optimization-based methods, while our work focuses on the design of a stronger regressor. Hence, our method is complementary to SPIN since we can combine them together by simply plugging our network into SPIN.</p><p>For more comprehensive comparison, <ref type="table" target="#tab_4">Table 2</ref> reports pose estimation performance across different actions on Human3.6M. Compared with SPIN and other methods, our method can be more robust to challenging actions such as Sitting and Sitting Down. We believe these benefits come from our decomposition design which enables our network to capture more detailed information for joint poses and produce more accurate reconstruction results. We can also see from the last row of <ref type="table" target="#tab_4">Table 2</ref> that, by simply replacing rotation matrices with the 6D representations <ref type="bibr" target="#b67">[68]</ref> for pose parameters as SPIN do, our method can achieve results on par with or even better than SPIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Comparison on In-the-wild Datasets</head><p>Reconstructing 3D human model on real-world images is much more challenging due to factors such as extreme poses Image HMR <ref type="bibr" target="#b4">[5]</ref> Rong et al. <ref type="bibr" target="#b14">[15]</ref> SPIN <ref type="bibr" target="#b42">[43]</ref> Ours <ref type="figure">Fig. 9</ref>. Qualitative comparison of reconstruction results on the COCO dataset. and heavy occlusions. In our network, the aggregated refinement module and PartDrop training strategy are proposed to enhance its robustness and generalization. We conduct evaluation experiments on UP-3D, COCO, and 3DPW to demonstrate the efficacy of our method.</p><p>Evaluation on UP-3D. For comparison on the UP-3D dataset, we report quantitative results in the PVE of the reconstructed meshes in <ref type="table" target="#tab_5">Table 3</ref>. In comparison with previous methods, our method outperforms them across all subsets of UP-3D by a large margin. Our closest competitor BodyNet <ref type="bibr" target="#b33">[34]</ref> has the PVE value of 102.5 on LSP, while ours is 88.5. Moreover, BodyNet <ref type="bibr" target="#b33">[34]</ref> uses both 2D and 3D estimation as the intermediate representation, which is much more time-consuming than ours. Reconstruction results on UP-3D are visualized in <ref type="figure">Fig. 8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on COCO.</head><p>For evaluation on COCO, we train our model on the mixture of training data from DensePose-COCO and Human3.6M datasets, and perform both qualitative and quantitative comparison on the COCO validation set. We first show qualitative reconstruction results in <ref type="figure">Fig. 9</ref>, and make comparisons with HMR [5], Rong et al. <ref type="bibr" target="#b14">[15]</ref>, and SPIN <ref type="bibr" target="#b42">[43]</ref>. As we can see, our method has better generalization in real-world scenarios with more accurate and well-aligned reconstruction performances. Our method can produce reasonable results even in cases of extreme poses, occlusions, and incomplete human bodies, while competitors fail or produce visually displeasing results.</p><p>To perform quantitative evaluations on COCO, we project keypoints from the estimated SMPL models on the image plane, and compute the Average Percision (AP) based on the keypoint similarity with the ground truth annotations. We report keypoint localization APs of different approaches in <ref type="table" target="#tab_6">Table 4</ref>, where we also include 2D human pose estimation approaches <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> for comparison. It can be seen that, in terms of keypoint localization results, approaches for 3D human mesh recovery lag far behind those for 2D human pose estimation. Among approaches for human mesh recovery, our model achieves significantly higher APs than previous ones. Compared with the recent state-of-the-art method SPIN <ref type="bibr" target="#b42">[43]</ref>, our model improves the mean AP and AP 50 by 16.5% and 29.5%, respectively. We attribute such remarkable improvements to our decomposeand-aggregate design. To validate this, we upgrade the backbone of SPIN to HRNet-W64-C <ref type="bibr" target="#b69">[70]</ref>, a more powerful classification network, and denote it as SPIN-HRNet. As shown in <ref type="table" target="#tab_6">Table 4</ref>, though SPIN-HRNet has a stronger backbone with more parameters than our whole network, it brings much less gains over SPIN (3.9% improvement in mean AP from 17.3% to 21.2%). In contrast, our network decomposes the perception tasks and aggregates them efficiently, making our SMPL regressor more effective to handle challenging cases in real-world scenes. <ref type="table" target="#tab_6">Table 4</ref> also presents the comparison of our approach against our previous model DaNet-LSTM <ref type="bibr" target="#b10">[11]</ref>. DaNet-LSTM has the same network architecture with ours except that its aggregation procedure is performed sequentially along kinetic chains via LSTM. Based on DaNet-LSTM, we introduce the graph-based aggregation module and PartDrop strategy in this work. The graph-based aggregation performs feature refinement in parallel for all body parts, while PartDrop regularizes the network and encourages learning features from complementary body parts. These newly introduced designs can help to improve the robustness and generalization of our model. As shown in <ref type="table" target="#tab_6">Table 4</ref>, both two new components contribute to higher performance in this challenging dataset. By replacing the LSTM-based aggregation module with the graph-based one, our DaNet-GCN obtains a 6.8% improvement in AP 50 . By adopting the PartDrop strategy for training, we further have a 3.1% improvement in AP 50 . Taking these two updates together, our approach improves the AP 50 by 9.9% over DaNet-LSTM from 58.7% to 68.6%. We can also see from <ref type="table" target="#tab_6">Table 4</ref> that other dropping out strategies such as Dropout and DropBlock do not work Image SPIN <ref type="bibr" target="#b42">[43]</ref> Ours <ref type="figure" target="#fig_0">Fig. 11</ref>. Qualitative comparison of reconstruction results on the 3DPW dataset.</p><p>well as PartDrop and even degrade the performance. One intuitive explanation for this is that our PartDrop can better imitate the corrupted IUV maps in challenging cases. As we can observe from <ref type="figure" target="#fig_0">Fig. 10</ref> that the body parts are missing irregularly from the estimated IUV maps due to occlusions. PartDrop helps to produce more natural and well-aligned results in comparison with its alternatives.</p><p>Evaluation on 3DPW. In <ref type="table" target="#tab_8">Table 5</ref>, we report the results of our approach and other state-of-the-art approaches on the 3DPW test set. Here, we use the same datasets and training strategy as SPIN <ref type="bibr" target="#b42">[43]</ref> and do not use any data from 3DPW for training. Besides, the valid SMPL parameters fitted in SPIN are adopted as ground-truth labels for those in-thewild training datasets. As shown in <ref type="table" target="#tab_8">Table 5</ref>, our approach reduces the MPJPE-PA by 4.4 mm over SPIN to 54.8 mm, achieving the best performance among frame-based and even temporal approaches. <ref type="table" target="#tab_8">Table 5</ref> also includes SPIN-HRNet for comparison, where we can see that there is only a 0.7 mm reduction in MPJPE-PA over SPIN. <ref type="figure" target="#fig_0">Fig. 11</ref> depicts the qualitative results of our approach. We can observe that our model has better generalization performances on 3DPW in comparison with SPIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Running Time</head><p>During inference, our method takes about 93ms on a Titan Xp GPU, where the IUV estimation accounts for 60ms while the parameter prediction accounts for the rest 33ms. The running time and platform of different models are included in <ref type="table">Table 6</ref> for comparison. Numbers are obtained from respective literature or evaluated using their official implementation. Overall, our method has a moderate computation cost among regression-based reconstruction methods. <ref type="table">Table 6</ref> Comparison of running time (ms) with state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Run Time GPU HMR <ref type="bibr" target="#b4">[5]</ref> 40 GTX 1080 Ti Pavlakos et al. <ref type="bibr" target="#b5">[6]</ref> 50 Titan X NBF <ref type="bibr" target="#b6">[7]</ref> 110 Titan Xp BodyNet <ref type="bibr" target="#b33">[34]</ref> 280 Modern GPU CMR <ref type="bibr" target="#b39">[40]</ref> 33 RTX 2080 Ti DenseRaC <ref type="bibr" target="#b40">[41]</ref> 75 Tesla V100</p><p>Ours 93 Titan Xp </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To evaluate the effectiveness of the key components proposed in our method, we conduct ablation experiments on Human3.6M under various settings. We will begin with our baseline network by removing the local streams, aggregated refinement module, and PartDrop strategy in our method. In other words, the baseline simply uses the global stream of DaNet to predict all parameters. Moreover, it adopts ResNet101 <ref type="bibr" target="#b66">[67]</ref> as the backbone network for parameter predictions such that the model size of the baseline is comparable to that of the networks used in ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Intermediate Representation</head><p>To show the superiority of adopting the IUV map as the intermediate representation, our baseline network adopts its alternatives for the shape and pose prediction tasks. Specifically, the IUV maps are replaced by the convolutional feature maps outputted from the last layer of the FCN or the part segmentation (i.e., Index channels of IUV maps). Note that there is actually no intermediate representation for the approach adopting feature maps as "intermediate representation". As observed from <ref type="table" target="#tab_9">Table 7</ref>, the approach adopting IUV maps as intermediate representations achieves the best performance. In our experiments, we found that the approach without using any intermediate representation is more prone to overfitting to the training set.</p><p>Effect of IUV Estimation Quality. We further conduct experiments to investigate the impact of the quality of dense estimation on the final shape and pose prediction performance. To this end, different architectures or initializations of the IUV estimators are adopted in ablation experiments to produce IUV maps with different qualities. Specifically, the IUV estimator adopts the pose estimation networks <ref type="bibr" target="#b82">[83]</ref> built upon ResNet-50 and ResNet-101 as alternative architectures, and these models are pretrained on ImageNet <ref type="bibr" target="#b85">[86]</ref> or COCO <ref type="bibr" target="#b70">[71]</ref>. Following the protocol of DensePose <ref type="bibr" target="#b38">[39]</ref>, we measure the quality of dense correspondence estimations via the pointwise evaluation <ref type="bibr" target="#b38">[39]</ref>, where the area under the curve at the threshold of 10cm (i.e., AUC 10 ) is adopted as the metric. <ref type="figure" target="#fig_0">Fig. 12(a)</ref> reports the reconstruction results of ablation approaches versus their qualities of IUV estimations. As we can see, networks with better IUV estimations consistently achieve better reconstruction performance. To investigate the performance upper bound of adopting IUV maps as intermediate representations, we also report the results of the approach using ground truth IUV maps as input with the removal of the IUV estimator. As shown in the rightmost result of <ref type="figure" target="#fig_0">Fig. 12(a)</ref>, the approach learning from the ground truth IUV maps achieves much better performance than using the estimated one outputted from networks, which means that there is still a large margin for improvement by adopting IUV maps as intermediate representations.</p><p>In contrast to the concurrent work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> obtaining IUV maps from the pretrained network of Dense-Pose <ref type="bibr" target="#b38">[39]</ref>, our approach augments the annotation of Hu-man3.6M with the rendered IUV maps so that our IUV estimator can be trained on Human3.6M with dense supervision, which enables our network to have a higher quality of IUV estimation. To verify this, the IUV estimator is firstly trained on DensePose-COCO or Human3.6M, and then frozen to generate IUV maps for the training of the reconstruction task on Human3.6M. As can be seen from <ref type="figure" target="#fig_0">Fig. 12(b)</ref>, approaches with the IUV estimators trained on Human3.6M consistently achieve better performances on both IUV estimation and model reconstruction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Decomposed Perception</head><p>The decomposed perception provides fined-grained information for detailed pose estimation. To validate the effectiveness of such a design, we report the performance of the approaches using one-stream and multiple streams in <ref type="table" target="#tab_11">Table 8</ref>, where the D-Net denotes the variant of our DaNet without using the aggregated refinement module and PartDrop strategy. Results in PVE-S and PVE-P are also reported in <ref type="table" target="#tab_11">Table 8</ref> for separately studying the efficacy of the decomposed design on the shape and pose predictions. It can be seen that the reconstruction performance metric PVE is actually dominated by the PVE-P metric. Comparison of the first and second rows in <ref type="table" target="#tab_11">Table 8</ref> shows that using multiple streams has barely effects on the shape prediction but brings a significant improvement in the pose prediction (i.e., the PVE-P value drops more than 14%). We also report results to validate the use of different ratios ? k and the simplification of partial IUV maps. In the 3rd and 4th rows of <ref type="table" target="#tab_11">Table 8</ref>, D-Net-ES adopts equal scales with all ? k s set to 0.5, while D-Net-AP adopts partial IUV maps with all body parts. As can be seen, such modifications degrade the performance, which is due to two facts that (i) the proportions of body parts are different and (ii) the rotational states of different body joints are relatively independent and  <ref type="table" target="#tab_3">(mm)   68  75  79  81  82  82  86  86  87  92  94  98   112  113   130   62   72  71  67  73  73  78  76  78  83  76  86   96   86   120   56  60  62  60  66  67  71  67  67  76   65   84  89   80   108   52  56  58  56  59  63  65  63  62  70   59   75  75  69</ref> 92 Baseline +Decomposition +PartDrop +Aggregation <ref type="figure" target="#fig_0">Fig. 13</ref>. Reconstruction performance of ablation approaches across different actions on the Human3.6M dataset.  involving irrelevant body parts could disturb the inference of the target joint rotations.</p><p>To visualize the reconstruction performance on different body areas, <ref type="figure" target="#fig_0">Fig. 14 depicts</ref> the average per-vertex error with respect to the surface areas of the human model. As shown in <ref type="figure" target="#fig_0">Fig. 14(a)</ref>, for the baseline network, the per-vertex errors of limb parts (hands, feet) are much higher than that of the torso. By comparing Figs. 14(a) and 14(b), we can conclude that our decomposed perception design alleviates the above issue and achieves much better reconstruction performance on limb parts. Reconstruction performances across different actions on Human3.6M are also reported in <ref type="figure" target="#fig_0">Fig. 13</ref> for comprehensive evaluations. We can see that the decomposed perception design reduces reconstruction errors consistently for all actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Part-based Dropout</head><p>The proposed Part-based Dropout (PartDrop) strategy drops IUV values in contiguous regions at the granularity level of body parts. Such a dropping out strategy can effectively regularize the neural network by removing semantic information from foreground areas of intermediate representations. In this subsection, we conduct experiments  to validate its effectiveness and evaluate the impact of the dropping rate on the reconstruction performance.</p><p>To validate the superiority of our PartDrop strategy, we adopt DropBlock <ref type="bibr" target="#b59">[60]</ref> and Dropout <ref type="bibr" target="#b57">[58]</ref> as alternative strategies to drop values from intermediate representations during training. For DropBlock, following the setting of <ref type="bibr" target="#b59">[60]</ref>, the size of the block to be dropped is set to 7 in our experiments. For fair comparison, only the foreground pixels are involved in counting the dropping rate. <ref type="figure" target="#fig_0">Fig. 15</ref> reports the performance of the full model reconstruction as well as its shape and pose components under different strategies across different dropping rates. It can be seen that the performance gains brought by dropping out strategies mainly come from the pose prediction tasks since the evaluation metric PVE is dominated by its pose component PVE-P. Among three strategies, Dropout is the worst and its performance deteriorates quickly when increasing the rate of dropping out. DropBlock works better than Dropout and brings marginal gains when the dropping rate is less than 20%. Though we can see from the PVE-S curves in <ref type="figure" target="#fig_0">Fig. 15(b)</ref> that DropBlock has comparable results with PartDrop on shape prediction when the dropping rate is larger than 40%, its pose prediction results degrade significantly as shown in <ref type="figure" target="#fig_0">Fig. 15(c)</ref>. We hypothesize that the removal of a large area of block makes DropBlock similar to PartDrop for the global perception but does harm to the local perception for pose prediction. Compared with these two alternatives, the proposed PartDrop is more robust to the dropping rate and achieves the best results at a dropping rate around 30%. The above comparison of unit-wise, block-wise, and partwise dropping strategies suggest that removing features in a structured manner is crucial to our reconstruction task, where PartDrop performs best among them. The efficacy of Pos.-aided Ref. <ref type="figure" target="#fig_0">Fig. 16</ref>. Example results of approaches without refinement, or using direct / position-aided refinement strategies. PartDrop can be also validated from the reconstruction error reduction shown in <ref type="figure" target="#fig_0">Fig. 13</ref> and <ref type="figure" target="#fig_0">Fig. 14(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Aggregated Refinement</head><p>Our aggregated refinement module is proposed to impose spacial structure constraints upon rotation-based pose features. As observed from <ref type="figure" target="#fig_0">Fig. 14(d)</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>, the aggregation in DaNet effectively reduces the reconstruction errors across all surface areas and human actions considerably. A straightforward strategy to refine the feature would be conducting refinement between the rotation features directly. In such a direct refinement strategy, the first and third steps of our refinement procedure are removed and the rotation features are directly refined by the graph convolution layers of the second step. The features outputted from the last refinement layer are also added with the original rotation features in a residual manner and then used to predict joint rotations. For fair comparison, the refinement layer number of the direct strategy is equal to the number of the layers involved in the three steps of the position-aided strategy.</p><p>Rotation Feature Space vs. Position Feature Space. The proposed position-aided refinement strategy performs refinement in the position feature space instead of the rotation feature space. The graphs A r2p and A p2r of the first and last refinement steps are customized to connect the rotation and position feature spaces. The graph A r2p collects rotation features to the position feature space, while the graph A p2r converts position features back to the rotation feature space. To validate their functions, we discard position supervisions from the objective L ref ine during refinement. We refer to <ref type="table" target="#tab_3">0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   (a)   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   (b)   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23</ref>  this strategy as the position-implicit refinement strategy since the position feature space is built in an implicit manner. The only difference between the direct and position-implicit refinement strategies is that, in the latter one, there are two mapping operations performed before and after the refinement. We report the results of the approaches using direct, position-implicit, position-aided strategies in <ref type="table" target="#tab_12">Table 9</ref> for comparison. It can be seen that the position-implicit strategy achieves inferior results than the position-aided strategy but better results than the direct strategy, which means that the implicit position space still works better than the rotation space for feature refinement. Example results of the approach using the direct or position-aided refinement strategy are also depicted in <ref type="figure" target="#fig_0">Fig. 16</ref> for comparison. We can see that the position-aided refinement helps to handle challenging cases and produce more realistic and wellaligned results, while the direct refinement brings marginal to no improvement. The reason behind the inferior performance of the direct refinement is that the correlation between rotation features is weak, and the messages of neighboring rotation features are generally irrelevant to refine the target rotation feature. Our refinement module builds an auxiliary position feature space for feature refinement, making it much more efficient than that in the original rotation feature space. To verify this, we extract the features before refinement from the rotation, implicit position, and position spaces of the three strategies mentioned above, and compute the correlations between features of different body joints. <ref type="figure" target="#fig_0">Fig. 17</ref> shows the comparison of correlation matrices of these three types of features. As observed from <ref type="figure" target="#fig_0">Fig. 17(a)</ref>, the correlation matrix of rotation features approximates to an identity matrix, meaning that the correlations between the rotation features of different joints are rather weak even for two adjacent joints. By contrast, for implicit position features in <ref type="figure" target="#fig_0">Fig. 17(b)</ref> and position features in <ref type="figure" target="#fig_0">Fig. 17(c)</ref>, the correlations between features of adjacent joints are much higher, making it more feasible to refine features with the messages from neighboring joints.</p><p>Benefit from Learnable Graph Edge and PartDrop. The learnable edge weighting matrix M of the refinement graph contributes to better balancing the importance of neighboring messages, while the PartDrop strategy helps to encourage the network to leverage more information from neighboring joints. To verify their effectiveness during feature refinement, <ref type="table" target="#tab_3">Table 10</ref> reports the results of the ablation approaches incrementally adopting the learnable edge in the refinement graph and the PartDrop strategy,  where D-Net+Direct and D-Net+Pos.-aided adopt the refinement module with the direct and position-aided strategy, respectively. It can be seen that, for the direct refinement, the performance gains mainly come from the PartDrop strategy. In contrast, for the position-aided refinement, the performance gains are attributed to both the learnable edge and the PartDrop strategy. <ref type="figure" target="#fig_0">Fig. 18</ref> depicts the learned edge weighting matrices of different ablation approaches. As observed, the learned edge weighting matrices of the direct refinement are relatively flat with lower values. When using the PartDrop strategy, the learnable values of most edges in the refinement graph rise for the position-aided refinement, while such a phenomenon is not observed for the direct refinement. We conjecture that the PartDrop strategy brings gains from two perspectives. First, PartDrop regularizes the backbone feature extractor to focus on more complementary regions in intermediate representations for better feature exploitation. Second, PartDrop encourages the refinement module to borrow more information from neighbors in the position feature space for better feature refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, a Decompose-and-aggregate Network is proposed to learn 3D human shape and pose from dense correspondences of body parts with the decomposed perception, aggregated refinement, and part-based dropout strategies. All these new designs contribute to better part-based learning and effectively improve the reconstruction performance by providing well-suited part perception, leveraging spatial relationships for part pose refinement, and encouraging the exploitation of complementary body parts. Extensive experiments have been conducted to validate the efficacy of key components in our method. In comparison with previous ones, our network can produce more accurate results, while being robust to extreme poses, heavy occlusions, and incomplete human bodies, etc. In future work, we may explore integrating dense refinement <ref type="bibr" target="#b13">[14]</ref> to further improve the shape and pose recovery results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of our main ideas. (a) A human image with a parametric body model. (b) Comparison of the raw RGB image, silhouette, segmentation, and IUV map. (c) Local visual cues are crucial for the perception of joint rotations. (d) Our DaNet learns 3D human shape and pose from IUV maps with decomposed perception, aggregated refinement, and part-based dropout strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of the proposed Decompose-and-aggregate Network (DaNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Visualization of (a) global, (b) partial, and (c) simplified partial IUV maps. Joint-centric RoI pooling. (a) The RoI pooling is implemented as an STN. (b) The evolution of ? k s of different body joints over learning iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Illustration of the aggregated refinement module. (a) Three steps of the proposed refinement strategy. (b) The kinematic tree with K = 24 joints in the SMPL model. The pelvis joint with 0 index is the root node of the tree. Joints belonging to the same kinematic chain are linked by the line with the same color. (c)(d)(e) Adjacency matrices of the graphs used in three steps for the feature collection, refinement, and conversion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 .</head><label>14</label><figDesc>Comparison of the average per-vertex error upon the model surface for ablation approaches on the Human3.6M dataset. (a) The baseline approach using one stream only. (b) The approach using multiple streams for decomposed perception. (c) The approach using decomposed perception and PartDrop strategies. (d) Our final approach with the aggregated refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 15 .</head><label>15</label><figDesc>Comparison of reconstruction performance for approaches using different dropping out strategies on the Human3.6M dataset. (a)(b)(c) report results with metrics of PVE, PVE-S, and PVE-P to reveal the quality of the full model recovery, shape recovery, and pose recovery across different dropping rates, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 17 .</head><label>17</label><figDesc>Correlation matrices of the features extracted from (a) rotation, (b) implicit position, and (c) position feature spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 18 .</head><label>18</label><figDesc>Visualization of learned edge weighting matrices under different training settings. (a)(b) Direct refinement without and with PartDrop. (c)(d) Position-aided refinement without and with PartDrop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b59">60]</ref>, we introduce a Part-based Dropout (PartDrop) strategy to drop out semantic information from intermediate representations during training. PartDrop has a dropping rate ? as the probability of dropping values in the estimated IUV maps. In contrast to other dropping out strategies such as Dropout<ref type="bibr" target="#b57">[58]</ref> and DropBlock<ref type="bibr" target="#b59">[60]</ref>, the proposed PartDrop strategy drops features in contiguous regions at the granularity level of body parts. Specifically, for each training sample, the index subset I drop of the body DropBlock<ref type="bibr" target="#b59">[60]</ref> and Dropout<ref type="bibr" target="#b57">[58]</ref> drop IUV values in part-wise, block-wise, and unit-wise manners, respectively. The corresponding binary masks are shown on the top row.parts to be dropped is randomly selected from {1, 2, . . . , P } with the probability of ?. Then, for both global and partial IUV maps, the estimated IUV values of selected body parts are dropped out by setting corresponding body parts as zeros:</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row><row><cell cols="4">Fig. 6. Comparison of different dropping out strategy. (a) Original IUV</cell></row><row><cell cols="2">map. (b)(c)(d) PartDrop (ours),</cell><cell></cell><cell></cell></row></table><note>H[p, :, :, :] = 0, for p ? I drop ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art methods on the Human3.6M dataset.</figDesc><table><row><cell>Method</cell><cell>PVE</cell><cell cols="2">MPJPE MPJPE-PA</cell></row><row><cell>Zhou et al. [54]</cell><cell>-</cell><cell>107.3</cell><cell>-</cell></row><row><cell>Tung et al. [4]</cell><cell>-</cell><cell>-</cell><cell>98.4</cell></row><row><cell>SMPLify [2]</cell><cell>202.0</cell><cell>-</cell><cell>82.3</cell></row><row><cell>SMPLify++ [3]</cell><cell>-</cell><cell>-</cell><cell>80.7</cell></row><row><cell>Pavlakos et al. [6]</cell><cell>155.5</cell><cell>-</cell><cell>75.9</cell></row><row><cell>HMR [5]</cell><cell>-</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell>NBF [7]</cell><cell>-</cell><cell>-</cell><cell>59.9</cell></row><row><cell>Xiang et al. [38]</cell><cell>-</cell><cell>65.6</cell><cell>-</cell></row><row><cell>Arnab et al. [17]</cell><cell>-</cell><cell>77.8</cell><cell>54.3</cell></row><row><cell>CMR [40]</cell><cell>-</cell><cell>-</cell><cell>50.1</cell></row><row><cell>HoloPose [14]</cell><cell>-</cell><cell>60.3</cell><cell>46.5</cell></row><row><cell>TexturePose [19]</cell><cell>-</cell><cell>-</cell><cell>49.7</cell></row><row><cell>DenseRaC [41]</cell><cell>-</cell><cell>76.8</cell><cell>48.0</cell></row><row><cell>SPIN [43]</cell><cell>-</cell><cell>-</cell><cell>41.1</cell></row><row><cell>DaNet-LSTM [11]</cell><cell>75.1</cell><cell>61.5</cell><cell>48.6</cell></row><row><cell>Ours</cell><cell>66.5</cell><cell>54.6</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Quantitative comparison of MPJPE-PA across different actions on the Human3.6M dataset. Method Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell cols="2">Pavlakos et al. [33] 47.5</cell><cell>50.5</cell><cell>48.3</cell><cell>49.3 50.7</cell><cell>55.2 46.1 48.0</cell><cell>61.1</cell><cell>78.1</cell><cell>51.1 48.3</cell><cell>52.9</cell><cell>41.5</cell><cell>46.4 51.9</cell></row><row><cell cols="2">Martinez et al. [24] 39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0 51.0</cell><cell>56.0 41.4 40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2 45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1 47.7</cell></row><row><cell>SMPLify [2]</cell><cell>62.0</cell><cell>60.2</cell><cell>67.8</cell><cell>76.5 92.1</cell><cell>77.0 73.0 75.3</cell><cell>100.3</cell><cell>137.3</cell><cell>83.4 77.3</cell><cell>79.7</cell><cell>86.8</cell><cell>81.7 82.3</cell></row><row><cell>HMR [5]</cell><cell>53.2</cell><cell>56.8</cell><cell>50.4</cell><cell>62.4 54.0</cell><cell>72.9 49.4 51.4</cell><cell>57.8</cell><cell>73.7</cell><cell>54.4 50.0</cell><cell>62.6</cell><cell>47.1</cell><cell>55.0 57.2</cell></row><row><cell>CMR [40]</cell><cell>41.8</cell><cell>44.8</cell><cell>42.6</cell><cell>46.6 45.9</cell><cell>57.2 40.8 40.6</cell><cell>52.2</cell><cell>66.0</cell><cell>46.6 42.8</cell><cell>51.7</cell><cell>36.9</cell><cell>44.6 48.2</cell></row><row><cell>SPIN [43]</cell><cell>37.6</cell><cell>42.4</cell><cell>38.8</cell><cell>42.6 40.4</cell><cell>45.9 36.1 36.7</cell><cell>48.7</cell><cell>58.6</cell><cell>41.2 37.9</cell><cell>46.6</cell><cell>33.8</cell><cell>38.4 41.1</cell></row><row><cell>DaNet-LSTM [11]</cell><cell>43.3</cell><cell>48.8</cell><cell>50.6</cell><cell>48.3 47.3</cell><cell>55.5 41.6 42.7</cell><cell>53.8</cell><cell>61.5</cell><cell>47.4 43.2</cell><cell>53.3</cell><cell>40.8</cell><cell>47.9 48.6</cell></row><row><cell>Ours</cell><cell>37.9</cell><cell>44.3</cell><cell>41.2</cell><cell>43.3 42.1</cell><cell>48.7 36.2 38.9</cell><cell>47.4</cell><cell>53.7</cell><cell>41.1 39.9</cell><cell>46.0</cell><cell>34.6</cell><cell>41.3 42.9</cell></row><row><cell>Ours-6D</cell><cell>35.7</cell><cell>40.4</cell><cell>39.0</cell><cell>40.3 40.5</cell><cell>47.4 35.1 34.9</cell><cell>45.2</cell><cell>51.7</cell><cell>39.6 37.8</cell><cell>43.4</cell><cell>34.4</cell><cell>39.8 40.5</cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HMR [5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NBF [7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMR [40]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Ours Fig. 8. Qualitative comparison of reconstruction results on the UP-3D dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Quantitative comparison of PVE with state-of-the-art methods on the UP-3D dataset.</figDesc><table><row><cell>Method</cell><cell>LSP</cell><cell>MPII</cell><cell>FashionPose</cell><cell>Full</cell></row><row><cell>SMPLify++ [3]</cell><cell cols="2">174.4 184.3</cell><cell>108.0</cell><cell>169.8</cell></row><row><cell>HMR [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>149.2</cell></row><row><cell>NBF [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>134.6</cell></row><row><cell>Pavlakos et al. [6]</cell><cell cols="2">127.8 110.0</cell><cell>106.5</cell><cell>117.7</cell></row><row><cell>BodyNet [34]</cell><cell>102.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Rong et al. [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>122.2</cell></row><row><cell>DaNet-LSTM [11]</cell><cell>90.4</cell><cell>83.0</cell><cell>61.8</cell><cell>83.7</cell></row><row><cell>Ours</cell><cell>88.5</cell><cell>82.1</cell><cell>60.8</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Quantitative comparison of keypoint localization AP with state-of-the-art methods on the COCO validation set. Results of HMR, CMR, and SPIN are obtained based on their publicly released code and models.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>OpenPose [82]</cell><cell>65.3</cell><cell>85.2</cell><cell>71.3</cell><cell>62.2</cell><cell>70.7</cell></row><row><cell>SimpleBaseline [83]</cell><cell>74.3</cell><cell>89.6</cell><cell>81.1</cell><cell>70.5</cell><cell>79.7</cell></row><row><cell>HRNet [84]</cell><cell>76.3</cell><cell>90.8</cell><cell>82.9</cell><cell>72.3</cell><cell>83.4</cell></row><row><cell>HMR [5]</cell><cell>18.9</cell><cell>47.5</cell><cell>11.7</cell><cell>21.5</cell><cell>17.0</cell></row><row><cell>CMR [40]</cell><cell>9.3</cell><cell>26.9</cell><cell>4.2</cell><cell>11.3</cell><cell>8.1</cell></row><row><cell>SPIN [43]</cell><cell>17.3</cell><cell>39.1</cell><cell>13.5</cell><cell>19.0</cell><cell>16.6</cell></row><row><cell>SPIN-HRNet [43]</cell><cell>21.2</cell><cell>45.3</cell><cell>18.0</cell><cell>22.5</cell><cell>20.9</cell></row><row><cell>DaNet-LSTM [11]</cell><cell>28.5</cell><cell>58.7</cell><cell>24.6</cell><cell>30.8</cell><cell>27.1</cell></row><row><cell>DaNet-GCN</cell><cell>31.9</cell><cell>65.5</cell><cell>27.5</cell><cell>33.2</cell><cell>31.2</cell></row><row><cell>+ Dropout</cell><cell>30.6</cell><cell>64.6</cell><cell>25.7</cell><cell>32.0</cell><cell>30.0</cell></row><row><cell>+ DropBlock</cell><cell>32.0</cell><cell>66.9</cell><cell>27.4</cell><cell>33.8</cell><cell>30.9</cell></row><row><cell cols="2">+ PartDrop (Ours) 33.8</cell><cell>68.6</cell><cell>29.9</cell><cell>36.0</cell><cell>32.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Compared with other methods, our DaNet could produce more satisfactory results under challenging scenarios.</figDesc><table><row><cell>Image</cell><cell>Est. IUV w./o. drop Dropout DropBlock PartDrop</cell></row><row><cell cols="2">Fig. 10. Comparison of different dropping out strategies in challenging</cell></row><row><cell cols="2">cases. From left to right: input images, estimated IUV maps, results of</cell></row><row><cell cols="2">models trained without dropping, with Dropout, DropBlock, and PartDrop</cell></row><row><cell>strategies.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Quantitative comparison with state-of-the-art methods on the 3DPW dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell>PVE</cell><cell cols="2">MPJPE MPJPE-PA</cell></row><row><cell>Temporal</cell><cell cols="2">Kanazawa et al. [16] 139.3 Doersch et al. [85] -Arnab et al. [17] -Sun et al. [44] -VIBE [20] 113.4</cell><cell>116.5 ---93.5</cell><cell>72.6 74.7 72.2 69.5 56.5</cell></row><row><cell></cell><cell>HMR [5]</cell><cell>-</cell><cell>130.0</cell><cell>76.7</cell></row><row><cell>Frame-based</cell><cell>CMR [40] Rong et al. [15] SPIN [43] SPIN-HRNet [43] DaNet-LSTM [11]</cell><cell>-152.9 114.8 112.4 114.6</cell><cell>--96.9 95.4 92.2</cell><cell>70.2 -59.2 58.5 56.9</cell></row><row><cell></cell><cell>Ours</cell><cell>110.8</cell><cell>85.5</cell><cell>54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Performance of approaches adopting different intermediate representations on the Human3.6M dataset. The IUV estimators trained on Human3.6M with dense supervisions have higher IUV estimation qualities. IUV estimators are all pretrained on COCO and then trained on different datasets. Different IUV estimators are denoted as ?( ) or ?[ * ], where ? is the architecture, and * denote the pretrained and training datasets. IU V GT stands for taking ground-truth IUV as input. ImgN et, DP , and H36M abbreviate ImageNet, DensePose-COCO, and Human3.6M, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="5">PVE MPJPE MPJPE-PA</cell></row><row><cell></cell><cell></cell><cell cols="2">ConvFeat</cell><cell>98.9</cell><cell></cell><cell cols="2">82.5</cell><cell>60.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Segmentation</cell><cell>90.4</cell><cell></cell><cell cols="2">74.6</cell><cell>57.1</cell></row><row><cell></cell><cell></cell><cell>IUV</cell><cell></cell><cell>87.8</cell><cell></cell><cell cols="2">71.6</cell><cell>55.4</cell></row><row><cell cols="2">100</cell><cell>Res50(ImgNet) Res50(COCO) Res101(ImgNet) Res101(COCO)</cell><cell>HR(ImgNet) HR(COCO) IUV_GT</cell><cell>PVE MPJPE MPJPE-PA</cell><cell cols="2">100</cell><cell cols="2">Res50[DP] Res101[DP] HR[DP]</cell><cell>Res50[H36M] Res101[H36M] HR[H36M]</cell><cell>PVE MPJPE MPJPE-PA</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell></row><row><cell>Error (mm)</cell><cell>60 70 80</cell><cell></cell><cell></cell><cell></cell><cell>Error (mm)</cell><cell>70 80</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">46 47 48 49 50 51 52</cell><cell>100</cell><cell></cell><cell>33</cell><cell>34</cell><cell>35</cell><cell>50</cell><cell>51</cell><cell>52</cell></row><row><cell></cell><cell></cell><cell cols="2">AUC10 (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC10 (%)</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="9">Fig. 12. Reconstruction performance on Human3.6M versus the IUV</cell></row><row><cell cols="9">estimation quality for approaches adopting IUV estimators with dif-</cell></row><row><cell cols="9">ferent architectures and training strategies. (a) Higher IUV estimation</cell></row><row><cell cols="9">qualities generally contribute to better reconstruction performance. IUV</cell></row><row><cell cols="9">estimators are all trained on Human3.6M but initialized with different</cell></row><row><cell cols="3">models. (b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>Performance of approaches using different perception strategies on the Human3.6M dataset.</figDesc><table><row><cell>Method</cell><cell cols="5">PVE PVE-S PVE-P MPJPE MPJPE-PA</cell></row><row><cell>Baseline</cell><cell>87.8</cell><cell>38.0</cell><cell>76.3</cell><cell>71.6</cell><cell>55.4</cell></row><row><cell>D-Net</cell><cell>74.3</cell><cell>36.3</cell><cell>64.0</cell><cell>61.8</cell><cell>48.5</cell></row><row><cell>D-Net-ES</cell><cell>76.1</cell><cell>36.6</cell><cell>65.5</cell><cell>63.1</cell><cell>49.8</cell></row><row><cell>D-Net-AP</cell><cell>76.8</cell><cell>36.8</cell><cell>65.8</cell><cell>63.4</cell><cell>49.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc>Performance of approaches using different feature refinement strategies on the Human3.6M dataset.</figDesc><table><row><cell cols="4">Refinement Strategy PVE MPJPE MPJPE-PA</cell></row><row><cell>w/o Ref.</cell><cell>71.7</cell><cell>59.1</cell><cell>46.1</cell></row><row><cell>Direct Ref.</cell><cell>70.3</cell><cell>58.1</cell><cell>45.5</cell></row><row><cell>Pos.-implicit Ref.</cell><cell>69.2</cell><cell>56.5</cell><cell>44.7</cell></row><row><cell>Pos.-aided Ref.</cell><cell>66.5</cell><cell>54.6</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc>Ablation study of using learnable graph edge and PartDrop strategies on the Human3.6M dataset.</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell cols="6">PVE MPJPE MPJPE-PA</cell></row><row><cell></cell><cell>D-Net</cell><cell></cell><cell></cell><cell cols="3">74.3</cell><cell>61.8</cell><cell></cell><cell>48.5</cell></row><row><cell></cell><cell cols="3">+ PartDrop</cell><cell cols="3">71.7</cell><cell>59.1</cell><cell></cell><cell>46.1</cell></row><row><cell></cell><cell cols="3">D-Net+Direct</cell><cell cols="3">72.1</cell><cell>59.4</cell><cell></cell><cell>46.9</cell></row><row><cell></cell><cell cols="3">+ LearntEdge</cell><cell cols="3">72.7</cell><cell>59.6</cell><cell></cell><cell>47.0</cell></row><row><cell></cell><cell cols="3">+ PartDrop</cell><cell cols="3">70.3</cell><cell>58.1</cell><cell></cell><cell>45.5</cell></row><row><cell></cell><cell cols="3">D-Net+Pos.-aided</cell><cell cols="3">70.8</cell><cell>57.1</cell><cell></cell><cell>45.9</cell></row><row><cell></cell><cell cols="3">+ LearntEdge</cell><cell cols="3">68.9</cell><cell>55.8</cell><cell></cell><cell>44.9</cell></row><row><cell></cell><cell cols="3">+ PartDrop</cell><cell cols="3">66.5</cell><cell>54.6</cell><cell></cell><cell>42.9</cell></row><row><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell cols="2">0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell cols="2">0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell>0 4 5 9 14 18 19 23 22 21 20 17 16 15 13 12 11 10 8 7 6 3 2 1</cell><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</cell><cell>0.0 0.1 0.2 0.3 0.4 0.5</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell>(d)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the associate editor and reviewers for their helpful comments to improve this manuscript. This work was supported in part by the National Natural Science Foundation of China <ref type="figure">(</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Danet: Decompose-and-aggregate network for 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1337" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5340" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shape-aware human pose and shape reconstruction using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4352" to="4362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="975" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hs-nets: Estimating human body shape from silhouettes with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>?ztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards accurate 3d human body reconstruction from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2821" to="2840" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Moulding humans: Non-parametric 3d human shape estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarseto-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manafas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7739" to="7749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Densebody: Directly regressing dense 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10153</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The stitched puppet: A graphical model of 3d human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3537" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6821" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6799" to="6808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">220</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2131" to="2143" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
