<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSAST: Self-Supervised Audio Spectrogram Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
							<email>yuangong@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Cheng-I</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SSAST: Self-Supervised Audio Spectrogram Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study (Gong, Chung, and Glass 2021)   showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based selfsupervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST. Code at https://github.com/YuanGongND/ssast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>inductive bias such as spatial locality or translation equivariance, and are more data-driven. In the audio and speech domain, the recently proposed Audio Spectrogram Transformer (AST) (Gong, <ref type="bibr" target="#b10">Chung, and Glass 2021)</ref> and the Keyword Transformer <ref type="bibr" target="#b3">(Berg, O'Connor, and Cruz 2021</ref>) also achieve new state-of-the-art performance on audio scene classification and keyword spotting. Despite the strong performance, a critical issue of such pure self-attention based models is they tend to require more training data than CNNs <ref type="bibr" target="#b9">(Dosovitskiy et al. 2021)</ref>. For example, the ViT outperforms CNNs only when the training data volume is larger than about 100 million samples. AST also does not perform well when it is trained from scratch, and the success of AST strongly relies on supervised pretraining. Since labeled speech and audio data is limited, AST uses cross-modal pretraining with ImageNet data <ref type="bibr" target="#b8">(Deng et al. 2009</ref>). However, in practice, supervised pretraining on ImageNet data is complex <ref type="bibr" target="#b11">(He et al. 2019)</ref> and expensive, and also constrains the vision and audio models to have a similar architecture and use the same patch size and shape. Further, the validity and transferability of such cross-modal pretraining for a specific audio or speech task are unclear.</p><p>While annotating audio and speech data is expensive, we can easily get web-scale unlabeled audio and speech data from radio or YouTube. This motivates us to explore Self-Supervised AST (SSAST) that leverages unlabeled data to alleviate the data requirement problem. In this paper, we present a novel joint discriminative and generative Masked Spectrogram Patch Modeling (MSPM) based self-supervised learning (SSL) framework that can significantly improve AST performance with limited labeled data. Previous self-supervised learning methods such as wav2vec <ref type="bibr" target="#b26">(Schneider et al. 2019)</ref> or autoregressive predictive coding (APC) <ref type="bibr" target="#b7">(Chung et al. 2019</ref>) use an objective that predicts future or masked temporal spectrogram frames, thus potentially learning only the temporal structure of the spectrogram. In contrast, the objective of MSPM is to predict a specific frequency band in a specific time range (i.e., a "spectrogram patch") given the neighboring band and time information, which allows the model to learn both the temporal and frequency structure. The spectrogram patch can be an arbitrary shape and size, e.g., it can be a conventional time frame or a square patch.</p><p>In addition, most previous SSL research considers either only speech or only audio events, but in this work, we show that the SSL model can be generalized to both speech and audio tasks. Specifically, we pretrain our model using both Librispeech and AudioSet, and evaluate the model on a variety of speech and audio tasks including audio event classification, keyword spotting, speaker identification, and speech emotion recognition. Our experiments demonstrate the effectiveness of the proposed MSPM framework: a model pretrained with MSPM can significantly outperform fromscratch models for all 6 benchmarks we evaluated with an average improvement of 60.9%, and the performance can even match or outperform supervised pretrained models. The contributions of this work are two-fold: 1. We propose MSPM, a novel patch-based joint discriminative and generative self-supervised learning framework. With MSPM pretraining, our SSAST model matches or outperforms previous supervised pretrained AST. To the best of our knowledge, MSPM is the first patch-based self-supervised learning framework in the audio and speech domains, and SSAST is the first selfsupervised pure self-attention based audio classification model. Further, we conduct extensive experiments to thoroughly investigate the design choices and quantify the performance impact of each factor. 2. We show that pretraining with both speech and audio datasets noticeably improves the models' generalization ability, and leads to better performance than pretraining with dataset from a single domain. As a consequence, our SSAST model performs well on both speech and audio downstream tasks. Previous work typically only uses datasets in a single domain for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Self-Supervised Audio Spectrogram Transformer</head><p>In this section, we first review the AST architecture and then discuss the proposed joint discriminative and generative masked spectrogram patch prediction (MSPM) selfsupervised learning framework, and the design details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AST Model Architecture</head><p>As shown in <ref type="figure">Figure 1</ref>, we intentionally follow as close as possible to the original AST architecture to make a fair performance comparison. First, the input audio waveform of t seconds is converted into a sequence of 128-dimensional log Mel filterbank (fbank) features computed with a 25ms Hanning window every 10ms. This results in a 128 ? 100t spectrogram as input to the AST. We then split the spectrogram into a sequence of 16 ? 16 patches. We flatten each 16 ? 16 patch to a 1D 768-dimensional patch embedding with a linear projection layer. We refer to this linear projection layer as the patch embedding layer and the output as patch embedding E. Since the Transformer architecture does not capture the input order information and the patch sequence is also not in temporal order, we add a trainable positional embedding (also of size 768) P to each patch embedding to allow the model to capture the spatial structure of the 2D audio spectrogram. The resulting sequence is then input to the Transformer. A Transformer consists of several encoder and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head><p>Linear Projection  <ref type="figure">Figure 1</ref>: The proposed self-supervised AST. The 2D audio spectrogram is split into a sequence of 16?16 patches without overlap, and then linearly projected to a sequence of 1-D patch embeddings E. Each patch embedding is added with a learnable positional embedding P and then input to the Transformer encoder. The output of the Transformer O is used as the spectrogram patch representation. During selfsupervised pretraining, we randomly mask a portion of spectrogram patches and ask the model to 1) find the correct patch at each masked position from all masked patches; and 2) reconstruct the masked patch. The two pretext tasks aim to force the AST model to learn both the temporal and frequency structure of the audio data. During fine-tuning, we apply a mean pooling over all patch representation {O} and use a linear head for classification. decoder layers. Since the AST is designed for classification tasks, we only use the encoder of the Transformer that has an embedding dimension of 768, 12 layers, and 12 heads, which are the same as those in original AST (Gong, <ref type="bibr" target="#b10">Chung, and Glass 2021)</ref>. We refer to the output of the Transformer encoder as patch representation O. During fine-tuning and inference, we apply a mean pooling over the sequence of patch representation {O} to get the audio clip level representation, and then use a linear head for classification.</p><formula xml:id="formula_0">E [1] E [2] E [3] E [4] E [5] E [6] E [7] E [8] P [1] P [2] P [3] P [</formula><p>While we aim to follow the architecture of the original AST, we made two modifications for self-supervised learning. First, in the original AST, a [CLS] token is appended to the beginning of the input sequence of the Transformer encoder, and the output representation of the [CLS] token is used as the audio clip level representation. In this work, we apply mean pooling over all patch representation {O} as the audio clip level representation. This is because the original Figure 2: Illustration of the proposed patch-level masking with different cluster factor C but same total masking area. The model is forced to learn more global spectrogram structure with a larger C, and more local structure with a smaller C. To make the model learn both local and global structure, we use random C during pretraining. Compared with framelevel masking SSL methods that potentially only learn temporal frame structure, patch-based masking allows the model to learn both temporal and frequency spectrogram structure.</p><p>AST uses supervised pretraining and the supervision is applied on the [CLS] token, thus the output representation of the [CLS] learns to summarize the entire sequence during pretraining and can be used as audio clip level representation. In contrast, for our self-supervised pretraining framework, supervision is applied to each individual patch representation, and the mean of all patch representations is a better summary of the audio clip. Second, in the original AST, spectrogram patches are split with overlap, and the overlap was shown to improve model performance. In this work, we split the patch without overlap during pretraining to not allow the model to use overlapped edges as a shortcut for the task prediction instead of learning a meaningful representation. In the fine-tuning and inference steps, we split the patch with an overlap of 6 in the same fashion as the original AST.</p><p>While we pretrain the model using fixed-length audio data (10 seconds), AST supports variable length input by simply interpolating or truncating the positional embedding to the downstream task audio length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Discriminative and Generative Masked Spectrogram Patch Modeling</head><p>In this section, we introduce the proposed self-supervised pretraining framework. We first show our masking strategy and then discuss the pretext task (i.e., the self-supervised learning task in the pretraining stage) in detail.</p><p>Masked Patch Sampling As mentioned above, during pretraining, we use a fixed-length audio of 10s and convert it to spectrogram of size 1024 ? 128. AST splits the spectrogram into 512 16?16 patches (8 in the frequency dimension and 64 in the time dimension). Thanks to this special design of AST, we are able to mask spectrogram patches rather than the entire time frames during pretraining, which allows the  </p><formula xml:id="formula_1">draw index i ? unif {1,</formula><formula xml:id="formula_2">for X ? D do 8: split X into 512 patches x = {x 1 , x 2 , ..., x 512 } 9: E = M patchembedding (x) 10: draw C ? unif {3, 6} 11: I = SampleMaskIndex(C, N ) 12: E I = E mask</formula><p>Mask the Patch Embeddings 13:</p><formula xml:id="formula_3">O = M transf ormer (E + P ) 14: L d = 0, L g = 0 15: for i ? I do 16: r i = M reconstruction head (O i ) 17: c i = M classif ication head (O i ) 18: L += L d (x i , c i , x I ) + ?L g (x i , r i ) 19: L = L / N 20:</formula><p>update M to minimize L return M model to learn both the temporal and frequency structure of the data. In addition, as shown in <ref type="figure">Figure 2</ref>, we use a cluster factor C to control how masked patches cluster. Specifically, we first randomly select a patch, and then mask the square centered at the patch with a side length of C, e.g., if C = 3, we mask a cluster of 9 patches that has a total size of 48?48. The model is forced to learn more global spectrogram structure with a larger C, and more local structure with a smaller C. To make the model learn both local and global structure, we use random C ? [3, 5] during pretraining. We show the details in Algorithm 1 line 1-5. Note that while we mainly focus on using 16?16 patches in this paper, MSPM actually supports patches of arbitrary size and shape.</p><p>Joint Discriminative and Generative Masked Spectrogram Patch Modeling As opposed to prior work that either used discriminative (e.g., wav2vec) or generative training objectives (e.g., APC), in this work, we propose to use a joint discriminative and generative objective for pretraining.</p><p>As shown in Algorithm 1, each input spectrogram X is split into 512 patches x converted to corresponding patch embeddings E (line 8-9). We then randomly generate a set I of N masked patch position indexes as previously described (line 10-11). For each patch that needs to be masked, we replace its patch embedding with a learnable mask embedding E mask (line 12). We add positional embeddings to the patch embeddings and input them to the Transformer encoder (line 13). For each masked patch x i , we get the corresponding Transformer encoder output O i . We then input O i to a classification head and a reconstruction head and get output c i and r i , respectively (line 16-17). Both the classification and reconstruction heads are two-layer MLPs that map O i (768) to the same dimension as x i (256). We expect r i to be close to x i , and the model can match correct (x i , c i ) pairs. Therefore, we use the InfoNCE loss <ref type="bibr" target="#b19">(Oord, Li, and Vinyals 2018)</ref> L d for the discriminative objective and mean square error (MSE) loss L g for the generative objective:</p><formula xml:id="formula_4">L d = ? 1 N N i=1 log( exp(c T i x i ) N j=1 exp(c T i x j ) ) (1) L g = 1 N N i=1 (r i ? x i ) 2 (2)</formula><p>Where N is the number of masked patches. We then sum up L d and L g with a weight ?. In this work, we set ? = 10.</p><formula xml:id="formula_5">L = L d + ?L g<label>(3)</label></formula><p>Finally, we update the weights of the AST model M to minimize L with the optimizer (line 19-20). Note that for the discriminative task, the negative samples are sampled from the same spectrogram, i.e., the model aims to pick the correct patch for each masked position from all patches being masked. On one hand, this increases the difficulty of the pretext task to avoid the model learning trivial things such as recording environment for prediction; on the other hand, this also avoids building a memory bank of patches from different spectrograms and makes the algorithm less computationally intensive and less affected by the mini-batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretraining Datasets</head><p>In contrast to previous efforts that only use either speech dataset (e.g., in APC, wav2vec) or audio event dataset <ref type="bibr" target="#b25">(Saeed, Grangier, and Zeghidour 2021;</ref><ref type="bibr" target="#b17">Niizumi et al. 2021)</ref>, in this work, we propose to use both speech and audio event datasets for pretraining to explore if the pretrained model can generalized to both speech and audio classification tasks. For both datasets, we only use the audio data and abandon the labels for self-supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AudioSet-2M</head><p>We use the AudioSet full training set (AudioSet-2M) <ref type="bibr" target="#b10">(Gemmeke et al. 2017</ref>) as our audio pretraining dataset. AudioSet is a multi-label audio event classification dataset that contains 2 million 10-second audio clips excised from YouTube videos with 527 sound classes including human sounds, animal sounds, sounds of things, music, natural sounds, environment sounds etc. It is worth mentioning that while about half of AudioSet-2M audio clips contain speech, speech might only appear in a small part of each clip as most AudioSet clips contain more than one We pretrain three AST models with a fixed number of 100, 250, and 400 masked patches, respectively, and evaluate their classification and reconstruction performance with various masked patch numbers from 50 to 500 on the validation set. While the AST model is pretrained with a fixed number of masked patches, we find it can perform well with a different number of masked patches in inference. As expected, the performance of the model drops with the increase of the number of masked patches, e.g., the AST models achieve over 80% accuracy when the evaluation masked patch number is 50, but only around 35% when the evaluation masked patch number is 400. This indicates the pretext tasks are neither trivial nor impossible.</p><p>sound. Therefore, AudioSet potentially does not have good coverage of speech and might not be sufficient to pretrain a good model for downstream speech tasks.</p><p>Librispeech In order to improve the coverage of speech data, we further use the Librispeech <ref type="bibr" target="#b20">(Panayotov et al. 2015)</ref> 960-hour training set as our speech pretraining dataset. Librispeech contains public domain audio books data in English, read by over 1,000 speakers, and is commonly used to train and evaluate speech recognition systems.</p><p>For both AudioSet and Librispeech data, we cut or pad each waveform to 10sec. We use 1,953k AudioSet samples and 281k Librispeech samples, and a total of 2,234k samples. We mix and shuffle the two datasets during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance of Pretext Tasks</head><p>For pretraining the AST, we use a batch size of 24, an initial learning rate of 1e-4, and cut the learning rate into half if the pretext task performance on the validation set stops improving for 8k iterations. We optimize the network using the Adam optimizer <ref type="bibr" target="#b13">(Kingma and Ba 2015)</ref>. We train the model for up to 800k iterations (?8.5 epochs). We tested different numbers of masked patches of 100, 250, and 400. We pretrain SSAST on 4? NVIDIA GTX Titan X or GTX Titan X Pascal GPUs, the pretraining takes about 10 days.</p><p>We show the masked spectrogram patch modeling performance in <ref type="figure" target="#fig_2">Figure 3</ref>. While the AST model is pretrained with a fixed number of masked patches, we find it can perform well with a different number of masked patches during inference. As expected, the performance of the model drops with the increase of the number of masked patches, e.g., the AST models achieve over 80% accuracy when the evaluation masked patch number is 50, but only around 35% when the evaluation masked patch number is 400, indicating the pretext tasks are neither trivial nor impossible. In general, the model pretrained with more masked patches performs better on the pretext tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Downstream Tasks and Datasets</head><p>We evaluate the pretrained model on 6 commonly used audio and speech benchmarks. We use the same three benchmarks (AudioSet-20K, ESC-50, and Speech Commands V2) that the original AST has been tested on and use exactly the same setting intentionally to make a fair comparison. To further evaluate the model performance on downstream speech tasks and compare with previous self-supervised models that focus on speech, we test the pretrained AST on three additional benchmark Speech Commands V1, VoxCeleb 1, and IEMOCAP for keyword spotting, speaker identification, and emotion recognition, respectively. We report mean Average Precision (mAP) for the AudioSet-20K task and accuracy for all other tasks.</p><p>AudioSet-20K (AS) We use the AudioSet balanced training set and evaluation set for the multi-label audio event classification task. The AudioSet-20K training set is a classbalanced subset of AudioSet-2M that contains 20,785 audios. We test the model on the AudioSet evaluation set, which is disjoint with AudioSet-20K and AudioSet-2M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ESC-50 (ESC)</head><p>We use the ESC-50 dataset <ref type="bibr" target="#b23">(Piczak 2015)</ref> for the single-label audio event classification task. ESC-50 is an audio classification dataset consists of 2,000 5-second environmental audio recordings organized into 50 classes.</p><p>Speech Commands V2 (KS2) We use the Speech Commands V2 (Warden 2018) for the keyword spotting task. The Speech Command V2 dataset consists of 105,829 1-second recordings of 35 common speech commands.</p><p>Speech Commands V1 (KS1) We also use the Speech Commands V1 (Warden 2018) for the keyword spotting task, which is similar to Speech Commands V2, but only contains 10 classes of keywords, 1 class of silence, and an unknown class to include the false positive.</p><p>VoxCeleb 1 (SID) We use the VoxCeleb 1 dataset (Nagrani et al. 2020) that contains speech from 1,251 speakers for the speaker identification task. The task goal is to classify each utterance by its speaker identity where speakers are in the same predefined set for both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEMOCAP (ER)</head><p>We use the IEMOCAP dataset <ref type="bibr" target="#b4">(Busso et al. 2008</ref>) that contains about 12 hours of emotional speech for the speech based emotion recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Downstream Fine-tuning Details</head><p>To make a fair comparison with previous work, for the AudioSet-20K, ESC-50, and Speech Commands V2 experiments, we train and evaluate the model using the exact same training and evaluation settings with the original AST. Specifically, we use mixup training <ref type="bibr" target="#b27">(Tokozume, Ushiku, and Harada 2018)</ref>, SpecAugment <ref type="bibr" target="#b21">(Park et al. 2019)</ref>, an initial learning rate of 5e-5, 1e-4, and 2.5e-4 and train the model with 25, 50, and 30 epochs for AudioSet-20K, ESC-50, Speech Commands V2, respectively.</p><p>For the three benchmarks Speech Commands V1, Vox-Celeb1, and IEMOCAP that the original AST has not been tested on, we use the standard SUPERB <ref type="bibr" target="#b31">(Yang et al. 2021)</ref> training and testing framework. Specifically, we search the learning rate from 1e-5 to 1e-3 for out SSAST model and all baseline models and train the model for up to 20k and 40k iterations for Speech Commands V1 and VocCeleb1, respectively. We use a fixed learning rate of 1e-4 and max iteration of 10k for IEMOCAP. Please refer to the AST and SUPERB papers for more details. For all downstream experiments, we use the end-to-end fine-tuning setting, i.e., we do not freeze any layer of the pretrained AST.</p><p>For supervised pretrained models, we use the output of [CLS] token as the audio clip representation because supervision is given to the output of [CLS] in pretraining while we use mean pooling for self-supervised models as supervision is given to individual token in pretraining, keeping pretraining and fine-tuning consistent can slightly improve the performance and make the comparison fairer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Performance on Downstream Tasks</head><p>We compare the following models in our experiments:</p><p>1. AST-Scratch: AST model with appropriate initialization but without any pretraining. 2. AST-IM+KD: AST model with supervised ImageNet pretraining, proposed in (Gong, Chung, and Glass 2021). The model is pretrained with the ImageNet 2012 dataset in a supervised manner. In addition, during ImageNet pretraining, knowledge distillation from another convolution neural network is applied, which can noticeably improve the performance <ref type="bibr" target="#b28">(Touvron et al. 2020</ref>). This is a strong baseline that achieves state-of-the-art results on AudioSet-20K, ESC-50, and Speech Commands V2. 3. AST-AudioSet: AST model with supervised AudioSet-2M pretraining on the audio event classification task. 4. SSAST 250: The proposed self-supervised AST model pretrained with 250 masked patches. 5. SSAST 400: The proposed self-supervised AST model pretrained with 400 masked patches.</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, we evaluate the above-mentioned 7 models on 6 benchmarks. Key findings include: First, the proposed self-supervised training framework can significantly boost the performance of AST with an average improvement of 60.9%, e.g., SSAST achieves 0.310 mAP on the AudioSet-20K while AST-Scratch only achieves 0.148 mAP. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the proposed self-supervised framework helps AST train faster and better. Further, the  improvement is consistent over all audio and speech benchmarks, demonstrating the proposed self-supervised training framework is effective and generalizable. Second, AudioSet-2M supervised pretraining is quite strong for audio event classification tasks (AS and ESC) that are in the same domain with AudioSet, but performs poorly on speech tasks, showing the limitation of supervised pretraining. Surprisingly, cross-domain supervised ImageNet pretraining with knowledge distillation performs quite well on all tasks, and still achieves the best performance on the AudioSet-20K task. Third, even when compared with strong supervised baselines, the proposed SSAST models still get the best results on all benchmarks except AS, showing the proposed self-supervised model potentially can be used as a powerful generic audio classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Performance Impact of Pretraining Settings</head><p>We set the AST pretrained with 400 masked patches, joint discriminative and generative objectives, on both AudioSet-2M and Librispeech as the base model. We then change one factor at a time to observe the performance impact. Impact of the Number of Masked Patches As shown in <ref type="table" target="#tab_3">Table 2</ref>, upper section, we find masking 100 patches is too simple a task, and leads to the worst performance for all downstream tasks. Masking 400 patches leads to better performance on audio event classification tasks, while masking 250 patches leads to better performance on speech tasks, but the overall performance is similar. <ref type="table" target="#tab_3">Table 2</ref>, middle section, we find that a discriminative objective leads to better performance than the generative objective for all tasks, but joint discriminative and generative objective always achieves the best performance, indicating that the discriminative and generative objectives are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Pretext Tasks As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Pretraining Data</head><p>We pretrain the AST model using 1) AudioSet-20K, 2) AudioSet-2M only, 3) Librispeech only, and 4) both AudioSet-2M and Librispeech, and compare the performance of the pretrained models on the downstream tasks. As shown in <ref type="table" target="#tab_3">Table 2</ref>, bottom section, we have the following key findings: First, increasing the pretraining data volume improves the performance of downstream tasks, e.g., AudioSet-2M pretrained model always outperforms AudioSet-20K pretrained model, but the proposed self-supervised framework can still noticeably improve the AST model with limited pretraining data, e.g., when pretrained and fine-tuned on the same AudioSet-20K data, the proposed SSAST model achieves 0.257 mAP, and significantly outperforms the AST-Scratch model. Second, with the same AudioSet-2M pretraining data, the proposed self-supervised framework leads to similar or even better results compared with the supervised pretraining method, particularly for the speech tasks, showing that the proposed selfsupervised framework is more generalizable. Third, as ex- <ref type="figure">Figure 5</ref>: Performance correlation between pretraining tasks and downstream tasks (upper: audio classification tasks, lower: speech tasks). We save the checkpoint models at iteration <ref type="bibr">20,</ref><ref type="bibr">40,</ref><ref type="bibr">80,</ref><ref type="bibr">200,</ref><ref type="bibr">400</ref>, and 600 during pretraining, then fine-tune and evaluate these checkpoint models on the downstream tasks. For better visualization, we normalize the performance of each task in the range [0, 1]. We observe that the model pretrained with more iterations generally performs better on downstream tasks, which further confirms that the pretraining pretext tasks can benefit all downstream tasks. pected, a model pretrained with AudioSet-2M is better for audio classification and a model pretrained with Librispeech is better for speech tasks, but training with both sets always leads to the best results, showing that it is beneficial to combine pretraining datasets in audio and speech domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Correlation between Pretraining and Downstream Tasks</head><p>We save the checkpoint models at <ref type="bibr">iteration 20, 40, 80, 200, 400</ref>, and 600 during pretraining, then fine-tune and evaluate these checkpoint models on the downstream tasks. We observe the performance of pretraining tasks and downstream tasks are highly correlated, i.e., the model pretrained with more iterations generally performs better on downstream tasks, which further confirms that the pretraining pretext tasks benefit all downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Performance Impact of AST Model Size</head><p>In all previous experiments, we use the original AST (Gong, Chung, and Glass 2021) architecture to make a direct performance comparison. We refer to this model as the base AST model. In this section, we further test the following AST architectures to study the impact of model size.  For each model architecture, we compare the performance of the from-scratch model and the self-supervised pretrained SSAST model (pretrained with 400 masked patches) and show the results in <ref type="table" target="#tab_4">Table 3</ref>. Key findings are as follows:</p><p>First, the MSPM self-supervised pretraining consistently enhances the performance of all three model architectures, showing that MSPM is model size agnostic. Small models that are unlikely to be over-parameterized also get performance improvement with MSPM pretraining.</p><p>Second, when trained from scratch, the larger AST model does not always get the best performance, e.g., the small AST model outperforms the base AST model on AS, KS1, and KS2 tasks. This is as expected since larger models are harder to train with limited data. However, we find that with MSPM self-supervised pretraining, larger AST models always perform better, demonstrating that MSPM can unlock the potential of models with higher capacity. This also suggests that further scaling up the base AST model can potentially achieve even better performance.</p><p>We also observe that using a larger learning rate for the last linear layer during fine-tuning improves the performance for tiny and small SSAST models on the AS task, e.g., for small SSAST model, using a learning rate of 5e-3 for the last linear layer and 5e-5 for all other layers leads to an mAP of 0.308 while using a learning rate of 5e-5 for the entire model leads to an mAP of only 0.272. Nevertheless, we find this trick is only useful for tiny and small selfsupervised pretrained models for some downstream tasks, it does not improve the performance of from-scratch models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Comparing Patch-based and Frame-based AST</head><p>In all previous experiments, we follow the original AST (Gong, Chung, and Glass 2021) to split the audio spectrogram into 16 ? 16 square patches. In (Gong, Chung, and Glass 2021), it was found that splitting the spectrogram into frame-like rectangle patches in the temporal order leads to better performance when the model is trained from scratch. However, ImageNet supervised pretrained model performs significantly better than the from-scratch model, which also constrains the original AST to use square patches. In contrast, our proposed MSPM self-supervised pretraining supports any patch size and shape including a conventional frame. As discussed in Section 2, heuristically, square patch based pretraining could capture correlation in frequency bands in addition to time frames, which is potentially useful when the input has a complex frequency structure (e.g., natural sounds). For clarity, we refer to the AST model that uses square patches and frame-like rectangle patches as patchbased AST model and frame-based AST model, respectively. In this section, we compare patch-based and framebased AST models in both from-scratch setting and selfsupervised pretraining setting. Specifically, the two models have exactly the same architecture except the patch splitting layer, for the patch-based AST model, we use 16 ? 16 patches as described in Section 2; for the frame-based AST model, instead of splitting the spectrogram into 16 ? 16 patches, we split the spectrogram into 128 ? 2 patches in the temporal order (128 is the number of frequency bins of the spectrogram). Patches are split without overlap during pretraining and are split with an overlap of 1 on the time dimension during fine-tuning. This makes a fair comparison as the area of the patch is the same and the number of patches after splitting is similar. In the pretraining setting, both models are pretrained using the method described in Section 2. The only pretraining setting difference is that we do not cluster the masked frames for frame-based AST because this would lower the pretext and downstream task performance, instead, we just random sample the masked frame for frame-based AST pretraining. We test models pretrained with 250 and 400 masked patches (frames) and show the results in <ref type="table" target="#tab_5">Table 4</ref>. Key findings are as follows:</p><p>First, when trained from scratch, frame-based AST always performs better than patch-based AST (except ER), which is consistent with the finding in (Gong, Chung, and Glass 2021) and as expected because 1-D temporal structure is easier to learn than 2-D temporal-frequency structure. Second, after MSPM self-supervised pretraining, framebased AST still outperforms patch-based AST on speech tasks (KS1, KS2, SID, and ER) but the advantage becomes much smaller. Patch-based AST performs better on audio tasks (AS and ESC). MSPM significantly improves the performance of both patch-based and frame-based AST, but the improvement is noticeably larger for patch-based AST (except ER), which verifies our hypothesis that square patch based pretraining can be more effective, particularly for data that has a complex frequency structure such as natural sounds. Our experiment also demonstrates that MPSM is patch shape agnostic, it also works well with framebased AST and makes frame-based SSAST a strong model for speech tasks. In contrast, previous ImageNet pretraining only supports square patches.  3.9 Comparing with Existing Speech Self-supervised Pretraining Frameworks</p><p>Finally, we compare the performance of SSAST with existing speech self-supervised pretraining frameworks. Since these frameworks are designed for speech tasks and are only pretrained on speech datasets, we only compare with them on the speech benchmarks. Specifically, we compare three SSAST models with previous models: 1) SSAST-Patch (Librispeech): Patch-based SSAST model pretrained on only Librispeech (same pretraining data with previous speech self-supervised models); 2) SSAST-Patch: Patchbased SSAST model pretrained on both AudioSet and Librispeech; and 3) SSAST-Frame SSAST model described in Section 3.8 that uses frame-like patches and is pretrained on both AudioSet and Librispeech.</p><p>Comparing with APC and wav2vec 1.0 We first compare SSAST models with autoregressive predictive coding (APC) <ref type="bibr" target="#b7">(Chung et al. 2019</ref>), a generative pretraining framework, and wav2vec 1.0 <ref type="bibr" target="#b26">(Schneider et al. 2019</ref>), a discriminative pretraining framework. We evaluate APC and wav2vec 1.0 in both fine-tuned and frozen settings and report the best result. As shown in <ref type="table" target="#tab_6">Table 5</ref>, SSAST models match or outperform APC and wav2vec 1.0 on all three benchmarks.</p><p>Comparing with wav2vec 2.0 and HuBERT We then compare SSAST models with the state-of-the-art wav2vec 2.0 <ref type="bibr" target="#b1">(Baevski et al. 2020)</ref> and HuBERT <ref type="bibr" target="#b12">(Hsu et al. 2021)</ref> models. Specifically, we compare the base model that is pretrained on Librispeech 960 dataset. Due to the complexity of finding optimal hyperparameters and the large computation cost for fine-tuning these two models, we only report the results in the frozen setting. As shown in <ref type="table" target="#tab_6">Table 5</ref>, frozen wav2vec and HuBERT can already match or outperform fine-tuned SSAST for speech tasks. Nevertheless, it is worth noting that although wav2vec 2.0 and HuBERT perform better, they are pre-trained with 64/32 GPUs and hence have larger batch sizes than our SSAST that is trained with 4 GPUs. The computational resource difference could greatly impact the performance, e.g., for HuBERT, using 8 GPUs leads to 40% WER while 32 GPUs leads to below 20% WER. With more computational resources and larger batch size, SSAST potentially can achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Pure Transformer Based Models Self-attention models, especially the Transformer <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref>, have been widely used in natural language processing. Recently, pure Transformer models, e.g., Vision Transformer <ref type="bibr" target="#b9">(Dosovitskiy et al. 2021;</ref><ref type="bibr" target="#b28">Touvron et al. 2020;</ref><ref type="bibr" target="#b32">Yuan et al. 2021)</ref> and Audio Spectrogram Transformer (Gong, Chung, and Glass 2021), are found to outperform CNN based models for vision tasks and audio classification. Such models differ from CNN models or CNN-Attention hybrid models in that they do not contain non-degenerated convolutions <ref type="bibr" target="#b6">(Chen, Xie, and He 2021)</ref> and have less inductive bias such as spatial locality and translation equivariance. However, it is found that such pure Transformer models require a lot of training data to perform well <ref type="bibr" target="#b9">(Dosovitskiy et al. 2021)</ref>.</p><p>Self-Supervised Learning In the vision domain, selfsupervised Vision Transformer has been studied in <ref type="bibr" target="#b5">(Caron et al. 2021;</ref><ref type="bibr" target="#b6">Chen, Xie, and He 2021;</ref><ref type="bibr" target="#b0">Atito, Awais, and Kittler 2021)</ref>. In addition, patch based self-supervised framework has been extensively studied in the vision domain, e.g., in <ref type="bibr" target="#b18">(Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b29">Trinh, Luong, and Le 2019;</ref><ref type="bibr" target="#b2">Bao, Dong, and Wei 2021)</ref>. However, to the best of our knowledge, the self-supervised Audio Spectrogram Transformer and patch based self-supervised learning framework has not been studied in the audio and speech domain. Previous self-supervised learning frameworks in the speech domain are mainly based on CNN, RNN, or CNN-Transformer hybrid models with the pretext task of predicting past, current, or future frames <ref type="bibr" target="#b7">(Chung et al. 2019;</ref><ref type="bibr" target="#b19">Oord, Li, and Vinyals 2018;</ref><ref type="bibr" target="#b15">Liu et al. 2020;</ref><ref type="bibr" target="#b26">Schneider et al. 2019)</ref>. In contrast, the proposed MSPM framework allows the model to learn both the temporal and frequency structure of the spectrogram. Further, most previous research only focuses on learning either a speech or audio representation, only a few efforts <ref type="bibr" target="#b25">(Saeed, Grangier, and Zeghidour 2021;</ref><ref type="bibr" target="#b17">Niizumi et al. 2021</ref>) studied learning a general audio and speech representation. However, both efforts pretrain the model with only AudioSet. In contrast, we explore pretraining the AST model with both AudioSet and Librispeech. Finally, we pretrain the model with joint discriminative and generative objectives, which is also novel in the audio and speech domain and only has been explored in <ref type="bibr" target="#b22">(Pascual et al. 2019;</ref><ref type="bibr" target="#b13">Jiang et al. 2020;</ref><ref type="bibr" target="#b24">Ravanelli et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper aims to reduce the need for large amounts of labeled data for the AST self-attention based audio and speech classification model by leveraging self-supervised learning. We propose MSPM, a novel patch-based joint discriminative and generative pretraining framework. In order to make the pretrained model generalize to both audio and speech tasks, we pretrain AST using both AudioSet and Librispeech, and evaluate on six downstream benchmarks including audio event classification, keyword spotting, speaker identification, and emotion recognition.</p><p>With extensive experiments, we observe the following key findings. First, the proposed MSPM self-supervised pretraining framework significantly improves the performance of AST for all downstream tasks with an average improvement of 60.9%. Our SSAST model can match or even outperform previous supervised pretrained models and shows better generalization capability, indicating that the proposed MSPM can replace supervised pretraining that requires a large amount of labeled data. Second, we find that pretraining the model with both generative and discriminative objectives leads to a better performance than using a single objective, similarly, pretraining the model on both speech and audio datasets leads to better performance than using data from a single domain. Third, the flexibility of MSPM on patch shape allows us to explore frame-based AST. We find that frame-based AST always outperforms patch-based AST in the from-scratch setting, but patch-based pretraining leads to a larger improvement from the random-initialized models. After MSPM pretraining, the patch-based AST wins on the audio tasks while the frame-based AST wins on the speech tasks. We plan to investigate the reason for this difference in our future work. Finally, we find MSPM allows us to scale up the AST model, with MSPM pretraining, larger AST always performs better. In contrast, in the from-scratch setting, scaling up the model may cause a performance drop. Nevertheless, the current version of SSAST is pretrained with a small batch size due to computational resource limitations. In the future, we plan to further investigate the scaling law of AST.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>set I c = [C 2 -1 indexes neighboring i, i] 4: I = I ? I c 5: I = I[1 : N ] Guarantee to mask exactly N patches return I MSPM (D, M) Input: D, M, Number of Masked Patches N 6: for every epoch do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Prediction accuracy (upper) and reconstruction MSE (lower) of the masked patch modeling pretext tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparing learning curves of AST trained from scratch and self-supervised AST on the AudioSet-20K task. The self-supervised framework helps AST train faster and better. Using a different learning rates, or increasing training epochs does not improve the AST-scratch performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1.</head><label></label><figDesc>Tiny Model: The Transformer encoder has 12 layers with 3 attention heads and an embedding dimension of 192. The tiny model has 6M parameters. 2. Small Model: The Transformer encoder has 12 layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of self-supervised AST with baseline models on various benchmarks.</figDesc><table><row><cell>Model</cell><cell>Task</cell></row><row><cell></cell><cell>AS ESC KS2 KS1 SID</cell><cell>ER</cell></row><row><cell>AST-Scratch</cell><cell cols="2">14.8 41.9 92.6 87.2 30.1 51.9</cell></row><row><cell cols="2">Supervised Pretraining Baselines</cell></row><row><cell cols="3">AST-IM + KD 34.7 88.7 98.1 95.5 41.1 56.0</cell></row><row><cell cols="3">AST-AudioSet 28.6 86.8 96.2 91.6 35.2 51.9</cell></row><row><cell cols="2">Proposed Self-Supervised AST</cell></row><row><cell>SSAST 250</cell><cell cols="2">30.4 86.7 98.1 96.2 66.6 57.1</cell></row><row><cell>SSAST 400</cell><cell cols="2">31.0 88.8 98.0 96.0 64.2 59.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the impact of number of masked patches, pretext task, and pretraining data.</figDesc><table><row><cell>Setting</cell><cell>Task</cell></row><row><cell></cell><cell>AS ESC KS2 KS1 SID</cell><cell>ER</cell></row><row><cell>From Scratch</cell><cell cols="2">14.8 41.9 92.6 87.2 30.1 51.9</cell></row><row><cell cols="2"># Masked Patches</cell></row><row><cell>100</cell><cell cols="2">28.7 85.3 98.0 94.9 62.1 57.3</cell></row><row><cell>250</cell><cell cols="2">30.4 86.7 98.1 96.2 66.6 57.1</cell></row><row><cell>400 (Default)</cell><cell cols="2">31.0 88.8 98.0 96.0 64.3 59.6</cell></row><row><cell>Pretext Task</cell><cell></cell></row><row><cell cols="3">Discriminative 30.6 85.6 98.0 94.2 61.4 57.5</cell></row><row><cell>Generative</cell><cell cols="2">16.1 74.2 96.6 93.3 40.1 54.3</cell></row><row><cell cols="3">Joint (Default) 31.0 88.8 98.0 96.0 64.3 59.6</cell></row><row><cell cols="2">Pretraining Data</cell></row><row><cell cols="3">AudioSet-20K 25.7 82.2 97.6 93.8 43.8 55.4</cell></row><row><cell>AudioSet 2M</cell><cell cols="2">29.0 84.7 97.8 94.8 57.1 56.8</cell></row><row><cell>AudioSet 2M Supervised</cell><cell cols="2">28.6 86.8 96.2 91.6 35.2 51.9</cell></row><row><cell>Librispeech</cell><cell cols="2">22.9 80.0 97.8 95.6 60.8 58.3</cell></row><row><cell cols="3">Joint (Default) 31.0 88.8 98.0 96.0 64.3 59.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of AST model of different sizes ( * use larger learning rate for the last linear classification layer).</figDesc><table><row><cell>Model</cell><cell>Task</cell></row><row><cell></cell><cell>AS ESC KS2 KS1 SID ER</cell></row><row><cell>Tiny-Scratch</cell><cell>15.1 34.8 92.4 87.7 24.2 50.8</cell></row><row><cell cols="2">Tiny-SSAST 27.1  *  79.5 97.2 94.8 55.1 55.7</cell></row><row><cell cols="2">Small-Scratch 16.5 37.8 93.3 87.4 23.8 51.2</cell></row><row><cell cols="2">Small-SSAST 30.8  *  85.4 97.7 95.4 60.9 58.7</cell></row><row><cell>Base-Scratch</cell><cell>14.8 41.9 92.6 87.2 30.1 51.9</cell></row><row><cell>Base-SSAST</cell><cell>31.0 88.8 98.0 96.0 64.2 59.6</cell></row><row><cell cols="2">with 6 attention heads and an embedding dimension of</cell></row><row><cell cols="2">384. The small model has 23M parameters.</cell></row><row><cell cols="2">3. Base Model: The model described in Section 2.1 that</cell></row><row><cell cols="2">is used as the default model throughout the paper. The</cell></row><row><cell cols="2">Transformer encoder has 12 layers with 12 attention</cell></row><row><cell cols="2">heads and an embedding dimension of 768. The base</cell></row><row><cell cols="2">model has 89M parameters.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of frame and patch based AST models.</figDesc><table><row><cell>Model</cell><cell>Task</cell></row><row><cell></cell><cell>AS ESC KS2 KS1 SID ER</cell></row><row><cell>Frame-Scratch</cell><cell>16.6 53.7 96.0 91.7 54.9 51.2</cell></row><row><cell>Patch-Scratch</cell><cell>14.8 41.9 92.6 87.2 30.1 51.9</cell></row><row><cell cols="2">SSAST-Frame-250 27.1 84.0 98.0 96.6 73.6 58.3</cell></row><row><cell>SSAST-Patch-250</cell><cell>30.4 86.7 98.1 96.2 66.6 57.1</cell></row><row><cell cols="2">SSAST-Frame-400 29.2 85.9 98.1 96.7 80.8 60.5</cell></row><row><cell>SSAST-Patch-400</cell><cell>31.0 88.8 98.0 96.0 64.2 59.6</cell></row><row><cell cols="2">Frame-Improvement 12.6 32.2 2.1 5.0 25.9 9.3</cell></row><row><cell cols="2">Patch-Improvement 16.2 46.9 5.4 8.8 34.1 7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of SSAST and existing speech selfsupervised pretraining frameworks ( * frozen setting results).</figDesc><table><row><cell>Model</cell><cell>Task</cell></row><row><cell></cell><cell>KS1 SID</cell><cell>ER</cell></row><row><cell>APC (Chung et al. 2019)</cell><cell cols="2">94.0 60.4 59.3</cell></row><row><cell>Wav2vec (Schneider et al. 2019)</cell><cell cols="2">96.2 56.6 59.8</cell></row><row><cell cols="3">Wav2vec 2.0 (Baevski et al. 2020)  *  96.2 75.2 63.4</cell></row><row><cell>HuBERT (Hsu et al. 2021)  *</cell><cell cols="2">96.3 81.4 64.9</cell></row><row><cell>SSAST-Patch (Librispeech Only)</cell><cell cols="2">95.6 60.8 58.3</cell></row><row><cell>SSAST-Patch</cell><cell cols="2">96.0 64.3 59.6</cell></row><row><cell>SSAST-Frame</cell><cell cols="2">96.7 80.8 60.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments and suggestions. This work is partly supported by Signify.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Downstream Task Dataset and Evaluation Protocol Details</head><p>We show more details about the downstream datasets and evaluation protocols.</p><p>AudioSet-20K We use the AudioSet balanced training set and evaluation set for the multi-label audio event classification task. The AudioSet-20K training set is a class-balanced subset of AudioSet-2M that contains 20,785 audio clips. For this task, we use the mean averaged precision (mAP) as the matrix. We test the model on the AudioSet evaluation set, which is disjoint with both AudioSet-20K and AudioSet-2M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ESC-50</head><p>We use the ESC-50 dataset <ref type="bibr" target="#b23">(Piczak 2015)</ref> for single audio event classification task. ESC-50 is an audio classification dataset consists of 2,000 5-second environmental audio recordings organized into 50 classes. For this task, we follow the standard 5-fold cross-validation to evaluate our model and report the accuracy. The difference between ESC-50 and AudioSet-20K is that each ESC-50 audio clip only contains a single event and the total data volume size is 10 times smaller than AudioSet-20K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Commands V2-35</head><p>We use the Speech Commands V2-35 (Warden 2018) for the keyword spotting task. The Speech Command V2-35 dataset consists of 105,829 1second recordings of 35 common speech commands. The training, validation, and test set contains 84,843, 9,981, and 11,005 samples, respectively. We fine-tune the pretrained model on the training set, select the model using the validation set, and report the accuracy on the test set.</p><p>Speech Commands V1 We also use the Speech Commands V1 (Warden 2018) for the keyword spotting KS task, which is similar to Speech Commands V2, but only contains 10 classes of keywords, 1 class of silence, and an unknown class to include the false positive. For Speech Commands V1, we use the SUPERB evaluation framework <ref type="bibr" target="#b31">(Yang et al. 2021</ref>) and report the accuracy on the test set.</p><p>VoxCeleb 1 We use VoxCeleb 1 dataset <ref type="bibr" target="#b16">(Nagrani et al. 2020</ref>) for the speaker identification task. The VoxCeleb 1 dataset contains 352 hours of speech from 1,251 speakers. The goal of this task is to classify each utterance for its speaker identity where speakers are in the same predefined set for both training and testing. For VoxCeleb 1, we use the SUPERB evaluation framework <ref type="bibr" target="#b31">(Yang et al. 2021</ref>) and report the accuracy on the test set.</p><p>IEMOCAP We use the IEMOCAP dataset <ref type="bibr" target="#b4">(Busso et al. 2008)</ref> for the speech based emotion recognition (ER) task that contains about 12 hours of emotional speech from 10 speakers. For IEMOCAP, we use the SUPERB evaluation framework <ref type="bibr" target="#b31">(Yang et al. 2021)</ref>, which follows the conventional IEMOCAP evaluation protocol that drops the unbalance emotion classes to leave the four roughly balanced classes (neutral, happy, sad, angry), conduct 5-fold crossvalidation, and report the mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Details</head><p>Utterance Representation For the reason mentioned in Section 2.1 of the main manuscript, for supervised pretrained baseline (AST-IM+KD and AST-AudioSet), we use the output representation of the [CLS] token as the utterance representation because supervision is given to the output of [CLS] in pretraining while we use the average patch representation of all patches as the utterance representation (i.e., applying a mean pooling over O) for AST-Scratch and SSAST because supervision is given to individual token in pretraining, keeping pretraining and fine-tuning settings consistent can slightly improve the performance and make the comparison fairer. Model without pretraining might need more epochs to train, therefore, we train AST-Scratch models with more epochs until convergence for a fair comparison.</p><p>Model Initialization We use default PyTorch initialization for weights for all layers for the AST models, which is the same as the original AST paper (Gong, Chung, and Glass 2021).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computing Software and Infrastructure</head><p>We run our experiments with Python 3.7.4, PyTorch 1.9.0, and CUDA 10.2. We run our experiments on 4? NVIDIA GTX Titan X or GTX Titan X Pascal GPUs; Intel Xeon CPU E5-2623 v3 or E5-2620 v4 CPUs, and 128GB memory.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BEiT: BERT Pre-Training of Image Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Cruz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. Gong, Y</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>AST: Audio Spectrogram Transformer. Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HuBERT: How much can a bad teacher benefit ASR pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6533" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13991</idno>
	</analytic>
	<monogr>
		<title level="m">Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Voxceleb: Large-scale speaker verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101027</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06695</idno>
		<title level="m">BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning problem-agnostic speech representations from multiple self-supervised tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03416</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitask self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6989" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive learning of general-purpose audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<title level="m">wav2vec: Unsupervised pre-training for speech recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selfie: Selfsupervised pretraining for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
	</analytic>
	<monogr>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<editor>NIPS. Warden, P</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><forename type="middle">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01051</idno>
		<title level="m">SUPERB: Speech processing Universal PERformance Benchmark</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
