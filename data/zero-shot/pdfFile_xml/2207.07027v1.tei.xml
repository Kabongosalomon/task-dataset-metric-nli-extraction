<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images Engineering Division NYU Abu Dhabi</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022">2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
							<email>nasirhayat6160@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Healthcare</forename><forename type="middle">Abu</forename><surname>G42</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhabi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><forename type="middle">J</forename><surname>Uae</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><forename type="middle">E</forename><surname>Shamout</surname></persName>
							<email>farah.shamout@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Dhabi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uae</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">NYU Grossman School of Medicine New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images Engineering Division NYU Abu Dhabi</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of Machine Learning Research</title>
						<meeting>Machine Learning Research						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="25"/>
							<date type="published" when="2022">2022</date>
						</imprint>
					</monogr>
					<note>Machine Learning for Healthcare</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal fusion approaches aim to integrate information from different data sources. Unlike natural datasets, such as in audio-visual applications, where samples consist of "paired" modalities, data in healthcare is often collected asynchronously. Hence, requiring the presence of all modalities for a given sample is not realistic for clinical tasks and significantly limits the size of the dataset during training. In this paper, we propose MedFuse, a conceptually simple yet promising LSTM-based fusion module that can accommodate uni-modal as well as multi-modal input. We evaluate the fusion method and introduce new benchmark results for in-hospital mortality prediction and phenotype classification, using clinical time-series data in the MIMIC-IV dataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more complex multi-modal fusion strategies, MedFuse provides a performance improvement by a large margin on the fully paired test set. It also remains robust across the partially paired test set containing samples with missing chest X-ray images. We release our code for reproducibility and to enable the evaluation of competing models in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans perceive the world through multi-modal data <ref type="bibr" target="#b33">(Ngiam et al., 2011)</ref>. To date, most of the successful models learning from perceptual data in healthcare are uni-modal, i.e. they rely on a single data modality <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>. Multi-modal learning has been widely explored in the context of audio-visual applications <ref type="bibr">(Vaezi Joze et al., 2020)</ref> and natural image datasets <ref type="bibr">(Zellers et al., 2021;</ref><ref type="bibr">Hayat et al., 2020)</ref>, but less so in healthcare. The main goal of multi-modal fusion is to exploit relevant information from different modalities to improve performance in downstream tasks <ref type="bibr" target="#b3">(Baltru?aitis et al., 2018)</ref>. Multi-modal fusion strategies can be characterized as early, joint, or late fusion <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>. The joint fusion paradigm is the most promising, since its core idea is to model interactions between the representations of the input modalities.</p><p>We highlight two main challenges facing multi-modal joint fusion in healthcare. First, many of the state-of-the-art approaches make a strong assumption that all modalities are available for every sample during training, inference, or both <ref type="bibr" target="#b36">(P?lsterl et al., 2021)</ref>. Although some clinical studies follow suit of this assumption <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>, obtaining paired data is not feasible since daily clinical practice produces heterogeneous data with varying sparsity. For example, physiological data is more frequently collected than chest Xray images in the Intensive Care Unit (ICU) setting. These two modalities are the key focus of our study because they play a very important role in clinical prediction tasks <ref type="bibr" target="#b8">(Harutyunyan et al., 2019;</ref><ref type="bibr" target="#b29">Lohan, 2019)</ref>. Developing a unified fusion model for those two modalities also presents its own challenges as they (i) have significantly different input dimensions, (ii) require modality-specific feature extractors due to differences in information and noise content <ref type="bibr" target="#b32">(Nagrani et al., 2021)</ref>, and (iii) are not temporally aligned and hence cannot be paired easily. Considering those challenges, our primary aim is to propose a fusion architecture that can deal with partially paired data, in order to achieve favorable performance in downstream prediction tasks.</p><p>The second challenge is that there are no well-studied publicly available multi-modal clinical benchmarks. Therefore, most studies rely on a single data modality to perform clinical prediction tasks <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>, or use privately curated multi-modal datasets <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>. Here, our secondary aim is to introduce new multi-modal benchmark results for two popular clinical prediction tasks using the publicly available Medical Information Mart for Intensive Care (MIMIC)-IV <ref type="bibr" target="#b21">(Johnson et al., 2021)</ref> and MIMIC-CXR <ref type="bibr" target="#b22">(Johnson et al., 2019)</ref> datasets, and we also release the code for reproducibility. We compare our approach to vanilla early and joint fusion as well as open-source state-of-theart joint fusion approaches <ref type="bibr">(Vaezi Joze et al., 2020;</ref><ref type="bibr" target="#b36">P?lsterl et al., 2021)</ref>. In summary, we make the following contributions:</p><p>? We propose MedFuse, a new LSTM-based <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997)</ref> multimodal fusion approach. Conventional joint fusion strategies concatenate feature representations of multiple modalities as a single feature representation, and then process that concatenated representation for downstream tasks, such as using a classifier. On the contrary, we treat the multi-modal representation as a sequence of uni-modal representations (or tokens), such that the fusion module aggregates these representations through the recurrence mechanism of LSTM. We assume a sequential structure to leverage the recurrent inductive bias of LSTM and to handle input sequences of variable length, in case of a missing modality. The fusion module is agnostic to the architecture of the modality-specific extractors and can handle missing data during training and inference.</p><p>? To evaluate the proposed approach, we link two open-access real-world datasets: MIMIC-IV <ref type="bibr" target="#b21">(Johnson et al., 2021)</ref>, which contains clinical time-series data collected in the ICU, and MIMIC-CXR <ref type="bibr" target="#b22">(Johnson et al., 2019)</ref>, which contains chest X-ray images. We pre-process the data and introduce new benchmark results for two tasks <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>: in-hospital mortality prediction and phenotype classification.</p><p>The results show that the model's performance remains robust across uni-modal samples and improves for paired multi-modal samples. The model achieves state-of-the-art results without imposing any assumptions on correlation between modalities.</p><p>? Considering the lack of multi-modal learning benchmarks in healthcare, we release our data pre-processing and benchmark code to allow reproducibility of the results and enable the evaluation of competing models in the future. The code can be found at: https://github.com/nyuad-cai/MedFuse. An overview of the proposed work is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Overview of the proposed work. We first extract and link the datasets from MIMIC-IV and MIMIC-CXR based on the task definition (i.e., inhospital mortality prediction, or phenotype classification). The data splits of the training, validation, and test sets are summarized for each task, and the prevalence of positive and negative labels for in-hospital mortality is shown. Phenotype classification involves 25 labels as shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizable Insights about Machine Learning in the Context of Healthcare</head><p>State-of-the-art multi-modal fusion approaches typically investigate synchronous sources of information using natural datasets, such as audio, visual, and textual modalities. In healthcare, data is often sparse and heterogeneous and hence modalities are not always paired. Our work overcomes the challenge of missing data by proposing a flexible fusion approach that is agnostic to the modality-specific encoders. Therefore, it can be used for other types of input data, beyond chest X-ray images and clinical time-series data. It also highlights the value of processing a sequence of uni-modal representations, compared to the conventional concatenation strategy in joint fusion. Overall, the work highlights the promise of multi-modal fusion in healthcare to improve performance in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Routine clinical practice produces large amounts of data from different sources (i.e. modalities), including medical images, laboratory test results, measurements of vital signs, and clinical notes <ref type="bibr" target="#b2">(Asri et al., 2015)</ref>. Advances in deep learning have enabled building predictive models using subsets of modalities, typically clinical time-series data <ref type="bibr" target="#b41">(Shickel et al., 2017)</ref> and medical images <ref type="bibr" target="#b28">(Litjens et al., 2017)</ref>. Here, we provide an overview of related work on multi-modal fusion in healthcare using imaging and non-imaging data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-modal Learning</head><p>Multi-modal learning has been widely explored for jointly learning representations of multiple modalities <ref type="bibr" target="#b3">(Baltru?aitis et al., 2018)</ref>. Example tasks include visual grounding , language grounding through visual cues <ref type="bibr" target="#b54">(Zhang et al., 2021b)</ref>, action recognition , video classification <ref type="bibr" target="#b32">(Nagrani et al., 2021)</ref>, image captioning , or visual-question answering <ref type="bibr">(Zellers et al., 2021)</ref>. Since machine learning studies typically investigate different combinations of audio, visual, and textual modalities, many of the existing methods are driven by the assumption that the modalities share intrinsic and structural information. This is not always true for heterogeneous data in healthcare. Hence, due consideration should be given to learning with multiple medical data modalities, since conventional assumptions for non-medical data are not necessarily applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modal Fusion with Medical Images</head><p>There is an increasing interest in advancing the fusion of multi-modal medical images <ref type="bibr" target="#b13">(Hermessi et al., 2021)</ref>. The images usually represent different views of the same organ or lesion of interest, acquired using one or more sensors, whereby the images share the same set of labels. Proposed methods mainly focus on pixel-level fusion of complementary views acquired through multiple sensors to obtain a unified composite representation of the raw images <ref type="bibr" target="#b19">James and Dasarathy, 2014)</ref>. Various feature-and prediction-level fusion approaches were proposed for improved classification <ref type="bibr" target="#b37">(Puyol-Ant?n et al., 2021;</ref><ref type="bibr" target="#b48">Wu et al., 2019;</ref><ref type="bibr" target="#b53">Zhang et al., 2021a)</ref> or segmentation performance <ref type="bibr" target="#b13">(Hermessi et al., 2021)</ref>. Since textual reports are a natural byproduct of radiology exams, they were also used as additional modalities for tasks like visual-question answering <ref type="bibr" target="#b26">(Li et al., 2020;</ref><ref type="bibr" target="#b40">Sharma et al., 2021)</ref>, report generation <ref type="bibr">(Sonsbeek and Worring, 2020)</ref>, or zero-shot image classification <ref type="bibr" target="#b11">(Hayat et al., 2021b;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-modal Fusion with Clinical Data and Medical Images</head><p>Several studies investigated the fusion of medical images and clinical data extracted from the patient's Electronic Health Records (EHR) for various applications <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>. For example, a stream of work covers tasks pertaining to cancer, such as recurrence prediction <ref type="bibr" target="#b14">(Ho et al., 2021)</ref>, lesion detection <ref type="bibr" target="#b39">(Shao et al., 2020)</ref>, or patient survival prediction (Vale-Silva and Rohr, 2020). Other tasks include detection of pulmonary embolism <ref type="bibr" target="#b17">(Huang et al., 2020b)</ref>, predicting the progression of Alzheimer's disease <ref type="bibr" target="#b25">(Lee et al., 2019)</ref>, diagnosis of neurological disease <ref type="bibr" target="#b49">(Xin et al., 2021)</ref>, or diagnosis of cervical dysplasia <ref type="bibr" target="#b50">(Xu et al., 2016)</ref>. While these studies highlight the impact of using multiple data fine-tuning during fusion <ref type="figure">Figure 2</ref>: Overview of network with MedFuse module. First, we pre-train the modality-specific encoders and classifiers independently for each input modality. Specifically, we train f ehr and g ehr using the clinical time-series data and f cxr and g cxr using the chest X-ray images. Next, we project the chest X-ray latent representation v cxr to v * cxr , in order to match the dimension of v ehr . We pass v ehr and v * cxr as an input sequence to the LSTM-based f f usion , and we classify its last hidden state h f usion to compute the overall prediction y f usion . f f usion , f ehr , f cxr , g f usion , and ? are fine-tuned together for fusion. modalities on downstream performance, many curate datasets for specific tasks and share the assumption that the images and selected clinical features are paired.</p><p>Some studies specifically focused on the integration of clinical data and chest X-ray images. For example, the integration of the two modalities showed a favorable impact on the predictive performance in prognostication tasks among patients with COVID-19 <ref type="bibr" target="#b38">(Shamout et al., 2021;</ref><ref type="bibr" target="#b20">Jiao et al., 2021)</ref>. Some studies jointly refine a common latent representation after aggregating encoded features of each modality <ref type="bibr" target="#b7">(Grant et al., 2021;</ref><ref type="bibr" target="#b20">Jiao et al., 2021)</ref>, while others combine predictions computed by each modality through weighted averaging (i.e., late fusion) <ref type="bibr" target="#b38">(Shamout et al., 2021;</ref><ref type="bibr" target="#b20">Jiao et al., 2021)</ref>. While late fusion enables the computation of predictions even for incomplete samples, it requires that the two modalities are assigned the same labels, which is not always feasible. Closely related to our work is that of <ref type="bibr" target="#b10">Hayat et al. (2021a)</ref>, where they propose a dynamic training approach for partially paired clinical time-series data and chest X-ray images for the task of phenotype classification. However, their method is not scalable since it incorporates an additional classifier (and prediction) for every possible combination of input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We define a two stage-approach (i) to learn modality-specific perceptual models to extract the latent features (Section 3.1), and (ii) integrate these features through a joint multimodal fusion module, MedFuse (Section 3.2). The overall architecture is shown in <ref type="figure">Figure 2</ref>. Without loss of generality, we focus here on two modalities only and denote the clinical time-series data as ehr and the chest X-ray images as cxr when defining the methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modality-specific Encoders</head><p>One of the main sources of heterogeneity in healthcare is the varying dimensionality of the input modalities, which makes it challenging to develop a unified encoder for all input modalities. Another difference is the target space, since we do not assume that the modalities must be assigned the same set of labels. Hence, we first define modality-specific encoders as follows.</p><p>For a given instance, let x ehr ? R t?d represent the clinical time-series data associated with ground-truth labels y ehr , where t is the number of time steps and d is the number of features derived from the clinical variables. We implement the encoder, f ehr , for the clinical time-series modality as two stacked layers of an LSTM network <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997</ref>) with a dropout layer. We compute a latent feature representation v ehr ? R m consisting of the last hidden state of the stacked LSTM, where m = 256. We then apply a classifier, g ehr , to compute the predictions, such that? ehr = g ehr (v ehr ). To fine-tune the encoder, we optimize the following loss:</p><formula xml:id="formula_0">L ehr (y ehr ,? ehr ) = BCE(y ehr ,? ehr ),<label>(1)</label></formula><p>where BCE is the Binary Cross-Entropy loss. Let x cxr ? R w?h?c represent the chest X-ray image belonging to the same instance associated with the ground-truth labels y cxr , where w is the width dimension, h is the height dimension, and c is the number of channels. In all of our experiments, h = 224, w = 224, and c = 3, as we replicate each image across three channels. We implement the encoder, f cxr , as a ResNet-34 <ref type="bibr" target="#b12">(He et al., 2016)</ref> to compute v cxr ? R n , which is the feature representation after the average pooling layer of the convolutional network where n = 512. Similarly, we then apply a classifier, g cxr , to compute the predictions, such that y cxr = g cxr (v cxr ) and optimize the following loss to fine-tune the encoder:</p><p>L cxr (y cxr ,? cxr ) = BCE(y cxr ,? cxr ).</p><p>(2)</p><p>The encoders can hence be independently pre-trained using their respective labels and losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The MedFuse Module</head><p>To fuse the modalities, we first dismiss the classifiers, g ehr and g cxr , and keep the the pretrained modality-specific encoders, f ehr and f cxr . Since the latent space dimensions of the two modalities are different, we use a projection layer, ?, that projects v cxr to the same dimensionality as v ehr :</p><formula xml:id="formula_1">v * cxr = ?(v cxr )<label>(3)</label></formula><p>such that v * cxr ? R m . We then create an input sequence consisting of the the uni-modal feature representations of the sample:</p><formula xml:id="formula_2">v f usion = [v ehr , v * cxr ].<label>(4)</label></formula><p>We parameterize a multi-modal fusion network, f f usion , as a single LSTM layer with input dimension of 256 and a hidden dimension of 512, that aggregates the multi-modal sequence through recurrence. The motivation for using an LSTM is two-fold. First, it follows the intuition of decision-making, where clinicians examine information from each modality sequentially, or one at a time. This allows the LSTM module to initially learn from v ehr , and then update its internal state using information in v * cxr . Second, it can handle input sequences of variable number of modalities, so it inherently deals with missing modalities. In the case that the chest X-ray image is missing during training or inference, the network processes a single-element sequence, [v ehr ].</p><p>The last hidden state, h f usion , of f f usion is then processed using a classifier g f usion that computes the final fusion predictions, such that? f usion = g f usion (h f usion ). We jointly train the encoders f ehr and f cxr , the projection layer ?, the fusion module f f usion , and the classifier g f usion , by optimizing the following loss:</p><formula xml:id="formula_3">L f usion (y f usion ,? f usion ) = BCE(y f usion ,? f usion ),<label>(5)</label></formula><p>where y f usion = y ehr , since we assume that the clinical time-series data modality is the base modality associated with the prediction task of interest, and is always present during training and inference. All classifiers g ehr , g cxr , and g f usion consist of a single linear layer followed by sigmoid activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Benchmark Tasks</head><p>For our experiments, we extract the clinical time-series data from MIMIC-IV <ref type="bibr" target="#b21">(Johnson et al., 2021)</ref> along with the associated chest X-ray images in MIMIC-CXR <ref type="bibr" target="#b22">(Johnson et al., 2019)</ref>. Here we describe the two tasks and provide more details on each:</p><p>? Phenotype classification: The goal of this multi-label classification task is to predict whether a set of 25 chronic, mixed, and acute care conditions are assigned to a patient in a given ICU stay. For a given instance, x ehr contains clinical time-series data collected during the entire ICU record, and y ehr is a vector of 25 binary phenotype labels. We link each instance with the last chest X-ray image collected during the same ICU stay. MIMIC-III contains International Classification of Diseases (ICD) version 9 (ICD-9) codes, whereas MIMIC-IV contains both ICD-9 and ICD-10. In the original benchmark paper <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>, the 25 phenotype labels were initially defined using the Clinical Classifications Software (CCS) for ICD-9 <ref type="bibr" target="#b47">(WHO et al., 1988)</ref>. Since ICD-9 and ICD-10 codes are aggregated to different CCS categories, we mapped all ICD-10 codes to ICD-9 using the guidelines provided by the Centers for Medicare &amp; Medicaid Services 1 , and then map them to CCS categories. We evaluate this task using the Area Under the Receiver Operating Characteristic (AUROC) curve and the Area Under the Precision Recall curve (AUPRC). ? In-hospital mortality prediction: The goal of this binary classification task is to predict in-hospital mortality after the first 48 hours spent in the ICU. Hence, for a given instance, x ehr contains clinical time-series data collected during the first 48 hours of the ICU record, and y ehr is a binary label indicating in-hospital mortality. Since the task requires a minimum of 48 hours, we exclude ICU stays that are shorter than 48 hours. Here, we pair each instance with the last chest X-ray image collected during the first 48 hours of ICU stay. We evaluate this task using AUROC and AUPRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Pre-processing of Clinical Time-series Data</head><p>We modified the extraction and data pre-possessing pipeline of <ref type="bibr" target="#b8">Harutyunyan et al. (2019)</ref>, which was originally implemented in TensorFlow <ref type="bibr" target="#b1">(Abadi et al., 2015)</ref>, and introduce a new version for MIMIC-IV using Pytorch <ref type="bibr" target="#b34">(Paszke et al., 2019)</ref>. To make a fair comparison and illustrate the efficacy of multi-modal learning, we use the same set of 17 clinical variables. Amongst those, five are categorical (capillary refill rate, Glasgow coma scale eye opening, Glasgow coma scale motor response, Glasgow coma scale verbal response, and Glasgow coma scale total) and 12 are continuous (diastolic blood pressure, fraction of inspired oxygen, glucose, heart rate, height, mean blood pressure, oxygen saturation, respiratory rate, systolic blood pressure, temperature, weight, and pH). For all the tasks, we regularly sample the input every two hours, discretize and standardize the clinical variables to obtain the input for f ehr as in previous work <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>. After data pre-processing and one-hot encoding of the categorical features, we obtain a vector representation of size 76 at each time-step of the clinical time-series data, such that for a given instance, x ehr ? R t?76 and t depends on the instance and task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Data Splits</head><p>Using the patient identifier of the clinical time-series data, we randomly split the dataset into 70% for training, 10% for validation, and 20% for test set, as shown in <ref type="figure">Figure 1</ref>. We report final results on the test sets and compute 95% confidence intervals with 1000 iterations via the bootstrap method <ref type="bibr" target="#b6">(Efron and Tibshirani, 1994)</ref>. Here, we denote the clinical timeseries data as EHR and the chest X-ray images as CXR. (EHR+CXR) PARTIAL contains paired and partially paired samples (i.e. samples where chest X-ray is missing). (EHR + CXR) PAIRED contains data samples where both modalities are present. For example, the (EHR + CXR) PARTIAL training set for patient phenotyping contains 8056 samples associated with chest X-rays amongst 42628 samples. We extract from MIMIC-CXR chest X-ray images and split them based on a random patient split. We then transfer images from the training set to either the validation or test set, in case are were associated with patients in the validation or test splits of the clinical time-series data. This procedure resulted with 325188 images in the training set, 15282 images in the validation set, and 36625 images in the test set. We define y cxr as a vector of 14 binary radiology labels extracted from radiology reports through CheXpert <ref type="bibr" target="#b18">(Irvin et al., 2019)</ref>. We denote this uni-modal dataset as CXR UNI and it is fixed across all tasks. We introduce an additional notation for CXR PAIRED , which includes only chest X-ray images within (EHR+CXR) PAIRED , and EHR PARTIAL , which includes only clinical time-series data within (EHR + CXR) PARTIAL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Strategy with the MedFuse Module</head><p>The training strategy consists of two steps: pre-training of the modality-specific encoders followed by jointly fine-tuning the encoders and fusion module. During the pre-training stage, we train the image encoder using the full uni-modal training dataset CXR UNI with concatenate concatenate fine-tuning during fusion end-to-end training <ref type="figure">Figure 3</ref>: Architecture of early and joint fusion baselines. In early fusion (left), the encoders are first pre-trained. Then, we freeze them and fine-tune the projection layer and fusion classification module. In joint fusion (right), the encoders and classification module are randomly initialized and trained end-to-end.</p><p>the 14 radiology labels. We also pre-train the clinical time-series data encoder for each task independently using the training sets EHR PARTIAL , since each task is associated with its own set of inputs and labels. After pre-training the modality-specific encoders, we discard the uni-modal classifiers and fine-tune the encoders, projection layer, and MedFuse using (EHR + CXR) PARTIAL . We compare this training strategy to fine-tuning the fusion module with randomly initialized feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Models</head><p>We compare the performance of our proposed multi-modal approach to several existing baselines:</p><p>? Early fusion: The vanilla early fusion approach commonly used in recent work <ref type="bibr" target="#b16">(Huang et al., 2020a)</ref>  <ref type="figure">(Figure 3</ref> (left)) assumes the presence of paired data modalities during training and inference. We train two versions. In the first version, we pre-train modality-specific networks independently: f cxr and g cxr with the CXR PAIRED training set, and f ehr and g ehr with the EHR PAIRED training set. We then freeze the encoders f cxr and f ehr , concatenate their latent feature representations, and finetune a projection layer and a fully connected classification network, denoted as g cl using the (EHR + CXR) PAIRED training set. In the second version, we use the (EHR + CXR) PARTIAL training set for fine-tuning the projection layer and g cl . Inspired by <ref type="bibr">Kyono et al. (2021)</ref>, we learn a vector to substitute for missing chest X-ray images. ? Joint fusion: In this setting, we train a network end-to-end including the modalityspecific encoders (f cxr and f ehr ) and a classification network applied to the concatenated latent representations of the two encoders <ref type="figure">(Figure 3 (right)</ref>). We train two versions. In the first version, we train a randomly initialized network end-to-end using (EHR + CXR) PAIRED . In the second version, we train a randomly initialized network end-to-end using (EHR + CXR) PARTIAL with a learnable vector to substitute for any missing chest X-ray images.  <ref type="formula">2020)</ref>, this approach also assumes paired input data. We apply an MMTM <ref type="table">Table 1</ref>: Performance results in the uni-modal vs multi-modal setting. Here, we compare the stacked LSTM network for the clinical time-series data only, with our network using MedFuse. In the first four rows, we summarize the AUROC and AUPRC results in the paired setting with uni-modal (EHR PAIRED ) and multi-modal data ((EHR + CXR) PAIRED ). In the last two rows, we show the results on the partially paired test set, with the uni-modal subset (EHR PARTIAL ) and multi-modal data ((EHR + CXR) PAIRED ). All results shown below are for MedFuse (OPTIMAL). Best results are shown in bold. module after the first LSTM layer in the clinical time-series modality, and either the third or the fourth ResNet layer. We train a randomly initialized network with the MMTM module end-to-end using the (EHR+CXR) PAIRED training set, and closely follow the training strategy described in the original paper. 2 ? Dynamic Affine Feature Map Transform (DAFT): Also requiring paired input data, we use the general purpose DAFT module <ref type="bibr" target="#b36">(P?lsterl et al., 2021)</ref> to rescale and shift the feature representations after the first LSTM layer using the chest X-ray representation computed either through the third or fourth layer of ResNet. Similarly, we use (EHR+CXR) PAIRED , and follow the training approach in the original work's respository. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modalities</head><p>We also compare it with a uni-modal two-layer LSTM network trained with clinical timeseries data only, and the method proposed by <ref type="bibr" target="#b10">Hayat et al. (2021a)</ref> (Unified) trained with (EHR + CXR) PARTIAL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Training and Selection</head><p>We perform hyperparameter tuning over 10 runs for our proposed network with MedFuse and each of the baseline models and their different versions. In each run, we randomly sample a learning rate between 10 ?5 and 10 ?3 , and then choose the model and learning rate that achieve the best AUROC on the respective validation set. For the baselines with architectural choices (i.e. MMTM and DAFT), we choose the architecture that achieves the best performance on the validation set, and report its results on the test set. We use the Adam optimizer (Kingma and Ba, 2014) across all experiments with a batch size of 2. https://github.com/haamoon/mmtm 3. https://github.com/ai-med/DAFT/ and AUPRC results for our proposed approach with MedFuse and the baseline models. We include results for early and joint fusion when trained with either (EHR + CXR) PAIRED or (EHR + CXR) PARTIAL , where the latter uses a learnable vector in the case of a missing chest X-ray image. We also show results of our proposed approach when we finetune the fusion module with (EHR + CXR) PARTIAL and randomly initialized encoders (RI) or pre-trained encoders (PT), and the best version of the latter when using the optimal number of uni-modal samples during fine-tuning (OPTIMAL). Best results are shown in bold. 16. We set the maximum number of epochs to 50 and use early stopping if the validation AUROC does not improve for 15 epochs. We also apply image augmentations as described in Appendix A.1.</p><p>With the best learning rate chosen via hyperparameter tuning, we vary the percentage of samples with EHR only data in the (EHR + CXR) PARTIAL training set, fine-tune MedFuse accordingly and evaluate it on the validation set. We select the best model based on the best AUROC performance on the (EHR + CXR) PARTIAL validation set, and report its results on the test set. We denote this chosen model as MedFuse (OPTIMAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we describe the results for a number of experiments to provide insights on our proposed approach. The learning rates that achieved the best results are summarized in Appendix A.2 for all models. The results on the validation set in the experiments where we vary the percentage of uni-modal samples during training are shown in Appendix A.3. The optimal percentages are 10% for in-hospital mortality prediction, and 20% for phenotype classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance Results in the Uni-modal &amp; Multi-modal Settings</head><p>In <ref type="table">Table 1</ref>, we compare our proposed approach to the uni-modal stacked LSTM. As expected, we first observe that the performance of the uni-modal LSTM improves on the EHR PAIRED test set, in terms of AUROC and AUPRC for both tasks, when using the larger EHR PARTIAL training set. Our proposed approach using MedFuse achieves the best performance on the paired test set when the chest X-ray images are used during training and inference as an auxiliary modality (0.770 AUROC and 0.481 AUPRC for phenotype <ref type="table" target="#tab_3">Table 3</ref>: Performance results on the (EHR + CXR) PARTIAL test set. We compare our proposed approach with MedFuse with early and joint fusion when trained with (EHR + CXR) PARTIAL , including samples with missing chest X-ray images (substituted with a learnable vector). All methods were trained with the full (EHR + CXR) PARTIAL training set, except for MedFuse (OPTIMAL) which uses the optimal number of uni-modal samples during fine-tuning. Best results are shown in bold. classification, and 0.865 AUROC and 0.594 AUPRC for in-hospital mortality). We note similar, but less significant trends, in the larger partially paired test set, which may be due to the fact that only 18.8% and 26.2% of samples are paired in the phenotyping and in-hospital mortality test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Results in the Paired Setting</head><p>Since the baseline models were originally designed for paired input, we evaluate all models on the (EHR + CXR) PAIRED test set as shown in <ref type="table" target="#tab_1">Table 2</ref>. First, we observe that early fusion and joint fusion perform comparably across both tasks when trained with (EHR + CXR) PAIRED , with early fusion achieving a slightly better performance in terms of AUROC. We also note that training early fusion using (EHR + CXR) PARTIAL leads to a drop in AUROC and AUPRC across both tasks, while joint fusion only improves for phenotype classification. Second, we observe that the Unified approach by <ref type="bibr" target="#b10">Hayat et al. (2021a)</ref> achieves the best performance amongst all baseline approaches, with 0.765 AU-ROC and 0.461 AUPRC for phenotype classification, and 0.835 AUROC and 0.495 AUPRC for in-hospital mortality prediction. Third, we observe that our proposed approach with MedFuse (OPTIMAL) achieves the best performance across both tasks, with 0.770 AUROC and 0.481 AUPRC for phenotype classification, and 0.865 AUROC and 0.594 AUPRC for in-hospital mortality prediction. We also performed an ablation study where we randomly dropped the chest X-ray modality in the paired test set. The results are shown in Appendix A.4. We also compared the use of substituting the missing modality with zeros or with a learnable vector for early and joint fusion and the results are shown in Appendix A.5. The two techniques perform comparably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance Results in the Partially Paired Setting</head><p>In  <ref type="figure">Figure 4</ref>: Performance results across different subsets of labels for the phenotype classification task in the (EHR + CXR) PAIRED test set. The multi-modal approach with MedFuse achieves the highest AUROC and AUPRC gains for the mixed conditions followed by chronic conditions, compared to the results achieved by the uni-modal stacked LSTM with EHR PAIRED ). task, although early fusion achieves a better AUPRC. Our approach outperforms joint fusion in the in-hospital mortality setting (0.861 compared to 0.841 AUROC and 0.501 compared to 0.482 AUPRC), and performs comparably for phenotype classification. Overall, MedFuse (OPTIMAL), fine-tuned with paired samples and only 10% of uni-modal samples for inhospital mortality prediction and 20% of uni-modal samples for phenotype classification, achieves the best performance (0.768 AUROC and 0.429 AUPRC for phenotype classification and 0.874 AUROC and 0.567 AUPRC for inhospital mortality prediction). We also performed an ablation study where we varied the percentage of uni-modal samples in the partially paired setting. The results are shown in Appendix A.6.</p><p>We also compared the performance of MedFuse to an ensemble consisting of (i) MedFuse for paired samples, and (ii) a uni-modal LSTM for samples with missing chest X-rays. While the results are comparable, as shown in Appendix A.7, the results imply that an ensemble of strong models may be better suited for some tasks, such as phenotyping. This however requires the training of two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Phenotype-wise Analysis</head><p>In <ref type="figure">Figure 4</ref>, we show the AUROC (left) and AUPRC (right) results across different categories of phenotype labels: acute, mixed, and chronic conditions. The label types and their prevalence are listed in <ref type="table">Table 4</ref>. We note that our approach mostly improves the performance in terms of AUROC and AUROC for mixed and chronic conditions, which are generally hard to predict through uni-modal clinical time-series data <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>. In particular, across mixed conditions, the AUROC increases from 0.749 to 0.800, and the AUPRC increases from 0.458 to 0.565. For chronic conditions, the AUROC increases from 0.717 to 0.745 and the AUPRC increases from 0.487 to 0.512. We observe relatively smaller improvements for acute conditions, where the AUROC increases from 0.761 to 0.772 and the AUPRC increases from 0.432 to 0.433. In <ref type="table">Table 4</ref>, we report the performance across all 25 labels for the paired test set using uni-modal and multi-modal data. We observe an improvement across a number of thorax-related phenotypes, such as pneumonia and pleurisy, which are usually clinically assessed using chest imaging <ref type="bibr" target="#b30">(Long et al., 2017)</ref>. This further highlights the importance of using the chest X-ray images as auxiliary information along with the clinical time-series data. <ref type="table">Table 4</ref>: Performance results across the different phenotype labels on (EHR + CXR) PAIRED test set, compared to the uni-modal stacked LSTM with EHR PAIRED . We report the performance results for the individual phenotypes using AUROC and AUPRC, and show the prevalence of labels in the (EHR + CXR) PARTIAL training set, and the (EHR + CXR) PAIRED test set. The labels and results in bold indicate that MedFuse achieved a performance improvement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">In-hospital Mortality Age-wise Analysis</head><p>We evaluate the performance of our approach across different age groups, as shown in <ref type="table" target="#tab_4">Table 5</ref>, and compare it to the uni-modal stacked LSTM. We observe that the AUROC and AUPRC improve across age groups 40-60, 60-80, and &gt;80 years, while the AUROC decreases for the 18-40 years. The latter result needs further investigation with a larger dataset, since the test sets only contain 11 positive samples for the youngest age group. Additionally, there are variations in the relative improvements. For example, the AUPRC increases by 24% for the 40-60 years group, compared to 1.3% in the 60-80 years group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we present a multi-modal fusion approach, named MedFuse, and new benchmark results for integrating partially paired clinical time-series data and chest X-ray images. We evaluate it for two popular benchmark tasks, namely in-hospital mortality prediction and phenotype classification, using publicly available datasets MIMIC-IV and MIMIC-CXR.</p><p>Our study has several strengths. First, our approach is simple and easy to implement. The results show that the proposed approach performs better than the uni-modal LSTM baseline, as it considers chest X-ray images, when available, as an additional source of information. In addition, the approach outperforms several baselines, and the phenotypewise and age-wise analysis provide some insight as to where it improves performance. We conclude that the proposed method is overall a better choice than the baseline methods because (i) the LSTM-based fusion module can inherently deal with missingness (i.e., partially paired data), and (ii) the combination of the architecture and the training procedure provides performance gains. Otherwise, the size of the partially paired training set does not seem to be correlated with the performance improvements, as illustrated with the validation set results in Appendix A.3. The results overall highlight the promise of multi-modal fusion in improving the performance of clinical prediction models. Multi-modal learning is also generally more closely aligned with the decision-making process of clinicians, who consider multiple sources of information when assessing a patient.</p><p>Moreover, in contrast with conventional multi-modal approaches that assume paired input, our proposed method is more flexible since it can process samples with missing chest X-ray images. There is a rising interest in learning cross-modal interactions between modalities during training time and in reconstructing missing modalities <ref type="bibr" target="#b33">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b49">Xin et al., 2021;</ref><ref type="bibr" target="#b31">Ma et al., 2021;</ref><ref type="bibr" target="#b43">Sylvain et al., 2021;</ref><ref type="bibr" target="#b31">Ma et al., 2021)</ref>. In contrast with natural multi-modal datasets, assuming a high degree of correlation in such settings is not a trivial task in healthcare especially when the modalities do not necessarily share the same labels, and this is an area of future work. The difficulty stems from the sparse and asynchronous nature of medical data, i.e. it would be difficult to use a biopsy report for skin tissue to reconstruct common thorax diseases features <ref type="bibr" target="#b10">(Hayat et al., 2021a)</ref>. Additionally, some of the existing work for learning cross-modal interactions assumes the presence of all modalities during training <ref type="bibr" target="#b43">(Sylvain et al., 2021)</ref>.</p><p>Another strength is that the approach can be easily scaled to more than two modalities with no amendments to the fusion loss function, compared to existing work where the complexity of the computation increases with the number of modalities <ref type="bibr" target="#b10">(Hayat et al., 2021a)</ref>. However, this requires evaluation and is an area of future work. We also do not assume any correlation among the input modalities, in terms of information content or assigned labels.</p><p>Furthermore, we formalize and introduce new benchmark results for two popular tasks that are typically evaluated in the context of clinical time-series data only <ref type="bibr" target="#b8">(Harutyunyan et al., 2019)</ref>. By gaining access to the MIMIC-IV and MIMIC-CXR datasets <ref type="bibr" target="#b21">(Johnson et al., 2021</ref><ref type="bibr" target="#b22">(Johnson et al., , 2019</ref>, researchers can utilize our open-access data pre-processing pipeline and introduce new results for direct comparison.</p><p>Limitations. The study also has its own limitations. To begin with, we focus on tasks pertaining to the integration of clinical time-series data and chest X-ray images from a single data source, and we evaluate our work on two benchmark tasks due to limited resources.</p><p>The original work by <ref type="bibr" target="#b8">Harutyunyan et al. (2019)</ref> includes two other tasks, decompensation prediction and length of stay prediction, which we would like to evaluate our method on in the future. We also do not run any experiments on settings where the clinical time-series data may be missing, but the chest X-ray image is available. In future work, this requires the definition of additional benchmark tasks where the chest X-ray image is the primary modality. Since we currently evaluate our method with two input modalities only, another interesting next step would be to use more than two to further evaluate the robustness of the model, considering its scalability. In its current formulation, the model also lacks interpretability, since we mainly focus on fusion within the scope of this paper. We later plan to explore incorporating attention layers <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> at the input level of the feature encoders to evaluate the importance of features within each modality, and within the fusion module to evaluate the overall informativeness of each modality. On a related note, our work can benefit from performing instance-level analysis. However, this requires clinical expertise that bridges between chest X-ray image and clinical time-series analysis, which we are currently missing. To realize the full potential of multi-modal learning, there is more work to be done to understand the clinical underpinnings of multi-modal fusion. Overall, the study highlights an extremely worthwhile direction to further leverage the value of multi-modal learning in healthcare, especially as the diversity and quantity of medical data continues to increase. Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Image Augmentations</head><p>For the chest X-ray images, we apply a series of transformations during pre-training and fine-tuning across all experiments and tasks. Specifically, we resize each image to 256 ? 256 pixels, randomly apply a horizontal flip, and apply a set of random affine transformations, such as rotation, scaling, shearing, and translation. We then apply a random crop to obtain an image of size 224 ? 224 pixels. During validation and testing, we perform image resizing to 256 ? 256 and apply a center crop to 224 ? 224 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyperparameter Search Results</head><p>The results of hyperparameter tuning are shown in <ref type="table">Table A1</ref>. We summarize the learning rates that achieved the best performance for each model. <ref type="table">Table A1</ref>: Learning rates that achieved the best results during hyperparameter search.</p><p>We conducted 10 runs for each model with randomly sampled learning rates between 10 ?5 and 10 ?3 . For MMTM and DAFT, we additionally selected the version that achieved the best validation set AUROC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Phenotyping 7.347 ? 10 ?5 1.452 ? 10 ?5 * We trained two versions of MMTM for each task, where the MMTM module is placed after the third or fourth ResNet layer. Placing it after the fourth layer achieved the best performance for both tasks. * * We trained two versions of DAFT for each task, where we transform the LSTM representation either after the third or fourth ResNet layer. Placing it after the third layer achieved the best performance for phenotype classification, whereas placing it after the fourth layer achieved the best performance for in-hospital mortality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Percentage of Uni-modal Samples within the Training Set</head><p>We also run experiments where we vary the percentage of uni-modal samples during finetuning. The best AUROC results for both tasks on the validation set are shown in <ref type="figure" target="#fig_3">Figure A1</ref>. For in-hospital mortality (shown in red), we notice that a relatively smaller portion of uni-modal samples (10%) achieves the best performance. For patient phenotyping (shown in blue), we observe a similar trend where the best AUROC is achieved with only 20% of uni-modal samples. We fix the sampling percentage that achieves the best validation AUROC across all experiments, unless noted otherwise. Hence, this highlights that the best performance gains of MedFuse are achieved even with a small percentage of uni-modal samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Percentage of Uni-modal Samples within the Paired Test Set</head><p>We also performed an ablation study where we randomly dropped the chest X-ray modality for a percentage of samples in the paired test set. The results are shown in <ref type="figure">Figure A2</ref>. We observe that the as the percentage of dropping increases, the AUROC decreases for both tasks.  <ref type="figure">Figure A2</ref>: Performance on the test set with randomly dropped CXR modality in the paired test set. The plot shows the AUROC on the paired test set for different percentages of randomly dropped CXR modality from paired test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Missing Modality with Early and Joint Fusion</head><p>We also ran initial experiments to compare the learnable vector with imputing zeros for a missing chest X-ray modality. The results are shown in <ref type="table" target="#tab_1">Table A2</ref>. We note that the results are comparable with no obvious differences. <ref type="table" target="#tab_1">Table A2</ref>: Missing modality with early and joint fusion. We report the AUROC and AUPRC results on the entire test set (EHR PARTIAL ), including samples with missing chest X-ray images (substituted with a zeros or a learnable vector) All methods below were pre-trained using the (EHR PARTIAL ) training set and a fixed learning rate of 0.0001. We performed another ablation study where we varied the number of uni-modal samples in the partially paired test set. The results are shown in <ref type="figure">Figure A3</ref>. Hence, including 0% of uni-modal test samples is equivalent to the fully paired test set. We observe an increase in the AUROC in the in-hospital mortality task, as the percentage of uni-modal samples increase, but a more a consistent AUROC in the phenotyping task. We do however observe that the widths of the confidence intervals decrease as the percentage of uni-modal samples increases across both tasks.  <ref type="figure">Figure A3</ref>: Performance on the test set when varying the percentage of uni-modal samples. The plot shows the AUROC on the partial test set for different percentages of randomly selected uni-modal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Ensemble of uni-modal and multi-modal models</head><p>We ran another experiment to compare the performance of MedFuse to that of an ensemble of two models: an EHR only model that computes predictions for partial input (i.e., not associated with a chest X-ray) using LSTM, and a paired model that computes predictions for paired input using MedFuse. The results are shown in <ref type="table" target="#tab_3">Table A3</ref>. We observe that the ensemble slightly outperforms MedFuse for phenotyping only. This implies that an ensemble of strong models may be better suited for some tasks, such as phenotyping, which however requires the training of two models. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>? Multi-modal Transfer Module (MMTM): Originally proposed by Vaezi Joze et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>+ CXR) PAIRED 0.747 (0.720, 0.773) 0.446 (0.404, 0.493) 0.825 (0.798, 0.853) 0.506 (0.436, 0.574) Joint (EHR + CXR) PARTIAL 0.754 (0.727, 0.780) 0.458 (0.415, 0.506) 0.819 (0.785, 0.850) 0.479 (0.413, 0.552) MMTM (Vaezi Joze et al., 2020) 0.734 (0.707, 0.761) 0.428 (0.387, 0.476) 0.819 (0.788, 0.846) 0.474 (0.402, 0.544) DAFT (P?lsterl et al., 2021) 0.737 (0.710, 0.764) 0.434 (0.393, 0.482) 0.828 (0.799, 0.854) 0.492 (0.427, 0.572) Unified (Hayat et al., 2021a) 0.765 (0.742, 0.794) 0.461 (0.417, 0.511) 0.835 (0.808, 0.861) 0.495 (0.424, 0.567) MedFuse (RI) 0.748 (0.721, 0.774) 0.452 (0.408, 0.501) 0.817 (0.785, 0.846) 0.471 (0.404, 0.545) MedFuse (PT) 0.756 (0.729, 0.782) 0.466 (0.420, 0.515) 0.841 (0.813, 0.868) 0.544 (0.477, 0.609) MedFuse (OPTIMAL) 0.770 (0.745, 0.795) 0.481 (0.436, 0.531) 0.865 (0.837, 0.889) 0.594 (0.526, 0.655)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>640, 0.687) 0.552 (0.517, 0.590) 0.708 (0.686, 0.730) 0.581 (0.543, 0.618) Chronic kidney disease chronic 0.206 0.240 0.748 (0.727, 0.771) 0.457 (0.419, 0.505) 0.768 (0.747, 0.789) 0.485 (0.445, 0.533) Chronic obstructive pulmonary disease chronic 0.143 0.148 0.673 (0.640, 0.703) 0.272 (0.231, 0.319) 0.747 (0.721, 0.776) 0.344 (0.302, 0.398) Complications of surgical/medical care acute 0.189 0.226 0.728 (0.703, 0.752) 0.464 (0.420, 0.513) 0.722 (0.698, 0.747) 0.439 (0.395, 0.487) Conduction disorders mixed 0.100 0.115 0.719 (0.688, 0.750) 0.252 (0.210, 0.304) 0.854 (0.822, 0.882) 0.632 (0.570, 0.692) Congestive heart failure; nonhypertensive mixed 0.255 0.295 0.760 (0.738, 0.781) 0.592 (0.553, 0.632) 0.823 (0.805, 0.843) 0.679 (0.643, 0.715) Coronary atherosclerosis and related chronic 0.311 0.337 0.740 (0.719, 0.763) 0.603 (0.563, 0.643) 0.779 (0.760, 0.799) 0.631 (0.593, 0.668) Diabetes mellitus with complications mixed 0.114 0.120 0.885 (0.866, 0.902) 0.534 (0.473, 0.596) 0.883 (0.862, 0.902) 0.534 (0.473, 0.599) Diabetes mellitus without complication chronic 0.172 0.211 0.758 (0.731, 0.781) 0.430 (0.386, 0.481) 0.748 (0.724, 0.772) 0.414 (0.370, 0.463) Disorders of lipid metabolism chronic 0.404 0.406 0.689 (0.666, 0.713) 0.598 (0.562, 0.635) 0.707 (0.685, 0.729) 0.613 (0.577, 0.649) Essential hypertension chronic 0.418 0.433 0.678 (0.655, 0.699) 0.617 (0.583, 0.650) 0.703 (0.682, 0.725) 0.634 (0.600, 0.667) Fluid and electrolyte disorders acute 0.371 0.454 0.737 (0.716, 0.757) 0.696 (0.666, 0.727) 0.733 (0.713, 0.754) 0.687 (0.657, 0.720) Gastrointestinal hemorrhage acute 0.070 0.071 0.751 (0.712, 0.785) 0.194 (0.145, 0.254) 0.747 (0.708, 0.783) 0.221 (0.165, 0.287) Hypertension with complications chronic 0.215 0.222 0.736 (0.714, 0.758) 0.430 (0.391, 0.475) 0.764 (0.742, 0.786) 0.465 (0.421, 0.511) Other liver diseases mixed 0.125 0.169 0.716 (0.687, 0.743) 0.359 (0.313, 0.409) 0.730 (0.704, 0.759) 0.398 (0.353, 0720, 0.772) 0.453 (0.409, 0.502) 0.770 (0.745, 0.795) 0.481 (0.436, 0.531)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A1 :</head><label>A1</label><figDesc>Performance on the validation set when varying the sampling percentage for uni-modal training samples. The plot shows the AUROC on the validation set for different percentages of randomly selected uni-modal training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance results on the (EHR + CXR) PAIRED test set. We show the AUROC</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>, we evaluate our proposed approach with MedFuse as well as early and joint fusion on the partially paired test set. Compared to early fusion, our proposed approach trained with the full (EHR + CXR) PARTIAL training set achieves a better performance for phenotype classification (0.758 compared to 0.748 AUROC and 0.418 compared to 0.394 AUPRC). It performs comparably with early fusion in the in-hospital mortality prediction</figDesc><table><row><cell>AUROC</cell><cell>0.70 0.75 0.80 0.85</cell><cell></cell><cell></cell><cell>Uni-modal Multi-modal</cell><cell>AUPRC</cell><cell>0.40 0.60 0.55 0.45 0.50</cell><cell></cell></row><row><cell></cell><cell>0.65</cell><cell>Acute</cell><cell>Mixed</cell><cell>Chronic</cell><cell></cell><cell>0.35</cell><cell>Acute</cell><cell>Mixed</cell><cell>Chronic</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance of MedFuse across different age groups for in-hospital-mortality on the (EHR + CXR) PAIRED ) test set, compared to the uni-modal stacked LSTM with EHR PAIRED . We compare the AUROC and AUPRC for the different age groups. The results in bold indicate improved performance with multi-modal data.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">EHR PAIRED</cell><cell cols="2">(EHR+CXR) PAIRED</cell></row><row><cell cols="2">Age group Positive fraction</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell>18-40</cell><cell>0.078 (11/141)</cell><cell cols="4">0.941 (0.859, 0.988) 0.613 (0.332, 0.883) 0.917 (0.820, 0.980) 0.521 (0.272, 0.841)</cell></row><row><cell>40-60</cell><cell>0.119 (44/369)</cell><cell cols="4">0.796 (0.719, 0.865) 0.403 (0.277, 0.541) 0.822 (0.751, 0.885) 0.499 (0.360, 0.629)</cell></row><row><cell>60-80</cell><cell>0.159 (98/616)</cell><cell cols="4">0.846 (0.805, 0.885) 0.576 (0.478, 0.670) 0.868 (0.830, 0.900) 0.583 (0.484, 0.678)</cell></row><row><cell>&gt; 80</cell><cell>0.227 (56/247)</cell><cell cols="4">0.789 (0.722, 0.850) 0.549 (0.428, 0.677) 0.841 (0.784, 0.891) 0.616 (0.491, 0.731)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A3 :</head><label>A3</label><figDesc>MedFuse compared to an ensemble evaluation. We report the AUROC and AUPRC results on the partially paired test set.</figDesc><table><row><cell>Task</cell><cell cols="2">Phenotyping</cell><cell cols="2">In-hospital mortality</cell></row><row><cell>Method</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell cols="5">Ensemble 0.770 (0.759, 0.782) 0.431 (0.410, 0.454) 0.870 (0.857, 0.884) 0.547 (0.509, 0.589)</cell></row><row><cell>MedFuse</cell><cell cols="4">0.768 (0.756, 0.779) 0.429 (0.408, 0.452) 0.874 (0.860, 0.888) 0.567 (0.529, 0.607)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022 N. Hayat, K.J. Geras &amp; F.E. Shamout.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported in part by the NYUAD Center for Artificial Intelligence and Robotics, funded by Tamkeen under the NYUAD Research Institute Award CG010. We would also like to thank the High Performance Computing (HPC) team at NYUAD for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.cms.gov/Medicare/Coding/ICD10/2018-ICD-10-CM-and-GEMs" />
	</analytic>
	<monogr>
		<title level="j">Centers for Medicare &amp; Medicaid Services</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Man?</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big data in healthcare: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiba</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajar</forename><surname>Mousannif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Al</forename><surname>Moatassime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Noel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Cloud Technologies and Applications (CloudTech)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end multi-modal video temporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning classification of cardiomegaly using combined imaging and non-imaging icu data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declan</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bart Lomiej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Papie?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Understanding and Analysis</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greg Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0103-9</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards dynamic multi-modal phenotyping using chest radiographs and physiological data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><forename type="middle">E</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shamout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-label generalized zero shot learning for the classification of disease in chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Lashen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><forename type="middle">E</forename><surname>Shamout</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Machine Learning for Healthcare Conference</title>
		<meeting>the 6th Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal medical image fusion review: Theoretical background and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haithem</forename><surname>Hermessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olfa</forename><surname>Mourali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezzeddine</forename><surname>Zagrouba</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sigpro.2021.108036</idno>
		<idno>0165-1684. doi</idno>
		<ptr target="https://doi.org/10.1016/j.sigpro.2021.108036" />
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">108036</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predictive models for colorectal cancer recurrence using multi-modal healthcare data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danliang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehul</forename><surname>Motani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning. 2021</title>
		<meeting>the Conference on Health, Inference, and Learning. 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Seyyedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imon</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-00341-z</idno>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">136</biblScope>
		</imprint>
	</monogr>
	<note>npj Digital Medicine</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health record: a case-study in pulmonary embolism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roham</forename><surname>Zamanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imon</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-78888-w</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">22147</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviana</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Medical image fusion: A survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Pappachen James</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belur</forename><forename type="middle">V</forename><surname>Dasarathy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2013.12.002</idno>
		<idno>1566-2535. doi</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2013.12.002" />
	</analytic>
	<monogr>
		<title level="m">Special Issue on Information Fusion in Medical Image Computing and Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prognostication of patients with covid-19 using artificial intelligence based on chest x-rays and clinical data: a retrospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Whae</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasey</forename><surname>Halsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><forename type="middle">My</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongcui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feyisope</forename><surname>Eweje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">T</forename><surname>Delworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">T</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Atalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><forename type="middle">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1016/S2589-7500(21)00039-X</idno>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="286" to="294" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Bulgarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.13026/s6n6-xd98</idno>
		<ptr target="https://doi.org/10.13026/s6n6-xd98." />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>mimic-iv&quot; (version 1.0). physionet</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mimic-cxr-jpg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">R</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ying Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>a large publicly available database of labeled chest radiographs</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Miracle: Causallyaware imputation via learning missing data mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trent</forename><surname>Kyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mildint: Deep learning-based multimodal longitudinal data integration framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwangsik</forename><surname>Nho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokyoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3389/fgene.2019.00617</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">617</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparison of pre-trained vision-and-language models for multimodal representation learning across medical images and reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="1999" to="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Medical image fusion method by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Li</surname></persName>
		</author>
		<idno>2666- 3074</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Cognitive Computing in Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I S?nchez. A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Thoracic Imaging: Basic to Advanced</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Lohan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-13-2544-17</idno>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="173" to="194" />
		</imprint>
	</monogr>
	<note>Imaging of icu patients</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emergency medicine evaluation of communityacquired pneumonia: history, examination, imaging and laboratory assessment, and risk scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brit</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Koyfman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of emergency medicine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Smil: Multimodal learning with severely missing modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized zero-shot chest x-ray diagnosis through trait-guided multi-view semantic embedding with self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angshuman</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2021.3054817</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>P?lsterl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Nuno</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wachinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-366</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A multimodal deep learning model for cardiac resynchronisation therapy response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Puyol-Ant?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baldeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Sidhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew P</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10662</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An artificial intelligence system for predicting the deterioration of covid-19 patients in the emergency department</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><forename type="middle">E</forename><surname>Shamout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakash</forename><surname>Kaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis</forename><surname>Law Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Witowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Dogra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kudlowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Azour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><forename type="middle">W</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yindalon</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-021-00453-0</idno>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task multi-modal learning for joint diagnosis and prognosis of human cancers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101795</idno>
		<idno>1361- 8415</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101795" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101795</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Medfusenet: an attentionbased multimodal deep learning model for visual question answering in the medical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<idno>2045-2322. doi: 10.1038/ s41598-021-98390-1</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">James</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azra</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1604" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards Automated Diagnosis with Attentive Multimodal Learning Using Electronic Health Records and Chest X-Rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sonsbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Worring</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60946-711</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="106" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cmim: Cross-modal information maximization for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Berthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Di</forename><surname>Jorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaux</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1190" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Hamid Reza Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multisurv: Long-term cancer survival prediction using multimodal deep learning. medRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Vale-Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohr</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.08.06.20169698</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">International classification of diseases ninth revision (icd-9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Who</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="343" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prediction of molecular subtypes of breast cancer using bi-rads features based on a &quot;white box&quot; machine learning approach in a multi-modal imaging setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoling</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzhou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejrad.2019.03.015</idno>
	</analytic>
	<monogr>
		<title level="j">European Journal of Radiology</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interpretation on deep multimodal fusion for diagnostic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Bowen Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN52387.2021.9534148</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for cervical dysplasia diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multimodal transformer with multi-view visual representation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4467" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Merlot: Multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predicting molecular subtypes of breast cancer using multimodal deep learning and incorporation of the attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Beets-Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritse</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Explainable semantic space by grounding language to vision with cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
