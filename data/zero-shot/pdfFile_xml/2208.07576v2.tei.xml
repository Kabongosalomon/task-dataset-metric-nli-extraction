<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Discovery via Contrastive Learning for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Bae</surname></persName>
							<email>whbae@cs.ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of British</orgName>
								<address>
									<settlement>Columbia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of British</orgName>
								<address>
									<settlement>Columbia</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alberta Machine Intelligence Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyug</forename><forename type="middle">Noh</forename><surname>4?</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Lawrence Livermore National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
							<email>dkim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object Discovery via Contrastive Learning for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Weakly Supervised Object Detection (WSOD)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly Supervised Object Detection (WSOD) is a task that detects objects in an image using a model trained only on image-level annotations. Current state-of-the-art models benefit from self-supervised instance-level supervision, but since weak supervision does not include count or location information, the most common "argmax" labeling method often ignores many instances of objects. To alleviate this issue, we propose a novel multiple instance labeling method called object discovery. We further introduce a new contrastive loss under weak supervision where no instance-level information is available for sampling, called weakly supervised contrastive loss (WSCL). WSCL aims to construct a credible similarity threshold for object discovery by leveraging consistent features for embedding vectors in the same class. As a result, we achieve new state-of-the-art results on MS-COCO 2014 and 2017 as well as PASCAL VOC 2012, and competitive results on PASCAL VOC 2007. The code is available at https://github.com/jinhseo/OD-WSCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection <ref type="bibr">[22,</ref><ref type="bibr">19,</ref><ref type="bibr">21,</ref><ref type="bibr">18]</ref> has seen huge improvements since the introduction of deep neural networks and large-scale datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. It is, however, very expensive and time-consuming to annotate large datasets with fine-grained object bounding boxes. Recent work has thus attempted to use more cost-efficient annotations in an approach called Weakly Supervised Object Detection (WSOD), such as image, point, or "scribble" labels.</p><p>Although WSOD methods can be trained with much less annotation effort, however, the resulting models still perform far below their fully-supervised counterparts. We identify three categories of reasons for this deterioration, summarized in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. First, part domination is when WSOD models focus only on   <ref type="bibr" target="#b6">[7]</ref> and MS-COCO <ref type="bibr" target="#b16">[17]</ref> due to the argmax-based method.</p><p>the discriminative part of an object, perhaps caused by the fundamentally illposed nature of framing WSOD as a Multiple Instance Learning (MIL) problem <ref type="bibr" target="#b5">[6]</ref> prone to local minima, as done by much previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">31,</ref><ref type="bibr">34]</ref>. The second major issue of WSOD is grouped instances, where neighbouring instances of objects in the same category are grouped into one large proposal, rather than proposed separately. As image-level annotations reveal only the presence of each object class, without any information about object location or counts (see <ref type="figure" target="#fig_1">Fig. 1(b)</ref>), it has become conventional to take only the single highest-score proposal as a "pseudo groundtruth" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b11">12]</ref>. This can help avoid false positives, but often causes missing objects, where less-obvious instances are ignored.</p><p>Current argmax-based algorithms for finding pseudo groundtruths turn out to be problematic even on extremely popular benchmark datasets. Labeling only one proposal per category misses 40% of labels on PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> (selecting 7,306 of 12,608 target objects on VOC07), and 60% on MS-COCO <ref type="bibr" target="#b16">[17]</ref> (533,396 of 894,204 objects on COCO14). Similar patterns hold for VOC12 and COCO17 (see <ref type="figure" target="#fig_1">Fig. 1(c)</ref>). Object detection models trained with this limited and, indeed, potentially confusing supervision are substantially hindered, and mining more pseudo-labels is problematic since they are likely to be false positives.</p><p>We introduce a novel multiple instance labeling method which addresses the limitations of current labeling methods in WSOD. Our proposed object discovery module explores all proposed candidates using a similarity measure to the highest-scoring representation. We further suggest a weakly supervised contrastive loss (WSCL) to set a reliable similarity threshold. WSCL encourages a model to learn similar features for objects in the same class, and to learn discriminative features for objects in different classes. To make sure the model learn appropriate features, we provide a large number of positive and negative instances for WSCL through three feature augmentation methods suitable for WSOD. This well-behaved embedding space allows the object discovery module to find more reliable pseudo groundtruths. The resulting model then detects less-discriminative parts of target objects, misses fewer objects, and better distinguishes neighbouring object instances, as we will demonstrate experimentally in Section 5.1. As a result, the proposed approach beats state-of-the-art WSOD performance on both MS-COCO and PASCAL VOC by significant margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weakly Supervised Object Detection</head><p>Bilen et al . <ref type="bibr" target="#b1">[2]</ref> introduced the first MIL-based end-to-end WSOD approach, known as WSDDN, which includes both classification and detection streams. Based on the MIL-based method, later works in WSOD have attempted to generate instance-level pseudo groundtruths in various ways. Self-Supervised Pseudo Labeling Approach. To use instance-level supervision, Tang et al . <ref type="bibr">[31]</ref> suggest Online Instance Classifier Refinement (OICR), which alternates between training instance classifier and selecting the most representative candidates. The online classifier rectifies initially detected instances through multiple stages and updates instance-level supervision determined by spatial relations. Tang et al .</p><p>[30] expand clusters from OICR to include adjacent proposals that belong to the same cluster. Kosugi et al . <ref type="bibr" target="#b14">[15]</ref> devise an instance labeling method to find positive instances based on a context classification loss, and to avoid negative instances using spatial constraints. Zeng et al . <ref type="bibr">[38]</ref> show that the bottom-up evidence, unlike top-down class confidence, helps to recognize class-agnostic object boundaries. Chen et al . <ref type="bibr" target="#b3">[4]</ref> propose a spatial likelihood voting (SLV) system to vote the bounding boxes with the highest likelihood in spatial dimension. Huang et al . <ref type="bibr" target="#b11">[12]</ref> propose Comprehensive Attention Self-Distillation (CASD) that learns a balanced representation via input-wise and layer-wise feature learning. CASD aggregate attention maps, generated by multiple transformations and extracted from different levels of feature maps. Multiple Instance Approach. Previous works focus on selecting valid pseudo groundtruths based on location information, but most still rely on the argmax labeling method which considers only one instance. Some attempts have been made, however, to provide multiple pseudo groundtruths. C-WSL <ref type="bibr" target="#b7">[8]</ref> uses perclass object count annotations, which can help effectively separate grouped instances. OIM <ref type="bibr" target="#b15">[16]</ref> exploits an object mining method to find undiscovered objects by calculating Euclidean distance between the core instance and its surrounding boxes. Ren et al . <ref type="bibr">[23]</ref> propose not only Multiple Instance Self-Training (MIST) to generate top-k scored proposals as pseudo groundtruths, but also parametric dropblock to adversarially drop out discriminative parts. Yin et al . <ref type="bibr">[37]</ref> introduce feature bank to provide one more pseudo groundtruth using the top-similarity scored instance. These algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">23,</ref><ref type="bibr">37]</ref> effectively find multiple instances per class, but their methods largely depend on heuristics, rather than learning.</p><p>Our proposed method instead explores all possible pseudo groundtruths in a more reliable way, with learning guided by a contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning</head><p>Contrastive losses have been successful in unsupervised and self-supervised learning for image classification tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. For object detection tasks, Xie et al . <ref type="bibr">[35]</ref> and Sun et al .</p><p>[28] demonstrate learning good embedding features via contrastive learning successfully improves the generalization ability of an object detector. One important factor of this success is to use effective mining strategies for positive and negative samples, which accelerates convergence and enhances the generalization ability of a model. In image classification tasks, these sample pairs are usually identified either by class labels <ref type="bibr" target="#b13">[14]</ref> if available, or pairing images with versions that have been randomly altered with methods such as cropping, color distortion, or Gaussian blur <ref type="bibr" target="#b2">[3]</ref>. Schroff et al .</p><p>[25] introduce a hard positive and negative mining strategy based on the distance between anchor and positive samples, with full supervision. However, it is difficult to mine positive and negative samples in WSOD setting, which assumes no instance-level labels are available. Therefore, we propose feature augmentations to sample positives and negatives for contrastive learning in the WSOD setting. To the best of our knowledge, our method is the first approach to incorporate contrastive learning into WSOD tasks. Our proposed weakly supervised contrastive loss guides a model to learn consistent feature representations for objects in the same class and discriminative representations for ones in different classes, through mining positive and negative samples and augmenting intermediate features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>As with most state-of-the-art WSOD models, our approach is also based on the MIL head of WSDDN <ref type="bibr" target="#b1">[2]</ref>, followed by the refinement head suggested by OICR <ref type="bibr">[31]</ref>. In this section, we describe how MIL and refinement heads work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extractor</head><p>Let a batch B = {I n , R n , Y n } N n=1 contain an image I n , proposals R n = {r 1 , . . . , r M n } with M n proposals for the image I n , and image-level labels Y n = [y n 1 , ..., y n C ] ? {0, 1} C where C is the number of classes. Given an image I n , a feature extractor generates features for downstream tasks as follows. A backbone network takes a given image as input and outputs a feature map, from which a Region of Interest (RoI) feature map f n ? R D?H?W ?M n is generated through an RoI pooling layer. Two fully-connected (FC) layers, which we denote ?(?), map f n to RoI feature vectors v n ? R D ? ?M n . To alleviate part domination, we also randomly mask out some blocks of the RoI map with Dropblock [23], generatingf n ; we then generate regularized feature vectors? n = ?(f n ). The MIL and refinement heads operate on? n . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple Instance Learning Head</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>(a), Multiple Instance Learning (MIL) head consists of classification and detection networks which take RoI feature vectors? n as input, and return classification scores X n cls ? R C?M n and detection scores X n det ? R C?M n . Here, the X n cls are computed by a softmax operation along the classes (rows), whereas X n det are computed along the regions (columns). Proposal scores X n ? R C?M n are the element-wise product of classification and detection scores: X n = X n cls ? X n det . The image score of the c-th class, ? n c , is obtained by the sum of proposal scores over all regions: ? n c = M n m=1 X n c,m . Given an image-level label Y n and image score ? n c , the multi-label classification loss L mil is</p><formula xml:id="formula_0">L mil = ? 1 N N n=1 C c=1 y n c log ? n c + (1 ? y n c ) log(1 ? ? n c ).<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Refinement Head</head><p>The goal of the refinement head is to integrate self-supervised training strategy into WSOD via instance-level supervision. At the k-th stage (k ? {1, ..., K}), an instance classifier generates proposal scores X n,k ? R (C+1)?M n where M n is the number of proposals and C + 1 adds a background class to the C classes. Instance-level supervision at the k-th stage is determined by previous stage; in particular, the first instance classifier takes supervision from the output of MIL head. Instance-level pseudo labels for the n-th image Y n,k ? R (C+1)?M n are then set to 1 if the corresponding proposal sufficiently overlaps the highest scored proposal, otherwise 0 as defined in <ref type="bibr" target="#b1">(2)</ref>. </p><p>Finally, the instance classification loss L cls is defined as</p><formula xml:id="formula_2">L cls = ? 1 N N n=1 1 K K k=1 1 M n M n m=1 C+1 c=1 w n,k m y n,k c,m log x n,k c,m<label>(3)</label></formula><p>where x n,k c,m denotes m-th proposal score of a class c at k-th stage, w n,k m denotes a loss weight defined as w n,k m = x n,(k?1) c,m n,k c following OICR [31], and K is the total number of refinement stages.</p><p>In addition to instance classification, some work [38,36] has improved localization performance by adding a bounding box regression loss. GivenM n pseudo groundtruth bounding boxes? n,k m , nearby predicted bounding boxes g n,k m , matched as in <ref type="bibr" target="#b1">(2)</ref>, are encouraged to align using a smooth L1 regression loss:</p><formula xml:id="formula_3">L reg = ? 1 N N n=1 1 K K k=1 1 M nM n m=1</formula><p>w n,k m smooth L1 (g n,k m ,? n,k m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>Most prior WSOD works <ref type="bibr">[31,</ref><ref type="bibr" target="#b11">12]</ref> consider only top-scoring proposals, as described in <ref type="bibr" target="#b1">(2)</ref>. This strategy, however, has significant challenges in achieving our goal of detecting all objects present. To alleviate this issue, we propose a novel approach called object discovery which secures reliable pseudo groundtruths, by transferring instance-level supervision from previous to the next stage as illustrated in Section 3.3 and <ref type="figure" target="#fig_2">Fig. 2</ref>(c). To further enhance the object discovery module, we also introduce a new similarity head which maps RoI feature vectors to an embedding space, guided by a novel weakly supervised contrastive loss (WSCL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity Head</head><p>Parallel to the MIL and refinement heads, we construct a similarity head ?(?) that takes augmented RoI feature vectors as input described in <ref type="figure" target="#fig_2">Fig. 2</ref>. We will explain how RoI features are augmented in Section 4.2. The similarity head consists of two FC layers which map the inputs to a 128-dimensional space, followed by a normalization step. Thus, the outputs of the similarity head are expressed as z n = ?(v n ) ? R 128?M n where ||z n m || 2 = 1. Note that the similarity head uses v n , whereas MIL and refinement head use the region-dropped? n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling Strategy for Object Discovery</head><p>Contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>, in general, focuses on making "positive" and "negative" pairs have similar and different feature embeddings, respectively. Although it is possible to augment images and pass each to the backbone to obtain pair of samples from RoI features in WSOD setting where no instance-level supervisions are available, it is computationally inefficient: most of the features from the backbone are not used as RoI features. Instead of augmenting images, we propose three feature augmentation methods to generate views of samples, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>(a): IoU sampling, random masking, and adding gaussian noise. IoU Sampling. The purpose of IoU sampling is to increase the number of samples by treating the proposals adjacent to the top-scoring proposalm n c in (2) as positives. The proposals that overlap more than a threshold ? IoU with the top-scoring proposal at each stage k are considered positive samples, and the corresponding embedding vectors are formulated as</p><formula xml:id="formula_5">M n,k c = {m | IoU (r m , rmn,k c ) &gt; ? IoU , m = 1, 2, ..., M n } Z n,c IoU = {?(?(f n m )) | m ? K?1 k=0 M n,k c }<label>(5)</label></formula><p>where M n denotes the total number of proposals in n-th image, ?(?) is the extractor of RoI feature vectors, and ?(?) denotes the similarity head. Random Masking. Random masking randomly drops some regions across all channels of a RoI feature map. We first generate a random map D : D i,j ? U (0, 1) ? R H?W . Then the binary mask D drop is determined by drop threshold ? drop , so if D &lt; ? drop , D drop is set to be 0, otherwise 1. Finally, a randomlymasked feature is obtained by taking spatial-wise multiplication of a RoI feature map f n m with D drop followed by the similarity head,</p><formula xml:id="formula_6">Z n,c mask = {?(?(f n m ? D drop )) | m ? K?1 k=0 M n,k c }.<label>(6)</label></formula><p>Here, random masking is applied to the all proposals from IoU sampling at all stages, K k=1 M n,k c , to obtain more positive samples. Adding Gaussian Noise. To add Gaussian random noise to RoI feature maps, we create a random noise map D noise : D i,j ? N (0, 1) ? R H?W . We add this to the RoI feature maps f n m by</p><formula xml:id="formula_7">Z n,c noise = {?(?(f n m + f n m ? D noise )) | m ? K?1 k=0 M n,k c }.<label>(7)</label></formula><p>Cross-Image Representations. Finally, we gather the augmented embedding vectors corresponding to the same object category from different images, as described in <ref type="figure" target="#fig_4">Fig. 3</ref>(b). We treat cross-batch representations from the same categories as positive examples in the mini-batch,  </p><formula xml:id="formula_8">S c = N n=1 (Z n,c IoU ? Z n,c mask ? Z n,c noise ).<label>(8)</label></formula><formula xml:id="formula_9">? car ? person ? dog ? (a) ? Batch images with initial outputs (ii) (?) ?(?) (ii) Random Masking (iii) Gaussian Noise Image ( ) Feature Map RoI Features (i) IoU Sampling , , ,<label>(iii)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Discovery</head><p>Using the augmented RoI features introduced in the previous section, we discover many reliable instance-level pseudo groundtruths missed by previous methods that take only the top-scoring proposals. Intuitively, even though the classification score of a proposal may be low, if its embedding vector is close to that of the top-scoring proposal, it is likely that the proposal shares the same class as the top-scoring proposal. Therefore, instead of solely relying on classification scores, we exploit similarity scores between the embedding vectors of all proposals and the top-scoring (argmax) proposal at each stage k, to discover additional pseudo grountruths as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>(c). To mine new pseudo groundtruths, we first compute a threshold ? n,k c that determines whether to label a proposal as a pseudo groundtruth for a class c at stage k:</p><formula xml:id="formula_10">? n,k c = 1 |S c | |Sc| i=1 sim(z n m n,k c , S c,i ),<label>(9)</label></formula><p>where z n m n,k c denotes embedding vectors of top-scoring proposals at stage k, S c,i denotes i-th element of S c , and sim(?, ?) is a dot product between inputs. Then, new pseudo groundtruth candidates? n,k c are determined as the ones having higher similarity to the top-scoring proposal than similarity threshold ? n,k c ,</p><formula xml:id="formula_11">M n,k c = {m | sim(z n m , z n m n,k c ) &gt; ? n,k c , m = 1, 2, ..., M n }.<label>(10)</label></formula><p>Finally, we add new pseudo groundtruths denoted asM n,k c at stage k after applying Non-Maximum Suppression (NMS) [24] to? n,k c . Consequently, we update instance-level supervision and embedding vectors for newly discovered pseudo groundtruthsM n,k c . We re-label instance-level supervision {y n,k c,m |m ?M n,k c } and its adjacent proposals as described in <ref type="bibr" target="#b1">(2)</ref>. New embedding vectors {z n m |m ?M n,k c } are add to S k c , as it is expected discriminative features help to measure precise similarity. Then, classification loss L cls in (3) and regression loss L reg in (4) are updated accordingly. After going through all the stages from 1 to K, we obtain S U = S K ? S as described in <ref type="figure" target="#fig_2">Fig. 2(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weakly Supervised Contrastive Loss</head><p>To learn more consistent feature representations for the proposals in the same class, we propose weakly supervised contrastive loss (WSCL) that learns representations by attracting positive samples closer together and repelling negative samples away from positives samples in the embedding space. From a collection</p><formula xml:id="formula_12">S U = {s i , t i } |S U | i=1</formula><p>where s i denotes i-th embedding vectors, t i denotes the pseudo label of s i , WSCL for i-th embedding vector denoted as L i wscl , is formulated as</p><formula xml:id="formula_13">L i wscl = ? 1 N ti ? 1 |S U | j=1,j? =i 1{t i = t j } ? log exp (s i ? s j /?) |S U | l=1,l? =i exp (s i ? s l /?) (11) where N ti := |S U | j=1 1{t i = t j }, and ? is a temperature parameter introduced in [14]. Note that S U = C c=1 S U c . Instance difficulty.</formula><p>Since confidence score of instance is noisy at early stages of training, we introduce instance difficulty ? to make training for WSCL easier. ? is the set of scores for all images in a batch where each score is the instance score from the MIL head over the sum of them at each image. Here, the size of ? is the same as S. Then, the re-weighted contrastive loss is formulated as,</p><formula xml:id="formula_14">L wscl = 1 |S U | |S U | i=1 ? i ? L i wscl , ? = N n=1 ? ? ? X n c,m / M n j=1 X n c,j | m ? K?1 k=0 M n,k c ? ? ?<label>(12)</label></formula><p>Total Loss. Finally, the total loss of training the proposed model is defined as</p><formula xml:id="formula_15">L total = L mil + L cls + L reg + ?L wscl<label>(13)</label></formula><p>where ? is a loss weight to balance scale with the other losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setting</head><p>Datasets. To verify the robustness of our method, we evaluate it on four object detection datasets; VOC07 and VOC12 in PASCAL VOC <ref type="bibr" target="#b6">[7]</ref>, and COCO14 and COCO17 in MS-COCO <ref type="bibr" target="#b16">[17]</ref>, following the convention in WSOD tasks. We use trainval sets containing 5,011 and 11,540 images for VOC07 and VOC12, respectively, and test sets that contain 4,951 and 10,991 images, for evaluation.</p><p>We further investigate the robustness of our method on MS-COCO datasets. For COCO14, we train our model on the train set of 82,783 images and test it with the validation set of 40,504 images. For COCO17, we split the dataset into the train set of 118,287 images and validation set of 5,000 images. We use only image-level annotations to train our model on all datasets. Evaluation Metrics. On VOC07 and 12 datasets, we evaluate our model on the test set using mean Average Precision (mAP) metric with standard IoU criterion (0.5). MS-COCO is more challenging than PASCAL VOC as it has significantly more instances per image (about 2 vs. 7) and more classes (20 vs. 80). For this reason, MS-COCO is often not considered in the WSOD literature. We report the performance on MS-COCO datasets following the standard COCO metric which includes several metrics, such as, average precision (AP) and average recall (AR) with varying IoU thresholds e.g., 0.5 and 0.75, and object sizes e.g., small (s), medium (m), and large (l ), but the most representative metric is the AP averaged over 10 IoU thresholds (from 0.5 to 0.95 for every 0.05 step). Implementation Details. In the experiments, we set ? IoU = 0.5, ? drop = 0.3, ? nms = 0.1, ? = 0.03 (? = 0.01 on COCO datasets) and ? = 0.2 for WSCL and K=3 for the number of refinement stages. But, as we will show in an ablation study, the performance is not sensitive to the choice of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Results</head><p>Comparison with state-of-the-arts. In <ref type="table" target="#tab_2">Table 1</ref>, we compare the proposed method with other state-of-the-art algorithms on COCO14 and 17. Regardless of backbone structure and dataset, our method achieves the new state-of-the-art performance for all the evaluation metrics. The fact that the performance of Ours in AR measurements are higher than the other methods implies that the proposed method successfully detects missing instances compared to the previous  for (AR s , AR m , AR l ) with VGG16 on COCO14. Despite the significant improvement in AR, AP also improves by a large margin regardless of different subcategory of AP and backbone structure. Our method gains on average of 2.95% for (AP 50 , AP 75 ) and on average of 2.23% for (AP s , AP m , AP l ) with VGG16 on COCO14. We observe similar tendency on COCO17 for all the backbones. As a result, our proposed method achieves the new state-of-the-art performance   <ref type="table" target="#tab_5">Table 3</ref>(a), each of OD and WSCL modules significantly improves OICR + baseline (+3.8 and +2.2) but the improvement is the highest when both OD and WSCL are applied simultaneously, which partially demonstrate that each module helps the other one as described in Section 4. Robust regardless of backbone. In <ref type="table" target="#tab_2">Table 1</ref>, we have shown that the proposed method performs well both in AP and AR metrics on MS-COCO datasets regardless of backbone structure. Although it is not a common practice to provide the performance with ResNet backbones on PASCAL VOC, we provide the performance of our method on ResNet in <ref type="table" target="#tab_5">Table 3</ref>(b). It shows that Ours with ResNet backbones significantly outperforms the previous state-of-the-art methods as with the case with VGG backbone shown in <ref type="table" target="#tab_4">Table 2</ref>. The performance of the previous methods are reported in [26].</p><formula xml:id="formula_16">- - - - - - - - - - C-MIDN [9] 9.6 21.4 - - - - - - - - - - WSOD2 [38] 10.8 22.7 - - - - - - - - - - MIST [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>Feature augmentation methods. To further verify the effectiveness of the proposed feature augmentation methods, we experiment with different combination of augmentation methods for the object discovery and WSCL modules in <ref type="table" target="#tab_5">Table 3</ref>(c). The performance significantly improves from 52.3% to 55.1% and 54.8% with random masking and Gaussian noise, respectively, even without IoU sampling. With IoU sampling prior to random masking and Gaussian noise, the performance consistently improves further by 2.3% and 2.6% for random masking and Gaussian noise. Using all the proposed feature augmentations, the performance reaches 58.7% that is 6.4% higher than the baseline. Sensitivity to hyperparameters. In <ref type="figure">Fig. 4</ref>, we provide the experiment results with different values of the hyperparameters we introduce. Regardless of hyperparameter, the performance is not sensitive to the choice of values around the optimal values we choose (? nms = 0.1, ? drop = 0.3, ? IoU = 0.5, ? = 0.03 and ? = 0.2). For instance, the gap between the highest and lowest performance for each hyperparameter is no more than 2.6% in mAP (highest with ? nms ), which demonstrates that our proposed method does not greatly depend on hyperparameter tuning. In (e), we use the same values of ? following the experiments conducted in other contrastive learning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative results</head><p>Three challenges of WSOD. In <ref type="figure">Fig. 5</ref>, we provide the qualitative results that show how our method addresses three main challenges of WSOD -part domination, grouped instances and missing objects (described in Section 1), compared to OICR [31]. The left columns show the results from OICR [31] whereas the right columns show the results from our method. The part domination shown in (a) is largely alleviated, especially for the categories with various poses such as dog, cat and person. We also observe that grouped instances are separated into multiple bounding boxes in (b). Lastly, our method successfully detects many of instances that are ignored with the argmax labeling method as shown in (c). , it also selects many false positives e.g., object-like background. Ours also captures many false positives in early stages of training (Iter: 0 -10K) but later in training, it mostly selects true positives (Iter: 20K -30K). Our method can even detect some objects categorized as "difficult" (red boxes in (e)), which are not considered for detection performance as they are too hard even for humans to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel multiple instance labeling method to replace the conventional argmax-based pseudo groundtruth labeling method for weakly supervised object detection (WSOD). To this end, we introduce a contrastive loss for the WSOD setting that learns consistent embedding features for proposals in the same class, and discriminative features for ones in different classes. With these features, it is possible to mine a large number of reliable pseudo groundtruths, which provide richer supervision for WSOD tasks. As a result, we achieve the new state-of-theart results on both PASCAL VOC and MS-COCO benchmarks. Acknowledgements.  In this appendix, we provide further results, both quantitative and qualitative, in the following order.</p><p>-Section A reports per-class Average Precision and Correct Localization results on PASCAL VOC datasets. -Section B compares different proposal generation methods.</p><p>-Section C demonstrates the robustness of proposed method using similarity threshold guided by WSCL. -Section D provides overall pipeline of Object Discovery.</p><p>-Section E provides additional qualitative results on PASCAL VOC and MS-COCO datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Performance on PASCAL VOC</head><p>In <ref type="table" target="#tab_7">Tables 4 and 5</ref>, we provide additional performance of per-class average precision (AP) using Selective Search (SS) [33] with VGG16 on VOC07 and VOC12 <ref type="bibr" target="#b6">[7]</ref>. Our method achieves the second-highest performance on VOC07 and the highest performance on VOC12. The proposed method successfully addresses the issue of missing objects with high performance for the classes with a large number of objects per image, such as cow, person and sheep in <ref type="figure" target="#fig_8">Fig. 7</ref>. In <ref type="table" target="#tab_12">Tables 6 and 7</ref>, we report the results of per-class Correct Localization (CorLoc) scores using SS with VGG16 on VOC07 and VOC12. CorLoc is an additional evaluation metric commonly reported in WSOD literature to measure localization accuracy, equivalent to precision (= true positives true positives+false positives ). More specifically, it measures the percentage of correct localization predictions where a prediction is treated as "correct" if the IoU between the prediction and corresponding ground truth is greater than or equal to 0.5. Our method achieves the third-best result in CorLoc on both VOC07 and VOC12. Our slightly worse performance in CorLoc than in mAP is because, as a multiple instance labeling method, our approach captures more proposals than argmax-based methods: this significantly increases recall, but may slightly decrease precision (the only thing measured by CorLoc). -  - - -  - - -   - -  <ref type="table" target="#tab_20">(Table 8</ref>) and MS-COCO <ref type="table" target="#tab_21">(Table 9</ref>) datasets. Note that COB generally captures the groundtruths the best among the three proposal generation methods whereas SS performs the worst. In general, the better the proposals are, the higher the performance of detection is regardless of model. In <ref type="table" target="#tab_20">Table 8</ref>, Ours performs the best with COB and then with MCG (COB: 61.8%, MCG: 58.7%, and SS: 56.1%), which is the same for CASD and MIST. Similarly, COB outperforms MCG with a large margin as observed on MS-COCO datasets as shown in <ref type="table" target="#tab_21">Table 9</ref>. We chose to report only the performance of SS and MCG in the main paper because additional boundary information is required to train COB, which violates the definition of image-level supervision. Based on this experiment, we believe MCG should be the default proposal generation method for both PASCAL VOC and MS-COCO datasets unlike the previous convention in WSOD.  </p><formula xml:id="formula_17">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_18">- - - - - - - - - - - - - - - - - - - 43.4 C-MIL[34] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_19">- - - - - - - - - - - - - - - - - - - 47.2 OIM[29] - - - - - - - - - - - - - - - - - - - - 45.3 SLV[4] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_20">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_21">- - - - - - - - - - - - - - - - - - - 65.0 C-MIDN[9] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_22">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_23">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_24">OICR[31] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_25">- - - - - - - - - - - - - - - - - - - 67.4 C-MIDN[9] - - - - - - - - - - - - - - - - - - - - 71.2 WSOD2[38] - - - - - - - - - - - - - - - - - - - - 71.9 OIM[29] - - - - - - - - - - - - - - - - - - - - 67.1 SLV[4] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_26">- - - - - - - - - - - - - - - - - - - 72.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Different Criterion for Object Discovery</head><p>In Section 4.3, we claimed that the similarity of two proposals in the embedding space can be large even though they are not similar in classification score. To justify the necessity of using additional similarity scores for the object discovery module, <ref type="table" target="#tab_2">Table 10</ref> compares object discovery based on classification score, with various threshold values, to similarity score. Recall that the "adaptive" threshold we used for similarity score is determined by the average value of similarity between the argmax and its augmented samples: ? n,k c = 1 |Sc| |Sc| i=1 sim(z n m n,k c , S c,i ). For object discovery based on classification score, we not only try fixed thresholds but also adaptive threshold defined as ? n,k c = 1</p><formula xml:id="formula_27">|S ? c | |S ? c | i=1 S ? c,i</formula><p>where S ? is the collection of classification scores that are calculated using the augmented features (same features for S).</p><p>In <ref type="table" target="#tab_2">Table 10</ref>, the performance of the object discovery based on classification score is significantly worse than similarity score. Moreover, the best-performing threshold ? n,k c = 0.4 (57.4%) is dramatically better than a similar threshold value ? n,k c = 0.2. Thus, unlike similarity score (as shown in Section 5.3), performance is also very sensitive to the choice of threshold. Note that we train the model with the same hyperparameters (? nms = 0.1, ? = 0.03) for fair comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Qualitative Results</head><p>In <ref type="figure" target="#fig_11">Fig. 9</ref>, we provide more qualitative results for the three challenges of WSOD on VOC07. Columns on the left and right of each pair correspond to qualitative results from OICR [31] and our model, respectively. In <ref type="figure" target="#fig_1">Fig. 10</ref>, we compare prediction results of OICR [31] on the left and Ours on the right. Our model shows much better results for COCO, which contains more instances per image. Although the issue of grouped instances is observed in some cases, our model correctly captures multiple objects and classifies them correctly, despite extremely complex backgrounds. <ref type="figure" target="#fig_1">Fig. 11</ref> shows failure cases of the proposed method. Our model misclassfies background objects that looks like a target class, for example human-like statues or dolls. In addition, the predicted boxes are separated in some cases, even though the object its full extent is captured.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Three challenges of WSOD [Bicycle( 1, 1 , 2 , 2 ), ?, Person( 1 , 1 , 2 , 2 ),?] The loss of information due to the argmax-based method (b) Different type of supervisions Weak supervision Full supervision</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Three challenges of WSOD: part domination, grouped instances, and missing objects. (b) Unlike full supervision, there is no location and count information in weak supervision. (c) A large number of target objects are ignored in PASCAL VOC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Overall architecture of the proposed method. Initial prediction in (a) collects top-scoring instances over all stages. Sampling step for Object Discovery in (b) iterates step (a) for all images in a batch, and applies feature augmentations described in Section 4.2. Object discovery in (c) mines additional pseudo groundtruths that are not recognized by the argmax method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Three steps for object discovery module. (a) applies feature augmentation methods to embedding vectors of top-scoring proposals. (b) collects all augmented embedding vectors through all images in a batch. (c) determines new pseudo groundtruths based on the similarity with the embedding vector of the top-scoring instance z n m n,k c and similarity threshold ? n,k c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Performance with different values of ? nms , ? drop , ? IoU , ? and ?.(a) Part domination (b) Group instances (c) Missing objects Qualitative results of OICR [31] and ours about the three challenges of WSOD: (a) part domination, (b) grouped instances and (c) missing objects. The images on the left and right indicate OICR and Ours, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of pseudo groundtruths generated by (a) OICR [31], (b) MIST [23] and (c) Ours which shows pseudo groundtruths at different training steps. "Difficult" objects are often captured by Ours as shown in (d). Selection of pseudo groundtruths. We visualize the pseudo groundtruths captured by OICR [31], MIST [23] and Ours in Fig. 6. OICR [31] selects only the top-scoring proposal per category ignoring all the other instances as shown in (a). Although multiple objects are captured by MIST [23] in (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grants funded by the Korea government (MSIT) (No.2017-0-00897, Development of Object Detection and Recognition for Intelligent Vehicles) and (No.B0101-15-0266, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis), as well as by support provided by the Natural Sciences and Engineering Research Council of Canada and the Canada CIFAR AI Chairs program. Junhyug Noh was supported by LLNL under Contract DE-AC52-07NA27344. 18. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd: Single shot multibox detector. In: European conference on computer vision. pp. 21-37. Springer (2016) 19. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3431-3440 (2015) 20. Maninis, K.K., Pont-Tuset, J., Arbel?ez, P., Van Gool, L.: Convolutional oriented boundaries. In: European conference on computer vision. pp. 580-596. Springer (2016) 21. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 779-788 (2016) 22. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. pp. 91-99 (2015) 23. Ren, Z., Yu, Z., Yang, X., Liu, M.Y., Lee, Y.J., Schwing, A.G., Kautz, J.: Instanceaware, context-focused, and memory-efficient weakly supervised object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10598-10607 (2020) 24. Rosenfeld, A., Thurston, M.: Edge and curve detection for visual scene analysis. IEEE Transactions on computers 100(5), 562-569 (1971) 25. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 815-823 (2015) 26. Shen, Y., Ji, R., Wang, Y., Chen, Z., Zheng, F., Huang, F., Wu, Y.: Enabling deep residual networks for weakly supervised object detection. In: European Conference on Computer Vision. pp. 118-136. Springer (2020) 27. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: ICLR (2015) 28. Sun, B., Li, B., Cai, S., Yuan, Y., Zhang, C.: Fsce: Few-shot object detection via contrastive proposal encoding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7352-7362 (2021) 29. Sun, G., Wang, W., Dai, J., Van Gool, L.: Mining cross-image semantics for weakly supervised semantic segmentation. In: European conference on computer vision. pp. 347-365. Springer (2020) 30. Tang, P., Wang, X., Bai, S., Shen, W., Bai, X., Liu, W., Yuille, A.: Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence 42(1), 176-191 (2018) 31. Tang, P., Wang, X., Bai, X., Liu, W.: Multiple instance detection network with online instance classifier refinement. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (July 2017) 32. Tang, P., Wang, X., Wang, A., Yan, Y., Liu, W., Huang, J., Yuille, A.: Weakly supervised region proposal network and object detection. In: The European Conference on Computer Vision (ECCV) (September 2018) 33. Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object recognition. International journal of computer vision 104(2), 154-171 (2013) 34. Wan, F., Liu, C., Ke, W., Ji, X., Jiao, J., Ye, Q.: C-mil: Continuation multiple instance learning for weakly supervised object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Analysis of a number of objects per image on PASCAL VOC datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>46.7 C-MIDN[9] 72.9 68.9 53.9 25.3 29.7 60.9 56.0 78.3 23.0 57.8 25.7 73.0 63.5 73.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of pseudo groundtruths generated by classification score vs. similarity score. The left and right images of each pair correspond to pseudo groundtruths based on classification and similarity scores, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>More qualitative results for the three challenges of WSOD on VOC07.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Qualitative results on COCO14. Failure cases of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the state-of-the-art algorithms on MS-COCODataset BackboneMethod AP AP 50 AP 75 AP s AP m AP l AR 1 AR 10 AR 100 AR s AR m AR l</figDesc><table><row><cell>PCL [30]</cell><cell>8.5</cell><cell>19.4</cell></row><row><cell>VGG16</cell><cell></cell><cell></cell></row><row><cell>COCO14</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the state-ofthe-art methods on PASCAL VOC.</figDesc><table><row><cell cols="4">Proposal Method VOC07 VOC12</cell></row><row><cell></cell><cell cols="2">WSDDN[2] 34.8</cell><cell>-</cell></row><row><cell></cell><cell>OICR[31]</cell><cell>41.2</cell><cell>37.9</cell></row><row><cell></cell><cell>PCL[30]</cell><cell>43.5</cell><cell>40.6</cell></row><row><cell></cell><cell>C-WSL[8]</cell><cell>46.8</cell><cell>43.0</cell></row><row><cell></cell><cell cols="2">WSRPN[32] 47.9</cell><cell>43.4</cell></row><row><cell></cell><cell>C-MIL[34]</cell><cell>50.5</cell><cell>46.7</cell></row><row><cell>SS[33]</cell><cell cols="2">C-MIDN[9] 52.6</cell><cell>50.2</cell></row><row><cell></cell><cell cols="2">WSOD2[38] 53.6</cell><cell>47.2</cell></row><row><cell></cell><cell>OIM[16]</cell><cell>50.1</cell><cell>45.3</cell></row><row><cell></cell><cell>SLV[4]</cell><cell>53.5</cell><cell>49.2</cell></row><row><cell></cell><cell>MIST[23]</cell><cell>54.9</cell><cell>52.1</cell></row><row><cell></cell><cell>CASD[13]</cell><cell>56.8</cell><cell>53.6</cell></row><row><cell></cell><cell>Ours</cell><cell>56.1</cell><cell>54.6</cell></row><row><cell></cell><cell>MIST[23]</cell><cell>56.5</cell><cell>53.9</cell></row><row><cell>MCG[1]</cell><cell>CASD[13]</cell><cell>57.4</cell><cell>-</cell></row><row><cell></cell><cell>Ours</cell><cell>58.7</cell><cell>56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experiment results with various settings on VOC07. (a) Different components of the proposed method. (b) Performance with ResNet backbones [26]. (c) Comparison of different combination of feature augmentation methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Method IoU Mask Noise mAP</cell></row><row><cell cols="3">OD WSCL mAP</cell><cell cols="3">Method R50-WS R101-WS</cell><cell>OICR +</cell><cell>?</cell><cell></cell><cell>52.3 56.8</cell></row><row><cell></cell><cell></cell><cell>52.3</cell><cell>OICR[31]</cell><cell>50.9</cell><cell>51.4</cell><cell></cell><cell></cell><cell>?</cell><cell>55.1</cell></row><row><cell>?</cell><cell></cell><cell>56.1</cell><cell>PCL[30]</cell><cell>50.8</cell><cell>53.3</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>54.8</cell></row><row><cell></cell><cell>?</cell><cell>54.5</cell><cell>C-MIL[34]</cell><cell>53.4</cell><cell>53.9</cell><cell>+ Ours</cell><cell>?</cell><cell>?</cell><cell>57.4</cell></row><row><cell>?</cell><cell>?</cell><cell>58.7</cell><cell>Ours</cell><cell>56.6</cell><cell>56.5</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>57.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>55.2</cell></row><row><cell cols="3">(a) Diff. components</cell><cell cols="3">(b) ResNet backbones</cell><cell></cell><cell>?</cell><cell>?</cell><cell>? 58.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) Feature augmentation</cell></row></table><note>The effectiveness of each component. To validate the effectiveness of each component of Object Discovery (OD) and WSCL modules, we provide experi- ment results on VOC07 with SS in Table 3(a). Note that we find the performance of OICR [31] can be further increased by adding the bounding box regression and dropblock [10] layers (45.0% ? 52.3% on VOC07), thus, we call OICR + Regression + Dropblock as OICR + , and use it as the baseline throughout the experiments unless specified otherwise. In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>35.Xie, E., Ding, J., Wang, W., Zhan, X., Xu, H., Sun, P., Li, Z., Luo, P.: Detco:Unsupervised contrastive learning for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.</figDesc><table><row><cell>[Appendix]</cell></row><row><cell>8392-8401 (2021) Object Discovery via Contrastive Learning for</cell></row><row><cell>36. Yang, K., Li, D., Dou, Y.: Towards precise end-to-end weakly supervised object detection network. In: Proceedings of the IEEE/CVF International Conference on Weakly Supervised Object Detection</cell></row><row><cell>Computer Vision (ICCV) (October 2019)</cell></row><row><cell>37. Yin, Y., Deng, J., Zhou, W., Li, H.: Instance mining with class feature banks for Jinhwan Seo 1 , Wonho Bae 2 , Danica J. Sutherland 2,3 , weakly supervised object detection. In: Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3190-3198 (2021) Junhyug Noh 4? , and Daijin Kim 1?</cell></row><row><cell>38. Zeng, Z., Liu, B., Fu, J., Chao, H., Zhang, L.: Wsod2: Learning bottom-up and top-1 Pohang University of Science and Technology down objectness distillation for weakly-supervised object detection. In: Proceedings 2 University of British Columbia of the IEEE/CVF International Conference on Computer Vision. pp. 8292-8300 3 Alberta Machine Intelligence Institute (2019) 4 Lawrence Livermore National Laboratory 39. Zitnick, C.L., Doll?r, P.: Edge boxes: Locating object proposals from edges. In: tohoaa@gmail.com, whbae@cs.ubc.ca, dsuth@cs.ubc.ca, European conference on computer vision. pp. 391-405. Springer (2014) noh1@llnl.gov, dkim@postech.ac.kr</cell></row><row><cell>https://github.com/jinhseo/OD-WSCL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Per-class AP results on VOC07</figDesc><table><row><cell>Method</cell><cell cols="6">Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV mAP</cell></row><row><cell cols="4">WSDDN[2] 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4</cell><cell>56.6</cell><cell>9.4</cell><cell>14.7 30.2 40.7 54.7 46.9 34.8</cell></row><row><cell>OICR[31]</cell><cell cols="3">58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8</cell><cell>65.5</cell><cell>15.7</cell><cell>24.1 41.7 46.9 64.3 62.6 41.2</cell></row><row><cell cols="4">C-WSL[8] 62.9 64.8 39.8 28.1 16.4 69.5 68.2 47.0 27.9 55.8 43.7 31.2 43.8</cell><cell>65.0</cell><cell>10.9</cell><cell>26.1 52.7 55.3 60.2 66.6 46.8</cell></row><row><cell cols="2">WSRPN[32] 60.3 66.2 45.0 19.6 26.6 68.1 68.4 49.4</cell><cell>8.0</cell><cell>56.9 55.0 33.6 62.5</cell><cell>68.2</cell><cell>20.6</cell><cell>29.0 49.0 54.1 58.8 58.4 47.9</cell></row><row><cell cols="4">C-MIL[34] 62.5 58.4 49.5 32.1 19.8 70.5 66.1 63.4 20.0 60.5 52.9 53.5 57.4</cell><cell>68.9</cell><cell>8.4</cell><cell>24.6 51.8 58.7 66.7 63.5 50.5</cell></row><row><cell cols="4">C-MIDN[9] 53.3 71.5 49.8 26.1 20.3 70.3 69.9 68.3 28.7 65.3 45.1 64.6 58.0</cell><cell>71.2</cell><cell>20.0</cell><cell>27.5 54.9 54.9 69.4 63.5 52.6</cell></row><row><cell cols="5">WSOD2[38] 65.1 64.8 57.2 39.2 24.3 69.8 66.2 61.0 29.8 64.6 42.5 60.1 71.2 70.7</cell><cell>21.9</cell><cell>28.1 58.6 59.7 52.2 64.8 53.6</cell></row><row><cell>OIM[29]</cell><cell cols="3">55.6 67.0 45.8 27.9 21.1 69.0 68.3 70.5 21.3 60.2 40.3 54.5 56.5</cell><cell>70.1</cell><cell>12.5</cell><cell>25.0 52.9 55.2 65.0 63.7 50.1</cell></row><row><cell>SLV[4]</cell><cell cols="3">65.6 71.4 49.0 37.1 24.6 69.6 70.3 70.6 30.8 63.1 36.0 61.4 65.3</cell><cell>68.4</cell><cell>12.4</cell><cell>29.9 52.4 60.0 67.6 64.5 53.5</cell></row><row><cell cols="4">MIST[23] 68.8 77.7 57.0 27.7 28.9 69.1 74.5 67.0 32.1 73.2 48.1 45.2 54.4</cell><cell>73.7</cell><cell>35.0</cell><cell>29.3 64.1 53.8 65.3 65.2 54.9</cell></row><row><cell>CASD[12]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Per-class AP results on VOC12 Method Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV mAP</figDesc><table><row><cell>OICR[31]</cell><cell>67.7 61.2 41.5 25.6 22.2 54.6 49.7 25.4 19.9 47.0 18.1 26.0 38.9</cell><cell>67.7</cell><cell>2.0</cell><cell>22.6 41.1 34.3 37.9 55.3 37.9</cell></row><row><cell cols="2">C-WSL[8] 74.0 67.3 45.6 29.2 26.8 62.5 54.8 21.5 22.6 50.6 24.7 25.6 57.4</cell><cell>71.0</cell><cell>2.4</cell><cell>22.8 44.5 44.2 45.2 66.9 43.0</cell></row><row><cell>WSRPN[32]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Per-class CorLoc results on VOC07 Method Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV CorLoc</figDesc><table><row><cell cols="2">WSDDN[2] 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8</cell><cell>82.4</cell><cell>25.5</cell><cell>41.6 61.5 55.9 65.9 63.7</cell><cell>53.5</cell></row><row><cell>OICR[31]</cell><cell>81.7 80.4 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5</cell><cell>88.8</cell><cell>21.8</cell><cell>57.9 76.3 59.9 75.3 81.4</cell><cell>60.6</cell></row><row><cell cols="2">C-WSL[8] 85.8 81.2 64.9 50.5 32.1 84.3 85.9 54.7 43.4 80.1 42.2 42.6 60.5</cell><cell>90.4</cell><cell>13.7</cell><cell>57.5 82.5 61.8 74.1 82.4</cell><cell>63.5</cell></row><row><cell cols="2">WSRPN[32] 77.5 81.2 55.3 19.7 44.3 80.2 86.6 69.5 10.1 87.7 68.4 52.1 84.4</cell><cell>91.6</cell><cell cols="2">57.4 63.4 77.3 58.1 57.0 53.8</cell><cell>63.8</cell></row><row><cell>C-MIL[34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>MIST[23] 87.5 82.4 76.0 58.0 44.7 82.2 87.5 71.2 49.1 81.5 51.7 53.3 71.4 92.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.2</cell></row><row><cell>SLV[4]</cell><cell>84.6 84.3 73.3 58.5 49.2 80.2 87.0 79.4 46.8 83.6 41.8 79.3 88.8 90.4</cell><cell>19.5</cell><cell cols="2">59.7 79.4 67.7 82.9 83.2 71.0</cell></row><row><cell></cell><cell></cell><cell>38.2</cell><cell>52.8 79.4 61.0 78.3 76.0</cell><cell>68.8</cell></row><row><cell>CASD[12]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Per-class CorLoc results on VOC12 Method Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV CorLoc</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>91.7 85.6 71.7 56.6 55.6 88.6 77.3 63.4 53.6 90.0 51.6 62.6 79.3 94.2</figDesc><table><row><cell></cell><cell></cell><cell>69.2</cell></row><row><cell>MIST[23] 32.7</cell><cell>58.8 90.5 57.7 70.9 85.7</cell><cell>70.9</cell></row><row><cell>CASD[12]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8 :</head><label>8</label><figDesc>Per-class AP results with different proposal generation methods on VOC07 Method Proposal Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV mAP</figDesc><table><row><cell>MIST[23]</cell><cell>SS</cell><cell cols="13">68.8 77.7 57.0 27.7 28.9 69.1 74.5 67.0 32.1 73.2 48.1 45.2 54.4</cell><cell>73.7</cell><cell>35.0</cell><cell cols="6">29.3 64.1 53.8 65.3 65.2 54.9</cell></row><row><cell>CASD[12]</cell><cell>SS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell>Ours</cell><cell>SS</cell><cell cols="13">65.8 79.5 58.1 23.7 28.6 71.2 75.0 71.7 31.7 69.8 45.2 55.7 57.2</cell><cell>75.7</cell><cell>29.6</cell><cell cols="6">24.3 61.0 55.3 71.7 72.0 56.1</cell></row><row><cell>MIST[23]</cell><cell>MCG</cell><cell cols="13">65.7 78.9 55.5 25.1 31.3 74.5 76.8 67.5 16.1 68.7 50.3 36.0 73.4</cell><cell>76.7</cell><cell>31.7</cell><cell cols="6">30.7 61.6 64.5 74.9 70.0 56.5</cell></row><row><cell>CASD[12]</cell><cell>MCG</cell><cell cols="13">65.1 70.5 55.6 42.8 31.3 72.4 71.7 75.5 16.0 64.1 60.2 68.4 71.5</cell><cell>70.7</cell><cell>39.6</cell><cell cols="6">27.5 58.3 53.9 63.6 69.2 57.4</cell></row><row><cell>Ours</cell><cell>MCG</cell><cell cols="14">69.2 81.5 56.4 28.5 30.5 77.6 79.1 71.6 13.0 70.8 48.8 56.9 74.9 78.4</cell><cell>34.9</cell><cell cols="6">27.6 61.4 65.4 74.4 73.4 58.7</cell></row><row><cell>MIST[23]</cell><cell>COB</cell><cell cols="13">65.1 74.8 57.5 34.0 45.0 77.8 80.6 56.1 20.5 71.2 50.0 51.9 58.0</cell><cell>78.2</cell><cell>27.2</cell><cell cols="6">32.6 62.2 63.4 72.9 69.8 57.4</cell></row><row><cell>CASD[12]</cell><cell>COB</cell><cell cols="13">69.1 71.1 63.2 48.5 40.0 76.4 74.2 77.1 17.6 67.4 59.9 76.1 74.4</cell><cell>70.4</cell><cell>20.8</cell><cell cols="6">30.2 59.4 58.3 67.2 68.1 59.4</cell></row><row><cell>Ours</cell><cell>COB</cell><cell cols="14">68.6 78.4 62.2 36.6 49.8 79.2 80.9 77.0 29.4 71.0 38.1 62.7 80.6 78.0</cell><cell>40.8</cell><cell cols="6">31.6 61.7 62.8 75.7 69.8 61.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 9 :</head><label>9</label><figDesc>Performance with different proposal generation methods on MS-COCO Dataset Backbone Method Proposal AP AP 50 AP 75 AP s AP m AP l AR 1 AR 10 AR 100 AR s AR m AR l 30.5 14.9 5.4 19.0 27.2 17.0 29.1 31.4 10.4 35.2 53.3 31.6 15.2 5.7 19.6 28.2 17.4 29.7 31.9 11.3 35.5 54.2</figDesc><table><row><cell></cell><cell></cell><cell>MIST [23]</cell><cell>MCG</cell><cell>11.4 24.3</cell><cell>9.4</cell><cell>3.6</cell><cell>12.2 17.6 13.5 22.6</cell><cell>23.9</cell><cell>8.5</cell><cell>25.4 38.3</cell></row><row><cell></cell><cell>VGG16</cell><cell>CASD [12] Ours</cell><cell>MCG MCG</cell><cell>12.8 26.4 13.7 27.7</cell><cell>-11.9</cell><cell>-4.4</cell><cell>-14.5 21.2 14.7 24.8 ---</cell><cell>-26.9</cell><cell>-8.8</cell><cell>-27.8 44.0 -</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>COB</cell><cell cols="5">15.1 29.3 13.8 4.5 15.9 23.4 16.0 26.5 28.2</cell><cell cols="2">8.9 29.5 46.5</cell></row><row><cell></cell><cell></cell><cell>MIST [23]</cell><cell>MCG</cell><cell>12.6 26.1</cell><cell>10.8</cell><cell>3.7</cell><cell>13.3 19.9 14.8 23.7</cell><cell>24.7</cell><cell>8.4</cell><cell>25.1 41.8</cell></row><row><cell>COCO14</cell><cell>ResNet50</cell><cell>CASD [12] Ours</cell><cell>MCG MCG</cell><cell>13.9 27.8 13.9 29.1</cell><cell cols="3">-11.8 4.9 16.8 22.3 15.5 26.1 -----</cell><cell>-28.0</cell><cell>-9.0</cell><cell>-31.8 46.6 -</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>COB</cell><cell cols="5">15.4 30.4 14.0 4.8 18.0 24.6 16.9 29.2 31.4</cell><cell cols="2">9.4 35.1 53.1</cell></row><row><cell></cell><cell></cell><cell>MIST [23]</cell><cell>MCG</cell><cell>13.0 26.1</cell><cell>10.8</cell><cell>3.7</cell><cell>13.3 19.9 14.8 23.7</cell><cell>24.7</cell><cell>8.4</cell><cell>25.1 41.8</cell></row><row><cell></cell><cell>ResNet101</cell><cell>Ours</cell><cell>MCG</cell><cell>14.4 29.0</cell><cell>12.4</cell><cell>4.8</cell><cell>17.3 23.8 15.8 27.0</cell><cell>30.0</cell><cell>9.2</cell><cell>33.6 51.0</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>COB</cell><cell cols="7">16.2 31.6 14.8 5.0 18.7 26.4 17.5 29.6 31.9 10.0 35.4 53.5</cell></row><row><cell></cell><cell></cell><cell>MIST [23]</cell><cell>MCG</cell><cell>12.4 25.8</cell><cell>10.5</cell><cell>3.9</cell><cell>13.8 19.9 14.3 23.3</cell><cell>24.6</cell><cell>9.7</cell><cell>26.6 39.6</cell></row><row><cell></cell><cell>VGG16</cell><cell>Ours</cell><cell>MCG</cell><cell>13.6 27.4</cell><cell>12.2</cell><cell>4.9</cell><cell>15.5 21.6 14.6 24.8</cell><cell>26.8</cell><cell>9.2</cell><cell>28.7 43.8</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>COB</cell><cell cols="5">15.6 29.9 14.3 5.1 17.2 25.1 16.4 27.1 28.7</cell><cell cols="2">9.8 30.5 47.8</cell></row><row><cell>COCO17</cell><cell cols="4">Ours Ours Ours 16.0 ResNet101 MCG 13.8 27.8 ResNet50 COB MCG 14.4 28.7 Ours COB 16.5</cell><cell cols="3">12.1 5.7 17.7 23.8 15.1 26.6 12.6 5.4 17.9 25.5 15.4 26.8</cell><cell>29.7 29.6</cell><cell cols="2">10.1 33.7 50.7 10.0 33.3 50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 10 :</head><label>10</label><figDesc>The results of different criteria for object discovery</figDesc><table><row><cell>Criterion</cell><cell>Threshold (? n,k c</cell><cell>) mAP</cell></row><row><cell></cell><cell>0.2</cell><cell>50.3</cell></row><row><cell></cell><cell>0.3</cell><cell>56.2</cell></row><row><cell></cell><cell>0.4</cell><cell>57.4</cell></row><row><cell>Classification Score</cell><cell>0.5 0.6</cell><cell>56.2 56.1</cell></row><row><cell></cell><cell>0.7</cell><cell>55.6</cell></row><row><cell></cell><cell>0.8</cell><cell>54.4</cell></row><row><cell></cell><cell>Adaptive</cell><cell>53.2</cell></row><row><cell>Similarity Score</cell><cell>Adaptive</cell><cell>58.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Pseudo Code: Sampling and Object Discovery</head><p>Along with <ref type="figure">Fig.2</ref> in the main paper, we provide the detailed procedure of sampling steps and object discovery. The main purpose of object discovery is to obtain more pseudo groundtruths in addition to the top-scoring proposals. if IoU (rm, rmn,k c ) &gt; ?IoU , ?m ? M n then 7:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling steps and object discovery</head><p>Dnoise : Di,j ? N (0, 1) ? R H?W 13:</p><p>Z </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SLV: Spatial likelihood voting for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12995" to="13004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">C-wsl: Count-guided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">C-midn: Coupled multiple instance detection network with segmentation guidance for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9834" to="9843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12890</idno>
		<title level="m">Dropblock: A regularization method for convolutional networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Comprehensive attention self-distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12023</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation by deep community learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1020" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-aware instance labeling for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Object instance mining for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11482" to="11489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
