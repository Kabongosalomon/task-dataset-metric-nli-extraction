<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
							<email>mihai.zanfir@imar.rocristian.sminchisescu@math.lth.se</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Lund University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep multitask architecture for fully automatic 2d and 3d human sensing (DMHS), including recognition and reconstruction, in monocular images. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The visual analysis of humans has a broad spectrum of applications as diverse as autonomous vehicles, robotics, human-computer interaction, virtual reality, and digital libraries, among others. The problem is challenging due to the large variety of human poses and body proportions, occlusion, and the diversity of scenes, angles of observation, and backgrounds humans are pictured against. The monocular case, which is central and intrinsic in many scenarios like the analysis of photographs or video available on the web, adds complexity as depth information is missing for * Authors contributed equally 3d reconstruction. This leads to geometric ambiguity and occlusion which are difficult to palliate compared to situations where multiple cameras are present.</p><p>A detailed analysis at both 2d and 3d levels, further exposes the need for both measurement and prior-knowledge, and the necessary inter-play between segmentation, reconstruction, and recognition within models that can jointly perform all tasks.</p><p>As training is essential, a major difficulty is the limited coverage of current datasets: 2d repositories like LSP <ref type="bibr" target="#b17">[18]</ref> or MPI-II <ref type="bibr" target="#b2">[3]</ref> exhibit challenging backgrounds, human body proportions, clothing, and poses, but offer single views, carry only approximate 2d joint location ground truth, and do not carry human segmentation or body part labeling information. Their size is also relatively small by today's deep learning standards. In contrast, 3d datasets like HumanEva <ref type="bibr" target="#b37">[38]</ref> or Human3.6M <ref type="bibr" target="#b15">[16]</ref> offer extremely accurate 2d and 3d anatomical joint or body surface reconstructions and a variety of poses captured under multiple viewpoints. Human 3.6M is also large-scale. However, being captured indoors, the 3d datasets typically lack the background and clothing variability that represent a strength of the 2d datasets captured in the wild. <ref type="bibr" target="#b0">1</ref> An open question is how one can leverage the separate strengths of existing 2d and 3d datasets towards training a model that can operate in realistic images and can offer accurate recognition and reconstruction estimates.</p><p>In this paper we propose one such deep learning model which, given a monocular RGB image, is able to fully automatically sense the humans at multiple levels of detail: figure-ground segmentation, body-part labeling at pixel level, as well as 2d and 3d pose estimation. By designing multi-task loss functions at different, recursive processing stages (human body joint detection and 2d pose estimation, semantic body part segmentation, 3d reconstruction) we are able to tie complete, realistic training scenarios by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model component training (complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability), leading to covariate shift and a lack of model expressiveness. In extensive experiments, including ablation studies performed using representative 2d and 3d datasets like LSP, HumanEva, or Hu-man3.6M, we illustrate the model and show that state-ofthe-art results can be achieved for both semantic body part segmentation and for 3d pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work relates to 2d and 3d monocular human pose estimation methods as well as semantic segmentation using fully-trainable deep processing architectures. As priorwork is comprehensive in each sub-domain <ref type="bibr" target="#b34">[35]</ref>, we will here mostly cover some of the key techniques directly relating to our approach, with an emphasis towards deep architectures and methodologies aiming to integrate the different levels of 2d and 3d processing.</p><p>The problem of 2d human pose estimation has been approached initially using the pictorial structures and deformable part models where the kinematic tree structure of the human body offers a natural decomposition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b11">12]</ref>. In recent years, the deep learning methodology had a great impact over the state-of-the-art pose estimation models where different hierarchical feature extraction architectures have been combined with spatial constraints between the human body parts <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref>. Recent deep architectures are obtained by cascading processing stages with similar structure but different parameters, where the system combines outputs of early layers with new information extracted directly from the image using learnt feature extractors. Such recursive schemes for 2d pose estimation appear in the work of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9]</ref> whereas a similar idea of feeding-back 3d estimates into 2d inference layers appears in <ref type="bibr" target="#b14">[15]</ref>. Note that the iterative processing frameworks of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref> are not deep, built on parts-based graphical models and random forests, respectively.</p><p>There is a vast literature on monocular 3d human pose estimation, including the analysis of kinematic ambiguities associated with the 3d pose <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b4">5]</ref>, as well as generative and discriminative methods for learning and inference <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. More recently, deep convolutional architectures have been employed in order to estimate 3d pose directly from images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> mostly in connection with 3d human motion capture datasets like Hu-manEva, Human3.6M, where the poses are challenging but the backgrounds are relatively simple. There is also interest in combining 2d and 3d estimation methods in order to obtain models capable of multiple task, and able to operate in realistic imaging conditions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2]</ref>. The most recent methods, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, rely on an a-priori 3d model that is fitted to anatomical body joint data obtained from an initial 2d pose estimate produced by a deep processing architecture, like the one of <ref type="bibr" target="#b12">[13]</ref> or <ref type="bibr" target="#b42">[43]</ref>. The methods rely on a state-of-theart discriminative person detection and 2d pose estimation and introduce a generative fitting component to search in a space of admissible body proportion variations. Both approaches fit a statistical body shape and kinematic model to data, using one or multiple views and the joint assignment constraints from 2d human pose estimation. These methods use the 3d to 2d anatomical landmark assignment provided by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> as initialization for 3d human pose estimation. While this is effective, as shown in several challenging evaluation scenarios, the initial 3d shape and kinematic configuration still needs to be initialized manually or set to a key initial pose. This is in principle still prone to local optima as the monocular 3d human pose estimation cost is non-linear and non-convex even under perfect 3d to 2d model-image assignments <ref type="bibr" target="#b39">[40]</ref>.</p><p>We share with <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref> the interest in building models that integrate 2d and 3d reasoning. We propose a fully trainable discriminative model for human recognition and reconstruction at 2d and 3d levels. We do not estimate human body shape, but we do estimate figure-ground segmentation, the semantic segmentation of the human body parts, as well as the 2d and 3d pose. The system is trainable, end-to-end, by means of multitask losses that can leverage the complementary properties of existing 2d and 3d human datasets. The model is fully automatic in the sense that both the human detection and body part segmentation and the 2d and 3d estimates are the result of recurrent stages of processing in a homogeneous, easy to understand and computationally efficient architecture. Our approach is complementary to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>: our model can benefit from a final optimizationbased refinement and it would be useful to estimate the human body shape. In contrast, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> can benefit from the semantic segmentation of the human body parts for their shape fitting, and could use the accurate fully automatic 2d and 3d pose estimates we produce as initialization for their 3d to 2d refinement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we present our multitask multistage architecture. The idea of using multiple stages of recurrent feed-forward processing is inspired by architectures like <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9]</ref> which focus separately on the 2d and 3d domains. However, we propose an uniform architecture for joint 2d and 3d processing that no prior method covers. Our choice of multi-task loss also makes it possible to exploit the complementary advantages of different datasets.</p><p>Conceptually, each stage of processing in our model produces recognition and reconstruction estimates and is con-strained by specific training losses. Specifically, each stage t is split into semantic processing S t , and 3d reconstruction, R t (see <ref type="figure" target="#fig_0">fig. 1</ref>). At the same time, the semantic module S t is divided in two sub-tasks, one focusing on 2d pose estimation, J t , and the other on body part labeling and figureground segmentation, B t (see <ref type="figure" target="#fig_1">fig. 2</ref>). The first one (i.e. J t ) feeds into the second one (i.e. B t ), while the semantic stages feed into the reconstruction stages. Each task consists of a total of six recurrent stages which take as input the image, the results of the previous stages of the same type (except for the first one), as well as inputs from other stages (2d pose estimation feeding into semantic body part segmentation and both feeding into 3d pose reconstruction). The inputs to each stage are individually processed and fused via convolutional networks in order to produce the corresponding outputs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2D Human Body Joint Detection</head><p>The 2d pose estimation task is based on a recurrent convolutional architecture similar to <ref type="bibr" target="#b45">[46]</ref>. Given an RGB image I ? R w?h?3 , we seek to correctly predict the locations of N J anatomically defined human body joints</p><formula xml:id="formula_0">p k ? Z ? R 2 , with k ? {1 . . . N J }. At each stage t ? {1 . . . T },</formula><p>where T is the total number of stages, the network outputs belief maps J t ? R w?h?N J . The first stage of processing operates only on image evidence (a set of seven convolution and three pooling layers producing features x) but for subsequent stages, the network also considers the information in the belief maps fed from the previous stage J t?1 with a slightly different image feature function x , defined as a set of four convolution and three pooling layers as in <ref type="bibr" target="#b45">[46]</ref>. These features are transformed through a classification function c t J to predict the body joint belief maps J t . The function c t J consists of a series of five convolutional layers, the first three of the form (11 ? 11 ? 128), followed by a (1 ? 1 ? 128) convolution and a final (1 ? 1 ? N J ) convolution that outputs J t . The loss function at each stage L t J minimizes the squared Euclidean distance between the predicted and ground truth belief maps, J t and J :</p><formula xml:id="formula_1">L t J = N J k=1 z?Z J t (z, k) ? J (z, k) 2 2<label>(1)</label></formula><p>In practice, this component of the model can be trained with data from both 2d and 3d datasets like LSP, where the ground truth is obtained manually, or HumanEva and Hu-man3.6M where the ground truth is obtained automatically based on anatomical markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Body Part Segmentation</head><p>In semantic body part segmentation (body part labeling) we assign each image location (u, v) ? Z ? R 2 one of N B anatomical body part labels (including an additional label for background), b l , where l ? {1 . . . N B }. At each stage t, the network predicts, for each pixel location, the probability of each body part being present, B t ? R w?h?N B . Differently from the previous task, our aim now is to classify each pixel location, not only to identify the body joints. The loss function used changes from a squared Euclidean loss to a multinomial logistic loss:</p><formula xml:id="formula_2">L t B = ? 1 |Z| z?Z log(B t z,B z )<label>(2)</label></formula><p>where B z is the ground truth label for each image location z = (u, v).</p><p>During the first stage of processing, we use convolutional representations based on the image (a series of convolution and pooling layers x with parameters tied from ?3.1) and the 2d pose belief maps J 1 in order to predict the current body labels B 1 . For each of the following stages, we also use the information present in the body labels at the previous stage, B t?1 , and rely on a series of four convolutional layers c t B that learn to combine inputs obtained by stacking image features x and B t?1 . The function c t B shares the same structure as the first four convolutions in c t J , but a classifier in the form of a (1 ? 1 ? N B ) convolution is applied after the fusion with the current 2d pose belief maps J t , in order to obtain semantic probability maps B t . An overview of our architecture together with the main dependencies is given in <ref type="figure" target="#fig_2">figure 3</ref>. Finally, we use an additional deconvolution layer <ref type="bibr" target="#b24">[25]</ref> of size 16 ? 16 ? N B , such that the loss can be computed at the full resolution of the input image I.</p><p>In practice, realistic data for training this component of the loss is not as easy to obtain as 2d body joint positions. Human3.6M offers such training data, but we are also able to generate it approximately for LSP (see ?4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Pose Reconstruction</head><p>This model component is designed for the stage-wise, recurrent reconstruction of 3d human body configurations represented as set of N R 3d skeleton joints, from a single monocular image I. The estimate is obtained from the internal representations R t . The 3d reconstruction module leverages information provided by the 2d semantic components S t , incorporating the joint and body part labeling feature maps J t and B t . Additionally, we insert a trainable function c t D , defined similarly to c t B , over image features, in order to obtain body reconstruction feature maps D t . The module follows a similar flow as the previous ones: it reuses estimates at earlier processing stages, R t?1 , together with S t and D t , in order to predict the reconstruction feature maps R t . The processing stages and dependencies of this module are shown in <ref type="figure" target="#fig_3">fig. 4</ref>.</p><p>Procedurally, we first fuse S t and D t , then apply a series of one (3 ? 3 ? 128) convolution, one (3 ? 3 ? 64) convolution, one (1 ? 1 ? 64) convolution, followed by a pooling layer (3 ? 3) and a (1 ? 1 ? 16) convolution. The output is concatenated with R t?1 and convolved by a (1 ? 1 ? 16) kernel that learns to combine the two components, producing the estimate R t . The feature maps are then transformed to the desired dimensionality of the 3d human body skeleton by means of a fully connected layer. The loss L t R is expressed as the mean per joint position error (MPJPE):</p><formula xml:id="formula_3">L t R = N R i=1 3 j=1 (f (R t , i, j) ? R (i, j)) 2 + 2 (3)</formula><p>where R are the 3d ground truth human joint positions, f (?) is the fully connected layer applied to R t and is a small constant that makes the loss differentiable. This loss component can be trained with data from Hu-manEva and Human3.6M, but not from LSP or other 2d datasets as these lack 3d ground truth information. Although the backgrounds in HumanEva and Human3.6M are not as challenging as those in LSP, the use of a multitask loss makes the complete 2d and 3d model competitive not only in a laboratory setting but also in the wild (see ?4 and <ref type="figure" target="#fig_6">fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Integrated Multi-task Multi-stage Loss</head><p>Given the information provided in the previous sections, we are now able to define the complete multitask, multistage loss function of the model as follows:</p><formula xml:id="formula_4">L = T t=1 (L t J + L t B + L t R )<label>(4)</label></formula><p>The loss allows us to conveniently train all the model component parameters, for different tasks, based on datasets where the annotations are challenging, or where annotations are missing entirely, as datasets with different coverage contribute to various components of the loss. Whenever we train using datasets with partial coverage, we could freeze the model components for which we do not have groundtruth. We can also simultaneously train all parameters, using datasets with partial and complete coverage: those examples for which we have ground truth at all levels will contribute to each of the loss components, whereas examples for which we have partial ground truth will only contribute to their corresponding losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate our method, we use 3 well-known datasets, the Leeds Sports Dataset (LSP) <ref type="bibr" target="#b17">[18]</ref>, HumanEva <ref type="bibr" target="#b37">[38]</ref> and Human3.6M <ref type="bibr" target="#b15">[16]</ref>.</p><p>The LSP dataset consists of 2d pose annotated RGB images depicting sports people (athletics, badminton, baseball, gymnastics, parkour, soccer, tennis, volleyball). We employ both the original release containing 1, 000 training images and 1, 000 testing images, as well as the extended training release containing an additional 10, 000 images.</p><p>We use the first release of the HumanEva(-I) dataset. This dataset was captured by an accurate 3d motion capture system in a laboratory environment. There are six actions performed in total by three different subjects. As standard procedure <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42]</ref>, we train our model on the train set and report results on the validation set, where we only consider every 5 th frame of the sequences walking, jog and box for all three subjects and a single frontal camera view.</p><p>Human80K is an 80,000 sample subset of the much larger 3.6 million human pose dataset Human3.6M <ref type="bibr" target="#b15">[16]</ref>. The dataset is captured in a laboratory environment with a motion capture setup, and contains daily activities and interaction scenarios (providing directions, discussion, eating, activities while seating, greeting, taking photo, posing, making purchases, smoking, waiting, walking, sitting on chair, talking on the phone, walking dog, walking together). The actions are performed by 11 actors, and captured by 4 RGB cameras. The dataset is fully annotated and it contains RGB data, 2d body part labeling ground truth masks as well as accurate 2d and 3d pose reconstructions.  Human80K consists of 55, 144 training and 24, 416 testing samples from Human3.6M. The samples from each original capture were selected such that the distance between eachother, in 3d space, is no more than 100 mm.</p><p>We use the Caffe <ref type="bibr" target="#b16">[17]</ref> framework to train and test our architecture. The complete recognition and reconstruction pipeline takes approximately 400 ms per frame, in testing, on an Nvidia TITAN X (Pascal) 12GB GPU. We evaluate the recognition (2d body part labeling) and 3d reconstruc-tion capabilities of our architecture. We use T = 6 stages in our architecture for each sub-task component model (joint detection, semantic segmentation, 3d reconstruction) and report results only for the final stage of each sub-task, as it is the best performing according to validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Body Part Labeling</head><p>In order to evaluate the 2d body part labeling task, we use the Human80K and LSP datasets. We introduce addi-tional annotations for LSP, as they are not available with the original release of the dataset which only provides the 2d anatomical joints. We create human body part annotations for LSP by using the annotated 2d skeleton joints and the kinematic tree. We produce circles for skeleton joints and ellipses for the individual body parts. We set the major axis to the size of the segment between the corresponding joints and estimate a constant value, for each body part, for the minor axis. The resulting masks (filtered by visual inspection) are of lower quality than those available for Hu-man3.6M. The reason for augmenting LSP, is to enhance the variability in human appearance, body proportions and backgrounds in Human80K. Thus, we want to leverage the good quality labeling and pose variation of Human80K with the diverse appearance variations found in LSP.</p><p>For evaluation on Human80K, we compare with the results of <ref type="bibr" target="#b14">[15]</ref> which represent the state-of-the-art for this task on this dataset. The authors of <ref type="bibr" target="#b14">[15]</ref> assume that the silhouette of the person (the figure-ground segmentation) is given, and perform the body part labeling only on the foreground mask as an inference problem over a total of 24 unique labels. Different from them, we do not make this assumption and consider the background as an extra class, thus building a model that predicts 25 classes.</p><p>To extend the evaluation on LSP, we consider multiple scenarios: (a) training on Human80K with (b) fine-tuning on LSP and (c) training our architecture on LSP and Hu-man80K simultaneously and test on both Human80K and LSP. In our setup, training using only LSP was not feasible, as in multiple experiments our network failed to converge. The labeling parameters (stages B) are initialized randomly, while the parameters corresponding to the 2d joint detection components, J, are initialized with the values of the network presented in <ref type="bibr" target="#b45">[46]</ref>, trained on MPI-II and LSP.</p><p>The performance of our body part labeling models for the Human80K and LSP is given in tables 1 and 2, respectively. We use the same evaluation metrics as in <ref type="bibr" target="#b14">[15]</ref>, i.e. average accuracy for the pixels contained within the ground truth silhouette and class normalized average accuracy. Basically, these two metrics apply only to the foreground classes. Additionally, we compute for all pixels, background and foreground, the average accuracy as well as the class normalized average accuracy.</p><p>From table 1 (Human80K testing), it can be noted that even though we solve a harder problem (by additionally estimating the <ref type="figure">figure-ground segmentation)</ref>, the class normalized average precision greatly improves, with more than 10% over <ref type="bibr" target="#b14">[15]</ref> for the models trained on (a) Human80K and (c) Human80K and LSP jointly. However, the model (b) initialized with parameters trained on Human80K, but finetuned on LSP, seems to have a performance drop caused by the low quality of the LSP body label annotations. In this scenario, as expected, the best performance is obtained by the model trained on Human80K, perhaps due to the fact that the test set distribution is better captured by the unaltered Human80K training set. This is not the case when testing on LSP, as it can be seen from table 2. The best performance is obtained by the model trained jointly on Human80K and LSP. This model is able to combine the pose variability and part labeling annotation quality of Hu-man80K with the background and appearance variability of LSP, making it adequate for both laboratory settings and for images of people captured against challenging backgrounds.</p><p>The proposed models for the 2d body part labeling task are trained using an initial learning rate set to 10 ?10 and reduced at every 5 epochs by a constant factor ? = 0.33. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Human Pose Reconstruction</head><p>For the evaluation of the 3d pose reconstruction task, we use HumanEva-I and Human3.6M (specifically, Hu-man80K), as they are both challenging datasets, containing complementary poses, and offering accurate 3d pose annotations. In all experiments, we use the same 3d evaluation metrics as other methods we compare against, and according to the standard practice. For the following experiments, note that our 2d semantic module components are trained on data from LSP and Human80K, whereas the 3d component of the module was pre-trained using Human80K.</p><p>We report results on the Human80K test set and investigate the impact of each of the input features J, B and D on the overall performance of the 3d reconstruction task (see <ref type="table">table 4</ref>). Notice that models that use the 'D-pipeline' for the 3d reconstruction part, would correspond to convolutional networks that feed-forward from the image features, but do not leverage the semantic segmentation of the human body parts.</p><p>We observe that our fully integrated system, DMHS R <ref type="figure">(J,B,D)</ref>, achieves the lowest error of 63.35 mm. Besides the interest in computing additional detailed semantic human representations, it can be seen that by feeding in the results from other tasks -the body joint belief maps J and the body labeling probability maps B -the error is reduced considerably, from 77.56 mm (for a model based on feed-forward processing and infering additional 2d joint position information) to 63.35 mm. Notice also our significant gains with respect to the previous state-of-the-art results on Human80K, as reported by <ref type="bibr" target="#b14">[15]</ref>.</p><p>For HumanEva-I, we use the DMHS R (J,B,D) architecture trained on Human80K and fine-tuned for a few epochs We report the performance of our network, trained on Human80K, LSP and both datasets jointly. Note that for LSP, the network was pre-trained on Human80K, as it otherwise failed to converge trained on LSP alone. We also compare with <ref type="bibr" target="#b14">[15]</ref>, where comparisons are possible only for accuracies computed on the person foreground as the model of <ref type="bibr" target="#b14">[15]</ref> does not predict the background label. Our model is able to predict the background class, thus we report the performance for the entire image (including the background class) as well as performance inside the silhouette (for classes associated to human body parts). Note that the best performance on Human80K is obtained with the network trained on Human80K, perhaps due to the noisy annotations added to LSP, and the naturally more similar training and testing distributions of Human80K.  <ref type="table">Table 3</ref>. 3d mean joint position error on the HumanEva-I dataset, computed between our predicted joints and the ground truth after an alignment with a rigid transformation. Comparisons with other competing methods show that DMHS achieves state-of-the-art performance.</p><p>(6) on the HumanEva-I training data. We use a subset of the training set containing 4, 637 samples. The fine-tuning step on HumanEva-I is performed in order to compensate for the differences in marker positioning with respect to Hu-man80K, and in order to account for the different pose distributions w.r.t Human80K. In table 3, we compare our results against several state-of-the-art methods. We follow the standard evaluation procedure in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42]</ref> and sample data from the validation set for walking, jogging and boxing activities. We use a single camera. We obtain considerable performance gains with respect to the previous state-of-theart methods on HumanEva, even though we only use a small subset of the available training set.</p><p>For this model, we use an initial learning rate set to 10 ?7 and reduce it at every 5 epochs by a constant factor ? = 0.66. Qualitative results of our method can be seen in <ref type="figure" target="#fig_6">fig. 5</ref> and <ref type="figure" target="#fig_7">fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed a deep multitask architecture for fully automatic 2d and 3d human sensing (DMHS), including recognition and reconstruction, based on monocular im-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Avg. MPJPE (mm) <ref type="bibr" target="#b14">[15]</ref> 92.00 DMHS R <ref type="figure">(</ref>  <ref type="table">Table 4</ref>. 3d mean joint position error on the Human80K dataset. Different components of our model are compared to <ref type="bibr" target="#b14">[15]</ref>. ages. Our system estimates the figure-ground segmentation and detects the human body joints, semantically identifies the body parts, and reconstructs the 2d and 3d pose of the person. The design of recurrent multi-task loss functions at multiple stages of processing supports the principled combination of the strengths of different 2d and 3d datasets, without being limited by their different weaknesses. In experiments we perform ablation studies, evaluate the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art-results can be achieved at all processing levels. We show that, even in the wild, our monocular RGB architecture is perceptually competitive to state-of-the art commercial RGB-D systems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Stage t of our architecture for recognition and reconstruction: figure-ground segmentation, 2d pose estimation, and semantic segmentation of body parts, all denoted by (S) and 3d reconstruction (R). The semantic task is detailed infig. 2 and fig. 3; the 3d reconstruction task is detailed infig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Stage t of our semantic task, including 2d joint detection (J), and labeling of the body parts (B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Our multitask multistage 2d semantic module S t , combines semantic body part labeling B t and 2d pose estimation J t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Our multitask multistage 3d reconstruction module R t , combines 3d processing with information from semantic modules, S t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>During the learning process, the training data is augmented by randomly rotating with angles between [?40 ? , +40 ? ], scaling by a factor in the range [0.5, 1.2] and horizontally flipping, in order to improve the diversity of the training set. Qualitative results of the semantic body part segmentation in challenging images are shown in fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Recognition and reconstruction results for images from Human3.6M and LSP. For each image we show the 2d pose estimate the semantic segmentation of body parts and the 3d pose estimation. Notice the difficulty of backgrounds and poses and the fact that the 2d and 3d models generalize well. Notice that in our architecture, errors during the early stages of processing (2d pose estimation) can be corrected later, during e.g. semantic body part segmentation or 3d pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparisons for segmentation and reconstruction between our RGB model (top row) and the ones of a commercial RGB-D Kinect for Xbox One system (bottom row). Our model produces accurate figure-ground segmentations, body part labeling, and 3d reconstruction for some challenging poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Body part labeling results for the Human80K test set.</figDesc><table><row><cell>Metric</cell><cell cols="4">DMHS B -Human80K DMHS B -LSP (ft) DMHS B -Human80K &amp; LSP [15]</cell></row><row><cell>Avg. Acc. (%) per pixel (fg)</cell><cell>79.00</cell><cell>53.31</cell><cell>75.84</cell><cell>73.99</cell></row><row><cell>Avg. Acc. (%) per pixel (fg + bg)</cell><cell>91.15</cell><cell>83.30</cell><cell>89.92</cell><cell>-</cell></row><row><cell>Avg. Acc. (%) per class (fg)</cell><cell>67.35</cell><cell>43.40</cell><cell>64.83</cell><cell>-</cell></row><row><cell>Avg. Acc. (%) per class (fg + bg)</cell><cell>68.56</cell><cell>45.61</cell><cell>66.13</cell><cell>53.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Body part labeling results for the LSP dataset. All models are initialized on the Human80K dataset, as networks trained only using LSP failed to converge. In this case the models trained jointly on Human80K and LSP produced the best results. This shows the importance of having accurate body part labeling annotations, obtained in simple imaging scenarios but for complex body poses, in combination with less accurate annotations but with more complex foreground-background appearance variations. D) 27.1 18.4 39.5 28.3 37.6 28.9 27.6 31.4 30.5 45.8 48.0 41.5 33.7</figDesc><table><row><cell>Metric</cell><cell></cell><cell cols="7">DMHS B -Human80K DMHS B -LSP (ft) DMHS B -Human80K &amp; LSP</cell></row><row><cell cols="2">Avg. Acc. (%) per pixel (fg)</cell><cell>50.52</cell><cell></cell><cell></cell><cell></cell><cell>60.54</cell><cell></cell><cell>61.16</cell></row><row><cell cols="2">Avg. Acc. (%) per pixel (fg + bg)</cell><cell>85.58</cell><cell></cell><cell></cell><cell></cell><cell>91.08</cell><cell></cell><cell>91.09</cell></row><row><cell cols="2">Avg. Acc. (%) per class (fg)</cell><cell>36.46</cell><cell></cell><cell></cell><cell></cell><cell>44.73</cell><cell></cell><cell>45.91</cell></row><row><cell cols="2">Avg. Acc. (%) per class (fg + bg)</cell><cell>38.77</cell><cell></cell><cell></cell><cell></cell><cell>46.88</cell><cell></cell><cell>48.01</cell></row><row><cell>Method</cell><cell>Walking</cell><cell>Avg.</cell><cell></cell><cell>Jog</cell><cell></cell><cell>Avg.</cell><cell></cell><cell>Box</cell><cell>Avg. All</cell></row><row><cell>[39]</cell><cell cols="6">65.1 48.6 73.5 62.4 74.2 46.6 32.2 51.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>[4]</cell><cell cols="8">45.4 28.3 62.3 45.33 55.1 43.2 37.4 45.2 42.5 64.0 69.3 58.6 49.7</cell></row><row><cell>[48]</cell><cell cols="6">35.8 32.4 41.6 36.6 46.6 41.4 35.4 41.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>[42]</cell><cell cols="2">37.5 25.1 49.2 37.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">50.5 61.7 57.5 56.6</cell><cell>-</cell></row><row><cell>DMHS R (J,B,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The situation is slightly more nuanced as some of the 3d datasets (e.g. Human3.6M) come with mixed-reality training setups where a moderately realistic graphics character was placed, in a geometrically correct setup, into a real scene and animated using human motion capture -arguably, though, a complete, fully realistic 2d and 3d training setting is still elusive.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work was supported in part by CNCS-UEFISCDI under PCE-2011-3-0438, JRP-RO-FR-2014-16.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast algorithms for large scale conditional 3d prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poseletbased contextual rescoring for human pose estimation via pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hern?ndez-Vela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="49" to="64" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural svm for visual localization and continuous state estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Hu-man3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning body pose via specialized maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Human Motion, Understanding, Modelling, Capture and Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Verlag</publisher>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiperson tracking by multicut and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
