<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain and View-point Agnostic Hand Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Alonso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
						</author>
						<title level="a" type="main">Domain and View-point Agnostic Hand Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand action recognition is a special case of action recognition with applications in human-robot interaction, virtual reality or life-logging systems. Building action classifiers able to work for such heterogeneous action domains is very challenging. There are very subtle changes across different actions from a given application but also large variations across domains (e.g. virtual reality vs life-logging). This work introduces a novel skeleton-based hand motion representation model that tackles this problem. The framework we propose is agnostic to the application domain or camera recording view-point. When working on a single domain (intra-domain action classification) our approach performs better or similar to current state-of-the-art methods on well-known hand action recognition benchmarks. And, more importantly, when performing hand action recognition for action domains and camera perspectives which our approach has not been trained for (cross-domain action classification), our proposed framework achieves comparable performance to intra-domain state-ofthe-art methods. These experiments show the robustness and generalization capabilities of our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>imply drastic view-point changes, e.g. egocentric vs. thirdperson view. On the other hand, fine grained details are essential. Different action categories are often quite similar and vary only subtly (e.g. pointing to different directions, sliding gestures, etc.). Moreover, hand skeleton joints present lower movement range than other full-body joints, increasing the correlation of skeleton joint motions and similar actions.</p><p>The main contribution of this work 1 is a novel motion representation model, summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>, designed to be robust to different application domains and view-points. It computes representations (motion descriptors) from labeled hand skeletons (motion sequences), that are later used for the final motion sequence classification. The main components of our motion representation model are: 1) a set of pose features adapted to hand motion; 2) a Temporal Convolutional Network (TCN) encoding the stream of hand pose features into per-frame descriptors; and 3) a summarization module that learns the relevance of each per-frame descriptor to describe the input motion sequence. The learned motion descriptors can be directly used to recognize the action categories (labels) they were trained for (intra-domain) with a simple Linear Classifier. More interestingly, they can also be used to build N-shot classifiers to recognize new unseen action categories recorded from radically different points of view (cross-domain) with a K-Nearest Neighbor Classifier.</p><p>Our experiments use the front view SHREC-17 dataset <ref type="bibr" target="#b10">[11]</ref>, the egocentric F-PHAB dataset <ref type="bibr" target="#b11">[12]</ref> and the third-person MSRA <ref type="bibr" target="#b12">[13]</ref> dataset, which include actions and gestures related to computer interaction, life-logging and sign language domains respectively. Our intra-domain classification results show that our framework gets better or similar performance than current state-of-the-art intradomain classifiers in well-known benchmarks. More importantly, our cross-domain classification approach obtains comparable accuracy to intra-domain methods by being trained just with the SHREC-17 dataset, and then evaluated on the F-PHAB and MSRA datasets. This demonstrates that our motion representation model generalizes well for different action domains and camera view-points. Besides, our approach shows a low latency, which allows its use for online and real-time applications.</p><p>II. RELATED WORK This section summarizes relevant works on the core topics of this work: pose modeling, skeleton-based action recognition models and generalization to unseen action categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose modeling for action recognition</head><p>Action recognition was first tackled by directly analyzing RGB videos <ref type="bibr" target="#b13">[14]</ref> or depth maps <ref type="bibr" target="#b14">[15]</ref>. Current approaches have settled the standard of extracting the intermediate representation of skeleton poses <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This representation has shown great performance since it encodes human poses regardless their appearance and surrounding and presents strong robustness to occlusions.</p><p>Certain works directly use the raw coordinates of skeleton joints (position of the joints in the Euclidean space) as input for full-body action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref> and for hand action recognition <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In order to achieve a standardized and generic skeleton pose descriptions, several full-body action recognition approaches propose different strategies, such as learning the most suitable view-point for each action <ref type="bibr" target="#b5">[6]</ref> or transforming all coordinates to a common coordinate system <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, this kind of transformations cannot be directly applied to hand action recognition, where orientation plays a key role.</p><p>In order to get more informative pose representations than the raw joint coordinates, many approaches propose to compute additional geometric (pose) features. Chen et al. <ref type="bibr" target="#b21">[22]</ref> use static features (distance and angles of pairs of joint coordinates) and temporal features (velocity and acceleration of joint coordinates). Zhang et al. <ref type="bibr" target="#b22">[23]</ref> calculate distances between joints and planes, and Yang et al. <ref type="bibr" target="#b8">[9]</ref> use joint distances and their motion speeds at different scales.</p><p>Our approach proposes a simplification of the skeleton representation reducing coordinate redundancy by using just a set of key joints. Then, simplified skeleton coordinates are standardized by applying scale and location invariant transformations. Specific geometric features are calculated to encode relevant translation information (lost in the standardization) and orientation aware information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action recognition models</head><p>As in many other fields, deep learning has become stateof-the-art in action recognition. Particularly relevant for this work, Recurrent Neural Networks (RNN) have been widely used to model temporal dependencies in hand action recognition. Ma et al. <ref type="bibr" target="#b17">[18]</ref> use a LSTM-based Memory Augmented Neural Network to model dynamic hand gestures. Chen et al. <ref type="bibr" target="#b23">[24]</ref> use a LSTM Network to combine skeleton coordinates, global motions and finger motion features. Li et al. <ref type="bibr" target="#b18">[19]</ref> combine a bidirectional Independently Recurrent Neural Network with a self-attention based graph convolutional network.</p><p>Other works make use of Convolutional Networks. Liu et al. <ref type="bibr" target="#b24">[25]</ref> recognize posture and action by using 3D convolutions. Yang et al. <ref type="bibr" target="#b8">[9]</ref> use 1D convolutions to process and fuse different hand motion features. Hou <ref type="bibr" target="#b16">[17]</ref> propose to focus on the most informative hand gesture features by using a ResNet-like 1D convolutional network with attention.</p><p>Our method uses a Temporal Convolutional Network (TCN) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> that implements 1D dilated convolutions to learn long-term temporal dependencies from variable-length input sequences, achieving comparable or better results than RNNs <ref type="bibr" target="#b25">[26]</ref>. TCNs have demonstrated good performance on full-body action recognition, both with unsupervised learning <ref type="bibr" target="#b20">[21]</ref> and supervised learning <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization to unseen action categories</head><p>Learning a model able to classify unseen categories is a challenging task. It is commonly tackled by encoding every new data sample into a descriptor and using a K-Nearest Neighbors classifier (KNN) to evaluate and assign labels according to the similarity between a few new category reference samples and the target samples <ref type="bibr" target="#b28">[29]</ref>.</p><p>Several works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref> address this problem for action recognition by extracting intermediate feature maps from a supervised action recognition model. Koneripalli et al. <ref type="bibr" target="#b29">[30]</ref> train an autoencoder to learn these descriptors in an unsupervised fashion. Ma et al. <ref type="bibr" target="#b17">[18]</ref> learn these descriptors directly in a semi-supervised manner by training an encoder with metric-learning techniques. Other works use word2vec <ref type="bibr" target="#b30">[31]</ref> and sent2vec <ref type="bibr" target="#b31">[32]</ref> approaches for descriptor learning.</p><p>Previous works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> are aimed to recognize unseen full-body action categories where no drastic camera view-points are found. Up to our knowledge, generalization to unseen hand view-points and domains is still to be studied. The present work uses metric-learning and specific data augmentation to learn meaningful hand sequence descriptors. Our framework performs accurate action recognition of sequences from unseen categories and recording view-points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HAND ACTION RECOGNITION FRAMEWORK</head><p>The core of the proposed framework is the motion representation model summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>. First, our approach calculates specific pose features for each skeleton (in our case already pre-computed and available in common datasets, see Section IV-A.1). These features are fed to a Temporal Convolutional Network to generate a set of motion descriptors. Additionally, a motion summarization module combines them, according to their relevance, into the final motion representation. In the following, we describe these steps, as well as how to train our motion representation model, both for intra-domain and cross-domain classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hand pose modeling</head><p>Human hand motion sequences are defined by sets of T hand skeleton poses X = {X 1 , ..., X T }, extracted from video frames. Each hand skeleton X t is composed by a set of J joint coordinates, X t = {x 1 , ..., x J }, x j R 3 (i.e. position of the joints in Euclidean space), which are logically connected by a set of B bones (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>1) Skeleton standardization: Since motion information has high variability across different action domains, we propose several steps to standardize the skeleton representation to help generalization of the motion representation model.</p><p>First, hand joints belonging to the same bones (fingers) are highly coupled and can be represented with a smaller number of degrees of freedom. Based on this assumption, we propose to use just a subset of 7 joints to define a hand pose (see <ref type="figure" target="#fig_1">Fig. 2</ref>), corresponding to the wrist, the top of the palm, and the tips of the 5 fingers; which we connect with a total of 6 hand bones, one for the palm and one more for each one of the fingers. This simpler skeleton representation makes the learning process easier and less prone to overfitting.</p><p>Secondly, since actions can be performed by different people with heterogeneous hand sizes and recorded at different scales, we standardize each skeleton pose X t to achieve scale-invariant skeleton representationsX t by applying, to all the hand coordinates, the transformation that makes the palm of size equal to 1:</p><formula xml:id="formula_0">X n = X n |P | ,<label>(1)</label></formula><p>where |P | is the euclidean distance between the wrist and the top of the palm (both joints included in original and simplified 7-joint formats). Finally, since actions must be recognized regardless the position where they are executed, we compute locationinvariant coordinates (relative coordinates) by translating the top of the palm to the origin of the reference coordinate system. Note that these relative hand coordinates describe properly the intra-relation of the hand joints, but they are now missing the information related to the hand motion direction. 2) Hand pose description: Different from full body motion sequences (e.g. walking) where their movement direction can be inferred from the relative coordinates of its bones (e.g. legs), hands can be translated through any direction without any change of their relative coordinates. Since the translation information is essential in certain actions (e.g. pointing to specific directions), we generate extra translation and orientation-aware features from the original hand skeletons:</p><p>? Difference of coordinates, defined as the difference of each joint coordinate with itself in the previous timestep. These features describe the translation direction and speed of each coordinate for each of the 3 axes:</p><formula xml:id="formula_1">d coord (t, j) = x j,t ? x j,t?1 , ?j J, t T<label>(2)</label></formula><p>? Difference of bone angles, defined as the difference of the elevation ? and azimuth ? of a bone b B with itself in the previous time-step. These features describe the rotation direction (with respect to the world coordinates) and rotation speed of each bone:</p><formula xml:id="formula_2">d ? (t, b ? ) = b ?,t ? b ?,t?1 , ?b B, t T (3) d ? (t, b ? ) = b ?,t ? b ?,t?1 , ?b B, t T<label>(4)</label></formula><p>Our final hand representation is a feature vector of size 54 ( <ref type="figure" target="#fig_0">Fig. 1</ref>.a) (7 ? 3 relative hand coordinates, 7 ? 3 coordinate difference features, and 6 ? 2 bone angle differences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion representation model</head><p>The core of our action recognition framework is a model that encodes the skeleton features from each frame, described in the previous section, into single motion descriptors with a Temporal Convolutional Network (TCN) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The TCN processes sequences of skeleton features, generating a descriptor at each time-step per-frame descriptors) that represents the motion performed up to that frame, i.e. with no information from the future (see <ref type="figure" target="#fig_0">Fig. 1</ref></p><formula xml:id="formula_3">.b).</formula><p>For a given motion sequence, the last descriptor generated by the TCN is frequently the one used to represent the action <ref type="bibr" target="#b19">[20]</ref>, since it encodes all the information up to that point. However, training motion sequences are not frequently segmented in time with high precision. In these cases, sequence endings contain frames that are not informative for the action they represent. Consequently, the last descriptor can introduce some noise that hinders the training.</p><p>To alleviate this issue, we learn the relevance of the temporal patterns of the actions. More precisely, we add a motion summarization module after the TCN (see <ref type="figure" target="#fig_0">Fig.  1</ref>.c), which combines all the per-frame descriptors generated for the input hand motion, up the TCN memory length, by performing a weighted average over them (details in <ref type="figure" target="#fig_2">Fig.  3</ref>). These weights represent how important each descriptor is for the final motion representation. They are learned with a simple Neural Network trained end-to-end along with the TCN. This network consists of a single 1D Convolutional layer with kernel 1 that reduces the per-frame descriptors dimensionality, and a single Fully Connected layer with a sigmoid activation layer, that takes as input all the simplified descriptors and outputs a vector of categorical probabilities (i.e. descriptor weights). These final weights are L1 normalized before performing the final descriptor summarization.</p><p>This summarization module efficiently describes hand motion sequences and helps the TCN to focus just on the meaningful data during training. However, there are real use cases where actions, at test time, present a longer length than our motion representation module can handle. In these cases, although the summarization module has been trained along with the TCN, it is better to discard it and classify individually all the per-frame descriptors generated by the TCN, which still contain meaningful motion representations.</p><p>So far, we have shown how to encode a motion sequence X into a robust simple descriptor z = f (X), where the function f represents our motion representation module ( <ref type="figure" target="#fig_0">Fig. 1</ref>). In the next two sections, we describe how to optimize these motion representations to perform intra-domain classification and cross-domain classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intra-domain classification</head><p>Intra-domain hand action classification aims to recognize the same actions categories (labels) seen during the learning phase, with no drastic variation on the camera viewpoint. For this classification, intra-domain class probabilities P = g(z) are predicted by a linear classifier g trained end-to-end along with our motion representation model f (represented in <ref type="figure" target="#fig_0">Fig. 1</ref>). Intra-domain classification is learnt by the optimization of the categorical cross-entropy loss:</p><formula xml:id="formula_4">CCE = ?? C c=1 y i,c log (p i,c ) ,<label>(5)</label></formula><p>which evaluates the predicted probabilities p i,c that belongs to a class c C, given their true label y i . Each training iteration include a mini-batch composed of motion sequences sampled uniformly for each action category (2 different samples per category in our experiments). To ensure the generalization to different motion artifacts, which can be hard to achieve with small datasets, each motion sequence within the mini-batch is included three times with different data augmentations. This data augmentation is applied to the per-frame skeletons X t , before the feature computation from Section III-A, as follows:</p><p>? Movement speed variation. Joint coordinates are randomly re-sampled by interpolation over the temporal dimension. This simulates different motion speeds, and thus, different sequence lengths. ? Frame skipping. Since contiguous video frames contain similar joint information, we only use one out of every three frames, reducing the data redundancy and making the learning process easier. Motion sequences are then initialized randomly between the three first frames. ? Random cropping. When the sampled motion sequence is longer than a defined maximum length (i.e. TCN memory lenght), it is randomly cropped. ? Random noise. Gaussian noise is added to the skeleton coordinates to simulate inaccurate joint estimations. ? Random rotation noise. The whole motion sequence is rotated randomly over the 3D axes. This rotation is limited to low angles, to simulate just subtle variations in the recording view-point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-domain classification</head><p>Cross-domain hand action classification aims to recognize motion sequences whose action category and recording camera view-point were not present in the training data. To obtain view-point agnostic motion representations, our motion representation model f is trained, via contrastive learning, to project motion descriptors in a space where descriptors belonging to the same action category (label) must be close to each other (similar descriptors), and far away from other category descriptors (dissimilar descriptors). This is achieved optimizing the normalized temperaturescaled cross-entropy loss (NT-Xent) <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_5">l i,j = ? log exp (sim (z i , z j ) /? ) 2N k=1 1 [k =i] exp (sim (z i , z k ) /? ) ,<label>(6)</label></formula><p>which is computed in each training iteration for each pair of actions i and j that belong to the same action category. NT-Xent maximizes the cosine similarity sim of both motion descriptors z i and z j and minimizes their similarity to the descriptors related to different action categories k. ? is a temperature parameter. The training of our motion representation model is performed with the same batch construction and data augmentation techniques described in Section III-C. Additionally, we add an extra data augmentation step that rotates randomly all the motion sequences of the mini-batch over the three axis. This batch augmentation simulates arbitrary camera recording perspectives, which is crucial to boost the performance achieved with the NT-Xent loss in different domains and camera view-points.</p><p>Once this generic motion representation model has been trained on a given source domain, we use a N-shot approach <ref type="bibr" target="#b28">[29]</ref> and generate motion descriptors for a small set of N reference motion sequences (motion reference set) from a different target domain, with no specific training on the latter. To perform action classification in this new domain, we use a simple K-Nearest Neighbors classifier (KNN) to assign a label to new sequences depending on their descriptor distance to the descriptors from the motion reference set. To improve the performance of the KNN, we extend our motion reference set by applying the same data augmentation strategies described in Section III-C, and we compute descriptors for all the new augmented sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section details the datasets used in the evaluation and our implementation details. Then, we expose the main framework design choices and evaluate its performance for cross-domain and intra-domain action recognition. Finally, we evaluate the time-performance of the presented approach.</p><p>A. Experimental setup 1) Datasets: The presented approach has been validated on three different datasets (see frame samples in <ref type="figure" target="#fig_3">Fig. 4)</ref>, with different application domains and camera view-points.</p><p>SHREC-17 <ref type="bibr" target="#b10">[11]</ref>: contains motion sequences (22-joint hand skeletons) related to human-machine interaction domains recorded from a frontal third-person view. The data is categorized with two levels of granularity, presenting 14 and 28 actions categories respectively. The dataset contains 1960 motion sequences for training and 840 sequences for validation. Actions are performed by 28 different users.</p><p>F-PHAB <ref type="bibr" target="#b11">[12]</ref>: contains motion sequences (21-joint hand skeletons) recorded from an egocentric view related to kitchen, office and social scenarios, which involve the interaction with different objects. Actions have been performed by 6 different users and labeled with 45 action categories. The dataset consists of 1175 motion sequences which are split into training and validation as stated by the authors [12]: 1:3, 1:1, 3:1 splits the motion sequences on different training:validation ratios (e.g. in the 1:3 split, 33% of the data is used for training and the remaining 66% is used for validation); cross-person 6-fold leave-one-out cross-validation, one fold for the each user motion sequences. Only the original cross-subject and 1:1 splits are available, for the other two data partitions we create three random data folds to perform 3-fold cross-validation.</p><p>MSRA <ref type="bibr" target="#b12">[13]</ref>: contains motion sequences (17-joint hand skeletons) of 17 different American Sign Language gestures performed by 9 different users. Each gesture sequence has a length of 500 frames recorded from a third-person view. For the classification of this data, we use the motion samples from the two first subjects as reference, leaving the remaining seven as the target samples, as suggested in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Implementation and training details:</head><p>Hand skeleton: Since each dataset used provides different skeleton joints format, we use the 20 joints that SHREC-17 and F-PHAB have in common (see <ref type="figure" target="#fig_1">Fig. 2</ref>), and our proposed 7-joint skeletons representation, described in Section III-A, suitable for the three datasets considered. Motion representation architecture: our motion representation model backbone is a TCN with two stacks of residual blocks with dilations of 1, 2 and 4 for the layers within each block, and convolutional filters of size 4, making a memory length of 32 frames long. Since the feature preprocessing filters out 2 out of 3 consecutive frames, this memory length covers 96 real frames. Our backbone uses 256 filters in each convolutional layer, generating motion sequence descriptors of size 256. The summarization module reduces their dimensionality to 64 with a single 1D convolutional layer and then a single perceptron layer of size 32 generates the final descriptor weights. When the sequence summarization module is not used, the descriptor generated by the TCN at the last motion time-step is used for the action representation (Last TCN descriptor). ? from Eq. 6 is set as 0.07.</p><p>KNN classifier: Our KNN classifier weights pairs of target-reference descriptors according to the inverse of their distance. We validate the use of different number of neighbors, i.e. 1, 3, 5, 7, 9, 11, and we report the results of the neighbor that optimizes the final classification accuracy. Additionally, the reference augmentation step increases the reference descriptors set randomly up to 40 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Framework design evaluation</head><p>This subsection analyzes and validates the main components of our framework using the cross-domain approach of Section III-D, since this setup is more demanding in terms of generalization capabilities. We train our base motion representation model on the front view SHREC-17 dataset. Then, we evaluate its accuracy on the egocentric F-PHAB validation splits (described in Section IV-A.1).</p><p>To analyze the effect of different design choices, we start representing the motion sequences with the last descriptor generated by the TCN at the last time-steps (no use of the motion summarization module).</p><p>First, we show the benefits of using our proposed hand skeleton simplification. <ref type="table" target="#tab_1">Table I</ref> shows in each column the accuracy obtained in each of the F-PHAB validation splits. Our proposed simplified 7-joint skeleton format reduces the coordinate redundancy and facilitates the generalization to other domains by reducing the overfitting on the source one. From now on, we set 7-joint skeleton format as default.    Results from <ref type="table" target="#tab_1">Table III</ref> show how our summarization module, from now on set as the default motion representation method, improves the classification accuracy with respect to the last descriptor of our TCN backbone. The summarization module suppresses noisy and non-informative per-frame descriptors, achieving a more informative motion representation. Interestingly, our motion summarization module learns good motion representations even when not many reference actions are available (splits 1:3). Another interesting finding is that augmenting the motion reference set helps to increase the accuracy in all the data splits by a noticeable margin.</p><p>However, we still find an accuracy drop when generalizing to actions of users not present in the motion reference set (cross-person splits). This is due to high inter-subject action variability of the F-PHAB dataset, and because no data from this dataset has been used to train our representation model. <ref type="figure" target="#fig_4">Figure 5</ref> shows the weights learned by our summarization module on the F-PHAB validation split (1:1). This plot illustrates the intuitive idea that later per-frame descriptors are more informative than earlier ones for final motion sequence representation. However, computed weights do not exhibit a continuous growth along time, probably because contiguous time descriptors contain similar information. Although final descriptors may encode information about the whole action, Action descriptor 1:3 1:   they may also encode motion not related with the action itself but with idle poses for example. Therefore, they are not always the most relevant for the final action representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-domain action classification</head><p>This experiment evaluates the cross-domain generalization of our framework by classifying motion sequences from action categories and camera view-points not seen in the training data. For this experiment, we train our motion representation model as defined in Section III-D only on the front view SHREC-17 dataset (28 labels), and we evaluate it on the egocentric F-PHAB dataset. Results from our framework correspond to the processing of 7-joint skeletons and the use of our proposed motion summarization module. <ref type="table" target="#tab_1">Table IV</ref> shows the accuracy of the best performing methods on the F-PHAB dataset, trained as an intra-domain problem (upper block), and the results of our cross-domain approach (bottom block). The later include the evaluation of DD-Net <ref type="bibr" target="#b8">[9]</ref>, one of the best performing methods on the SHREC-17 classification benchmark. We used the available public code to train it with the SHREC-17 dataset (20joint skeletons) as the authors state, extracting F-PHAB descriptors from its backbone and classifying them with our N-shot approach. Results from its evaluation show a lack of domain adaptation. Our method clearly outperforms the rest in this scenario.</p><p>The results show that our approach clearly outperforms the RGB <ref type="bibr" target="#b13">[14]</ref> and depth-based <ref type="bibr" target="#b14">[15]</ref> models trained on the target domain. It is noticeable that we also get better or comparable results than a regular LSTM network <ref type="bibr" target="#b33">[34]</ref> trained on the target dataset, specially when not many reference actions are available (1:3 split) or when not all the subjects are present in the reference split (cross-person splits). Although our cross-domain performance is behind the best intra-domain classification model <ref type="bibr" target="#b9">[10]</ref>, we show later in Section IV-E that we outperform them when training in the same domain. Remember that no specific training with the F-PHAB data splits has been performed in our evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-domain classification of long video sequences</head><p>In this experiment we use the MSRA dataset, with hand motion sequences much longer than the memory of our representation model. This helps to illustrate two characteristics of our method. First, the motion summarization module (1.c) not only helps to summarize the input motion, but also to enforce the TCN to generate informative per-frame descriptors (1.b). Second, per-frame descriptors can also be used to describe the input motion at each time-step and perform online and real-time recognition (see Section IV-F) This experiment uses the same model trained in section IV-C. We evaluate its cross-domain performance in the MSRA dataset. Since sequences are too long for our summarization, we perform the KNN classification of all the motion descriptors generated by the TCN at each time-step (1.b), denoted as online action classification. We report the average of class probabilities of the frames within a video sequence for comparison with previous works, denoted as video classification. For computational reasons, we randomly select just 8000 reference descriptors for the KNN evaluation. <ref type="table" target="#tab_8">Table V</ref> shows that, even though MSRA motion sequences do not correspond to the kind of motion seen in the training data, our approach achieves a high online per-frame classification. Moreover, a simple average of the predicted frame probabilities results in a 97.1% accuracy, comparable to current state-of-the-art results specifically trained on the MSRA dataset. In this case, reference motion data augmentation does not provide an edge, probably because MSRA motion sequences already contain enough hand pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Intra-domain classification and reference actions study</head><p>This experiment evaluates our method for intra-domain classification using the linear classifier from Section III-C.  1) SHREC-17 evaluation: <ref type="table" target="#tab_1">Table VI</ref> shows the classification accuracy of our framework trained and evaluated on the SHREC-17 dataset. Results show that, even though our method was designed for cross-domain classification, it gets comparable results to the state-of-the-art when trained with the target dataset. Note that we are using just 7 out of the 22 original skeleton joints, that helps generalization to other datasets but it might lose domain-specific information.</p><p>Model SHREC 14 SHREC 28 DD-Net <ref type="bibr" target="#b8">[9]</ref> 94.6 91.9 Two-stream NN <ref type="bibr" target="#b18">[19]</ref> 96  2) F-PHAB evaluation: <ref type="table" target="#tab_1">Table VII</ref> shows the classification accuracy of our framework trained and evaluated on each one of the F-PHAB data splits. DD-net results, are obtained by training on the F-PHAB dataset with the original code and following the original paper <ref type="bibr" target="#b8">[9]</ref>. Results show how we manage to outperform the current state-of-the-art in all the splits. Interestingly, our method excels even when less training data is available (1:3). This generalization is visible even on the high inter-subject variability (cross-person) <ref type="bibr" target="#b11">[12]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Time performance</head><p>The presented work is a lightweight solution, able to perform online and real-time hand action recognition (like in Section IV-D). Our base motion representation model (1.b) gets per-frame descriptors in 0.8 ms per time-step in GPU (NVIDIA GeForce GTX 1070) and 1 ms in CPU (Intel Core i7-6700). The motion summarization module <ref type="figure" target="#fig_0">(Fig. 1.c</ref>) and the linear classifier (intra-domain) from Section III-C can be used at a negligible cost. The KNN classifier (crossdomain) from Section III-D has a cost O(k * log(n)) that depends on the number of neighbors k and the size n of the motion reference set. For instance, the KNN classification on the 1:1 data split from F-PHAB (575 motion reference sequences), just takes 0.2 ms per motion descriptor when using 5 neighbors and 0.5 ms when augmenting the motion reference set 40 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The present work introduces a hand action recognition solution, specifically designed to be robust to different action domains and camera perspectives, and able to perform in online and real-time domains. Our framework extracts, from skeleton motion sequences, sets of pose features adapted to heterogeneous motion kinematics. Then, our motion representation model uses a Temporal Convolutional Network that generates per-frame motion descriptors, and a simple motion summarization module weights them, according to their relevance, generating the final motion representation. We trained and validated our motion representation model in two different conditions. In intra-domain classification, we achieve better or similar results than state of the art methods in well-known benchmarks. More importantly, in crossdomain classification, our approach is able to generalize to unseen target action domains and camera view-points, achieving comparable results to the state-of-the-art methods trained on the target data domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Motion representation model. (a) Hand pose modeling: pose features are extracted from the input hand skeleton stream. (b) Motion representation: a Temporal Convolutional Network (TCN) generates per-frame motion descriptors from the pose features. (c) Motion summarization: perframe motion descriptors are weighted and summarized to generate the final motion sequence descriptor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Hand skeleton simplification. Left diagram refers to a detailed hand skeleton of 20 joints (dots) connected by 19 bones (lines). Right diagram refers to our proposed hand skeleton simplification of 7 joints (dots) and 5 bones (lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Motion summarization module. Per-frame motion descriptors are simplified with a 1D Convolutional layer. They are grouped with a single perceptron layer to calculate their summarization weights. The motion sequence is summarized into a final motion descriptor by performing a weighted average over the initial per-frame motion descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(a) SHREC-17 depth sample frames (b) F-PHAB RGB sample frames (c) MSRA depth sample frames with skeleton joints Sample frames from the different evaluated data domains. (a) SHREC-17 dataset. Examples of actions grab, expand and rotation clockwise. (b) F-PHAB dataset. Examples of actions clean glasses, handshaking and pour juice. (c) MSRA dataset. Examples of the signs IP, RP and three.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Relevance weights generated by the summarization module for each action sample from the F-PHAB validation split (1:1). Red line on top shows average of all generated weights. Frames correspond to the action of pour liquid soap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Influence of the number of skeleton joints in the hand representation. Motion representation model trained on SHREC. Action recognition accuracy validated on F-PHAB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II</head><label>II</label><figDesc>shows the influence of using different classes to discriminate motion sequences while training the motion representation model. Higher class granularity (28 action categories) manages to improve the cross-domain performance by learning more informative motion descriptors. From now on we set this class granularity as default for training.</figDesc><table><row><cell>SHREC categories</cell><cell>1:3</cell><cell>1:1</cell><cell>3:1</cell><cell>cross-person</cell></row><row><cell>14</cell><cell>58.3</cell><cell>65.4</cell><cell>65.9</cell><cell>48.9</cell></row><row><cell>28</cell><cell>66.3</cell><cell>71.0</cell><cell>73.8</cell><cell>53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Influence of the training categories.</figDesc><table><row><cell>Motion</cell></row><row><cell>representation model trained on SHREC with 7-joint skele-</cell></row><row><cell>tons. Action recognition accuracy evaluated on F-PHAB.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Influence of the motion sequence summarization technique. Motion representation model trained on SHREC (7-joint skeletons and 28 labels). Action recognition accuracy evaluated on F-PHAB. Last TCN descriptor: descriptor generated by the TCN at the last time-step. Summarization: descriptor generated by our summarization module.</figDesc><table><row><cell>Model</cell><cell>1:3</cell><cell>1:1</cell><cell>3:1</cell><cell>cross-person</cell></row><row><cell>RGB [14]</cell><cell>-</cell><cell>75.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Depth [15]</cell><cell>-</cell><cell>70.61</cell><cell>-</cell><cell>-</cell></row><row><cell>LSTM [34]</cell><cell>58.75</cell><cell>78.73</cell><cell>84.82</cell><cell>62.06</cell></row><row><cell>DD-Net [9]</cell><cell>75.09</cell><cell>81.56</cell><cell>88.26</cell><cell>71.8</cell></row><row><cell>Gram Matrix [10]</cell><cell>-</cell><cell>85.39</cell><cell>-</cell><cell>-</cell></row><row><cell>Two-stream NN [19]</cell><cell>-</cell><cell>90.26</cell><cell>-</cell><cell>-</cell></row><row><cell>DD-Net [9]</cell><cell>59.6</cell><cell>63.7</cell><cell>67.5</cell><cell>51.2</cell></row><row><cell>Ours</cell><cell>70.6</cell><cell>75.5</cell><cell>77.7</cell><cell>58.4</cell></row><row><cell>Ours*</cell><cell>76.2</cell><cell>79.7</cell><cell>82.0</cell><cell>62.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* includes an augmented motion reference set</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>: F-PHAB accuracy comparison. Upper block:</cell></row><row><cell>results for methods trained on the F-PHAB dataset (intra-</cell></row><row><cell>domain classification). Bottom block: methods trained on</cell></row><row><cell>SHREC-17 dataset (cross-domain classification).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>MSRA accuracy comparison. Batched vs. online predictions. Our results correspond to our motion representation model trained on SHREC-17 data (cross-domain).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Intra-domain classification on SHREC-17.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII :</head><label>VII</label><figDesc>Intra-domain classification on F-PHAB.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code, learned models, supplementary video and data splits can be found in: https://sites.google.com/a/unizar.es/filovi/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of multimodal heading and pointing gestures for co-located mixed reality human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krupke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lubos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jonetzko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative model for intention recognition and manipulation assistance in teleoperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interaction relational network for mutual action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multimodal human-robot interaction manager for assistive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Monaikul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rysbek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zefran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-line simultaneous learning and recognition of everyday activities from virtual reality performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramirez-Amaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Inamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM multimedia asia</title>
		<meeting>the ACM multimedia asia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient temporal sequence comparison and classification using gram matrix embeddings on a riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shrec&apos;17 track: 3d hand gesture recognition using a depth and skeletal dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DOR-10th Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">First-person hand action benchmark with rgb-d videos and 3d hand pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatialtemporal attention res-tcn for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition using an enhanced network with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3680</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A two-stream neural network for pose-based hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2101</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One-shot action recognition in challenging therapy scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict &amp; cluster: Unsupervised skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning a 3d human pose distance metric from geometric pose descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On geometric features for skeletonbased action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mfanet: Motion feature augmented network for dynamic hand gesture recognition from skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">239</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d posturenet: A unified framework for skeleton-based posture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="143" to="149" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1803</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ISCA Speech Synthesis Workshop</title>
		<imprint>
			<biblScope unit="page" from="125" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1911</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rate-invariant autoencoding of time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koneripalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1901</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1911</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
