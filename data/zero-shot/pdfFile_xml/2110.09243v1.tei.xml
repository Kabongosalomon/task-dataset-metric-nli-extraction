<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging MoCap Data for Human Mesh Recovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Br?gier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging MoCap Data for Human Mesh Recovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training state-of-the-art models for human body pose and shape recovery from images or videos requires datasets with corresponding annotations that are really hard and expensive to obtain. Our goal in this paper is to study whether poses from 3D Motion Capture (MoCap) data can be used to improve image-based and video-based human mesh recovery methods. We find that fine-tune image-based models with synthetic renderings from MoCap data can increase their performance, by providing them with a wider variety of poses, textures and backgrounds. In fact, we show that simply fine-tuning the batch normalization layers of the model is enough to achieve large gains. We further study the use of MoCap data for video, and introduce PoseBERT, a transformer module that directly regresses the pose parameters and is trained via masked modeling. It is simple, generic and can be plugged on top of any state-of-the-art image-based model in order to transform it in a video-based model leveraging temporal information. Our experimental results show that the proposed approaches reach state-ofthe-art performance on various datasets including 3DPW, MPI-INF-3DHP, MuPoTS-3D, MCB and AIST. Test code and models will be available soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art methods that estimate 3D body pose and shape given an image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref> or a video <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref> have recently shown impressive results. A major challenge when training models for in-the-wild human pose estimation is data: collecting large sets of training images with groundtruth 3D annotations is cumbersome as it requires setting up IMUs <ref type="bibr">[47]</ref>, calibrating a multi-camera system <ref type="bibr" target="#b11">[12]</ref> or considering static poses <ref type="bibr" target="#b24">[25]</ref>. In practice, only 2D information such as 2D keypoint locations or semantic part segmentation can be manually annotated. Current methods therefore leverage this in-the-wild data with partial ground-truth an- * indicates equal contribution. Thibault Groueix is now at Adobe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input sequence SPIN + VIBE</head><p>88.1 78.9 PA-MPJPE (central frame, mm):  <ref type="bibr" target="#b19">[20]</ref> and the proposed method. We achieve stateof-the-art results for both image-and video-based pose estimation on a large variety of datasets (here a video sequence from AIST <ref type="bibr" target="#b42">[43]</ref>) by leveraging MoCap data. notations by defining their losses on the 2D reprojection of the 3D estimation <ref type="bibr" target="#b16">[17]</ref>, by running an optimization-based method <ref type="bibr" target="#b2">[3]</ref> beforehand and curate the obtained groundtruth <ref type="bibr" target="#b23">[24]</ref> or by running the optimization inside the training loop <ref type="bibr" target="#b20">[21]</ref>. This lack of annotated real-world data is even more critical for videos, making difficult the use of recent temporal models such as transformers <ref type="bibr" target="#b45">[46]</ref> which are known to require large datasets for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCap-SPIN + PoseBERT</head><p>At the same time, Motion Capture (MoCap), widely employed in the video-game and film industry, offers a solution to create large corpus of motion sequences with ac-curate ground-truth 3D poses. Recently, several of these MoCap datasets have been unified into the large AMASS dataset <ref type="bibr" target="#b28">[29]</ref>. Importantly, all the considered MoCap sequences were converted into realistic 3D human meshes represented by a rigged body model, concretely SMPL <ref type="bibr" target="#b26">[27]</ref>, a differentiable parametric model employed by most stateof-the-art human mesh recovery methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref>. In previous works, MoCap data have been used to train a discriminator for image-based <ref type="bibr" target="#b16">[17]</ref> or video-based <ref type="bibr" target="#b19">[20]</ref> models. In this case, these data are only used to force the model to predict a realistic output, without improving its capability to better estimate the poses by seeing more diverse examples. Another way to exploit MoCap data consists in rendering it, applying a texture and adding a background image as in <ref type="bibr" target="#b44">[45]</ref>, to generate synthetic images with groundtruth annotations. However, the inherent domain shift between synthetic and real-world images has limited the use of such data to pretraining <ref type="bibr" target="#b43">[44]</ref>.</p><p>In this paper, we study how MoCap data can be used in order to improve methods that estimate SMPL parameters in images or videos. For image-based models, we show that fine-tuning SPIN <ref type="bibr" target="#b20">[21]</ref>, a state-of-the-art method, while adding renderings from AMASS to its original training data of real images, improves its performance. We hypothesize this is a consequence of the model being exposed to a wider and more diverse set of poses. For added realism, we follow SURREAL <ref type="bibr" target="#b44">[45]</ref> and render textured SMPL models on top of random background images, increasing the invariance of the model to texture and background change. Surprisingly, we show that fine-tuning only the batch normalization layers of the pretrained SPIN model suffices to get state-of-theart performance for image-based models.</p><p>We further utilize MoCap data for learning better video models, and introduce PoseBERT, a transformer-based model tailored for pose estimation that directly regresses the pose parameters. PoseBERT takes as input estimated SMPL pose parameters and can be learned on the AMASS dataset using masked modeling, similar to BERT <ref type="bibr" target="#b7">[8]</ref>. This module can be plugged on top of any state-of-the-art image-based model in order to transform it into a video-based model. A qualitative comparison between SPIN and the proposed model is shown in 1. In summary, our contributions are:</p><p>? We leverage MoCap data to make image-based models more robust by fine-tuning using synthetic renderings. ? We introduce PoseBERT, a transformer model that regresses pose parameters and is trained using MoCap data via masked modeling. It is a plug-and-play unit that allows any image-based model to leverage temporal context. ? We exhaustively ablate all aspects of our methods and present consistent gains over the state of the art on several challenging datasets captured in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We review related work on the estimation of the SMPL parameters from images or videos as well as the use of synthetic data for human pose and shape estimation.</p><p>SMPL from images. SMPLify <ref type="bibr" target="#b2">[3]</ref> first introduced an optimization-based method to find the SMPL parameters that best explain a set of detected 2D keypoints by leveraging various priors, but it remains sensible to the initialization. Since then, most deep learning methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> process image crops around a person of interest to directly estimate these SMPL parameters. In order to handle real-world images, they are trained on a mix of real images where annotations are limited to 2D keypoints, and synthetic or MoCap images for which 3D poses are available. Losses, typically applied on the difference between SMPL 2D or 3D keypoint predictions and the corresponding annotations, can also be applied on the vertices <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> or on texture correspondences <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref>. In some works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, an additional discriminator ensures the realism of the predicted SMPL parameters. SPIN <ref type="bibr" target="#b20">[21]</ref> combines deep learning-based method with optimizationbased approaches by using the optimization stage to refine the prediction made by the network, which is later used in upcoming epochs. In I2L-MeshNet <ref type="bibr" target="#b31">[32]</ref>, vertices' locations are estimated with heatmaps for each mesh vertex coordinate instead of directly regressing the parameters while in Pose2Mesh <ref type="bibr" target="#b6">[7]</ref>, a 2D pose is first lifted to 3D to obtain a coarse mesh which is then iteratively refined. Both methods are trained on images where SMPL pseudo-ground-truth was obtained from 2D keypoints using SMPLify <ref type="bibr" target="#b2">[3]</ref>, which can result in inaccurate 3D data. Instead, we employ the final estimations in the SPIN training procedure to train with direct supervision and fully exploit synthetic data.</p><p>SMPL from videos. While Arnab et al. <ref type="bibr" target="#b1">[2]</ref> proposed an optimization-based strategy to handle human pose estimation in videos, recent methods are mostly based on deep learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b14">15]</ref>. In HMMR <ref type="bibr" target="#b17">[18]</ref>, features from consecutive frames are fed to a 1D temporal convolution, while VIBE <ref type="bibr" target="#b19">[20]</ref> uses recurrent neural network, namely Gated Recurrent Unit (GRU), together with a discriminator at the sequence level. The network is trained on different in-thewild videos and losses are similar to the ones employed for images and described above, i.e., mainly applied on keypoints. A similar architecture with GRU is used in TCMR <ref type="bibr" target="#b5">[6]</ref>, except that 3 independent GRUs are used and concatenated, one in each direction and one bi-directional in order to better leverage temporal information. MEVA <ref type="bibr" target="#b27">[28]</ref> estimates motion from videos by also extracting temporal features using GRUs and then estimates the overall coarse motion inside the video with Variational Motion Estimator (VME). Recently, Pavlakos et al. <ref type="bibr" target="#b35">[36]</ref> have proposed to use a transformer architecture <ref type="bibr" target="#b45">[46]</ref> in a concurrent work. To ob-(a) Different possible renderings for a ground-truth pose from AMASS <ref type="bibr" target="#b28">[29]</ref>. The same pose can be rendered with random camera viewpoint, texture, background to generate diverse synthetic training data.</p><p>(b) Pairs of augmented real images from COCO <ref type="bibr" target="#b25">[26]</ref>, LSPE <ref type="bibr" target="#b15">[16]</ref>, MPII <ref type="bibr" target="#b0">[1]</ref> and corresponding synthetic renderings depicting the same pose with a different background and body appearance. tain training data, i.e., in-the-wild videos annotated with 3D mesh information, they use the smoothness of the SMPL parameters over consecutive frames to obtain pseudo-groundtruth. In terms of architecture, the transformer is used to leverage temporal information by modifying the features. We also consider a transformer architecture but, in our case, it is directly applied to the sequences of SMPL parameters. It has the great advantage of being directly trainable on Mo-Cap data and pluggable to any image-based method.</p><p>Learning with synthetic data. Employing synthetic training images is a standard strategy to overcome the lack of large scale annotated data in computer vision <ref type="bibr" target="#b10">[11]</ref>. This is particularly the case for human 3D pose estimation as it is not possible to accurately annotate 3D information on a large corpus of in-the-wild images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Usually, a sim2real domain gap arises between synthetic and real examples that needs to be carefully handled to ensure generalization. For example, Chen et al. <ref type="bibr" target="#b4">[5]</ref> employed computer generated images of people to train a 3D pose estimation network with a domain mixer adversarially trained to discriminate between real and synthetic images, and therefore handle domain adaptation. More recently, SimPose <ref type="bibr" target="#b49">[50]</ref> proposed to train a model for 2.5D pose estimation using a mixed of synthetic and real data where 2D losses are applied to real data while 2.5D and 3D losses are applied to synthetic data only, which may affect generalization. Kundu et al. <ref type="bibr" target="#b22">[23]</ref> proposed to estimate an auto-encoder of 3D pose, together with a network that processes real images and output 2D poses. Losses are also applied on the interleaved network to allow training a model to estimate 3D pose in the wild. However, this does not allow the image feature extractor to see more variability in terms of poses. Closer to us is the work by Varol et al. <ref type="bibr" target="#b44">[45]</ref> who apply real textures on renderings of the SMPL parametric model completed with real background images to generate a large and varied dataset. In a subsequent work, they trained a network for volumetric body estimation <ref type="bibr" target="#b43">[44]</ref> by pretraining on this large synthetic data and fine-tuning on more limited real data. Doesh et al. <ref type="bibr" target="#b8">[9]</ref> show that motion through the optical flow in synthetic videos from <ref type="bibr" target="#b44">[45]</ref> allows to bridge the sim2real gap for 3D human pose estimation. In this paper, we also leverage SMPL renderings but fully exploit the synthetic data, i.e., without limiting its use to pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MoCap data for human mesh recovery</head><p>In this section we present two ways of using MoCap data for 3D mesh estimation in images and videos. We first explore MoCap data for regularizing image-based models via fine-tuning using synthetic renderings (Section 3.1). We then introduce PoseBERT, a pose estimation-oriented transformer architecture that is trained on MoCap data and enables the model to leverage temporal context (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improving image-based pose estimation</head><p>Existing datasets with ground-truth pose annotations for real-world images are not large enough to capture the variability of images/video sequences that can be encountered at test time; this can limit the generalization capabilities of a pose estimation method. We argue that one way to mitigate this issue is through the use of renderings from MoCap data. We leverage such synthetic data to regularize a strong image-based model like SPIN <ref type="bibr" target="#b20">[21]</ref>. SPIN is a 3D pose estimation model pretrained on datasets with paired RGB and 2D and/or 3D pose annotations. We propose to fine-tune this model using synthetic renderings and therefore expose it to a more diverse set of poses, viewpoints, textures and background changes from the ones seen during pretraining.</p><p>Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, we use a deep convolutional neural network encoder f for feature extraction from image crops followed by an iterative regressor r. Let x ? R d be an input crop, ? x = f (x) be the extracted feature and r(? x ) = {? x , ? x } the regressed prediction for the d ? -dimensional body model parameters ? x and the d ? -dimensional camera parameters ? x . The regressor r is initialized with the mean pose parameters ? mean and is run for a number of N  <ref type="figure">Figure 3</ref>: Regularizing image-based human mesh recovery model. We leverage synthetic renderings of MoCap data from AMASS <ref type="bibr" target="#b28">[29]</ref> together with real images to regularize SPIN <ref type="bibr" target="#b20">[21]</ref>. To fine-tune SPIN, we train the batchnormalization layers <ref type="bibr" target="#b12">[13]</ref> of the CNN backbone.</p><p>iterations. Similar to <ref type="bibr" target="#b20">[21]</ref>, our network uses the 6D representation proposed in <ref type="bibr" target="#b48">[49]</ref> for 3D joints orientation.</p><p>An overview of the fine-tuning process is depicted in <ref type="figure">Figure 3</ref>. We sample poses from AMASS <ref type="bibr" target="#b28">[29]</ref>, a large collection of MoCap data, and use them together with realworld images for fine-tuning the SPIN model. Given that we are starting from a state-of-the-art-model, apart from fine-tuning all of the backbone parameters, we also experiment with only fine-tuning a subset. Specifically, and motivated by a recent work <ref type="bibr" target="#b9">[10]</ref>, we experiment with only finetuning the parameters of the batch normalization <ref type="bibr" target="#b12">[13]</ref> layers, i.e. the affine transformation in each such layer, together with the running statistics.</p><p>Providing both real and synthetic data is crucial since fine-tuning only using synthetic data would hurt performance due to the synthetic/real data domain shift. We therefore fine-tune the SPIN model with batches that partially contain the same real data as used in SPIN during training, together with our synthetic renderings from AMASS poses. We initialize the weights with the released model from SPIN, and use direct supervision: for synthetic data we use the corresponding ground-truth SMPL parameters, for real data we use the final fits from the SPIN training procedure as pseudo ground-truth. We refer to a SPIN model fine-tuned this way as MoCap-SPIN.</p><p>Rendering synthetic humans. To generate a synthetic rendering of a human in a given pose, we render the corresponding SMPL <ref type="bibr" target="#b26">[27]</ref> model with a random texture from the SURREAL dataset <ref type="bibr" target="#b44">[45]</ref>, using a random background image from LSUN training set <ref type="bibr" target="#b47">[48]</ref> to provide further data augmentation. <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of such renderings.</p><p>We randomly sample SMPL pose and shape parameters from AMASS <ref type="bibr" target="#b28">[29]</ref>, which provides us with a great source of diverse poses. Camera 3D orientation is sampled uniformly considering a Tait-Bryan parametrization (?180 ? yaw, ?45 ? pitch, ?15 ? roll, with yaw and roll axes horizontal when considering the identity rotation) to model typical variability observed in real data. Synthetic renderings are cropped around the person in a similar manner as for real images, based on the location of 2D joints of the model. We model the fact that people are not always entirely visible in real crops by considering only upper body keypoints up to the hips or the knees in 20% of the cases. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates the variability of renderings given a single pose.</p><p>In order to verify that sampling diverse poses from large MoCap datasets is preferable over creating renderings of the poses that already appear in the commonly used datasets like COCO <ref type="bibr" target="#b25">[26]</ref>, MPII <ref type="bibr" target="#b0">[1]</ref>, LSP and LSPE <ref type="bibr" target="#b15">[16]</ref>, we further experiment with rendering poses from such datasets as well. Although restricted to the set of training poses, augmenting the dataset with synthetic renderings of the same poses can offer greater variability in textures and backgrounds. We use the pseudo-ground-truth (i.e., final fits from SPIN) pose, shape and camera parameters from the real data to render synthetic humans following the process presented above. Examples of pairs of real data and synthetic renderings are illustrated in <ref type="figure" target="#fig_1">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Capturing temporal context with transformers</head><p>In this section, we present PoseBERT, a transformerbased architecture that regresses directly the SMPL parameters for every pose in a temporal sequence. Although concurrent works have also utilized temporal models based on GRUs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref> or transformers <ref type="bibr" target="#b35">[36]</ref> for human mesh recovery, they utilize context to enhance the visual features, and still require an iterative regressor on top of the temporal model for regressing the SMPL parameters.</p><p>The basic PoseBERT block. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the architecture of PoseBERT. The input is T vectors, corresponding to a temporal sequence of poses of length T and the output is a sequence of SMPL pose parameters predictions, one for each frame. We propose to incorporate the regressor as part of the basic transformer block: we replace the feed-forward network that follows the multi-headed dot-product attention with a regressor r, similar to the one used in <ref type="bibr" target="#b16">[17]</ref>. This gives us a main advantage: at every layer of PoseBERT, we dynamically attend to parts of the input sequence and refine the regressed SMPL pose prediction accordingly.</p><p>Since we are learning from synthetic data, pose groundtruth is available. Despite the fact that the transformer only sees ground-truth poses during training, as we experimentally show in Section 4, the learned model can generalize to noisy poses during testing, i.e., when the input is instead the pose predictions from any image-based model.</p><p>The basic block of PoseBERT is repeated L times. Inside each block, the input is first fed to a multi-head scaled dotproduct attention mechanism and then to a regressor MLP. For the latter we use the architecture from <ref type="bibr" target="#b16">[17]</ref> and share the regressor parameters across the T inputs. Although the regressor can be iterative as in <ref type="bibr" target="#b16">[17]</ref>, given that this is a process that happens at every layer, i.e., L times, we find that one iteration is enough (see ablation in Section 4). Moreover, we experiment with versions of PoseBERT where the regressor parameters are shared across the L blocks, thus reducing the number of learnable parameters.</p><p>Like the original transformer <ref type="bibr" target="#b45">[46]</ref>, we use layer normalization before self-attention modules. For the first layer only, and similarly to <ref type="bibr" target="#b45">[46]</ref>, we add a 1-D positional encoding to the input which is learned from scratch. We also initialize the first layer regressor with the mean pose ? mean . Finally, we first learn a linear projection to D t dimensions before we feed the input to the transformer; the choice of this parameter directly influences the size of the model.</p><p>Learning PoseBERT parameters. Looking at context can help to correct errors of models based on single images. By inputting a sequence of poses to PoseBERT, we want to be able to learn temporal dynamics. To do so, we utilize the masked modeling task, one of the self-supervised tasks that the now ubiquitous BERT [8] model uses to learn language models. We also add random Gaussian noise to the input. <ref type="figure" target="#fig_4">Figure 5</ref> shows an overview of the training process. The inputs are first masked and noise is added. This is then given to PoseBERT as an input. PoseBERT outputs are then compared to the original clean inputs and similarly to the imagebased model; for every timestep there is a loss on the SMPL pose parameters as well as a 3D keypoint loss.</p><p>In masked modeling, part of the input is masked before it is fed to the model. The correct output is expected to be recovered using the rest of the sequence. In practice, and for each input sequence, we create a T -dimensional random binary vector, where each timestamp has a probability m/T % to be masked, where m is an hyperparameter To allow the model to learn temporal dynamics and similar to <ref type="bibr" target="#b7">[8]</ref> we mask part of the input and either replace it with a learnable masking token or a random pose. The mask is a random T -dimensional binary vector that specifies which timestamps will be masked.</p><p>that controls the number of frames of the input that will be masked. For the m timesteps that are randomly chosen to be masked, we replace the input pose with a learnable masked token and prohibit the self-attention module to attend to those timesteps. We ablate m in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>After presenting datasets and metrics in Section 4.1, we perform extensive evaluations and ablations of MoCap-SPIN (Section 4.2) and of PoseBERT (Section 4.3). We finally compare to the state of the art in Section 4.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets, training and metrics</head><p>MoCap data. We use AMASS <ref type="bibr" target="#b28">[29]</ref>, which is a collection of numerous Motion Capture datasets in a unified SMPL format, representing more than 45 hours of recording.</p><p>Real-world data (SPIN data). When fine-tuning the image-based model, we use the same real-world training data as SPIN <ref type="bibr" target="#b20">[21]</ref>, namely 2D pose estimation datasets like COCO <ref type="bibr" target="#b25">[26]</ref>, LSP and LSPE <ref type="bibr" target="#b15">[16]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>, as well as a 3D pose estimation dataset, MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref>. We were not able to use Human 3.6M <ref type="bibr" target="#b13">[14]</ref> due to license restriction.</p><p>Training. We first train MoCap-SPIN by fine-tuning the BN layers of a pre-trained SPIN model on SPIN data plus AMASS. We then train PoseBERT solely on MoCap data.</p><p>Test datasets and metrics. For evaluation, we use the 3DPW [47] test set, the MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref> test set, the MuPoTS-3D dataset <ref type="bibr" target="#b30">[31]</ref> and the AIST dataset <ref type="bibr" target="#b42">[43]</ref> that contains more challenging poses from people dancing. For image-based models, we also use the Mannequin Challenge Benchmark (MCB) <ref type="bibr" target="#b24">[25]</ref>. We report the mean per-joint error (MPJPE) before and after procrustes alignment (PA-MPJPE) in millimeters (mm). For 3DPW, following the related work, we also report the mean per-vertex position error (MPVPE). To measure the jittering of the estimations on the video datasets (3DPW, MPI-INF-3DHP, MuPoTS-3D, AIST), we follow <ref type="bibr" target="#b17">[18]</ref> and report the acceleration error, measured as the average difference between ground-truth and predicted acceleration of the joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DPW [47]</head><p>MuPoTS-3D <ref type="bibr" target="#b30">[31]</ref> AIST <ref type="bibr" target="#b42">[43]</ref> MPI </p><formula xml:id="formula_0">-INF-3D [30] MCB [25] MPJPE ? E ? MPVPE ? MPJPE ? E ? MPJPE ? E ? MPJPE ? E ? MPJPE ? SPIN [21</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of MoCap-SPIN</head><p>We first evaluate and ablate MoCap-SPIN, our approach to fine-tune a state-of-the-art image-based model using synthetic renderings of MoCap data. In <ref type="table" target="#tab_0">Table 1</ref> we report results on five datasets after fine-tuning the publicly available pretrained model of SPIN 1 . In the top section we report sanity checks, i.e., performance when fine-tuning the model with the same data used during pretraining (denoted as SPIN data). During early experiments, fine-tuning all model parameters using both real and synthetic renderings made the model quickly overfit and performance on the test datasets started to diverge as training progressed. To mitigate overfitting, we explored fine-tuning only subsets of the backbone parameters, e.g. only the regressor, only the last layers or to freeze the batch normalization layers and statistics. Inspired by a recent paper <ref type="bibr" target="#b9">[10]</ref> we also experimented with training only the batch normalization layer parameters, freezing the rest of the network.</p><p>Surprisingly, even without additional synthetic data, fine-tuning only the batch normalization layers provides a consistent boost on most datasets and metrics (second row of <ref type="table" target="#tab_0">Table 1</ref>). In fact, fine-tuning only the batch normalization layer parameters was the setting that enabled us to finetune using synthetic data. As shown in the bottom section of <ref type="table" target="#tab_0">Table 1</ref>, we observe large gains in performance across all datasets in both cases. This behavior about batch-norm statistics has already been shown by <ref type="bibr" target="#b39">[40]</ref>. When the renderings come from poses sampled from SPIN data, gains show that learning with diverse backgrounds and textures make the model more robust. The gains are however even larger when the renderings come from poses sampled from AMASS; sampling a more diverse set of poses seems to directly affect generalization performance. For the rest of the experiments, we will refer to MoCap-SPIN as the SPIN model whose batch normalization parameters were finetuned using both real data and MoCap renderings.</p><p>Ablating the balance between real and synthetic data in each batch. We study the impact of varying the percentage of synthetic renderings inside each batch. Results are shown  <ref type="figure">Figure 6</ref>: Varying the percentage of synthetic data. We report the MPJPE metric on four datasets when varying the percentage of AMASS rendering at each batch. We further plot the average across the four datasets (dashed black curve); the best performance overall is achieved when the batch is equally split between real and synthetic data.</p><p>in <ref type="figure">Figure 6</ref>. On all datasets, we observe a U shape curve as the percentage of synthetic data increases, meaning that the optimal value is not an extreme value (neither only real data nor only synthetic). Using half of synthetic data is a good compromise on all datasets.</p><p>Analysis of the performance gain. To better understand in which case our proposed MoCap-SPIN improves performance, we plot in <ref type="figure">Figure 7</ref> (left) an histogram of the gain compared to the original SPIN model on 3DPW in terms of MPJPE. We observe that most samples are improved by a few millimeters. The overall shape of the histogram has a shape similar to a Gaussian, meaning that some images have actually a higher error, and a few samples are significantly improved. The histogram on the right in <ref type="figure">Figure 7</ref> shows the impact of each bin on the final MPJPE metric.</p><p>Study of the real/synthetic domain gap. In order to study the impact of the domain gap between real and synthetic data, we consider the test set of 3DPW as an oracle and gradually make the renderings closer to this oracle. Sampling the SMPL pose parameters from 3DPW test set has low impact on the performance (0.3mm in MPJPE), meaning that the AMASS dataset sufficiently covers the pose space. We then additionally replace the randomized choice  <ref type="figure">Figure 7</ref>: Performance gain analysis on 3DPW. We study the gain of our MoCap-SPIN over SPIN <ref type="bibr" target="#b20">[21]</ref>. Left: Histogram of the gains in PA-MPJPE. Our method outperforms SPIN on 26393 test samples out of 35515. Right: Resulting performance gains for each bin in (left), obtained by multiplying the absciss of each bin with the fraction of samples inside it. Notice that the bins corresponding to small improvements have many samples, and are the foundation of our 4 points gained in PA-MPJPE over SPIN. <ref type="bibr">GT</ref>   of the global orientation by the ones of the corresponding relative pose from the 3DPW test set, thus rendering people in the same 3D global pose, but at random 2D positions in the image. This has an impact of 1 to 2mm, similar to the gain obtained when also keeping the exact same 2D positions by sampling the ground-truth camera parameters with the global pose. We finally keep the background image from the original data, in which case only the texture over the SMPL ground-truth is synthetically changed compared to the original images, and get an additional 1mm error reduction with a MPJPE of 85.2mm. Overall, MoCap-SPIN is close in performance to the strongest oracle because training only the batch-normalization layers of the feature extraction network f strongly constrains the learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of PoseBERT</head><p>In this section, we evaluate and ablate PoseBERT, our video-based model trained using MoCap data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study of different architectures.</head><p>As a first step we study the impact of some architecture design choices and report the PA-MPJPE on three datasets in <ref type="table" target="#tab_4">Table 3</ref>. As a default training strategy we mask 12.5% of the input poses for this ablation. First we note that PoseBERT leads to a consistent gain of 1 to 3mm on all datasets. Removing the positional encoding leads to a suboptimal performance indicating that incorporating temporal information within the network is a key design choice. Sharing the regressor allows to reduce the number of learnable parameters and leads to better  predictions. More importantly we notice that we can iterate over the regressor only a single time after each layer given that doing more iterations does not improve and even slightly decreases the performance. In terms of model size, the benefit of PoseBERT seems to be reached with a depth of L = 4 and an embedding dimension of D t = 512. We choose these hyperparameters since increasing the model complexity leads to minimal improvements. For the temporal length of the training sequence, we set T = 16, as longer sequences do not lead to further improvements.</p><p>Training strategies. We then study the impact of various training strategies on MoCap datasets in <ref type="table" target="#tab_7">Table 5</ref>. First, we study the impact of partially masking the input sequences, and observe that masking 12.5%, i.e., 2 frames out of 16, lead to smoother prediction (lower error acceleration) while the PA-MPJPE remains low. We also try adding Gaussian noise, with a standard deviation of 0.05 on top of the axisangle representation, and obtain a small additional boost of performance and smoother predictions. Increasing the standard deviation did not bring any benefit. Motivations for adding Gaussian noise is described in the Supplementary Material.</p><p>Plugging PoseBERT on top of existing approaches. One major key benefit of PoseBERT is that it can be plugged on top of any image-based model to transform it into a videobased model since it takes only SMPL sequences as input compared to other methods (VIBE, TCMR) which require <ref type="bibr">3DPW [47]</ref> MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref> MuPoTS-3D <ref type="bibr" target="#b30">[31]</ref> AIST <ref type="bibr" target="#b42">[43]</ref> Method    </p><formula xml:id="formula_1">E ? MPJPE ? MPVPE ? Accel ? E ? MPJPE ? Accel ? E ? MPJPE ? Accel ? E ? MPJPE ? Accel</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to the state of the art</head><p>Finally we compare our image-based and video-based methods against previous works in <ref type="table" target="#tab_6">Table 4</ref>. Single image. We compare MoCap-SPIN with HMR <ref type="bibr" target="#b16">[17]</ref>, GraphCMR <ref type="bibr" target="#b21">[22]</ref>, SPIN <ref type="bibr" target="#b20">[21]</ref>, I2L-MeshNet <ref type="bibr" target="#b31">[32]</ref>, and Pose2Mesh <ref type="bibr" target="#b6">[7]</ref> on four datasets. MoCap-SPIN outperforms all other methods on all datasets in PA-MPJPE, for instance by 6.1mm MPJPE on 3DPW.</p><p>Video. Finally we compare PoseBERT against other videobased methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>. We emphasize that compared to other approaches which require image-based features, Pose-BERT is trained only using MoCap data from AMASS and takes SMPL sequences as input. For 3DPW [47], Pose-BERT outperforms concurrent works on the PA-MPJPE and MPVPE metrics. We do not see any improvement by finetuning on 3DPW train. For MPI-INF-3DHP, PoseBERT demonstrates results on par with the state of the art while it is worth mentioning that, compared to all other methods, our temporal module does not use the associated training set. Finally on MuPoTS-3D <ref type="bibr" target="#b30">[31]</ref> and AIST <ref type="bibr" target="#b42">[43]</ref>, PoseBERT shows gains on all metrics over VIBE and TCMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we propose two ways of leveraging MoCap data to improve image and video-based human 3D mesh recovery. We first present MoCap-SPIN, a very strong baseline that reaches state-of-the-art performance for imagebased models on a number of datasets that is obtained by simply fine-tuning the batch normalization layer parameters of SPIN with the use of real data and synthetic renderings of MoCap data. We further introduce PoseBERT, a transformer module that directly regresses the SMPL pose parameters and is purely trained on MoCap data via masked modeling. Our experiments show that PoseBERT can be readily plugged on top of any image-based model to leverage temporal context and improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>In this supplementary material, we first present additional results and illustrations for MoCap-SPIN (Section 7) and PoseBERT (Section 8). We then present <ref type="figure" target="#fig_0">Figure 12</ref> an extended version of paper's <ref type="figure" target="#fig_0">Figure 1</ref>, depicting qualitative results for additional pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Additional results for MoCap-SPIN</head><p>In this section, we first illustrate our oracle experiments presented in <ref type="table" target="#tab_2">Table 2</ref> of the main paper as well as an interesting experiment where we evaluate SPIN on synthetic renderings (Section 7.1). We then present more results on the fine-tuning of MoCap-SPIN (Section 7.2). In particular, we ablate the choice of only fine-tuning the batchnormalization layers. We finally study the impact of using feature alignment loss when using paired data (Section 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Real to synthetic renderings domain gap</head><p>Illustration of the oracle experiment renderings. <ref type="figure" target="#fig_7">Figure 8</ref> illustrates the different kind of renderings used in our oracle experiments ( <ref type="table" target="#tab_2">Table 2</ref> of the main paper). Starting from a real test image (left), we gradually remove some parts of the ground-truth real data, i.e., background, camera translation, global orientaiton and pose. The latter case is MoCap-SPIN's setting, where we do not use any test data information, and only use renderings from the AMASS dataset.</p><p>SPIN on synthetics. The gap between in-the-wild images and renderings of MoCAP data is central to our fine-tuning strategy. We aim to learn pose estimation from synthetic renderings and generalize to in-the-wild image. To better understand the domain gap, we study an opposite but informative problem : how does a model trained on in-thewild images generalize to synthetic renderings? To that end, we generated several versions of 3DPW. For each data sample of 3DPW, we render the ground-truth pose using the same camera parameters, using either black or random background, and either metallic or SURREAL textures. We compare the performance of SPIN on each dataset version in <ref type="figure">Figure 9</ref>. We observe two phenomenons. First, SPIN performs better on black backgrounds than random backgrounds. Segmenting the human in the crop does make the prediction task easier. Second, SPIN performs better when SURREAL textures are used instead of metallic textures. Indeed, SURREAL textures makes the renderings look more realistic and reduces the domain gap. Most importantly, the strong performance of SPIN on renderings using random backgrounds and SURREAL textures (-4.5mm of PA-MPJPE) motivates MoCap-SPIN. Indeed, we empirically observe that SPIN, a model trained on in-the-wild images, can generalize to synthetic data. This means that the the domain gap between in-the-wild and synthetic data is not huge. In the main paper, we show that MoCap-SPIN (our fine-tuning that uses additional synthetic renderings) brings a 4mm improvement in PA-MPJPE over SPIN on the real-image version of 3DPW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Training different parts of the MoCap-SPIN</head><p>network.</p><p>In this section, we study the impact of fine-tuning only the batch-normalization layers of MoCap-SPIN. In <ref type="table">Table 7</ref>, we study the performance of fine-tuning different groups of parameters of SPIN. Fine-tuning only the batchnormalization layers outperforms other settings. In particular, fine-tuning all the parameters leads to overfitting, which we show on 3DPW in <ref type="figure" target="#fig_0">Figure 10</ref> and is the central motivation to only fine-tuning batch-normalization layers.  <ref type="table">Table 7</ref>: Additional ablation on MoCap-SPIN trainable parameters. We compare fine-tuning only the batchnormalization layers <ref type="bibr" target="#b9">[10]</ref> with other subsets of SPIN parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Feature alignment loss on paired data</head><p>SPIN is pretrained on a set of datasets that contain 2D and 3D pose ground-truth data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26]</ref>. MoCap-SPIN leverages additional poses from Amass <ref type="bibr" target="#b28">[29]</ref>. We compare in <ref type="table" target="#tab_2">Table 2</ref> of the main paper adding AMASS data against rendering poses from the original SPIN datasets. In the latter case, we utilize the pseudo ground-truth pose information of image x to render an image s of a synthetic human in the same pose. In practice, we use the final fits from the SPIN model <ref type="bibr" target="#b20">[21]</ref> as pseudo ground-truth SMPL parameters for these real-world datasets. For every pair x, s, we extract feature vectors ? x , ? s . Following the domain adaptation literature, we argue that maximizing the similarity between the two features can lead to better domain invariance. The benefits of the latter are twofold: (a) features from synthetic images are better aligned with features from real images, therefore synthetic renderings can be used for learning and (b) features should be more robust to background appearance changes.</p><p>We therefore use a feature alignment loss L align = ??s(? x , ? s ) for every pair x, s, where s(? x , ? s ) is a similarity function between the two feature vectors that can be defined in a number of ways. For example, one can define s as the negative of the mean square error or L2 dis-   <ref type="figure">Figure 9</ref>: Using SPIN pretrained model to study the domain gap between real and synthetic renderings on 3DPW.</p><p>We run SPIN on various synthetic renderings of 3DPW [47] using different backgrounds and textures. SPIN generalizes to renderings with random backgrounds and SURREAL textures.</p><p>tance, the cosine similarity, or the recently popular contrastive loss <ref type="bibr" target="#b3">[4]</ref>. In practice, we use :</p><formula xml:id="formula_2">s MSE (? x , ? s ) = ?||? x ? ? s || 2 2 s cosine (? x , ? s ) = ? x ? ? s max( ? x 2 ? ? s 2 , ) s contr (? x , ? s ) = ? log exp(? T x ? s ) y?N exp(? T x ? y ) ,<label>(1)</label></formula><p>where N is the set of all synthetic images in the batch. For the contrastive loss case, we first 2-2 normalize the features and symmetrise the loss by setting s(? x , ? s ) = s contr (? x , ? s ) + s contr (? s , ? x ). The contrastive loss is essentially a softmax function bringing the correct real-synth features closer to each other and further from all other features from synthetic images in the same batch.</p><p>We report results in <ref type="table">Table 8</ref>. For each loss type, we conduct an hyperparameter search to select the best weight ?. Using the L2 or cosine losses to align the feature do not yield an improvement. We view this as a valuable negative result. Aligning features with the contrastive loss yields a clear quantitative improvement on 3DPW, MPI-INF-3D and MCB and outperforms MoCap-SPIN by 4.2mm of MPJPE on AIST. We explored combining this experiment with MoCap-SPIN i.e. using additional data from amass and aligning paired samples with a contrastive loss, but did not observe performance gains compared to MoCap-SPIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Variance of MoCap-SPIN performance.</head><p>We fine-tune MoCap-SPIN 10 times with different random seeds and report the result in <ref type="table">Table 9</ref>. We observe that our fine-tuning strategy gives consistent and stable improvements over SPIN in each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DPW [47]</head><p>MuPoTS-3D <ref type="bibr" target="#b30">[31]</ref> AIST <ref type="bibr">[</ref>  <ref type="table">Table 8</ref>: Additional experiments with paired real/synthetic data. For brevity, we denote the PA-MPJPE metric as E. We experiment with different losses to align features between a real image and its synthetic rendering using a pseudo groundtruth. We conducted a hyperparameter search for each type of feature alignment loss. We weight by 10 ?3 the L2 and cosine loss, and by 10 ?2 the contrastive loss. <ref type="table">Table 9</ref>: Variance of MoCap-SPIN. We fine-tune SPIN with 10 difference random seed and report the mean, min , max, median and standard deviation of each metric. We conclude that our experiments are stable.</p><formula xml:id="formula_3">3DPW [47] MuPoTS-3D [31] AIST [43] MPI-INF-3D [30] MoCap-SPIN MPJPE ? E ? MPVPE ? MPJPE ? E ? MPJPE ? E ? MPJPE ? E ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Implementation details for MoCap-SPIN</head><p>Our codebase is based on the official Pytorch [34] SPIN release: https://github.com/nkolot/SPIN. In particular, we use the same set of pose losses with the same weights. For MoCap-SPIN, in addition to updating the 5.3 ? 10 4 running statistics, we train the 5.3 ? 10 4 parameters of the batch-norm affine layers out of the 2.7 ? 10 7 SPIN trainable parameters (0.2%). We fine-tune for 14k iterations using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref>, with batches of size 64, with a learning rate of 3 ? 10 ?5 , divided by 10 after 10k and 12k iterations. Training takes about 10 hours on a NVIDIA V100 graphics card. The speed bottleneck is on the CPU side and speed could be improved with more dataloader workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Additional results for PoseBERT</head><p>In this section, we first study the impact on finetuning PoseBERT on real-world training data (Section 8.1) as well as the impact of other architectures and training strategies (Section 8.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Fine-tuning PoseBERT on real-world data</head><p>While we propose to train PoseBERT from scratch using MoCap data only, we also study the impact of fine-tuning on real-world training sets from several datasets, see results in  <ref type="table" target="#tab_0">Table 10</ref>: Fine-tuning on real data. We study the impact of fine-tuning PoseBERT on different training sets. We reported the PA-MPJPE on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">PoseBERT: other training strategies</head><p>In addition to masking and adding Gaussian noise on the input, we have also investigated other training strategies as reported in <ref type="table" target="#tab_0">Table 11</ref>. First we compare against the common practice of having the iterative regressor <ref type="bibr" target="#b35">[36]</ref> on top of the temporal module. PoseBERT shows a gain ranging from 1.4 mm to 0.4mm compare to the baseline described above. Then we increase the temporal window of the input sequence by reducing the frames per second while keeping the sequence length fixed. We observe that increasing the time span does not bring significant improvement and even leads to decrease performances. We also study the impact of incorporating random poses or joints compared to random Gaussian noise as proposed in the main paper. We note that both noise types bring a small improvement compared to Gaussian noise but for simplicity we do not include them during the training scheme of our best model. <ref type="figure" target="#fig_0">Figure 10</ref>: Fine-tuning all parameters lead to overfitting. We compare fine-tuning all the parameters of SPIN against only the batch-normalization layers. Fine-tuning all parameters leads to a clear case of overfitting. On 3DPW, the best performance is reached after 500 iterations, with batches of size 64. On the other hand, fine-tuning only the batch-norm parameters (MoCap-SPIN) leads to monotonically decreasing errors. We report the MPJPE (top) and PA-MPJPE (bottom) on 3DPW test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Motivation and details on adding Gaussian noise</head><p>We see this as a form of data augmentation for the axisangle pose representation. We utilized histograms of axisangle errors in radians shown in <ref type="figure" target="#fig_0">Figure 11</ref> to estimate the standard deviation of the noise to be added (we used 0.10 to cover the error distribution) and simply sample a highdimensional noise vector and add it to the ground truth input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Comparison on the use of 3DPW-train</head><p>In the main paper we follow the standard evaluation protocol of 3DPW [47] which does not allow using 3DPW-train  <ref type="table" target="#tab_0">Table 11</ref>: Additional ablation on the PoseBERT hyperparameters. We first study the impact of having the regressor incorporated into the transformer. We also study the impact of the frame per second for training. However we also provide in <ref type="table" target="#tab_0">Table 12</ref> a comparison against methods using 3DPW-train for training. We do not get any improvement by incorporating 3DPW into our training set but we still get results on part with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">PoseBERT as a plug-and-play module</head><p>Here, we further present results when running Pose-BERT on top of the more recent ROMP method <ref type="bibr" target="#b40">[41]</ref>. Adding PoseBERT on top, we get a decrease of MPJPE (resp. PA-MPJPE) from 91.1 (resp. 56.5) to 90.2 (resp. 55.3) for 3DPW (using the cropping strategy of SPIN). Finally, to test it beyond SMPL inputs, we appended Pose-BERT on top of 2D/3D pose predictions from LCRNet++ <ref type="bibr" target="#b37">[38]</ref> and see significant gains on 3DPW, a decrease of MPJPE (resp. PA-MPJPE) from 125.8 (resp. 68.8) to 108.2   <ref type="table" target="#tab_0">Table 12</ref>: Comparison with state-of-the-art video models. "2D/3D" corresponds to the mix of MPI-INF-3DHP/PennAction/PoseTrack/H36M datasets.</p><p>(resp. 58.5). We believe that these additional results further strengthen our argument that PoseBERT is a robust, plugand-play module for temporal modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative comparison between SPIN+VIBE baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of synthetic human renderings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The PoseBERT architecture. Both input and output are SMPL pose parameters for a temporal sequence of T poses. PoseBERT basic block is repeated L times. The regressor parameters are shared across the T inputs and the L blocks. We regress the pose starting from the mean pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Learning PoseBERT with masked modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label></label><figDesc>https://github.com/nkolot/SPIN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Real test image? ????????????????????????????????????????????????????????? ?AMASS rendering GT relative pose GT global orientation GT camera translation GT background Data samples used in oracle experiments. Left: real image from 3DPW [47] test set. Right: synthetic rendering of a pose from AMASS [29] MoCap dataset. Middle: synthetic renderings exploiting parts of ground truth annotations, for oracle experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>(fps, 30 by default) and the percentage of random poses/joints (0 by default) with the PA-MPJPE metric on 3DPW, MPI-INF-3DHP and MuPoTS-3D when using PoseBERT on top of MoCap-SPIN, with masking 12.5% of the input sequences (2 frames with T=16 frames) and using a model of size D = 512 and L = 4. Histogram of SPIN axis-angle errors. On 3DPW train set, in radians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative comparison of image-based (left) and video-based (right) pose estimation methods (extension of paper's Figure 1). Note the left/right legs flipping produced by SPIN, and how PoseBERT improves predictions by leveraging temporal information and MoCap-based motion priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fine-tuning the pretrained SPIN model. For brevity, we denote the PA-MPJPE metric as E. Top section presents sanity checks, i.e., when fine-tuning the model with the same data used during pretraining (denoted as SPIN data). Bottom section presents results further adding synthetic renderings during fine-tuning, either coming from the SPIN data, or from AMASS(MoCap-SPIN). Results in the bottom part are obtained when fine-tuning only the batch normalization layers.</figDesc><table><row><cell>] fine-tune all parameters fine-tune batch-norm layers</cell><cell>97.2 95.1 94.8</cell><cell>59.6 58.6 58.1</cell><cell>116.8 112.1 111.6</cell><cell>154.6 154.8 153.7</cell><cell>83.0 82.8 82.5</cell><cell>126.2 129.6 126.4</cell><cell>76.2 76.6 76.0</cell><cell>104.3 102.7 102.4</cell><cell>68.0 66.6 67.1</cell><cell>155.4 150.6 149.7</cell></row><row><cell>Using synthetic data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ renderings of SPIN data + renderings of MoCap data (MoCap-SPIN)</cell><cell>93.5 90.8</cell><cell>58.6 55.6</cell><cell>109.7 105.0</cell><cell>152.2 152.3</cell><cell>82.1 81.0</cell><cell>123.8 125.1</cell><cell>76.0 75.7</cell><cell>98.0 100.8</cell><cell>67.4 66.7</cell><cell>150.0 145.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Oracle study on 3DPW. Starting from MoCap- SPIN (first row), we gradually add characteristics from the ground-truth (GT) 3DPW test set to the synthetic render- ings, to make it closer to the real test data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation on the PoseBERT hyperparameters.</figDesc><table /><note>We study the impact of the positional encoding, sharing the regressor, the number of regressor iterations per layer (1 by default), the depth L of the network (4 by default), the number of channels (D t =512 by default) and the length of the sequences (T=16 by default) with the PA-MPJPE metric on 3DPW, MPI-INF-3DHP and MuPoTS-3D when using PoseBERT on top of MoCap-SPIN, with masking 12.5% of the input sequences (2 frames with T=16 frames).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on 3DPW [47], MPI-INF-3DHP<ref type="bibr" target="#b29">[30]</ref>, MuPoTS-3D<ref type="bibr" target="#b30">[31]</ref> and AIST<ref type="bibr" target="#b42">[43]</ref>. For brevity, we denote the PA-MPJPE metric as E. Methods are organized by input type (single image and video). All methods listed follow the standard protocol and do not use the 3DPW training data. Unless otherwise stated, results are copied from the corresponding papers. The symbol ? denotes results obtained by running the model released by the authors.</figDesc><table><row><cell>Masking % MoCap-SPIN</cell><cell cols="4">3DPW E ? Accel ? E ? MPI-INF-3DHP Accel ? 55.6 32.5 66.7 29.5</cell><cell cols="2">MuPoTS-3D E ? Accel ? 81.0 23.5</cell></row><row><cell>0% 12.5% 25% 37.5%</cell><cell>53.3 53.2 53.3 53.9</cell><cell>9.6 7.8 8.3 9.0</cell><cell>62.3 63.8 64.2 65.0</cell><cell>9.8 8.7 9.0 9.2</cell><cell>80.0 80.3 80.3 80.8</cell><cell>13.8 12.8 13.3 14.0</cell></row><row><cell cols="2">12.5% + Noise 52.9</cell><cell>8.3</cell><cell>63.3</cell><cell>8.7</cell><cell>79.9</cell><cell>13.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation on the PoseBERT pretraining strategy.We study the impact of masking the input sequences and adding Gaussian noise. E refers to the PA-MPJPE metric.</figDesc><table><row><cell>3DPW</cell><cell cols="2">MPI-INF-3DHP MuPoTS-3D</cell><cell>AIST</cell></row><row><cell cols="2">SPIN [21] + PoseBERT 57.3 (? 2.3) 64.3 (? 3.7) 59.6 68.0</cell><cell>83.0 80.9 (? 2.1)</cell><cell>76.2 74.6 (? 1.6)</cell></row><row><cell cols="2">VIBE [20] + PoseBERT 54.9 (? 1.6) 64.4 (? 1.0) 56.5 65.4</cell><cell>83.4 81.0 (? 2.4)</cell><cell>76.0 74.5 (? 1.5)</cell></row><row><cell cols="2">MoCap-SPIN + PoseBERT 52.9 (? 2.7) 63.8 (? 2.9) 55.6 66.7</cell><cell>81.0 79.9 (? 1.1)</cell><cell>75.7 74.1 (? 1.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Adding PoseBERT on top of various methods. We report the PA-MPJPE metric (lower is better) on four video datasets. The gains in mm are shown in parenthesis.</figDesc><table><row><cell>image-based features as input. In Table 6, we report the PA-MPJPE on the 4 video datasets. We observe that when plugging PoseBERT on top of SPIN, it leads to a consis-tent improvement of 2.3mm on 3DPW, 3.7mm on MPI-INF-3DHP, 2.1mm on MuPoTS-3D and 1.6mm on AIST. Inter-estingly, this improvement is higher than the one obtained when using VIBE on MPI-INF-3DHP, MuPoTS-3D and AIST. When using MoCap-SPIN as image-based model, we observe a similar consistent improvement on all datasets. Actually, one can even plug PoseBERT on top of a model that already leverages videos, such as VIBE [20], and we observe a similar consistent gain, which suggests that Pose-BERT is complementary to the way temporal consistency of features is exploited in VIBE. Finally, to test it beyond</cell></row></table><note>SMPL inputs, we appended PoseBERT on top of 2D/3D pose predictions from LCRNet++ [38] and see significant gains on 3DPW, a decrease of MPJPE (resp. PA-MPJPE) from 125.8 (resp. 68.8) to 108.2 (resp. 58.5). More details are given in the supplementary materiel.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>We observe that finetuning leads to a boost on the associated test sets but may decrease performances on other test sets. For example, finetuning on the MPI-INF-3DPH training set improves the results on MPI-INF-3DPH and MuPoTs-3DPH respectively by 3.3 and 0.5 mm but it decreases the performance of PoseBERT on 3DPW by 1 mm. This can be explained by the domain shift between these specific training sets.</figDesc><table><row><cell></cell><cell cols="4">3DPW MPI-INF-3DHP MuPoTS-3D AIST</cell></row><row><cell>MoCap-SPIN</cell><cell>55.6</cell><cell>66.7</cell><cell>81.0</cell><cell>71.6</cell></row><row><cell>MoCap-SPIN + PoseBERT + finetuning on MPI-INF-3DPH + finetuning on 3DPW + finetuning on AIST</cell><cell>53.2 53.9 52.9 53.9</cell><cell>63.8 60.5 63.8 65.6</cell><cell>80.3 79.8 79.9 80.9</cell><cell>69.7 71.5 70.9 69.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it SMPL: automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Beyond static features for temporally consistent 3D human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08627</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021. 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The reasonable effectiveness of synthetic visual data. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards accurate markerless human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeletor: Skeletal transformers for robust body-pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised cross-modal alignment for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambareesh</forename><surname>Revanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Govind</forename><surname>Vitthal Waghmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">Mysore</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMPLy benchmarking 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Br?gier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Combaluzier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SMPL: a skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AMASS: Archive of Motion Capture As Surface Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3D human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03713</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Human mesh recovery from multiple shots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09843</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3D human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Black</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Hamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simpose: Effectively learning densepose and surface normals of people from simulated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
