<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20202-11-03">2020 2-3 November 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Turpault</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<region>Inria, Loria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">AI Perception</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">AI Perception</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">AI Perception</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Serizel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<region>Inria, Loria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Music Technology Group</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Seetharaman</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Interactive Audio Lab</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Tokyo, Japan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="20202-11-03">2020 2-3 November 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sound event detection</term>
					<term>synthetic soundscapes</term>
					<term>sound separation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Sound event detection (SED) is the task of describing, from an audio recording, what happens and when each single sound event is occurring <ref type="bibr" target="#b0">[1]</ref>. This is something that we, as humans, do rather naturally to obtain information about what is happening around us. However, trying to reproduce this with a machine is not trivial, as the SED algorithm needs to cope with several problems, including audio signal degradation due to additive noise or overlapping events <ref type="bibr" target="#b1">[2]</ref>. Indeed, in real-world scenarios, the recordings provided to the SED systems contain not only target sound events, but also sound events that can be considered as "noise" or "interference." Also, several target sound events can occur simultaneously.</p><p>In the past, the overlapping sound events problem has been tackled from the classifier point of view. This can be done by training the SED as a multilabel system in which case the most energetic sound events are usually detected more accurately than the rest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Some other approaches tried to deal more explicitly with this problem using either a set of binary classifiers <ref type="bibr" target="#b4">[5]</ref>, using factorization techniques on the input of the classifier <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, or exploiting Part of this work was made with the support of the French National Research Agency, in the framework of the project LEAUDS Learning to understand audio scenes (ANR-18-CE23-0020) and the French region Grand-Est. Experiments presented in this paper were carried out using the Grid5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000). spatial information when available <ref type="bibr" target="#b7">[8]</ref>. The additive noise problem is usually solved by training SED systems on noisy signals. This may be effective to some degree when the noise level is low, but much less so when the noise level increases <ref type="bibr" target="#b3">[4]</ref>.</p><p>Sound separation (SS) seems like a natural candidate to solve these two issues. SS systems are trained to predict the constituent sources directly from mixtures. Thus, sound separation can both decrease the level of interfering noise and enable a SED system to detect quieter events in overlapping acoustic mixtures. Until recently, SS has been mainly applied to specific classes of signals, such as speech or music. However, recent works has shown that sound separation can also be applied to separating sounds of arbitrary classes, a task known as "universal sound separation" <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this paper, we propose to combine a universal SS algorithm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> used as a pre-processing to the DCASE 2020 SED baseline <ref type="bibr" target="#b11">[12]</ref>. We investigate the impact of the data used to train the SS on the SED performance. We also explore different ways to combine the separated sound sources at different stages of SED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM AND BASELINES DESCRIPTION</head><p>We aim to solve a problem similar to that of DCASE 2019 Task 4 <ref type="bibr" target="#b12">[13]</ref>. Systems are expected to produce strongly-labeled outputs (i.e. detect sound events with a start time, end time, and sound class label), but are provided with weakly labeled data (i.e. sound recordings with only the presence/absence of a sound event included in the labels without any timing information) for training. Multiple events can be present in each audio recording, including overlapping target sound events and potentially non-target sound events. Previous studies have shown that the presence of additional sound events can drastically decrease the SED performance <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sound event detection baseline</head><p>The SED baseline system uses a mean-teacher model which is a combination of two models: a student model and a teacher model (both have the same architecture). The student model is the final model used at inference time, while the teacher model is aimed at helping the student model during training and its weights are an exponential moving average of the student model's weights. A more detailed description can be found in Turpault and Serizel <ref type="bibr" target="#b11">[12]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sound separation baseline</head><p>The baseline SS model uses a similar architecture to an existing approach for universal sound separation with a fixed number of sources <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which employs a convolutional masking network using STFT and analysis and synthesis. The training loss is negative stabilized signal-to-noise ratio (SNR) <ref type="bibr" target="#b13">[14]</ref> with a soft-threshold SNRmax. Going beyond previous work, the model in this paper is able to handle variable number sources by using different loss functions for active and inactive reference sources that encourage the model to only output as many nonzero sources as exist in the mixture. Additional source slots are encouraged to be all-zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SOUND EVENT DETECTION AND SEPARATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sound separation for sound event detection</head><p>Overlapping sound events are typically more difficult to detect as compared to isolated ones. SS can be used for SED by first separating the component sounds in a mixed signal and then applying SED on each of the separated tracks. The decisions obtained on separated signals may be more accurate than the ones on the mixed signal. On the other hand, separation of sounds is not a trivial problem and may introduce artifacts which in turn may make sound SED harder. So, it is necessary to jointly investigate SS and SED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sound event detection on separated sources</head><p>In the approaches described here, SS provides several audio clips that contain information related to the sound sources composing the original (mixture) clip. Each of these new audio clips (separated sound sources) are used together with the mixture clip within the SED. We compare three different approaches to integrate the information from these audio clips at different levels of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Early integration</head><p>This approach is similar to the SED baseline except that all the audio clips (mixture and separated sound sources) are concatenated as input channels to form a new tensor ( <ref type="figure" target="#fig_1">Figure 1a</ref>). The first channel always contains the mixture clip while the separated sound source clips are provided with no particular order. The model is trained like the SED baseline using the annotations of the mixture clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Middle integration</head><p>We re-use the CNN block from the SED baseline to extract embeddings from the mixture clip and the separated sound sources clips ( <ref type="figure" target="#fig_1">Figure 1b</ref>). The embeddings are concatenated along the feature axis and fed into a fully connected layer before training a new RNN classifier within a mean-teacher student framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Late integration</head><p>For this approach, we apply the SED baseline on the mixture clip and the separated sound source clips ( <ref type="figure" target="#fig_1">Figure 1c</ref>). The SED output for each of these clips are obtained from the yDSL,c, the raw outputs of the classifier corresponding to each sound class c among the C sound classes. The combined raw output (before thresholding and post-processing) for each class c is obtained as follows:</p><formula xml:id="formula_0">yDSL,c = y q M,c + y q SS,c 2 1/q<label>(1)</label></formula><p>where yM,c and ySS,c are the raw classifier outputs for the sound class c obtained on the mixture clips and the separated sound source clips, respectively. The sound source/mixture combination weight is q. The classifier output for the sound class c is obtained from the raw classifier outputs on each individual separated sound sources as follows:</p><formula xml:id="formula_1">ySS,c = 1 Ns Ns s=1 y p s,c 1/p<label>(2)</label></formula><p>where Ns is the number of separated sound source clips obtained from the SS, ys,c is the raw classifier output for the sound class c obtained for the separated sound source clip s and p is the sound sources combination weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BASELINES SETUP AND DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DESED dataset</head><p>The dataset used for the SED experiments is DESED 1 , a flexible dataset for SED in domestic environments composed of 10-sec audio clips that are recorded or synthesized <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>. The recorded soundscapes are taken from AudioSet <ref type="bibr" target="#b14">[15]</ref>. The synthetic soundscapes are generated using Scaper <ref type="bibr" target="#b15">[16]</ref>. The foreground events are obtained from FSD50k <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The background textures are obtained from the SINS dataset (activity class "other") <ref type="bibr" target="#b18">[19]</ref> and TUT scenes 2016 development dataset <ref type="bibr" target="#b19">[20]</ref>. The dataset includes a synthetic validation set simulated from different isolated those in the training set (SYN VAL) , a validation set and a public evaluation set composed of recorded clips (REC VAL and REC EVAL) that are used to adjust the hyperparameters and evaluate the SED, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">FUSS dataset</head><p>The Free Universal Sound Separation (FUSS) 2 dataset <ref type="bibr" target="#b20">[21]</ref> is intended for experimenting with universal sound separation <ref type="bibr" target="#b8">[9]</ref>, and is used as training data for the SS system. Audio data is sourced from freesound.org. Using labels from FSD50k <ref type="bibr" target="#b17">[18]</ref>, gathered through the Freesound Annotator <ref type="bibr" target="#b21">[22]</ref>, these source files have been screened such that they likely only contain a single type of sound. Labels are not provided for these source files, and thus the goal is to separate sources without using class information. To create reverberant mixtures, 10 second clips of sources are convolved with simulated room impulse responses. Each 10 second mixture contains between 1 and 4 sources. Source files longer than 10 seconds are considered "background" sources. Every mixture contains one background source, which is active for the entire duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sound event detection baseline</head><p>The SED baseline 3 architecture and parameters are described extensively in Turpault et al. <ref type="bibr" target="#b11">[12]</ref>. The performance obtained with this baseline on DESED is presented in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sound separation baseline</head><p>The SS system is trained on 16-kHz audio <ref type="bibr" target="#b3">4</ref> . The input to the SS network is the magnitude of the STFT using window size 32ms and hop of 8ms. These magnitudes are processed by an improved timedomain convolutional network (TDCN++) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which is similar to Conv-TasNet <ref type="bibr" target="#b22">[23]</ref>. Like Conv-TasNet, the TDCN++ consists of four repeats of 8 residual dilated convolution blocks, where within each repeat the dilation of block is 2 for = 0, .., 7. The main differences between the TDCN++ and Conv-Tasnet are (1) bin-wise normalization instead of global layer normalization, which averages only over basis frames instead of frames and frequency bins, (2) trainable scalar scale parameters multiplied after each dense layer, which are initialized with 0.9 i , and <ref type="formula">(3)</ref>    This TDCN++ network predicts four masks that are the same shape as the input STFT. Each mask is multiplied with the complex input STFT, and a source waveform is computed by applying the inverse STFT. A weighted mixture consistency projection layer <ref type="bibr" target="#b23">[24]</ref> is applied to the separated waveforms to be consistent with the input mixture waveform where the per-source weights are predicted by an additional dense layer using the penultimate output of TDCN++.</p><p>To separate mixtures with variable numbers of sources, different loss functions are used for active and inactive reference sources. For active reference sources (i.e. non-zero reference source signals), the soft-threshold for SNR is 30 dB, equivalent to the error power being below the reference power by 30 dB. For non-active reference sources (i.e. all-zero reference source signals), the soft-threshold is 20 dB measured relative to the mixture power, thus gradients are clipped when the error power is 20 dB below the mixture power. Thus, for a N -source mixture, a M -output model with M ? N should output M non-zero sources, and M ? N all-zero sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation metrics</head><p>SS systems are evaluated in terms of scale-invariant SNR (SI-SNR) <ref type="bibr" target="#b24">[25]</ref>. Since FUSS mixtures can contain one to four sources, we report two scores to summarize performance: multi-source SI-SNR improvement (MSi), which measures the separation quality of mixtures with two or more sources, and single-source SI-SNR (1S), which measures the separation model's ability to reconstruct single-source inputs.</p><p>SED systems are evaluated according to an event-based F1score with a 200 ms collar on the onsets and a collar on the offsets that is the greater of 200 ms and 20% of the sound event's length. The overall F1-score is the unweighted average of the class-wise F1-scores (macro-average). F-scores are computed on a single operating point (decision thresholds=0.5) using the sed eval library <ref type="bibr" target="#b25">[26]</ref>.</p><p>SED systems are also evaluated with poly-phonic sound event detection scores(PSDS) <ref type="bibr" target="#b26">[27]</ref>. PSDS are computed using 50 operating points (linearly distributed from 0.01 to 0.99) with the following parameters: detection tolerance parameter (?DTC = 0.5), ground truth intersection parameter (?GTC = 0.5), cross-trigger tolerance parameter (?CTTC = 0.3), maximum False Positive rate (emax = 100). The weight on the cost trigger cost is set to qCT = 1 and the weight on the class instability cost is set to qST = 1. <ref type="table">Table 4</ref>: SS and SED performance for various SS tasks. "Bgd" is DESED background, "Fgd" is DESED foreground, and "Fmx" is FUSS mixture. Confidence intervals: ? 1.2 (F1-score) and ? 0.015 (PSDS) on the validation set and ? 2.3 (F1-score) on the synthetic set.   <ref type="table" target="#tab_2">Table 2</ref> displays SS and SED performance on the FUSS test set and REC VAL. For SS we do a full cross-evaluation between the dry and reverberant versions. From this we can see that the reverberant FUSS-trained model achieves the best separation scores across both dry and reverberant conditions. However, in terms of SED performance, the dry FUSS-trained separation model yields the best performance in terms of both F1 and PSDS. This may be due to the synthetic room impulse responses used to create reverberant FUSS being mismatched to the real data in REC VAL. Thus, we opt to use the dry version of FUSS in the proceeding experiments. Besides training SS systems on FUSS, we also constructed a number of tasks consisting of data from both DESED and FUSS, described in <ref type="table" target="#tab_3">Table 3</ref>. Some tasks are trained with permutationinvariant training (PIT) <ref type="bibr" target="#b27">[28]</ref> or groupwise PIT. <ref type="table">Table 4</ref> reports the results of evaluating these models on the BgFgFm task. For models with more than three outputs, the sources for corresponding classes are summed together. For example, sources 1 through 5 are summed together for the PIT and GroupPIT models, and sources 0 through 9 for the Classwise model, to produce the separated estimate of the DESED foreground mixture. The BgFgFm-trained SS model achieves the best SS scores, since it is matched to the task. This model also achieves the highest F1 score on the SYN VAL set, although this is not statistically significant. However, on REC VAL, the Classwise model achieves the best F1 score. However, notice that the dry FUSS SS model achieves the overall best F1 and PSDS scores of 39.2 and 0.574 in <ref type="table" target="#tab_2">Table 2</ref>. This suggests that the DESED+FUSS-trained SS models do not generalize as well, since they are trained on more specific synthetic data compared to FUSS-trained models. <ref type="figure" target="#fig_2">Figure 2</ref> displays the impact of the late integration parameters p and q on the SED performance. Intuitively when the SS models aims at separating sources that corresponds to target sound events, the parameter p should be high so the source aggregation is close to a max pooling across sources. This is what can be observed on <ref type="figure" target="#fig_2">Fig.  2a</ref> for the PIT model. For the FUSS-trained SS separated sources do not correspond to target sources and the integration is better for low values of p. This however is not confirmed on REC VAL <ref type="figure" target="#fig_2">(Fig. 2b)</ref>. This could be due to the mismatch between training and test for the SS leading to sound sources that are not properly separated. The SED performance depending on the parameter q is presented on <ref type="figure" target="#fig_2">Figure 2c</ref>. A high value for the parameter q means focusing only on the mixture or on the separated sounds and leads to degraded performance for all the SS models. The best performance is then obtained with the FUSS-trained SS and p = 2 and q = 2 (40.7% F1-score and 0.570 PSDS on REC EVAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we proposed to use a SS algorithm as pre-processing to a SED system applied to complex mixtures including non-target events and background noise. We proposed to retrain the generic SS on task specific datasets. The combination has shown to have potential to improve the SED performance in particular when using a late integration to combine the prediction obtained from the separated sources. However, the benefits still remain limited most probably because of the mismatch between the SS training conditions and the SED test conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>We would like to thank the other organizers of DCASE 2020 task 4: Daniel P. W. Ellis and Ankit Parag Shah.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2007.03932v1 [cs.SD] 8 Jul 2020 (a) Early integration. (b) Middle integration. (c) Late integration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Integration between the SS and SED (gray waveform represents mixture, and colored waveforms represent separated sources).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>impact of the late integration weights on the SED performance (vertical bars represent confidence intervals)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance for the SED baseline<ref type="bibr" target="#b11">[12]</ref> on DESED.</figDesc><table><row><cell></cell><cell cols="2">F1-Score PSDS</cell></row><row><cell>REC VAL</cell><cell>37.8</cell><cell>0.540</cell></row><row><cell>REC EVAL</cell><cell>39.0</cell><cell>0.552</cell></row><row><cell>SYN VAL</cell><cell>62.6</cell><cell>0.695</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>additional residual connections between blocks, with connection pattern 0 ? 8, 0 ? 16, 0 ? 24, 8 ? 16, 8 ? 24, 16 ? 24.</figDesc><table /><note>2 https://github.com/google-research/ sound-separation/tree/master/datasets/fuss3 https://github.com/turpaultn/dcase20_task4/4 https://github.com/google-research/ sound-separation/tree/master/models/dcase2020_ fuss_baseline</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>SS and SED performance for FUSS-trained SS models: MSi (multi-source SI-SNR improvement) and 1S (single-source SI-SNR). Confidence intervals: ? 1.2 (F1-score) and ? 0.015 (PSDS).</figDesc><table><row><cell></cell><cell cols="2">FUSS test set</cell><cell></cell><cell>REC VAL</cell></row><row><cell>FUSS</cell><cell>Rev.</cell><cell>Dry</cell><cell></cell><cell>Late Integration</cell></row><row><cell cols="2">training MSi</cell><cell>1S MSi</cell><cell>1S</cell><cell>F1-Score PSDS</cell></row><row><cell>Rev.</cell><cell cols="3">12.5 37.6 10.4 32.1</cell><cell>38.2</cell><cell>0.565</cell></row><row><cell>Dry</cell><cell cols="3">10.4 31.2 10.2 31.8</cell><cell>39.2</cell><cell>0.574</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DESED+FUSS tasks.</figDesc><table><row><cell>Task</cell><cell>Sources</cell></row><row><cell>DmFm</cell><cell>DESED mix, dry FUSS mix</cell></row><row><cell>BgFgFm</cell><cell>DESED bg, DESED fg mix, dry FUSS mix</cell></row><row><cell>PIT</cell><cell>DESED bg, dry FUSS mix, 5 DESED fg sources</cell></row><row><cell cols="2">Classwise DESED bg, 10 DESED classes, dry FUSS mix</cell></row><row><cell cols="2">GroupPIT DESED bg, 5 DESED fg sources, 4 dry FUSS srcs</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://project.inria.fr/desed/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational analysis of sound scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detection of overlapping acoustic events using a temporallyconstrained probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ICASSP. [Online</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature learning with deep scattering for urban sound analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 23rd European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="724" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sound event detection in synthetic domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tut database for acoustic scene classification and sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 24th European Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1128" to="1132" />
		</imprint>
		<respStmt>
			<orgName>EU-SIPCO</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of overlapping acoustic events using a temporallyconstrained probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6450" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overlapping sound event detection with supervised nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multichannel sound event detection using 3d convolutional neural networks for learning inter-channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving universal sound separation using sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Foreground-Background Ambient Sound Scene Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olvera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-02567542" />
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training sound event detection on a heterogeneous dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Parag</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised sound separation using mixtures of mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12701</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaper: A library for soundscape synthesis and augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="344" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Freesound technical demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM</title>
		<meeting>ACM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="411" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FSD50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The SINS database for detection of daily activities in a home environment using an acoustic sensor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lauwereins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Adhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brouckxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Waterschoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verhelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TUT database for acoustic scene classification and sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7760424/" />
	</analytic>
	<monogr>
		<title level="m">2016 24th European Signal Processing Conference (EUSIPCO). IEEE</title>
		<imprint>
			<biblScope unit="page" from="1128" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What&apos;s all the FUSS about free universal sound separation data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Freesound datasets: a platform for the creation of open audio datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="486" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SDR-half-baked or well done</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A framework for the robust evaluation of sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azcarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krstulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
