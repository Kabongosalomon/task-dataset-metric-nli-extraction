<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paragraph-based Transformer Pre-training for Multi-Sentence Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><forename type="middle">Di</forename><surname>Liello</surname></persName>
							<email>luca.diliello@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>2 Amazon Alexa AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
							<email>sidgarg@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
							<email>lucas@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<title level="a" type="main">Paragraph-based Transformer Pre-training for Multi-Sentence Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained transformers <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr">Clark et al., 2020)</ref> have become the de facto standard for several NLP applications, by means of fine-tuning on downstream data. The most popular architecture uses self-attention mechanisms for modeling long range dependencies between compounds in the text, to produce deep contextualized representations of the input. There are several downstream NLP applications that require reasoning across multiple inputs candidates jointly towards prediction. Some popular examples include (i) Answer Sentence Selection (AS2) <ref type="bibr">(Garg et al., 2020)</ref>, which is a Question Answering (QA) task that requires selecting the best answer from a set of candidates for a question; and (ii) Fact Verification <ref type="bibr" target="#b15">(Thorne et al., 2018)</ref>, which reasons whether a claim is supported/refuted by multiple evidences. Inherently, these tasks can utilize information from multiple candidates (answers/evidences) to support the prediction of a particular candidate. * Work done as an intern at Amazon Alexa AI ? Work completed at Amazon Alexa AI Pre-trained transformers such as BERT are used for these tasks as cross-encoders by setting them as sentence-pair classification problems, i.e, aggregating inferences independently over each candidate. Recent studies <ref type="bibr" target="#b26">(Zhang et al., 2021;</ref><ref type="bibr" target="#b16">Tymoshenko and Moschitti, 2021)</ref> have shown that these tasks benefit from encoding multiple candidates together, e.g., encoding five answer candidates per question in the transformer, so that the cross-attention can model dependencies between them. However, Zhang et al. only improved over the pairwise cross-encoder by aggregating multiple pairwise cross-encoders together (one for each candidate), and not by jointly encoding all candidates together in a single model.</p><p>In this paper, we first show that popular pretrained transformers such as RoBERTa perform poorly when used for jointly modeling inference tasks (e.g., AS2) using multi-candidates. We show that this is due to a shortcoming of their pre-training objectives, being unable to capture meaningful dependencies among multiple candidates for the finetuning task. To improve this aspect, we propose a new pre-training objective for 'joint' transformer models, which captures paragraph-level semantics across multiple input sentences. Specifically, given a target sentence s and multiple sentences (from the same/different paragraph/document), the model needs to recognize which sentences belong to the same paragraph as s in the document used.</p><p>Joint inference over multiple-candidates entails modeling interrelated information between multiple short sentences, possibly from different paragraphs or documents. This differs from related works <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b25">Zaheer et al., 2020;</ref><ref type="bibr" target="#b22">Xiao et al., 2021)</ref> that reduce the asymptotic complexity of transformer attention to model long contiguous inputs (documents) to get longer context for tasks such as machine reading and summarization.</p><p>We evaluate our pre-trained multiple-candidate based joint models by (i) performing AS2 on ASNQ <ref type="bibr">(Garg et al., 2020)</ref>, WikiQA <ref type="bibr" target="#b23">(Yang et al., 2015)</ref>, TREC-QA <ref type="bibr" target="#b18">(Wang et al., 2007)</ref> datasets; and (ii) Fact Verification on the FEVER <ref type="bibr" target="#b15">(Thorne et al., 2018)</ref> dataset. We show that our pre-trained joint models substantially improve over the performance of transformers such as RoBERTa being used as joint models for multi-candidate inference tasks, as well as when being used as cross-encoders for sentence-pair formulations of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-Sentence Inference: Inference over a set of multiple candidates has been studied in the past <ref type="bibr" target="#b2">(Bian et al., 2017;</ref><ref type="bibr" target="#b0">Ai et al., 2018)</ref>. The most relevant for AS2 are the works of Bonadiman and Moschitti <ref type="bibr" target="#b29">(2020)</ref> and <ref type="bibr" target="#b26">Zhang et al. (2021)</ref>, the former improving over older neural networks but failing to beat the performance of transformers; the latter using task-specific models (answer support classifiers) on top of the transformer for performance improvements. For fact verification, Tymoshenko and Moschitti (2021) propose jointly embedding multiple evidence with the claim towards improving the performance of baseline pairwise cross-encoder transformers. Transformer pre-training Objectives: Masked Language Modeling (MLM) is a popular transformer pre-training objective <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref>. Other models are trained using token-level <ref type="bibr">(Clark et al., 2020;</ref><ref type="bibr" target="#b6">Joshi et al., 2020;</ref><ref type="bibr" target="#b9">Liello et al., 2021</ref><ref type="bibr">) and/or sentence-level (Devlin et al., 2019</ref><ref type="bibr" target="#b8">Lan et al., 2020;</ref> objectives. REALM <ref type="bibr" target="#b5">(Guu et al., 2020)</ref> uses a differentiable neural retriever over Wikipedia to improve MLM pre-training. This differs from our pre-training setting as it uses additional knowledge to improve the pre-trained LM. DeCLUTR <ref type="bibr" target="#b3">(Giorgi et al., 2021)</ref> uses a contrastive learning objective for cross-encoding two sentences coming from the same/different documents in a transformer. DeCLUTR is evaluated for sentencepair classification tasks and embeds the two inputs independently without any cross-attention, which differs from our setting of embedding multiple candidates jointly for inference. Modeling Longer Sequences: Beltagy et al. <ref type="bibr" target="#b29">(2020)</ref>; <ref type="bibr" target="#b25">Zaheer et al. (2020)</ref> reduce the asymptotic complexity of transformer attention to model very long inputs for longer context. For tasks with short sequence lengths, LongFormer works on par or slightly worse than RoBERTa (attributed to re- duced attention computation). These works encode a single contiguous long piece of text, which differs from our setting of having multiple short candidates, for a topic/query, possibly from different paragraphs and documents. <ref type="bibr">DCS (Ginzburg et al., 2021)</ref> proposes a cross-encoder for the task of document-pair matching. DCS is related to our work as it uses a contrastive pre-training objective over two sentences extracted from the same paragraph, however different from our joint encoding of multiple sentences, DCS individually encodes the two sentences and then uses the InfoNCE loss over the embeddings. CDLM <ref type="bibr">(Caciularu et al., 2021)</ref> specializes the Longformer for documentpair matching and cross-document coreference resolution. While the pre-training objective in CDLM exploits information from multiple documents, it differs from our setting of joint inference over multiple short sentences.</p><p>3 Multi-Sentence Transformers Models 3.1 Multi-sentence Inference Tasks AS2: We denote the question by q, and the set of answer candidates by C={c 1 , . . . c n }. The objective is to re-rank C and find the best answer A for q. AS2 is typically treated as a binary classification task: first, a model f is trained to predict the correctness/incorrectness of each c i ; then, the candidate with the highest likelihood of being correct is selected as an answer, i.e., A=argmax n i=1 f (c i ). Intuitively, modeling interrelated information between multiple c i 's can help in selecting the best answer candidate <ref type="bibr" target="#b26">(Zhang et al., 2021)</ref>. Fact Verification: We denote the claim by F , and the set of evidences by C={c 1 . . . c n } that are retrieved using DocIR. The objective is to predict whether F is supported/refuted/neither using C (at least one evidence c i is required for supporting/refuting F ). <ref type="bibr" target="#b16">Tymoshenko and Moschitti (2021)</ref> jointly model evidences for supporting/refuting a claim as they can complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Encoder Architecture</head><p>For jointly modeling multi-sentence inference tasks, we use a monolithic transformer crossencoder to encode multiple sentences using selfattention as shown in <ref type="figure" target="#fig_0">Fig 1.</ref> To perform joint inference over k sentences for question q or claim F , the model receives concatenated sentences [s 0 . . . s k ] as input, where the first sentence is either the question or the claim (s 0 =q or s 0 =F ), and the remainder are k candidates s i =c i , i={1 . . . k}. We pad (or truncate) each sentence s i to the same fixed length L (total input length L?(k + 1)), and use the embedding for the <ref type="bibr">[CLS]</ref> / <ref type="bibr">[SEP]</ref> token in front of each sentence s i as its embedding (denoted by E i ). Similar to Devlin et al., we create positional embeddings of tokens using integers 0 to L(k+1)?1, and extend the token type ids from {0, 1} to {0 . . . k} corresponding to (k + 1) input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference using Joint Transformer Model</head><p>We use the output embeddings [E 0 . . . E k ] of sentences for performing prediction as following: Predicting a single label: We use two separate classification heads to predict a single label for the input to the joint model [s 0 . . . s k ]: (i) IE 1 : a linear layer on the output embedding E 0 of s 0 (similar to BERT) referred to as the Individual Evidence (IE 1 ) inference head, and (ii) AE 1 : a linear layer on the average of the output embeddings [E 0 , E 1 , . . . , E k ] to explicitly factor in information from all candidates, referred to as the Aggregated Evidence (AE 1 ) inference head. For Fact Verification, we use prediction heads IE 1 and AE 1 . Predicting Multiple Labels: We use two separate classification heads to predict k labels, one label each for every input [s 1 . . . s k ] specific to s 0 : (i) IE k : a shared linear layer applied to the output embedding E i of each candidate s i , i ? {1 . . . k} referred to as k-candidate Individual Evidence (IE k ) inference head, and (ii) AE k : a shared linear layer applied to the concatenation of output embedding E 0 of input s 0 and the output embedding E i of each candidate s i , i ? {1 . . . k} referred to as kcandidate Aggregated Evidence (AE k ) inference head. For AS2, we use prediction heads IE k and AE k . Prediction heads are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-training with Paragraph-level Signals</head><p>Long documents are typically organized into paragraphs to address the document's general topic from different viewpoints. The majority of trans- former pre-training strategies have not exploited this rich source of information, which can possibly provide some weak supervision to the otherwise unsupervised pre-training phase. To enable joint transformer models to effectively capture dependencies across multiple sentences, we design a new pre-training task where the model is (i) provided with (k + 1) sentences {s 0 . . . s k }, and (ii) tasked to predict which sentences from {s 1 . . . s k } belong to the same paragraph P as s 0 in the document D. We call this pre-training task Multi-Sentences in Paragraph Prediction (MSPP). We use the IE k and AE k prediction heads, defined above, on top of the joint model to make k predictions p i corresponding to whether each sentence s i , i?{1 . . . k} lies in the same paragraph P ? D as s 0 . More formally:</p><formula xml:id="formula_0">p i = 1 if s 0 , s i ? P in D 0 otherwise ?i={1, . . . , k}</formula><p>We randomly sample a sentence from a paragraph P in a document D to be used as s 0 , and then (i) randomly sample k 1 sentences (other than s 0 ) from P as positives, (ii) randomly sample k 2 sentences from paragraphs other than P in the same document D as hard negatives, and (iii) randomly sample k 3 sentences from documents other than D as easy negatives (note that k 1 +k 2 +k 3 = k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our joint transformers on three AS2 and one Fact Verification datasets 1 . Common LM benchmarks, such as GLUE <ref type="bibr" target="#b17">(Wang et al., 2018)</ref>, are not suitable for our study as they only involve sentence pair classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Pre  <ref type="bibr" target="#b23">(Yang et al., 2015)</ref> where the questions are derived from query logs of the Bing search engine, and the answer candidate are extracted from Wikipedia. We use the most popular clean setting (questions having at least one positive and one negative answer).</p><p>? TREC-QA: A popular AS2 dataset <ref type="bibr" target="#b18">(Wang et al., 2007)</ref> containing factoid questions. We only retain questions with at least one positive and one negative answer in the development and test sets.</p><p>? FEVER: A dataset for fact extraction and verification <ref type="bibr" target="#b15">(Thorne et al., 2018)</ref> to retrieve evidences given a claim and identify if the evidences support/refute the claim. As we are interested in the fact verification sub-task, we use evidences retrieved by Liu et al. using a BERT-based DocIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details and Baselines</head><p>We use k=5 for our experiments (following <ref type="bibr" target="#b26">(Zhang et al., 2021)</ref> and <ref type="bibr" target="#b16">(Tymoshenko and Moschitti, 2021)</ref>), and perform continued pre-training starting from RoBERTa-Base using a combination of MLM and our MSPP pre-training for 100k steps with a batch size of 4,096. We use two different prediction heads, IE k and AE k , for pre-training. For evaluation, we fine-tune all models on the downstream AS2 and FEVER datasets using the corre-  sponding IE k and AE k prediction heads. We consider the pairwise RoBERTa-Base cross-encoder and RoBERTa-Base LM used as a joint model with IE k and AE k prediction heads as the baseline for AS2 tasks. For FEVER, we use several baselines: GEAR <ref type="bibr" target="#b30">(Zhou et al., 2019)</ref>, KGAT , Transformer-XH <ref type="bibr" target="#b27">(Zhao et al., 2020)</ref>, and three models from <ref type="bibr" target="#b16">(Tymoshenko and Moschitti, 2021)</ref>: (i) Joint RoBERTa-Base with IE 1 prediction head, (ii) Pairwise RoBERTa-Base with max-pooling, and (iii) weighted-sum heads. For complete experimental details, refer to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Answer Sentence Selection: The results for AS2 tasks are presented in <ref type="table" target="#tab_0">Table 1</ref>, averaged across five independent runs. From the table, we can see that the RoBERTa-Base when used as a joint model for multi-candidate inference using either the IE k or AE k prediction heads performs inferior to RoBERTa-Base used as a pairwise cross-encoder. Across five experimental runs, we observe that finetuning RoBERTa-Base as a joint model faces convergence issues (across various hyper-parameters) indicating that the MLM pre-training task is not sufficient to learn text semantics which can be exploited for multi-sentence inference.</p><p>Our MSPP pre-trained joint models (with both IE k , AE k heads) get significant improvements over the pairwise cross-encoder baseline and very large improvements over the RoBERTa-Base joint model. The former highlights modeling improvements stemming from joint inference over multiple-  candidates, while the latter highlights improvements stemming from our MSPP pre-training strategy. Across all three AS2 datasets, our joint models are able to get the highest P@1 scores while also improving the MAP and MRR metrics.</p><p>To demonstrate that our joint models can effectively use information from multiple candidates towards prediction, we perform a study in <ref type="table" target="#tab_2">Table 2</ref> where the joint models are used to re-rank the top-k candidates ranked by the pairwise RoBERTa-Base cross-encoder. Our joint models can significantly improve the P@1 over the baseline for all datasets. The performance gap stems from questions for which the pairwise RoBERTa model was unable to rank the correct answer at the top position, but support from other candidates in the top-k helped the joint model rank it in the top position. Fact Verification: The results for the FEVER task are presented in <ref type="table" target="#tab_4">Table 3</ref> and show that our joint models (pre-trained with both the IE k and AE k heads and fine-tuned with the IE 1 and AE 1 heads) outperform all previous baselines considered, including the RoBERTa-Base joint model directly applied for multi-sentence inference. Compute Overhead: We present a simplified latency analysis for AS2 (assuming sentence length L) as follows: a pairwise cross-encoder uses k transformer steps with input length 2L, while our model uses 1 step with input length (k+1)?L.</p><p>Since transformer attention scales quadratic on input length, our model should take (k+1) 2 4k times the inference time of the cross-encoder, which is 1.8 when k=5. However, when we fine-tune for Wik-iQA on one A100-GPU, we only observe latency increasing from 71s?81s (only 14.1% increase).  linearly with input length, reducing overheads of self-attention. Refer to Appendix C.3 for details. Qualitative Examples: We present some qualitative examples from the three AS2 datasets highlighting cases where the pairwise RoBERTa-Base model is unable to rank the correct answer on the top position, but our pre-trained joint model (Joint MSPP IE k ? FT IE k ) can do this using supporting information from other candidates in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The input embeddings and feedforward layers vary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we have presented a multi-sentence cross-encoder for performing inference jointly on multiple sentences for tasks like answer sentence selection and fact verification. We have proposed a novel pre-training task to capture paragraph-level semantics. Our experiments on three answer selection and one fact verification datasets show that our pre-trained joint models can outperform pairwise cross-encoders and pre-trained LMs when directly used as joint models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-training Datasets</head><p>We use the Wikipedia 2 , BookCorpus 3 , OpenWeb-Text <ref type="bibr" target="#b4">(Gokaslan and Cohen, 2019)</ref> and CC-News 4 datasets for performing pre-training of our joint transformer models. We do not use the STORIES dataset as it is no longer available for research use 5 . After decompression and cleaning we obtained 6GB, 11GB, 38GB and 394GB of raw text respectively from the BookCorpus, Wikipedia, OpenWeb-Text and CC-News.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Finetuning Datasets</head><p>We evaluate our joint transformers on three AS2 and one Fact Verification datasets. The latter differs from the former in not selecting the best candidate, but rather explicitly using all candidates to predict the target label. Here are the details of the finetuning datasets that we use for our experiments along with data statistics for each dataset:   <ref type="bibr" target="#b7">(Kwiatkowski et al., 2019)</ref> dataset by converting it from a machine reading to an AS2 dataset. This is done by labelling sentences from the long answers which contain the short answer string as positive correct answer candidates and all other answer candidates as negatives. We use the dev. and test splits released by Soldaini and Moschitti 7 .</p><p>? WikiQA: An AS2 dataset released by <ref type="bibr">Yang et al. 8</ref> where the questions are derived from query logs of the Bing search engine, and the answer candidate are extracted from Wikipedia. This dataset has a subset of questions having no correct answers (all-) or having only correct answers (all+). We remove both the all-and all+ questions for our experiments ("clean" setting).</p><p>? TREC-QA: A popular AS2 dataset released by <ref type="bibr">Wang et al..</ref> For our experiments, we trained on the train-all split, which contains more noise but also more question-answer pairs. Regarding the dev. and test sets we removed the questions without answers, or those having only correct or only incorrect answer sentence candidates. This setting refers to the "clean" setting <ref type="bibr" target="#b12">(Shen et al., 2017)</ref>, which is a TREC-QA standard.</p><p>? FEVER: A popular benchmark for fact extraction and verification released by <ref type="bibr">Thorne et al.</ref> The aim is to retrieve evidences given a claim, and then identify whether the retrieved evidences support or refute the claim or if there is not enough information to make a choice. For supporting/refuting a claim, at least one of the retrieved evidences must support/retrieve the claim. Note that the performance on FEVER depends crucially on the retrieval system and the candidates retrieved. For our experiments, we are interested only in the fact verification sub-task and thus we exploit the evidences retrieved by <ref type="bibr">Liu et al. using</ref>   <ref type="table">Table 6</ref>: Statistics for the FEVER dataset where evidences has been retrieved using .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Complete Experimental Details</head><p>Following standard practice, the token ids, positional ids and token type ids are embedded using separate embedding layers, and their sum is fed as the input to the transformer layers. We use k=5 for our experiments (following <ref type="bibr">Zhang et al.; Tymoshenko and Moschitti)</ref>, and perform continuous pre-training starting from the RoBERTa-Base checkpoint using a combination of MLM and our MSPP pre-training objective for 100,000 steps with a batch size of 4096. We use a triangular learning rate with 10,000 warmup steps and a peak value of 5 * 10 ?5 . We use Adam optimizer with ? 1 = 0.9, ? 2 = 0.999 and = 10 ?8 . We apply a weight decay of 0.01 and gradient clipping when values are higher than 1.0. We set the dropout ratio to 0.1 and we use two different prediction heads for pre-training: IE k and AE k . We follow the strategy of <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b8">Lan et al., 2020)</ref>, and equally weight the the two pre-training loss objectives: MLM and MSPP. For evaluation, we fine-tune all models on the downstream AS2 and FEVER datasets: using the same IE k and AE k prediction heads exploited in pre-training for AS2 and using either IE 1 or AE 1 prediction heads for FEVER. We finetune every model with the same maximum sequence length equal to 64 * (k + 1) = 384 tokens. For ASNQ we train for up to 6 epochs with a batch size of 512 and a learning rate of 10 ?5 with the same Adam optimizer described above but warming up for only 5000 steps. We do early stopping on the MAP of the development set. For WikiQA and TREC-QA, we created batches of 32 examples and we used a learning equal to 2 * 10 ?6 and 1000 warm up steps. We train for up to 40 epochs again with early stopping on the MAP of the development set. On FEVER, we use a batch size of 64, a learning rate of 10 ?5 , 1000 warm up steps and we do early stopping checking the Accuracy over the development set. We implemented our code based on HuggingFace's Transformers library <ref type="bibr" target="#b21">(Wolf et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Baselines</head><p>For AS2, we consider two baselines: (i) pairwise RoBERTa-Base model when used as a crossencoder for AS2, and (ii) RoBERTa-Base LM when used as a joint model with IE k and AE k prediction heads independently for AS2 tasks.</p><p>For FEVER, we use several recent baselines from Tymoshenko and Moschitti: (i) GEAR <ref type="bibr" target="#b30">(Zhou et al., 2019)</ref>, (ii) KGAT , (iii) Transformer-XH <ref type="bibr" target="#b27">(Zhao et al., 2020)</ref>, (iv) joint RoBERTa-Base with IE 1 prediction head (Tymoshenko and Moschitti, 2021), (v) pairwise RoBERTa-Base when used as a cross-encoder with max-pooling head <ref type="bibr" target="#b16">(Tymoshenko and Moschitti, 2021)</ref>, (vi) pairwise RoBERTa-Base when used as a cross-encoder with weighted-sum head (Tymoshenko and Moschitti, 2021). We used metrics from Torchmetrics <ref type="bibr">(Detlefsen et al., 2022)</ref> to compute MAP, MRR, Precision@1 and Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Metrics</head><p>The performance of AS2 systems in practical applications is typically (Garg and Moschitti, 2021) measured using the Accuracy in providing correct answers for the questions (the percentage of correct responses provided by the system), also called the Precision-at-1 (P@1). In addition to P@1, we use Mean Average Precision (MAP) and Mean Reciprocal Recall (MRR) to evaluate the ranking produced of the set of candidates by the model.</p><p>For FEVER, we measure the performance using Label Accuracy (LA), a standard metric for this dataset, that measures the accuracy of predicting support/refute/neither for a claim using a set of evidences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complete Results and Discussion</head><p>C.1 Results on AS2 with cascaded pairwise and Joint re-ranker</p><p>Below we present results of evaluating our joint models to re-rank the top-k candidates ranked by the pairwise RoBERTa-Base cross-encoder. Our joint models can significantly improve the P@1, MAP and MRR over the baseline for all datasets. The performance gap stems from questions for which the pairwise RoBERTa model was unable to rank the correct answer at the top position, but support from other candidates in the top-k helped the joint model rank it in the top position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Results on FEVER</head><p>Here we present complete results on the FEVER dataset in   Ash, 2020) which uses additional DocIR components and data (MNLI <ref type="bibr" target="#b20">(Williams et al., 2018)</ref>) for fine-tuning, (iv) DREAM <ref type="bibr">(Zhong et al., 2020)</ref> that uses the XL-Net model. Note that comparing our joint models with (iii) and (iv) is unfair since they use additional retrieval components, datasets and larger models. We just include these results here for the sake for completeness. Interestingly, our joint models outperform DREAM and DOMLIN++ on the dev set without using additional retrieval and larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Compute Overhead of Joint Models</head><p>Change in Number of Model Parameters: The transformer block of our joint inference model is identical to pre-trained models such as RoBERTa, and contains the exact same number of parameters. Classification heads IE 1 , IE k and AE 1 all operate on the embedding of a single token, and are identical to the classification head of RoBERTa (AE k operates on the concatenation of two token embeddings, and contains double the number of parameters as the RoBERTa). The maximum sequence length allowed for both the models is the same (512). The exact number of parameters of our joint model with AE k and the RoBERTa model are 124, 062, 720 and 124, 055, 040 respectively. Change in Inference Latency: While our joint model provides a longer input sequence to the transformer, it also reduces the number of forward passes that need to be done by a pairwise crossencoder. A simplified latency analysis for AS2 (assuming each sentence has a length L): pairwise cross-encoder will need to make k forward passes of the transformer with a sequence of length 2L (q with each candidate c i ), while our joint model will only need to make 1 forward pass of the transformer with input length (k+1)?L (q with k candidates). Transformer self-attention is quadratic in input sequence length, so this should lead to the inference time of out joint model being (k+1) 2 4k times the inference time of the cross-encoder. However, the input embedding layer and the feedforward layers are linear in input sequence length, so this should lead to a reduction in the inference time of our joint model by <ref type="bibr">(k+1)</ref> 2k times the inference time of the cross-encoder. Empirically, when we fine-tune for WikiQA on one A100-GPU, we only observe latency increasing from 71s?81s (increase of only 14.1%).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Multi-sentence 'Joint' transformer model. E i refers to embedding for the question/each candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Inference heads for joint transformer model. E i refers to embedding for the question/each candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ASNQQ:</head><label></label><figDesc>Who invented the submarine during the civil war? A1: H.L. Hunley , often referred to as Hunley , was a submarine of the Confedera A2: Hunley , McClintock , and Baxter Watson first built Pioneer , which was tested in February 1862 in the Mississippi River and was later towed to Lake Pontchartrain for additional trials . A3: She was named for her inventor, Horace Lawson Hunley , shortly after she was taken into government service under the control of the Confederate States Army at Charleston , South Carolina. A4: 1864 painting of H.L. Hunley by Conrad Wise Chapman History Confederate States Name : H.L. Hunley Namesake : Horace Lawson Hunley Builder : James McClintock Laid down : Early 1863 Launched : July 1863 Acquired : August 1863 In service: February 17 , 1864 Out of service : February 17, 1864 Status : Awaiting conservation General characteristics Displacement : 7.5 short tons ( 6.8 metric tons ) Length : 39.5 ft A5: Johan F. Carlsen was born in AEr?sk?bing April 9, 1841.WikiQA Q: What is the erb/heart? A1: Heart valves are labeled with "B", "T", "A", and "P".First heart sound: caused by atrioventricular valves -Bicuspid/Mitral (B) and Tricuspid (T). A2: Second heart sound caused by semilunar valves -Aortic (A) and Pulmonary/ Pulmonic (P). A3: Front of thorax , showing surface relations of bones , lungs (purple), pleura (blue), and heart (red outline). A4: In cardiology, Erb's point refers to the third intercostal space on the left sternal border where sS2 is best auscultated . A5: It is essentially the same location as what is referred to with left lower sternal border (LLSB).TREC-QAQ: When was the Khmer Rouge removed from power ? A1: Sihanouk was named head of state after the Khmer Rouge seized power in 1975, but was locked in his palace by the communists as they embarked on their brutal attempt to create an agrarian utopia . A2: When a Vietnamese invasion drove the Khmer Rouge from power in 1979, Duch fled with other Khmer Rouge leaders into the jungles. A3: Religious practices were revived after the Khmer Rouge were driven from power by a Vietnamese invasion in 1979 A4: Moreover, 20 years after the Khmers Rouges were ousted from power, Cambodia still struggles on the brink of chaos , ruled by the gun , not by law . A5: Sihanouk resigned in 1976 , but the Khmer Rouge kept him under house arrest until they were driven from power by an invading Vietnamese army in 1979 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Joint MSPP IE k ? FT IE k 63.0 (0.3) 67.2 (0.2) 73.7 (0.2) 82.7 (2.2) 88.5 (1.5) 89.0 (1.5) 91.7 (2.2) 91.1 (0.5) 95.2 (1.3) Results (std. dev. in parenthesis) on AS2 datasets. MSPP, FT refer to our pre-training task and fine-tuning respectively. We indicate the prediction head (IE k /AE k ) used for both pre-training and fine-tuning. We underline statistically significant gains over the baseline (Student t-test with 95% confidence level). and choose k 1 =1, k 2 =2, k 3 =2 as the specific values for creating positive and negative candidates for s 0 . For complete details refer to Appendix A.<ref type="bibr" target="#b7">Kwiatkowski et al., 2019)</ref>, where the candidate answers are from Wikipedia pages and the questions are from search queries of the Google search engine. We use the dev. and test splits released bySoldaini and Moschitti.   </figDesc><table><row><cell>Code</cell><cell>and</cell><cell>pre-trained</cell><cell>model</cell><cell>checkpoints:</cell></row><row><cell cols="5">https://github.com/amazon-research/</cell></row><row><cell cols="4">wqa-multi-sentence-inference</cell><cell></cell></row></table><note>-training: To eliminate any improvements stemming from usage of more data, we perform pre-training on the same corpora as RoBERTa: En- glish Wikipedia, the BookCorpus, OpenWebText and CC-News. For our proposed pre-training, we randomly sample sentences from paragraphs as s 0 ,1(Ours) Joint MSPP AE k ? FT AE k 63.? WikiQA: An AS2 dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>P@1 of joint models for AS2 when re-ranking answers ranked in top-5 by pairwise RoBERTa-Base. Statistically significant results (Student t-test 95%) are underlined. Complete results in Appendix C.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on FEVER dev and test sets. For our method, prediction heads (IE 1 /AE 1 ) are only used for fine-tuning (FT), while for pre-training (Pre) we use (IE k /AE k ) heads. '-' denotes models not released publicly, and results not reported in the paper. Statistically significant results (Student t-test 95%) are underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Examples from AS2 datasets where the pairwise RoBERTa-Base model is unable to rank a correct answer for the question at the top position, but our joint model (Joint MSPP IE k ? FT IE k ) can. We present answers {A1, . . . , A5} in their ranked order by the pairwise RoBERTa-Base model. For all these examples we highlight the top ranked answer by the pairwise RoBERTa-Base model in red since it is incorrect.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Thuy Vu, and Alessandro Moschitti. 2020. Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7780-7788.</figDesc><table><row><cell></cell><cell>Siddhant Garg, Dvir Ginzburg, Itzik Malkiel, Oren Barkan, Avi Caciu-</cell></row><row><cell></cell><cell>laru, and Noam Koenigstein. 2021. Self-supervised</cell></row><row><cell></cell><cell>document similarity ranking via contextualized lan-</cell></row><row><cell></cell><cell>guage models and hierarchical inference. In Find-</cell></row><row><cell></cell><cell>ings of the Association for Computational Linguis-</cell></row><row><cell></cell><cell>tics: ACL-IJCNLP 2021, pages 3088-3098, Online.</cell></row><row><cell></cell><cell>Association for Computational Linguistics.</cell></row><row><cell cols="2">Daniele Bonadiman and Alessandro Moschitti. 2020.</cell></row><row><cell cols="2">A study on efficiency, accuracy and document struc-</cell></row><row><cell cols="2">ture for answer sentence selection. In Proceed-</cell></row><row><cell cols="2">ings of the 28th International Conference on Com-</cell></row><row><cell cols="2">putational Linguistics, pages 5211-5222, Barcelona,</cell></row><row><cell cols="2">Spain (Online). International Committee on Compu-</cell></row><row><cell>tational Linguistics.</cell><cell></cell></row><row><cell cols="2">Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew Pe-</cell></row><row><cell cols="2">ters, Arie Cattan, and Ido Dagan. 2021. CDLM:</cell></row><row><cell cols="2">Cross-document language modeling. In Findings</cell></row><row><cell cols="2">of the Association for Computational Linguistics:</cell></row><row><cell cols="2">EMNLP 2021, pages 2648-2662, Punta Cana, Do-</cell></row><row><cell cols="2">minican Republic. Association for Computational</cell></row><row><cell>Linguistics.</cell><cell></cell></row><row><cell cols="2">Kevin Clark, Minh-Thang Luong, Quoc V. Le, and</cell></row><row><cell>Christopher D. Manning. 2020.</cell><cell>Electra: Pre-</cell></row><row><cell cols="2">training text encoders as discriminators rather than</cell></row><row><cell cols="2">generators. In International Conference on Learn-</cell></row><row><cell>ing Representations.</cell><cell></cell></row><row><cell cols="2">Nicki Skafte Detlefsen, Jiri Borovec, Justus Schock,</cell></row><row><cell cols="2">Ananya Harsh Jha, Teddy Koker, Luca Di Liello,</cell></row><row><cell cols="2">Daniel Stancl, Changsheng Quan, Maxim Grechkin,</cell></row><row><cell cols="2">and William Falcon. 2022. Torchmetrics -measur-</cell></row><row><cell cols="2">ing reproducibility in pytorch. Journal of Open</cell></row><row><cell>Source Software, 7(70):4101.</cell><cell></cell></row><row><cell cols="2">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell></row><row><cell cols="2">Kristina Toutanova. 2019. BERT: Pre-training of</cell></row><row><cell cols="2">deep bidirectional transformers for language under-</cell></row><row><cell cols="2">standing. In Proceedings of the 2019 Conference</cell></row><row><cell cols="2">of the North American Chapter of the Association</cell></row><row><cell cols="2">for Computational Linguistics: Human Language</cell></row><row><cell cols="2">Technologies, Volume 1 (Long and Short Papers),</cell></row><row><cell cols="2">pages 4171-4186, Minneapolis, Minnesota. Associ-</cell></row><row><cell>ation for Computational Linguistics.</cell><cell></cell></row><row><cell cols="2">Siddhant Garg and Alessandro Moschitti. 2021. Will</cell></row><row><cell cols="2">this question be answered? question filtering via</cell></row><row><cell cols="2">answer model distillation for efficient question an-</cell></row><row><cell cols="2">swering. In Proceedings of the 2021 Conference on</cell></row><row><cell cols="2">Empirical Methods in Natural Language Processing,</cell></row><row><cell cols="2">pages 7329-7346, Online and Punta Cana, Domini-</cell></row><row><cell cols="2">can Republic. Association for Computational Lin-</cell></row><row><cell>guistics.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Statistics for ASNQ, WikiQA and TREC-QA</cell></row><row><cell>datasets.</cell></row><row><cell>? ASNQ: A large-scale AS2 dataset (Garg et al.,</cell></row><row><cell>2020) 6 where the candidate answers are from</cell></row><row><cell>Wikipedia pages and the questions are from search</cell></row><row><cell>queries of the Google search engine. ASNQ</cell></row><row><cell>is a modified version of the Natural Questions</cell></row><row><cell>2 https://dumps.wikimedia.org/enwiki/</cell></row><row><cell>20211101/</cell></row><row><cell>3 https://huggingface.co/datasets/</cell></row><row><cell>bookcorpusopen</cell></row><row><cell>4 https://commoncrawl.org/2016/10/</cell></row><row><cell>news-dataset-available/</cell></row><row><cell>5 https://github.com/tensorflow/models/</cell></row><row><cell>tree/archive/research/lm_commonsense#</cell></row><row><cell>1-download-data-files</cell></row><row><cell>6 https://github.com/alexa/wqa_tanda</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>a BERT-based DocIR 9 .</figDesc><table><row><cell cols="4">Split # Claims # Evidences Avg. # E/C</cell></row><row><cell>Train</cell><cell>145,406</cell><cell>722,473</cell><cell>4.97</cell></row><row><cell>Dev</cell><cell>19,998</cell><cell>98,915</cell><cell>4.95</cell></row><row><cell>Test</cell><cell>19,998</cell><cell>98,839</cell><cell>4.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>, by also presenting some addi-</cell></row><row><cell>tional baselines such as: (i) pairwise BERT-Base</cell></row><row><cell>cross-encoder (Tymoshenko and Moschitti, 2021),</cell></row><row><cell>(ii) joint BERT-Base cross-encoder with IE 1 pre-</cell></row><row><cell>diction head, (iii) DOMLIN++ (Stammbach and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Complete results of our joint models for AS2 datasets when re-ranking the answer candidates ranked in top-k by Pairwise RoBERTa-Base. MSPP, FT refer to our pre-training task and finetuning respectively. We indicate the prediction head (IE k /AE k ) used for both pre-training and finetuning. Joint Pre IE k + FT IE 1 81.21 (0.24) 74.39 (Ours) Joint Pre IE k + FT AE 1 81.10 (0.15) 74.25 (Ours) Joint Pre AE k + FT IE 1 81.18 (0.14) 73.77 (Ours) Joint Pre AE k + FT AE 1 81.21 (0.16) 74.13</figDesc><table><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>GEAR</cell><cell>70.69</cell><cell>71.60</cell></row><row><cell>KGAT with RoBERTa-Base</cell><cell>78.29</cell><cell>74.07</cell></row><row><cell>Transformer-XH</cell><cell>78.05</cell><cell>72.39</cell></row><row><cell>Pairwise BERT-Base</cell><cell>73.30</cell><cell>69.75</cell></row><row><cell>Pairwise RoBERTa-Base + MaxPool</cell><cell>79.82</cell><cell>-</cell></row><row><cell>Pairwise RoBERTa-Base + WgtSum</cell><cell>80.01</cell><cell>-</cell></row><row><cell>Joint BERT-Base</cell><cell>73.67</cell><cell>71.01</cell></row><row><cell>Joint RoBERTa-Base + FT IE 1</cell><cell>79.25</cell><cell>73.56</cell></row><row><cell cols="3">(Ours) Methods with larger models and/or sophisticated retrieval</cell></row><row><cell>DOMLIN++</cell><cell>77.48</cell><cell>76.60</cell></row><row><cell>DREAM</cell><cell>79.16</cell><cell>76.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Complete Results on FEVER dev and test sets. For our method, prediction heads (IE 1 /AE 1 ) are only used for finetuning (FT), while for pre-training (Pre) we use the (IE k /AE k ) heads. '-' denotes models that are not publicly released and have no reported results on the test split in their published paper. Statistically significant results (T-Test 95%) are underlined.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/alexa/ wqa-cascade-transformers 8 http://aka.ms/WikiQA 9 https://github.com/thunlp/KernelGAT/ tree/master/data</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Datasets</head><p>We present the complete details for all the datasets used in this paper along with links to download them for reproducibility of results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep listwise context model for ranking refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3209985</idno>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A compare-aggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3133089</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1987" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeCLUTR: Deep contrastive learning for unsupervised textual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="879" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Openwebtext corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">REALM: Retrieval-augmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient pre-training objectives for transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Di Liello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Gabburo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained fact verification with kernel graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cascade transformer: an application for efficient answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.504</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5697" to="5708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">2020. efever: Explanations and summaries forautomated fact checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Stammbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Ash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TTO</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strong and light baseline models for fact-checking joint inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.426</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4824" to="4830" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? a quasi-synchronous grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">PRIMER: pyramid-based masked sentence pre-training for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2110.08499</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL -Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint models for answer verification in question answering systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3252" to="3262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning over semantic-level graph for fact checking</title>
		<idno type="DOI">10.18653/v1/2020.acl-main.549</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="6170" to="6180" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GEAR: Graph-based evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
