<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyuan</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Article submission</term>
					<term>IEEE</term>
					<term>IEEEtran</term>
					<term>journal</term>
					<term>L A T E X</term>
					<term>paper</term>
					<term>template</term>
					<term>typesetting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table structure</ref> <p>recognition is a crucial part of document image analysis domain <ref type="bibr" target="#b0">[1]</ref>. Its difficulty lies in the need to parse the physical coordinates and logical indices of each cell at the same time. However, the existing methods are difficult to achieve both these goals, especially when the table splitting lines are blurred or tilted. In this paper, we propose an accurate and end-to-end transformer-based table structure recognition method, referred to as TRUST. Transformers are suitable for table structure recognition because of their global computations, perfect memory, and parallel computation. By introducing novel Transformer-based Query-based Splitting Module and Vertexbased Merging Module, the table structure recognition problem is decoupled into two joint optimization sub-tasks: multi-oriented table row/column splitting and table grid merging. The Query-based Splitting Module learns strong context information from long dependencies via Transformer networks [2], accurately predicts the multi-oriented table row/column separators, and obtains the basic grids of the table accordingly. The Vertex-based Merging Module is capable of aggregating local contextual information between adjacent basic grids, providing the ability to merge basic girds that belong to the same spanning cell accurately. We conduct experiments on several popular benchmarks including PubTabNet <ref type="bibr" target="#b2">[3]</ref> and SynthTable [4], our method achieves new state-of-the-art results. In particular, TRUST runs at 10 FPS on PubTabNet, surpassing the previous methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T ABLE Structure Recognition aims to recognize the internal structure of a table. It is a fundamental task in document understanding and has numerous practical applications <ref type="bibr" target="#b4">[5]</ref>, such as question answering, dialogue systems, table-to-text, etc. With the increasing number of documents containing tables, automated reading of tables within these images has become an urgent task.</p><p>Through studied for years, table structure recognition is still a very open research problem. The main difficulty lies in the need to parse the exact bounding box and logical index of each cell at the same time. In particular, four types of degradation and variations cause various problems in most current table structure recognition systems, as illustrated in <ref type="figure" target="#fig_1">Fig.1</ref>. First, spanning cells that occupy at least two rows or columns are more important than other simple cells on tables  because spanning cells are more likely to be table headers in a table <ref type="bibr" target="#b5">[6]</ref>. Second, parsing unlined tables or partially lined tables is more difficult than lined tables, because there are no explicit visual cues that delimit cells, columns, and rows. Third, empty cells are easier to neglect and more difficult to be located than non-empty cells in tables. Fourth, rotation and linear perspective transformation may degrade strongly the performance of table structure recognition. Recent efforts have been devoted to improving the performance of table structure recognition which can be summarized into three categories: (1) Component-based Approaches (2) Sequence-based Approaches (3) Splitting-based Approaches. Unfortunately, the Component-based approaches such as DeepDeSRT <ref type="bibr" target="#b6">[7]</ref>, TableNet <ref type="bibr" target="#b7">[8]</ref> and LGPMA <ref type="bibr" target="#b9">[9]</ref> still suffer from boundary ambiguity problems in unlined tables and cannot achieve decent performance in complex scenarios such as tables with empty cells. Besides, the Sequencebased Approaches such as EDD <ref type="bibr" target="#b2">[3]</ref> strongly depend on a large amount of data for end-to-end training and the generalization will drop sharply when encountering unseen data. Moreover, they often fail to regress accurate cell boundaries. Significantly, the Splitting-based Approaches provide strong generative capabilities for different kinds of table images because they mainly focus on capturing global and local visual context in tables such as the row or column separators or the linking relationships between a pair of adjacent basic cells, which will not change very large among different kinds of table images. Also, the Splitting-based approaches can attain more accurate cell locations compared with Component-based approaches and Sequence-based approaches. Our proposed TRUST follows the Splitting-based Approaches.</p><p>However, recent Splitting-based approaches such as SP-LERGE <ref type="bibr" target="#b10">[10]</ref> and SEM <ref type="bibr" target="#b11">[11]</ref> may suffer from following disadvantages: (1) The pipeline of SEM is inefficient, which may involve time-consuming Region of Interest (RoI) <ref type="bibr" target="#b12">[12]</ref> operations and context features extraction via BERT <ref type="bibr" target="#b13">[13]</ref>. (2) SPLERGE trained two isolate split-model and merge-model which may increase the difficulty of optimization compared with training in an end-to-end fashion. (3) Existing Splittingbased approaches can not handle well tables with rotation and linear perspective transform.</p><p>In this paper, we propose an end-to-end Transformer-based table structure recognition method. Our method addresses the challenges in table structure recognition via an innovative encoder-decoder architecture as illustrated in <ref type="figure" target="#fig_2">Fig.2</ref>. The Convolutional Neural Networks with FPN are used as the backbone feature extractor. We enable table structure recognition with a Query-based Splitting Module, which introduces angle classification and starting point prediction for multi-oriented row/column separators. Through these predicted separators, a fine grid structure of the table is generated.</p><p>In addition, we design a novel Vertex-based Merging Module, to calculate features of all intersection of row separators and column separators, a.k.a vertices. With these features of vertices, a self-attention mechanism is built to scan all vertices and predicts which basic grid pairs should be merged in four directions including (top-left, top-right), (top-right, down-right), (down-left, down-right) and (top-left, down-left) around vertices. Vertex-based Merging Module helps to merge adjacent grids together to recover the spanning table cells more accurately, regardless of unlined tables or tables with empty cells. Our model is trained in an end-to-end fashion and the results show the effectiveness of our method.</p><p>The major contributions of this work can be summarized in the following three points: 1) We present a novel end-to-end framework named TRUST to tackle the tasks of table structure recognition, which leverages multi-headed self-and crossattention mechanisms between the visual feature maps and row/column features to capture contextual information from long dependencies more efficiently and effectively. Furthermore, we design a novel Query-based Splitting Module and Vertex-based Merging Module to extract semantic features of the row/column separators and vertices, leading to more accurate table structure recognition in a split-merge manner. 2) Our Splitting-based TRUST can handle well most categories of tables, including those that are unlined or partially lined, and those with empty cells or spanning cells. Moreover, TRUST can recognize the structure of rotating tables, which is not solved very well by the previous Splitting-based methods.</p><p>3) We develop an end-to-end trainable table structure recognition method that demonstrates superior performance over some public datasets including the PubTab-Net and SynthTable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Quite a number of table recognition techniques have been reported in recent years <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b16">[16]</ref>, and most of them can be broadly classified into three categories. The first one follows a bottom-up approach which first detects text parts or basic cell parts and then links them up to form a table structure through graph neural networks or post-processing. The second follows a sequence decoding framework which treats table recognition as a image-to-sequence problem. The third follows a splitmerge approach which obtains the basic table grids through dense splitting lines prediction, and then merge some of them to form spanning cells.</p><p>Component-based Approaches. Many conventional methods follow a bottom-up approach that first detects text or basic cell parts and then connects them to form a table structure. Popular table structure recognition methods include DeepDeSRT <ref type="bibr" target="#b6">[7]</ref>, ReS2Tim <ref type="bibr" target="#b15">[15]</ref>, DeepTabStR <ref type="bibr" target="#b16">[16]</ref>, etc. More recent methods explore Graph Neural Networks to link the basic components. For example, TIES <ref type="bibr" target="#b3">[4]</ref> combines CNN <ref type="bibr" target="#b17">[17]</ref> and GNN <ref type="bibr" target="#b14">[14]</ref> to construct a bottom-up model to recognize the table structure. TabStruct-Net <ref type="bibr" target="#b18">[18]</ref> first detects individual cells and then links them to get table structure by graphs. Similarly, in NCGM <ref type="bibr" target="#b19">[19]</ref>, it leverages graphs and modality interaction to boost the multi-modal representation for complex scenarios. Though Component-based Approaches are efficient, the big challenge is that these methods often fail to detect spanning cells and require extra cell detection networks which reduce the efficiency.</p><p>Sequence-based Approaches. Methods that directly reconstruct table structure in image-to-sequence manners become popular recently as reconstructing table structure in one shot avoids the extra linking process. EDD <ref type="bibr" target="#b2">[3]</ref> utilizes an attentionbased encoder-dual-decoder architecture to convert images of tables into HTML code. Its structure decoder reconstructs the table structure and directly recognizes cell content by the cell decoder at the same time. Though direct recognition of table structure is efficient, the big challenge is that these methods depend largely on the amount of trainable data and often fail to regress accurate cell locations.</p><p>Splitting-based Approaches. Splitting-based methods divide table structure recognition into two phases. They split the table into basic grid elements in which adjacent ones are then merged to recover spanning cells. For example, SPLERGE <ref type="bibr" target="#b10">[10]</ref> first predicts the basic table grid pattern using Row Projection Networks and Column Projection Networks with novel projection pooling and then combines them to get table structure. Similarly, in SEM <ref type="bibr" target="#b11">[11]</ref>, a splitter is applied to obtain the fine grid structure of the table by predicting the potential regions of the table row/column separators. It also enhances the representational power of each table cell by modeling the textual information via transformer networks and merging these table cells through the attention mechanism.</p><p>Our proposed TRUST follows the splitting-based approaches. Different from existing techniques, we predict row/column separators using a transformer decoder namely a Query-based Splitting Module, which can more effectively and efficiently deals with unconstrained table. Additionally, a novel Vertex-based Merging Module in which the vertex's representation is efficiently constructed of the learnable row/column representations from Query-based Splitting Module is introduced to merge table grids. Compared with the training of two independent modules in SPLERGE <ref type="bibr" target="#b10">[10]</ref>, the whole framework of TRUST can be trained in an end-to-end manner and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. Overview</head><p>We describe the details of our TRUST, As Shown in <ref type="figure" target="#fig_2">Fig.2</ref>, it consists of three main components: a CNN backbone, a Query-Based Splitting Module, and a Vertex-based Merging Module.</p><p>We use a ResNet18 <ref type="bibr" target="#b20">[20]</ref> as the visual feature encoder of TRUST which computes increasingly high-level visual features as the layers become deeper. To alleviate the size problem of table and text, we adopt the FPN <ref type="bibr" target="#b21">[21]</ref> strategy to merge features of different resolutions. Then in the Querybased Splitting Module, a transformer network is used as the feature decoder, in which visual features and learnable row/column position embedding features <ref type="bibr" target="#b22">[22]</ref> are jointly used to capture features in a horizontal direction and vertical direction, respectively. We apply FFNs to the learnable row/column representations in the previous stage, generating the row and column separators of arbitrary orientations in the table. Through these predicted separators, a fine grid structure of the table is generated and each cell in this grid is a basic element of the table. Finally, those generated basic grids are further merged if they belong to the same spanning cells by a Vertex-based Merging Module. The feature representation of each vertex can be efficiently constructed by fusing the associated row and column features extracted by the Querybased Splitting Module. After further enhancing the vertex representation via FFNs, they are used to predict the merging results between adjacent basic grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query-Based Splitting Module</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig.2</ref>, the proposed Query-Based Splitting Module takes visual features and row/column embedding features as inputs. In the Transformer decoder, visual features F V ? R H?W ?d obtained by the CNN encoder are firstly flattened to R (H?W )?d and fed to the Transformer decoder as the key and value of attention mechanism. In the meantime, the position indexes (0, 1, .., N ? 1) of rows and (0, 1, ..., M ? 1) of columns are fed to Embedding layers to get embedding features F embed ? R N/M ?d , which are used as the queries of attention mechanism. N and M represent the predefined maximum number of horizontal and vertical separators in the table.</p><p>Following the row/column Transformer decoder, three fully connected layer produces the final prediction (c i/j , o i/j , a i/j ) for each row/column queries. c i/j means whether the row/column query is classified as a horizontal/vertical separator or not, o i/j means the offset value of each predicted horizontal/vertical separator intersecting the left/top boundary of the table, and a i/j means the predicted rotation angle of each row/column separator. Note that the offset value and rotation angle are only meaningful when the row/column queries are classified as positive. Based on the forecast results, a fine grid structure of the table can be generated, as shown in the bottom-right of <ref type="figure" target="#fig_2">Fig.2</ref> The proposed Query-Based Splitting Module addresses the unconstrained tables better from two aspects. First, the use of the self-attention mechanism in Transformers helps to capture contextual information from global long dependencies, which is very helpful for cases with blurred splitting lines and plentiful empty cells. Second, the prediction output of the separator with rich attributes can well describe the scene of tilted table lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vertex-based Merging Module</head><p>We can accurately represent a simple merge-free table without spanning cells based only on the fine grid structure generated by Query-Based Splitting Module. However, when table contains spanning cells, the basic cells belonging to same spanning cell need to be merged. To solve this problem, we introduce Vertex-based Merging Module to model cell merging.</p><p>First, the intersection of each horizontal and vertical separator represents a table vertex, whose features can be efficiently obtained by the fusion of horizontal and vertical separator features. We have previously obtained the horizontal separator feature F r ? R N ?d and vertical separator feature F c ? R M ?d in Query-Based Splitting Module. We then expand them into a feature with a shape of N ?M ?d and add them together, getting the feature representation of N ?M vertices, where N and M represent the number of horizontal and vertical separators. To improve the perception of vertex features to the context information of table rows and columns, before the horizontal and vertical separator features are fused, we perform crossfeature enhancement on them. The enhancement method is shown at the bottom of <ref type="figure" target="#fig_2">Fig.2</ref>. When enhancing the horizontal separators, we use the horizontal separator feature F r ? R N ?d as query, the vertical separator feature F c ? R M ?d as key and value, and feed them into Transformer decoder layers to get the enhanced horizontal separator features. Using a similar operation, we can get the enhanced features of the vertical separators. The features of all vertices in the table are enhanced by the cross-attention mechanism between row and column separators. Each vertex will predict 4 attribute values, which are used to predict whether the 4 grid pairs around it should be merged. Specifically, 4 grid pairs include (top-left, top-right), (down-left, down-right), (top-left, down-left) and (top-right, down-right).</p><p>From the Query-based Splitting Module and Vertex-based Merging Module mentioned above, we can get the horizontal and vertical separators and vertices of the table, and further determine the information of the basic table grids and the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ground Truth and Loss Function</head><p>In the above section, we refer to the two components of the table, the splitting lines and vertexes, and their respective attributes. In this section, we will describe how attribute labels are derived and the design of the Loss function.</p><p>Ground Truth for Query-based Splitting Module. We expanded the definition of splitter in SPLERGE <ref type="bibr" target="#b10">[10]</ref> to support inclined separators. As illustrated in <ref type="figure" target="#fig_3">Fig.3</ref>, we use parallelogram to express the column table separators. The format of parallelogram can maximize the area of the separator regions without intersecting non-spanning cell content, and it is especially suitable for the inclined tables.</p><p>As described earlier, we predefined M queries for column table separators. In order to ease the difficulty of learning in query mode, we evenly distribute the column queries along the horizontal direction of the table image according to the index value of query. Therefore, the j ? th index corresponds to the (j * w/M ) horizontal position in the image, where the w means the width of table image.</p><p>Next, we use this predefined position information to determine whether the query falls in the area of a column separator. If so, the category of query is set to positive class. At the same time, the angle label is set to ?, which is the angle between the corresponding quadrilateral and the vertical direction. In addition, by drawing a vertical line with horizontal coordinate (j * w/M ) and the point intersecting the top boundary on the table is easily obtained, we can get the vertical offset value x. With the predefined horizontal position information and vertical offset value, plus the rotation angle, we can draw an accurate column splitting line. Using a similar operation method, we can also obtain the labels of n row split lines queries.</p><p>Once determining the positive horizontal and vertical queries of a table, the basic grids of the table are determined.</p><p>Ground Truth for Vertex-based Merging Module. For partially complex tables, a portion of the text region may span more than one base cell, so some basic cells need to be merged. Merge labels are reflected in attributes of intersection points. The attributes of the intersection points have the following four dimensions, that is, the upper, lower, left and right, which represent the four merging proposals respectively. The attributes of the intersection points indicate whether the adjacent basic cells around the intersection point need to be merged or not. If two cells need to be merged, the attributes of their common intersection point should be set to positive. For example, two basic cells with index (i, j) and index (i + 1, j) need to be merged, i means row i and j means column j. </p><formula xml:id="formula_0">+ 1 N vtx L bce (y lnk , c lnk )<label>(1)</label></formula><p>Here, y row is the label of all row queries, y i row = 1 if i-th query is labeled as positive,, and 0 otherwise. Likewise, y col is the label of all column queries. L bce is the binary crossentropy loss <ref type="bibr" target="#b23">[23]</ref> over the predicted row and column queries scores, respectively c row and c col , given by</p><formula xml:id="formula_1">L(y r/c , c r/c ) = ?(ylog(p c )) + (1 ? y)log(1 ? p c ))<label>(2)</label></formula><p>L loc is the Smooth L1 regression loss <ref type="bibr" target="#b24">[24]</ref> over the predicted start point geometries? and the groundtruth s:</p><formula xml:id="formula_2">L(?, s) = 0.5(? ? s) 2 , if |? ? s| &lt; 1 |? ? s| ? 0.5, otherwise<label>(3)</label></formula><p>As for rotated angle prediction, we limit the range of rotated angles to [?45 ? , +45 ? ] and one degree represented one prediction category, the loss of rotation angle is computed as L(y ang , c ang ) = ? </p><p>For link classification over all vertices, we also use binary cross-entropy, given by L(y lnk , c lnk ) = ?(ylog(p c )) + (1 ? y)log(1 ? p c )) <ref type="bibr" target="#b4">(5)</ref> Notice that we only consider the loss of adjacent grids needed to merge during training. Online Hard Example Mining(OHEM) <ref type="bibr" target="#b25">[25]</ref> is applied to L(y r/c , c r/c ) and L(y lnk , c lnk ) for balancing positive and negative samples.</p><p>The losses on row/column classification are normalized by N r/c , which is the number of positive and hard negative samples. The loss on angle classification and start point regression is normalized by the number of positive samples N pos . The loss on link classification is normalized by the number of positive and hard negative samples N vtx</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Inference Process</head><p>In the previous section, we introduced the structure of the model and the setting of labels. In this part, we will introduce how to get the final table structure through the output of the model. First, as shown in <ref type="figure" target="#fig_2">Fig.2</ref>, after we put the table image into the model, we can directly obtain the output results of the Query-Based Splitting Module(QBS) and Vertex-based Merging Module(VBM). Through the output of QBS, we can get the distribution probability of horizontal and vertical lines in the table image. Set threshold ?, We can get the distribution area of the splitting line, and the connected areas represent the distribution range of a splitting line. At the same time, we define the line unit with the highest score in the range as the final splitting line. After getting the split line, we can get the distribution of basic cells in the table, and we can get the position of the vertex. That means we can get the vertex information from the output of the VBM model. According to the vertex attribute, we can merge the basic cells to get the final table structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets</head><p>Quite a number of table structure recognition datasets have been reported in recent years, and most of them can be broadly classified into two categories: standard tables and unconstrained tables. We evaluate our proposed method on the following benchmarks which contain table data of various styles in various scenes and the benchmarks are as listed below. We also provide ablation studies to verify the effects of each proposed component.</p><p>PubTabNet <ref type="bibr" target="#b2">[3]</ref> PubTabNet is one of the most commonly used benchmarks for table structure recognition. It is a largescale complicated table collection that contains 500777 training images, 9115 validating images, and 9138 testing images. This dataset contains a large amount of three-line tables with multi-row/column cells, empty cells, etc. The images of the benchmark are extracted from scientific documents.</p><p>SynthTable. Unlike PubTabNet which contains mostly standard tables. SynthTable covers the unconstrained table in a natural scene, requiring the table structure recognizer to have both discriminative and generative capabilities. Therefore, we also use SynthTable proposed in TIES <ref type="bibr" target="#b3">[4]</ref>. SynthTable is a synthetic dataset that contains 1000 images for the training and 1000 images for the testing. The generated tables are harmoniously blended with the existing document background. where tables have a variety of orientations sizes and types of separators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Protocol</head><p>In the evaluation process, we focus on the accuracy of the logical structure of the table. We use Tree-Edit-Distance-based Similarity (TEDS <ref type="bibr" target="#b2">[3]</ref>) to evaluate the performance of our model for recognizing table logical structure. in addition to TEDs that consider both table structure and text content, we also evaluate performance on the structure TEDs metric that considers only the accuracy of table structure prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We use ResNet-18, pre-trained on ImageNet <ref type="bibr" target="#b17">[17]</ref>, as the backbone, and the whole networks are then fine-tuned endto-end using ADAM <ref type="bibr" target="#b26">[26]</ref> optimizer on the training sets of PubTabNet <ref type="bibr" target="#b2">[3]</ref> and SynthTable. For fine-tuning, images are resized to 640 ? 640 after random scaling, and the long size is resized to 640. Our model is trained for 20 epochs and the initialized learning rate is 0.0001. The batch size is set to 16. TRUST is implemented using PaddlePaddle <ref type="bibr" target="#b27">[27]</ref>, and we use Tesla A100 64GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>The proposed technique is evaluated over PubTabNet and SynthTable datasets. Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE <ref type="bibr" target="#b10">[10]</ref>, TabStruct-Net <ref type="bibr" target="#b18">[18]</ref>, EDD <ref type="bibr" target="#b2">[3]</ref>, GTE <ref type="bibr" target="#b28">[28]</ref>, LGPMA <ref type="bibr" target="#b9">[9]</ref>, FLAG-Net <ref type="bibr" target="#b29">[29]</ref>, etc. Unlike many state-of-the-art methods that perform evaluations only at TEDs, our method also test the Structure TEDs on PubTabNet.</p><p>Tab.I shows quantitative results on the PubTabNet dataset that contains mostly unlined tables. As the table shows, the TRUST achieves the best Structure TEDs 97.1% and TEDs 96.2% among all published methods for this widely studied dataset, TabStruct-Net <ref type="bibr" target="#b18">[18]</ref> has low TEDs because it cannot handle the problem of unlined tables. Our method detects row/column separators and accordingly alleviates the unlined table problem. Notice that the OCR results of PubTabNet are obtained by the public text detection method PSENet <ref type="bibr" target="#b30">[30]</ref> and text recognition method MASTER <ref type="bibr" target="#b31">[31]</ref> for a fair comparison. The superior performance of TRUST is largely due to the proposed Query-Based Splitting Module and Vertex-based Merging Module. The individual contributions of the Query-Based Splitting Module and Vertex-based Merging Module will be discussed in the ensuing Ablation Study.</p><p>Results on SynthTable. We also evaluate the SynthTable dataset proposed in TIES <ref type="bibr" target="#b3">[4]</ref> that mainly consists of tables in diverse categories. Different from Pubtabnet, tables in SynthTable have a more diverse style such as rotation and linear perspective transformation, and a more complex background. As Tab.V shows, our method achieves 99.2%, Speed analysis. We also evaluate the TRUST efficiency as shown in Tab.III. The runtime of TRUST is evaluated with NVIDIA Tesla A100 64GB. We can see that TRUST achieves 10 FPS, which is much faster than other methods such as EDD and SEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We conducted several experiments to evaluate the effectiveness of our design. These experiments mainly focus on evaluating two important modules in our TRUST: Query-based Splitting Module and Vertex-based Merging Module. Tab.II summarizes the results of TRUST with different settings on PubTabNet.</p><p>The Effectiveness of Query-Based Splitting Module. We designed this module to handle the row/column separators splitting problem. To evaluate this module, we replace the Query-Based Splitting Module with the Split Model proposed in SPLERGE <ref type="bibr" target="#b10">[10]</ref>. As Tab.II shows, the Split Model proposed in SPLERGE only achieves Structure TEDs 94.8% and TEDs 93.4%. By using the Query-Based Splitting Module, TRUST improves both Structure TEDs and TEDs by about 2.3% and 2.8%, respectively. The large improvement is largely due to the attention mechanism in the Query-Based Splitting Module that helps capture contextual information from long dependencies of both horizontal and vertical directions.   The Effectiveness of Cross Feature Enhancement. In this study, we evaluate the impact of cross-feature enhancement in the Vertex-based Merging Module by replacing them with feature enhancement from each own branch. As shown in Tab. IV, this results in substantial performance drops, e.g., 96.2% ? 88.0%TEDS without cross feature enhancement. suggesting that the proposed cross-feature enhancement in the Vertex-based Merging Module is an important contributor to the performance boost. F. Qualitative Results <ref type="figure" target="#fig_5">Fig.4</ref> shows a few sample images from the SynthTable dataset and the corresponding structure recognition results by TRUST. As <ref type="figure" target="#fig_5">Fig.4</ref> shows, TRUST is capable of recognizing most tables that have rotation, linear perspective transform, empty cells, spanning cells, invisible separators, etc. Its performance degrades slightly when tables appear with perspective distortion as shown in the third row in <ref type="figure" target="#fig_5">Fig.4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presents a robust and accurate table structure recognition method using innovative encoder-decoder architecture and Transformer networks. An encoder-decoder architecture is designed that can not only reconstruct the structure of tables in arbitrary orientations but also can accurately recognize the structure of complex tables that contains spanning cells. In addition, two innovative Query-Based Splitting Module and Vertex-Based Merging Module are designed which generates feature maps with contextual information from long dependencies in a more efficient and effective way. Additionally, Transformer networks are introduced to further increase the accuracy of table structure recognition. Extensive experiments over a number of public datasets show that the proposed TRUST achieves superior performance as compared with state-of-the-art, with a remarkable faster-running speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This paper was produced by the Baidu Inc. Haojie Li and Zhihui Wang are professors of Dalian University of technology (a) spanning cells (b) unlined table (c) empty cells (d) rotation and linear perspective transform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of different types of table structure recognition methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of the proposed TRUST. It consists of a CNN backbone, a Query-Based Splitting Module, and a Vertex-based Merging Module. The features of row/column separators are extracted and then generate row splitting lines and column splitting lines, forming a fine grid structure by the Query-Based Splitting Module. The row/column features are further fed into the Vertex-based Merging Module to predict the linking relations between adjacent basic cells merged information between the grids. From this information, we can form a variety of complex table structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Then the (top-left, bottom-left) attribute value of their common intersection point with index (i, j) is set to positive, and the (top-right, bottom-right) merged attribute value with index (i, j-1) is set to 1. The cell has the same index value as its bottom The label generation process of columns: the j-th query is a positive column query; red regions represent column separators; red points represent the start point of positive column queries and ? represents the rotation angle of positive column queries. The label generation process of rows is similar. right corner point. Loss Function. Our model is trained in an end-to-end fashion, where the training loss is a weighted combination of multiple functions from Query-Based Splitting Module and Vertex-based Merging Module. Overall, the loss function is a weighted sum of the three losses: L(y row , c row , y col , c col , y ang , c ang ,?, s, y lnk , c lnk ) = 1 N r L bce (y row , c row ) + 1 N c L bce (y col , c col ) + 1 N pos L ce (y ang , c ang ) + 1 N pos L loc (?, s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>+45 ang=? 45 y</head><label>45</label><figDesc>ang log(p cang )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of table structure recognition results made by TRUST: Images from first row are from PubTabNet. Images from middle row are from SynthTable. And final row is the example of bad cases. Blue lines indicate the predicted structure of tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>Comparison results of logical structure recognition on PubTabNet datasets</figDesc><table><row><cell>Method</cell><cell cols="2">Str-TEDs TEDs</cell></row><row><cell>EDD [3]</cell><cell>-</cell><cell>88.3%</cell></row><row><cell>TabStruct-Net [18]</cell><cell>-</cell><cell>90.1%</cell></row><row><cell>GTE [28]</cell><cell>-</cell><cell>93.0%</cell></row><row><cell>LGPMA [9]</cell><cell>96.7%</cell><cell>94.6%</cell></row><row><cell>FLAG-Net [29]</cell><cell>-</cell><cell>95.1%</cell></row><row><cell>Ours</cell><cell>97.1%</cell><cell>96.2%</cell></row><row><cell cols="3">96.9%, 93.6%, and 89.2% TEDs, respectively, outperforming</cell></row><row><cell cols="3">the state-of-the-art methods, Both EDD and SPLERGE have a</cell></row><row><cell cols="3">much lower TEDs because they cannot cope with tables with</cell></row><row><cell cols="3">rotation or linear perspective transform in category 4 due to the</cell></row><row><cell cols="3">limitation of rotation modeling. Our method models these sit-</cell></row><row><cell cols="3">uations through a Query-Based Splitting Module accordingly</cell></row><row><cell cols="3">alleviates rotation and linear perspective problems. Besides,</cell></row><row><cell cols="3">the proposed Vertex-based Merging Module explicitly merges</cell></row><row><cell cols="3">adjacent table grids, enabling it to recognize spanning cells.</cell></row><row><cell cols="3">This leads to up to 7.8% and 14.3% TEDs improvement over</cell></row><row><cell cols="2">EDD and SPLERGE, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The Effectiveness of Vertex-based Merging Module. We also conducted another experiment to evaluate the Vertexbased Merging Module. We found that, if we merge the basic cells by replacing the Vertex-based Merging Module with heuristic post-processing, the Structure TEDs and TEDs drop from 97.1% ? 88.3% and 96.2% ? 85.4%, respectively. We further replace the Vertex-based Merging Module with the merge model proposed in SPLERGE, and the Structure TEDs and TEDs drop from 97.1% ? 96.2% and 96.2% ? 95.3%, respectively. This suggests that the proposed Vertexbased Merging Module is critical to the merging results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .TABLE III .</head><label>IIIII</label><figDesc>Effectiveness of Query-Based Splitting Module and Vertex-Based Merging Module on PubTabNet and SynthTable. Split: split model proposed in SPLERGE [10], QBS: Query-Based Splitting Module, Heuristic: Heuristic Post-processing, Merge: merge model proposed in SPLERGE [10], VBM:Vertex-Based Merging Module Speed analysis. TRUST is the current fastest table structure recognition method with a speed of 10 FPS. The comparisons with the previous state-of-the-arts demonstrate the efficiency of our method.</figDesc><table><row><cell>Splitting Model</cell><cell>Merging Model</cell><cell cols="2">Performance(Pubtabnet/SynthTable(C4))</cell></row><row><cell cols="2"># Split [10] QBS Heuristic [10] Merge [10] VBM</cell><cell>Str-TEDs</cell><cell>TEDs</cell></row><row><cell>1</cell><cell></cell><cell>94.8% / 88.2%</cell><cell>93.4% / 85.9%</cell></row><row><cell>2</cell><cell></cell><cell>88.3% / 81.7%</cell><cell>85.4% /76.7%</cell></row><row><cell>3</cell><cell></cell><cell>96.2% / 90.8%</cell><cell>95.3% /86.6%</cell></row><row><cell>4</cell><cell></cell><cell>97.1% / 92.4%</cell><cell>96.2% /89.2%</cell></row><row><cell>Method</cell><cell>FPS</cell><cell></cell></row><row><cell cols="2">TabStruct-Net [18] 0.77</cell><cell></cell></row><row><cell>EDD [3]</cell><cell>1</cell><cell></cell></row><row><cell>SEM [11]</cell><cell>1.94</cell><cell></cell></row><row><cell>Ours</cell><cell>10</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>Effectivenes of Cross Feature Enhancement in Vertex-Based Merging Module on Pubtabnet.</figDesc><table><row><cell></cell><cell cols="2">Str-TEDs TEDs</cell></row><row><cell>with Cross Feature Enhancement</cell><cell>97.1%</cell><cell>96.2%</cell></row><row><cell>w/o Cross Feature Enhancement</cell><cell>90.6%</cell><cell>88.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V .</head><label>V</label><figDesc>Comparison results of logical structure recognition on SynthTable dataset, * represent models that were trained by us. C1 means standard tables with visible lines; C2 means standard tables without invisible lines; C3 means standard tables with spanning cells; C4 means unconstrained tables with rotation and linear perspective transform. TEDs TEDs Str-TEDs TEDs Str-TEDs TEDs Str-TEDs TEDs</figDesc><table><row><cell></cell><cell>C1</cell><cell></cell><cell>C2</cell><cell></cell><cell>C3</cell><cell></cell><cell>C4</cell><cell></cell></row><row><cell cols="2">Method Str-EDD  *  [32] 97.8%</cell><cell>96.0%</cell><cell>98.0%</cell><cell>93.4%</cell><cell>96.1%</cell><cell>93.2%</cell><cell>89.9%</cell><cell>81.4%</cell></row><row><cell>SP LERGE  *  [3]</cell><cell>97.8%</cell><cell>97.0%</cell><cell>94.4%</cell><cell>91.6%</cell><cell>95.5%</cell><cell>92.1%</cell><cell>85.6%</cell><cell>74.9%</cell></row><row><cell>TRUST w/o [10]</cell><cell>99.6%</cell><cell>99.0%</cell><cell>98.0%</cell><cell>97.0%</cell><cell>96.1%</cell><cell>93.6%</cell><cell>90.7%</cell><cell>81.0%</cell></row><row><cell>TRUST</cell><cell>99.7%</cell><cell>99.2%</cell><cell>98.1%</cell><cell>96.9%</cell><cell>96.0%</cell><cell>93.6%</cell><cell>92.4%</cell><cell>89.2%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Current status and performance analysis of table recognition in document images with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="663" to="87" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image-based table recognition: data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="564" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking table recognition using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of table recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Cordy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04729</idno>
		<title level="m">Complicated table structure recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="128" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lgpma: Complicated table structure recognition with local and global pyramid mask alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="99" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Split, embed and merge: An accurate table structure recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">108565</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Res2tim: Reconstruct syntactic structures from table images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeptabstr: deep learning based table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Fateh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T R</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Table structure recognition using top-down and bottom-up cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13359</idno>
		<title level="m">Neural collaborative graph machines for table structure recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Binary cross entropy with deep learning technique for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ruby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yendapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Trends Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Data and Domputing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, read and reason: Table structure recognition with flexible context aggregator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1084" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Master: Multi-aspect non-local network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">107980</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing table structures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
