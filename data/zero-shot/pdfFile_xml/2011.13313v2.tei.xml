<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polarization-driven Semantic Segmentation via Efficient Attention-bridged Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
							<email>*wangkaiwei@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">National Engineering Research Center of Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Polarization-driven Semantic Segmentation via Efficient Attention-bridged Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic Segmentation (SS) is promising for outdoor scene perception in safetycritical applications like autonomous vehicles, assisted navigation and so on. However, traditional SS is primarily based on RGB images, which limits the reliability of SS in complex outdoor scenes, where RGB images lack necessary information dimensions to fully perceive unconstrained environments. As preliminary investigation, we examine SS in an unexpected obstacle detection scenario, which demonstrates the necessity of multimodal fusion. Thereby, in this work, we present EAFNet, an Efficient Attention-bridged Fusion Network to exploit complementary information coming from different optical sensors. Specifically, we incorporate polarization sensing to obtain supplementary information, considering its optical characteristics for robust representation of diverse materials. By using a single-shot polarization sensor, we build the first RGB-P dataset which consists of 394 annotated pixel-aligned RGB-Polarization images. A comprehensive variety of experiments shows the effectiveness of EAFNet to fuse polarization and RGB information, as well as the flexibility to be adapted to other sensor combination scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of deep learning, outdoor scene perception and understanding has become a popular topic in the area of autonomous vehicles, navigation assistance systems for vulnerable road users like visually impaired pedestrians and mobile robotics <ref type="bibr" target="#b0">[1]</ref>. Semantic Segmentation (SS) is a task to assign semantic labels to each pixel of the images, i.e., object classification task at the pixel level, which is promising for outdoor perception applications <ref type="bibr" target="#b1">[2]</ref>. A multitude of SS neural networks have been proposed following the trend of deep learning like FCN <ref type="bibr" target="#b2">[3]</ref>, U-Net <ref type="bibr" target="#b3">[4]</ref>, ERFNet <ref type="bibr" target="#b4">[5]</ref>, SwiftNet <ref type="bibr" target="#b5">[6]</ref> and so on. However, the networks mentioned above are mainly focused on the segmentation of RGB images, which makes it hard to fully perceive complex surrounding scenes because of the limited color information. A lot of works concerning domain adaptation have been presented to cope with SS in conditions without enough optical information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Yet, a high-level security needs to be guaranteed for outdoor scene perception to support safety-critical applications like autonomous vehicles, where merely algorithm advancement is insufficient. Incorporating heterogeneous imaging techniques, multimodal semantic segmentation is of great necessity to be researched, which can leverage various optical information like depth, infrared and event-based data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In this paper, we employ polarization information as the supplement sensor information to advance the performance of RGB-based SS considering its optical characteristics for robust representation of diverse materials. The polarization information are promising to advance the segmentation of objects which possess polarization features in the outdoor. With the rationale, this work advocates polarization-driven multimodal SS, which is rarely explored in the literature.</p><p>To better explain the necessity of multimodal semantic segmentation that merely RGB sensors can not cope with complex outdoor scene perception, we conduct a preliminary investigation in an unexpected obstacle detection scenario. In outdoor scenes, many unexpected obstacles like tiny animals, boxes and so on are risk factors for secure driving. We choose Lost and Found dataset <ref type="bibr" target="#b10">[11]</ref> to perform an experiment. The dataset is acquired by a pair of cameras with a baseline distance of 23cm in 13 challenging outdoor traffic scenes by setting up 37 different categories of tiny obstacles, which possesses three types of data as shown in <ref type="figure">Fig. 1</ref>, i.e., RGB image, disparity image and ground-truth label. The dataset contains 3 categories, i.e., coarse annotations of passable areas, fine-grained annotations of unexpected tiny obstacles and background, whose resolution is 1024?2048. Among them, 1036 images are selected as the training set, while the remaining 1068 images are selected as the validation set. Considering the fact that outdoor scene perception application demands high efficiency, we select a real-time network SwiftNet <ref type="bibr" target="#b5">[6]</ref> to conduct the experiment. We only take the RGB images as the input information to train the network, where other training implementations will be described in Section 4.1. The detailed results are shown in <ref type="table" target="#tab_2">Table 3</ref>. The precision of obstacle and passable area segmentation are 26.4% and 85.4%. Their recall rate are 49.8% and 63.9%, and their Intersection over Union (IoU) are 20.9% and 56.2%, respectively. In addition, the qualitative results of the experiment are illustrated in <ref type="figure">Fig. 2</ref>. The results show that severe over-fitting has appeared, and we find that the model trained merely with RGB images can not satisfactorily detect small, unexpected obstacles. According to the toy experiment above, the model's performance is unacceptable when trained only with RGB images. Thereby, we consider it is necessary to incorporate additional sensor information for semantic segmentation to perceive outdoor traffic scenes. As mentioned above, we select polarization as the complementary information, whose potential has been shown in our previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> for water hazard detection. In this work, we leverage a novel single-shot RGB-P imaging sensor, and investigate polarization-driven semantic segmentation. To sufficiently fuse RGB-P information, we propose the Efficient Attention-bridged Fusion Network (EAFNet), enabling adaptive interaction of cross-modal features. In summary, we deliver the following contributions:</p><p>? Addressing polarization-driven semantic segmentation, we propose EAFNet, an efficient attention-bridged fusion network, which fuses multimodal sensor information with a lightweight fusion module, advancing many categories' accuracy, especially categories with polarization characteristics like glass, whose IoU is lifted to 79.3% from 73.4%. The implementations and codes will be made available at https://github.com/ Katexiang/EAFNet.</p><p>? With a single-shot polarization imaging sensor, we present an RGB-P outdoor semantic segmentation dataset. To the best of our knowledge, this is the first RGB-P outdoor semantic segmentation dataset, which will be made publicly accessible at http://www. wangkaiwei.org/download.html.</p><p>? We conduct a series of experiments to demonstrate the effectiveness of EAFNet with comprehensive analysis, along with a supplementary experiment that verifies EAFNet's generalization capability for fusing other sensing data besides polarization information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">From accurate to efficient semantic segmentation</head><p>Convolutional Neural Networks (CNNs) have been the mainstream solution to semantic segmentation since Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b2">[3]</ref> approached the dense recognition task in an end-to-end way. SegNet <ref type="bibr" target="#b13">[14]</ref> and U-Net <ref type="bibr" target="#b3">[4]</ref> presented encoder-decoder architectures, which are widely used in the following networks. Benefiting from deep classification models like ResNets <ref type="bibr" target="#b14">[15]</ref>, PSPNet <ref type="bibr" target="#b15">[16]</ref> and DeepLab <ref type="bibr" target="#b16">[17]</ref> constructed multi-scale representations and achieved significant accuracy improvements. Inspired by the channel attention method proposed in SENet <ref type="bibr" target="#b17">[18]</ref>, EncNet <ref type="bibr" target="#b18">[19]</ref> encoded global image statistics, while HANet <ref type="bibr" target="#b19">[20]</ref> explored heightdriven contextual priors. ACNet <ref type="bibr" target="#b20">[21]</ref> leveraged attention connections and bridged multi-branch ResNets to exploit complementary features. In another line, DANet <ref type="bibr" target="#b21">[22]</ref> and OCNet <ref type="bibr" target="#b22">[23]</ref> aggregated dense pixel-pair associations. These works have pushed the boundary of segmentation accuracy and attained excellent performances on existing benchmarks. In addition to accuracy, the efficiency of segmentation CNNs is crucial for real-time applications. Efficient networks were designed such as ERFNet <ref type="bibr" target="#b4">[5]</ref> and SwiftNet <ref type="bibr" target="#b5">[6]</ref>. They were built on techniques including early downsampling, filter factorization, multi-branch setup and ladder-style upsampling. Some efficient CNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> also leveraged attention connections, trying to improve the trade-off between segmentation accuracy and computation complexity. With these advances, semantic segmentation can be performed both swiftly and accurately, and thereby has been incorporated into many optical sensing applications such as semantic cognition system <ref type="bibr" target="#b25">[26]</ref> and semantic visual odometry <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">From RGB-based to multimodal semantic segmentation</head><p>While ground-breaking network architectural advances have been achieved in single RGB-based semantic segmentation on existing RGB image segmentation benchmarks such as Cityscapes <ref type="bibr" target="#b27">[28]</ref>, BDD <ref type="bibr" target="#b28">[29]</ref> and Mapillary Vistas <ref type="bibr" target="#b29">[30]</ref>, in some complex environments or under challenging conditions, it is necessary to employ multiple sensing modalities that provide complementary information of the same scene. Comprehensive surveys on multimodal semantic segmentation were presented in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. In the literature, researchers explored RGB-Depth <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, RGB-Infrared <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, RGB-Thermal <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, GRAY-Polarization <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and Event-based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>  semantic segmentation to improve the reliability of surrounding sensing and the applicability towards real-world applications. For example, RFNet <ref type="bibr" target="#b24">[25]</ref> fused RGB-D information on heterogeneous datasets, improving the robustness of SS in road-driving scenes with small-scale, unexpected obstacles.</p><p>In this work, we focus on RGB-P semantic segmentation by using a single-shot polarization camera. Traditional polarization-driven dense prediction frameworks were mainly dedicated to the detection of water hazards <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> or the perception in indoor scenes <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. In our previous works, we investigated the impact of loss functions on water hazard segmentation <ref type="bibr" target="#b41">[42]</ref>, followed by a comparative study on high-recall semantic segmentation <ref type="bibr" target="#b42">[43]</ref>. Inspired by <ref type="bibr" target="#b39">[40]</ref>, dense polarization maps were predicted from RGB images through deep learning <ref type="bibr" target="#b0">[1]</ref>. Instead, current polarization imaging technique makes it possible to sense pixel-wise polarimetric information in a single shot and has been integrated on perception platforms for autonomous vehicles <ref type="bibr" target="#b12">[13]</ref>. Following this line, we present a multimodal semantic segmentation system with single-shot polarization sensing. Notably, we found previous collections <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> of polarization images were mainly gray images without providing RGB information that are critical for segmentation tasks. Besides, they were limited in terms of data diversity and entailed careful calibration between different cameras. In contrast, we are able to bypass the complex calibration and naturally obtain multimodal data with single-shot polarization imaging. As an important contribution of this work, a novel outdoor traffic scene RGB-P dataset is collected and densely annotated, which covers not only specular scenes but also diverse unstructured surroundings. The dataset will be made publicly available to the community to foster polarimetry-based semantic segmentation. Moreover, our work is related to transparent object segmentation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>In addition, some polarization-driven objection detection methods have been explored in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. The designed modules re-encode raw polarization images for a better representation of polarization information. It may be beneficial when the input images are all of the same type like polarization raw images. After all, they are RGB images and possess similar data distribution. However, it can not be generalized into other sensors easily like RGB and disparity images, which have a distinct difference in data distribution. Unlike them, our work is to build a network architecture which can be flexibly adapted to different sensors besides RGB-polarization information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we derive the polarization image formation process and explain why polarization images contain rich information to complement RGB images for semantic segmentation. Then, we make a brief introduction of our integrated multimodal sensor and the novel RGB-P dataset. Finally, we present the Efficient Attention-bridged Fusion Network (EAFNet) for polarimetrybased multimodal scene perception. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Polarization image formation</head><p>Polarization is a significant characteristic of electromagnetic waves. When optical flux is incident upon a surface or medium, three processes occur: reflection, absorption and transmission. Analyzing the polarization of reflected light is possible to determine the optical proprieties of a given surface or medium. We illustrate the importance of polarization according to the Fresnel equation: </p><p>where r and t are the reflected and refracted portion of incoming light, the subscript label s and p represent perpendicular polarization and parallel polarization, n 1 and n 2 are refractive indexes of the two media material, and the and are the angle of incident light and refracted light, respectively. Inferred from Eq. (1), we find that the surface material's optical characteristics can affect the intensity of the two orthogonally polarized light. Therefore, the orthogonally polarized light can partially reflect the surface material.</p><p>The polarization image formation can be reducible to the model shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. In outdoor scenes, the light source is mainly sunlight. When the sunlight shines on the object like cars, polarized reflection occurs. Then, the reflected light with orthogonally polarized portion enters the camera with a polarization sensor, and the optical information with polarized characteristics are recorded by the sensor. The reason why the photoelectric sensor can record the polarized information is that the sensor's surface is covered by a polarization mask layer with four different polarization directions, and only the light with the same polarization direction can pass the layer.</p><p>Here, we make a brief introduction of polarization parameters like the Degree of Linear Polarization (DoLP) and the Angle of Linear Polarization (AoLP). They are the key elements that contribute to the advancement of multimodal semantic segmentation. They are derived by Stokes vectors S, which are composed of four parameters, i.e., S 0 , S 1 , S 2 and S 3 . More precisely, S 0 stands for the total light intensity, S 1 stands for the parallel polarized portion's superiority against the perpendicular polarized portion, and S 2 stands for 45 ? polarized portion's superiority against 135 ? polarized portion. S 3 , associated to circularly polarized light, is not involved in our work on multimodal semantic segmentation. They can be derived by:</p><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? ? ? ? ? 0 = 0 + 90 = 45 + 135 , 1 = 0 ? 90 , 2 = 45 ? 135 ,<label>(2)</label></formula><p>where I 0 , I 45 , I 90 and I 135 are the optical intensity values at the certain polarization direction, i.e., 0 ? , 45 ? , 90 ? and 135 ? . Here, DoLP and AoLP can be formulated as:</p><formula xml:id="formula_2">= ?? 2 1 + 2 2 0 ,<label>(3)</label></formula><formula xml:id="formula_3">= 1 2 ? arctan 1 2 .<label>(4)</label></formula><p>According to Eq. (3), the range of DoLP is from 0 to 1. For partially polarized light, DoLP ? (0, 1). For completely polarized light, DoLP = 1. Namely, DoLP stands for the degree of Linear Polarization. For AoLP, it ranges from 0 ? to 180 ? . AoLP can reflect object's silhouette information, because objects with the same material normally possess similar AoLP. Therefore, AoLP is a natural scene segmentation mask. In other words, objects of the same category or with the same material have similar AoLP. Different from RGB-based sensors whose output can be influenced by various outdoor environments like foggy weather or dust, polarization information from RGB-P sensor still keeps stable according to Eq. <ref type="formula" target="#formula_1">(2)</ref>, <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref>. Because the RGB images with four polarization directions from RGB-P sensor suffer similar degradation in the process of reaching the polarization filter in RGB sensor and the degradation will be cancelled out in the derivation of DoLP and AoLP. Therefore, the polarization stability against various outdoor environments is beneficial for scene perception. We generate a visualization of a set of DoLP and AoLP polarization images, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We find that the glass area and vegetation area are of high DoLP, but other areas are of low DoLP, which offers limited information, merely focused on the area with polarized characteristics. Besides, the left part of the vegetation and sky can not be distinguished merely depending on DoLP. On the contrary, for AoLP, we find areas of the same category show proper continuity of polarization information, which indicates great spatial priors for SS. AoLP offers a better representation of spatial information, which keeps a consistent distribution on the same category or materials like vegetation, sky, road and glass. In Section 4, we will further analyze the AoLP's great potential in providing extra spatial information for SS over DoLP with extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrated multimodal sensor and ZJU-RGB-P dataset</head><p>The RGB-P outdoor scene dataset is captured by an integrated multimodal sensor for autonomous driving <ref type="bibr" target="#b12">[13]</ref>, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The sensor is a highly integrated system which is a combination of polarization sensor, RGB sensor, infrared sensor and depth sensor. The sensor captures polarization information with an RGB-based imaging sensor LUCID_PHX050S. The difference between LUCID_PHX050S and Gray-Polarization sensors is that the former is covered with an extra Bayer array besides the polarization mask. Intensity and luminance are important for the detection quality with image sensors, but the use of polarization (eg., linear polarization ) filters naturally results in losing less of half of input intensity coming to the RGB sensor. The reason why the collected RGB-polarization images can keep the consistency against the changing outdoor environment is that we utilize the data acquisition and processing program, i.e., Arena, that can dynamically adjust the gain factor and exposure time according to the outdoor environment. In addition, the multimodal sensor integrates an embedded system which combines hardware and  software, by which we can attain various types of information like semantic information, infrared information, stereo depth information <ref type="bibr" target="#b48">[49]</ref>, monocular depth information <ref type="bibr" target="#b49">[50]</ref> and surface normal information <ref type="bibr" target="#b50">[51]</ref> by utilizing relevant estimation algorithms. While the sensor provides diverse modalities, this work focuses on using RGB and polarization information. <ref type="figure" target="#fig_4">Fig. 5(b)</ref> shows some examples of the sensor's output information. The highly integrated sensor can broaden RGB-based sensor's application scenarios <ref type="bibr" target="#b12">[13]</ref>. The infrared information can assist nighttime semantic segmentation, and the polarization-RGB-infrared multimodal sensor can offer precise depth information by pairing the sensors with different baselines. We leverage the multimodal sensor to attain pixel-aligned polarization and RGB images, and the main purpose of this work to adapt RGB-based SS to Polarization-driven multimodal SS. RGB-Polarization outdoor scene SS dateset is scarce in the literature. Some research groups have realized the importance of polarization information for outdoor perception. The Polabot dataset <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, a GARY-Polarization outdoor scene SS dataset, consists of around 180 pairs of images at a low resolution of 230?320. The limited image number and the low resolution make it hard to train a robust SS network for outdoor scenes. In addition, the dataset is short of RGB information which provide important texture features for classification tasks.</p><p>Addressing the scarcity, we build the first RGB-P outdoor scene dataset which consists of 394 annotated pixel-aligned RGB-Polarization images. We collect the images with abundant and complex scenes at Yuquan Campus, Zhejiang University, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. The scenes of the dataset cover road scenes around teaching building area, canteen area, library and so on to provide diverse scenes for reducing the risk of over-fitting in training SS models.</p><p>The resolution of our dataset is 1024?1224, which makes it possible to apply data augmentation like random crop and random rescale, which are crucial for improving data diversity and attaining robust segmentation <ref type="bibr" target="#b25">[26]</ref>. We label the dateset with 9 classes at the pixel level, i.e., Building, Glass, Car, Road, Vegetation, Pedestrian, Bicycle and Background using LabelMe <ref type="bibr" target="#b51">[52]</ref>. Here, we make a statistics of category distribution at the pixel level and draw a histogram as shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. We can learn from the histogram that the dataset has a diversity of categories, and the categories with a low pixel proportion will be the difficult categories for SS like glass and pedestrian. An example of the dataset is shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, which consists of four pixel-aligned  RGB images at four polarization directions and a SS label. But AoLP and DoLP are the ultimate polarization representations integrated into SS, so the four polarized RGB images need to be operated according to Eq. (3) and Eq. (4). We utilize the average of the four polarized RGB images as the RGB images to feed into EAFNet. In fact, the average of any two orthogonal directions RGB images can represent images captured by a conventional RGB sensor. Finally, we select 344 images as the training set, and the other 50 images as the validation set. We name it ZJU-RGB-P dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Efficient attention-bridged fusion network</head><p>In order to combine RGB and polarization features, we present EAFNet, an Efficient Attentionbridged Fusion Network to exploit multimodal complementary information, whose architecture is shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. Inspired by SwiftNet <ref type="bibr" target="#b5">[6]</ref> and our previous SFN <ref type="bibr" target="#b42">[43]</ref> with an U-shape encoder-decoder structure, EAFNet is designed to keep a similar architecture with downsampling paths to extract features and an upsampling module to restore the resolution, together with EAC modules to fuse features from RGB and polarization images. Here, we make a brief overview of EAFNet according to <ref type="figure" target="#fig_8">Fig. 9</ref>. EAFNet is designed to have a three-branch structure with downsampling paths of the same type. They are the RGB branch, the polarization branch and the fusion branch. In order to advance computation efficiency, we employ ResNet-18 <ref type="bibr" target="#b14">[15]</ref>, a light-weight encoder to extract and fuse features. After attaining the downsampled and fused features, SPP module, a spatial pyramid pooling module <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">53]</ref> is leveraged to enlarge valid receptive field. Then, a series of upsampling modules are leveraged to restore the feature  resolution. Like SwiftNet, EAFNet employs a series of convolution layers with kernel size of 1?1 to connect features between shallow layers and deep layers. The key innovation, here, lies in that EAFNet possesses the carefully designed fusion module, namely EAC Module, with inspiration gathered from the Efficient Channel Attention Network <ref type="bibr" target="#b53">[54]</ref>. With this architecture, EAFNet is a real-time network, whose inference speed on GTX 1080Ti reaches 24 FPS (Frame Per Second) at the resolution of 512?1024.</p><p>EAC Module is an efficient attention complementary module which is designed for extracting informative RGB features and polarization features, as shown in <ref type="figure" target="#fig_9">Fig. 10</ref>. It is an efficient version of the Attention Complementary Module (ACM) <ref type="bibr" target="#b20">[21]</ref>, which replaces fully connection layers with 1?1 convolution layers whose kernel sizes are adaptively determined according to the channel number of the corresponding feature maps. On the one hand, the structure reduces computation complexity compared with ACM due to the use of local cross-channel interactions other than all channel-pair interactions. On the other hand, local cross-channel interaction effectively avoids the problem of losing information caused by the dimension reduction in learning channel attention.</p><p>Assuming the input feature map is ? R ? ? , we first apply a global average pooling layer to process , where , and are the height, width and channel number of the input feature map, respectively. Then, we obtain a feature vector = [ 1 , 2 , . . . . . . , ] ? R 1? , where the subscript label represents the sequence number of features' channel. The k-th ( ? [1, ]) element of can be expressed as: Then, the vector needs to be reorganized by a convolution layer with an adaptive kernel size K to obtain a more meaningful vector = [ 1 , 2 , . . . . . . , ] ? R 1? . K is the key point to attain the local cross-channel interaction attention weights, which can be acquired using t:</p><formula xml:id="formula_4">= 1 ? ?? =1 ?? =1 ( , ) .<label>(5)</label></formula><formula xml:id="formula_5">t = int abs log 2 (C) + b ,<label>(6)</label></formula><p>where b and are hyper-parameters set as 1 and 2 in our experiments, respectively. If t is divisible by 2, K is equal to t, otherwise K is equal to t plus 1. With the growing of channel depth, the EAC module can attain interaction among more channels. To limit the range of , sigmoid activation function (?) is applied to it. (?) can be expressed as:</p><formula xml:id="formula_6">( ) = 1 1 + ? .<label>(7)</label></formula><p>Then, we can get the final attention weights = [ 1 , 2 , . . . . . . , ] ? R 1? . All the elements of are in the range of 0 and 1. In other words, each element of can be viewed as the key weight of the corresponding channel of the input feature map. Finally, we perform an outer product of and to get the adjusted feature map ? R ? ? . Thereby, RGB features and polarization features can be adjusted dynamically by the EAC module.</p><p>Fusion module is leveraged to fuse the adjusted feature maps from RGB branch and polarization branch following the EAC modules. As mentioned above, the fusion branch is the same as the RGB branch and the polarization branch. The main difference lies in the inputting feature flow. Assuming at the i-th dowmsampling stage, the RGB branch's feature map is ? R ? ? , and the polarization branch's feature map is R ? ? . <ref type="figure" target="#fig_10">Fig. 11</ref> illustrates one layer of the fusion branch for the fusion process. The left part is the RGB feature, and the right part is the polarization feature, while the feature flowing through the center arrow is the fusion feature from the previous fusion stage. Then, the fused feature +1 at the current stage can be expressed as:</p><formula xml:id="formula_7">+1 = * + * + ,<label>(8)</label></formula><p>where +1 working as the fusion feature is passed into the fusion branch to extract higher-level features. It should be noted that at the first fusion stage of our EAFNet architecture, it only has RGB feature and polarization feature as input information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and analysis</head><p>In this section, the implementation details and a series of experiments with comprehensive analysis are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>The experiments concerning polarization fusion are performed on ZJU-RGB-P dataset, while the preliminary experiment detailed in Section 1 and the supplement experiment detailed in Section 4.5 are performed on the Lost and Found dataset <ref type="bibr" target="#b10">[11]</ref>. The remaining implementation details are the same for all of the experiments. For data augmentation, we first scale the images with random factors between 0.75 and 1.25, then we randomly crop the images with a crop size of 768?768, followed with a random horizontal flipping. It is worth noting that AoLP's random horizontal flipping has a critical difference from DoLP and RGB images. According to Eq. (4), when the RGB images at four polarized directions are applied horizontal flipping, the AoLP will be:</p><formula xml:id="formula_8">= 180 ? ? ,<label>(9)</label></formula><p>where is the ultimate horizontal flipped AoLP image, and is merely the spatially horizontal flipped version of the initial AoLP image. After all the data augmentation, all the processed images are normalized to the range between 0 and 1.</p><p>We use Tensorflow and an NVIDIA GeForce GTX 1080Ti GPU to implement EAFNet and perform training. We use Adam optimizer <ref type="bibr" target="#b54">[55]</ref> with an initial learning rate of 4?10 ?4 . We decay the learning rate with cosine annealing to the minimum 2.5?10 ?3 of the initial learning rate until the final epoch. To combat over-fitting, we use the L2 weight regularization with a weight decay of 1?10 ?4 . Unlike prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, we have not adopted any pre-trained weights in order to investigate the effectiveness of multimodal SS fairly, with the aim to reach high performance even with limited pairs polarization images. We utilize the cross entropy loss to train all the models with a batch size of 8. We evaluate with the standard Intersection over Union (IoU) metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and analysis</head><p>Both of AoLP and DoLP can represent polarization information of scenes, but which is the better to be fused into polarization-driven SS remains an open question.</p><p>We have made a brief analysis of the superiority of AoLP over DoLP for polarization-driven SS in Section 3.1 intuitively. As preliminary investigation, according to <ref type="figure" target="#fig_2">Fig. 4</ref>, we find that AoLP's distribution has a remarkable difference to that of DoLP. Further, we present the statistics of the value distributions of DoLP and AoLP on the ZJU-RGB-P training set, as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. The majority of all pixels of the training set are with a small DoLP ranging from 0 to 0.4, while the portion of pixels whose DoLP values are larger than 0.4 is rather low, indicating that DoLP offers limited information, merely on categories with highly polarized characteristics. Different from DoLP, AoLP offers a uniform distribution. In other words, nearly all pixels of AoLP images possess meaningful features that are useful for SS. With the different distributions of DoLP and AoLP, it can be expected that their information are emphasized on different polarization characteristics. Inspired by this observation, we perform a series of experiments to investigate <ref type="table">Table 1</ref>. Accuracy analysis on ZJU-RGB-P including per-class accuracy in IoU (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Building Glass Car Road Vegetation Sky Pedestrian Bicycle mIoU the effectiveness of EAFNet to fuse polarization and RGB information, and whether AoLP is superior to DoLP in offering complementary information for RGB-P image segmentation. For the basic control experiment, we first train the RGB-only SwiftNet on ZJU-RGB-P dataset as our Baseline. Then, four sets of training are performed for comparison. As shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, EAFNet is a two-path network, where RGB images and polarization information are input into different paths. To explore the better polarization feature, we select AoLP (marked as EAF-A) and DoLP (marked as EAF-D) as the input polarization information, respectively. Considering the fact that both AoLP and DoLP can offer polarization information, we also concatenate AoLP and DoLP images along channel to build a polarization representation for training a variant model of EAFNet (marked as EAF-A/D). Finally, we build a three-path version of EAFNet, where we select one path as the RGB path and the other two paths as polarization paths. AoLP and DoLP are passed into the two polarization paths (marked as EAF-3Path).</p><p>All quantitative results of the experiments are shown in <ref type="table">Table 1</ref>. It can be seen that models combined with polarization information can advance the segmentation of objects with polarization characteristics like glass (73.4% to 79.3%), car (91.6% to 93.7%) and bicycle (82.5% to 86.0%). In addition, we observe that not only the IoU of classes with polarization characteristics are advanced, but other classes' IoU have been improved by a great extent when combined with polarization information, especially pedestrian (36.1% to 63.8%). Meanwhile, the mIoU is lifted to 85.7% from 80.3%.</p><p>Further, we compare and discuss among the groups that fuse polarization-based features. As shown in <ref type="table">Table 1</ref>, EAF-A is the optimal setting, while EAF-3Path is the worst group. Here, we analyze from the view of data distribution and model complexity. Our main focus is on the classes like glass and car, as the initial motivation of this study is to lift the segmentation performance of objects with polarization characteristics. Making a comparison between EAF-A and EAF-D, we find that the former can better advance the IoU of glass and car than the latter. It is the data distribution that counts. The analysis of the different distributions in previous sections shows that AoLP offers a better spatial representation like contour information than DoLP, while DoLP only offers meaningful information on areas with high polarization information. In this sense, AoLP provides richer priors and complementary information for RGB-P segmentation. The reason why EAF-A/D can attain higher IoU values on glass and car than EAF-D is that AoLP complements the spatial features of DoLP. However, it reaches a lower IoU on glass than EAF-A, because the interference between DoLP and AoLP features occurs, bringing some side effects and losing some serviceable information. EAF-3Path is the worst group as the complex architecture of this model prevents the model from exploiting the most informative features. Besides, the 3-path structure impairs the capacity of RGB features, which is critical for outdoor scene perception. Eventually, we conclude from the quantitative analysis that utilizing AoLP images to feed in the polarization path can greatly advance polarization-driven segmentation performance. For qualitative results, we use the Baseline RGB-only model and the EAF-A polarizationdriven model, i.e., SwiftNet and our EAFNet fed with AoLP on the ZJU-RGB-P validation set to produce a series of visualization examples, as illustrated in <ref type="figure" target="#fig_1">Fig. 13</ref>. We find that SwiftNet wrongly segments the pedestrians into cars in the first row of <ref type="figure" target="#fig_1">Fig. 13</ref> where EAFNet detects them correctly. In the second row, EAFNet successfully distinguishes glass from the car, while SwiftNet can not segment the full glass area. Moreover, SwiftNet even segments part of the car into road and part of the pedestrian into vegetation according to the last row of results in <ref type="figure" target="#fig_1">Fig. 13</ref>. The wrong segmentation results in outdoor traffic scenes can lead to terrible situations and even accidents once the model is selected to guide autonomous vehicles or assisted navigation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. In addition, we can learn from the first row that EAFNet can keep a good performance in low luminance. The area to the fore are in the shade with low luminance, where limited RGB information makes it hard for our eyes to perceive the scene, while AoLP offers sufficient spatial information to complement it, thereby advancing the performance of EAFNet. On the contrary, the area in the distance is illuminated by sun with high intensity annotated as background, where the complementary AoLP aids the segmentation of EAFNet, leading to accurate perception in such challenging scenarios. It is obvious that polarization-driven SS can complement the missing information merely based on RGB images. Therefore, multimodal SS is beneficial for semantic understanding in pursuit of robust outdoor scene perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of EAC module</head><p>EAC module is the key module of EAFNet, which can extract attention weights of RGB branch and AoLP branch. To better demonstrate the effect of EAC module, we visualize the fourth downsampling block's feature maps of RGB and AoLP branches, and their attention weights of EAC module as shown in <ref type="figure" target="#fig_2">Fig. 14.</ref> We only visualize feature maps of the former 16 channels. Here, (i, j) denotes the position at the i-th row and the j-th col of the feature map, which corresponds to the attention weight one by one, where some insightful results can be found. In the RGB branch, we find that the car and glass area have low responses in the feature maps. On the contrary, the corresponding area of the AoLP branch have high responses, <ref type="figure" target="#fig_1">especially at (2, 4), (3, 2) and (4,  4)</ref>. Then, their EAC module extracts their attention weights, respectively. Taking (3, 2) as an example to illustrate the complement process, this channel's attention weights are 0.5244 and 0.4214 for RGB and AoLP branches, respectively. Then, the corresponding feature map will be multiplied by the attention weight. Finally, the adjusted feature map are added up to build the  ultimate feature map, and it can be clearly seen that the feature maps spotlight the area of car perfectly.</p><p>As it can be seen in <ref type="figure" target="#fig_2">Fig. 14,</ref> the attention weights of RGB are higher than those of AoLP in most cases. Here, we evaluate the weights generated by EAC Module at all levels and illustrate the average of them as shown in <ref type="figure" target="#fig_4">Fig. 15</ref>. According to the curve, it can be easily observed that the RGB branch possesses higher weights than the AoLP branch at Layer1, Layer2, Layer3 and Layer4. On the contrary, the AoLP branch has a higher weight than the RGB branch at the first downsampling block, i.e., conv0. As mentioned before, AoLP offers a representation of spatial information and rich priors. Therefore, at the beginning of EAFNet, AoLP offers more distinguished features than RGB. With the features flowing into deeper layers, RGB branch becomes overwhelming. In addition, both of the weight curves keep a similar variation trend and reach the highest at Layer3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>To better illustrate that fusing polarization information is beneficial and to verify EAFNet's powerful fusion capacity, we have performed three extra training. First, we directly utilize AoLP images to train SwiftNet, denoted as SN-AoLP. Second, we utilize the concatenation of RGB and AoLP images to train SwiftNet, denoted as SN-RGB/A. Finally, we get rid of the EAC Moudle of EAFNet, and element-wise addition is utilized to fuse RGB features and AoLP features instead, <ref type="table">Table 2</ref>. Accuracy analysis of the ablation study on ZJU-RGB-P (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Building  denoted as EAF-wo-A. We also select Baseline and EAF-A of the previous experiment for the ablation study, as shown in <ref type="table">Table 2</ref>. We find that SN-AoLP can reach a decent performance, which is benefited from the spatial priors of AoLP. It can be learned from the comparison between Basline and SN-AoLP that RGB possesses more distinguished features than AoLP, while AoLP offers meaningful information as well. Besides, SN-RGB/A can indeed advance the segmentation of classes with polarization characteristics like glass (73.4% to 75.6%), but it does not yield remarkable benefits for all classes in contrast to our EAF-A. In addition, SN-RGB/A even causes a slight mIoU degradation to 80.2% from 80.3%. It is the difference between RGB and AoLP distributions that accounts for this degradation. The interference between RGB and AoLP has an adverse impact on the extraction of distinguishable features. We can learn from the comparison between EAF-wo-A and EAF-A that attention mechanism of EAFNet is highly effective. Combining with EAC Module, all classes have a remarkable elevation like glass (75.4% to 79.3%), pedestrian (40.6% to 60.4%) and so on. Making a comparison among all the groups, we draw a conclusion that EAF-A reaches the highest accuracy on all classes, indicating the effectiveness of our EAFNet and the designed polarization fusion strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization to other sensor fusion</head><p>To prove the flexibility of EAFNet to be adapted to other sensor combination scenarios besides polarization information, we utilize disparity images with EAFNet to investigate in the unexpected obstacle detection scenario, i.e., the preliminary investigation mentioned in Section 1.</p><p>According to <ref type="figure">Fig. 1</ref>, we can find that disparity images reflect the contours of the tiny obstacles. Thereby, combing RGB and disparity images with EAFNet is hopeful to address the devastating results of using merely RGB data. Considering the similar distribution between disparity and AoLP images, we train the EAFNet fed with pixel-aligned disparity and RGB images (marked as EAFNet-RGBD). We mark the baseline group as SwiftNet-RGB. All the training strategies are set according to Section 4.1. From the results in <ref type="table" target="#tab_2">Table 3</ref>, we observe a remarkable elevation on the performance with the aid of EAFNet and complementary disparity images, where the precision and IoU of obstacle segmentation are lift to 76.2% from 26.5% and 52.7% from 20.9%, respectively. It indicates that combing disparity images with EAFNet bears fruit. To have a better realization of EAFNet-RGBD's effects intuitively, we visualize an example as shown in <ref type="figure" target="#fig_5">Fig. 16</ref>, where EAFNet-RGBD segments obstacles and most of the road areas successfully, but SwiftNet-RGB ignores most of the road and obstacles. Therefore, it is essential to perform multimodal semantic segmentation with complementary sensing information like polarization-driven and depth-aware features to have a reliable and holistic understanding of outdoor traffic scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this paper, we propose EAFNet by fusing features of RGB and polarization images. We build ZJU-RGB-P with our integrated multimodal vision sensor, which is the first RGB-polarization semantic segmentation dateset to the best of our knowledge. EAFNet dynamically extracts attention weights of RGB and polarization branches, adjusts and fuses multimodal features, significantly advancing the segmentation performance, especially on classes with highly polarized characteristics like glass and car. Extensive experiments are conducted to prove the effectiveness of EAFNet for incorporating features from different sensing modalities and the flexibility to be adapted to other sensor combination scenarios like RGB-D perception. Therefore, EAFNet is a multimodal SS model that can be utilized in diverse real-world applications. In the future, there are two research paths that can be explored. One is to build more kinds of multimodal dataset based on the integrated multimodal vision sensor like RGB-Infrared dataset to address nighttime scene understanding. Another line is to enlarge the categories of ZJU-RGB-P to cope with the detection of transparent objects, ice and water hazards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research was granted from ZJU-Sunny Photonics Innovation Center (No. 2020-03). This research was also funded in part through the AccessibleMaps project by the Federal Ministry of Labor and Social Affairs (BMAS) under the Grant No. 01KM151112.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>arXiv:2011.13313v2 [cs.CV] 22 Jan 2021 An example of Lost and Found dataset: (a) is the RGB image. (b) is the disparity image. (c) is the label, where the blue area denotes the obstacles, the purple area denotes the passable area and the black area denotes the background. The effect of SwiftNet [6] trained merely with RGB images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Polarization image formation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) RGB images with four polarization directions (b) Polarization image Display of polarization characteristics: (a) RGB images at four polarization directions; (b) Polarization image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Integrated multimodal sensor (b) Output of the sensor Our integrated multimodal vision sensor for capturing polarization information. (a) The integrated multimodal sensor; (b) The output of the sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Diverse scenes in our ZJU-RGB-P dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Histogram of ZJU-RGB-P dataset's category distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>An example of ZJU-RGB-P dateset including RGB images at different polarization directions and the pixel-wise semantic segmentation label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Overview of EAFNet. RGB and polarization images are input to the network for extracting features separately. The EAC modules adaptively fuse the features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>The Efficient Attention Complementary module (EAC module). A is the input feature map, B is the average global feature vector, C is the feature vector after a convolution layer with an adaptive kernel size. D is the attention weights, i.e. the vector after activation function of C, and E is the adjusted feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>The structure of Fusion Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>The value distribution of ZJU-RGB-P training set's DoLP and AoLP: (a) The DoLP distribution; (b) The AoLP distribution. All the values are normalized to the range between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Qualitative result comparison between the RGB-only baseline and our EAFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>The EAC module's role in fusing features: An RGB image and an AoLP image are fed into EAFNet, then we visualize the fourth downsampling block's feature maps. Following that, EAC module extracts RGB and polarization branches' attention weights. Finally, the feature maps are adjusted by the attention weights and fused.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>The average attention weights of EAFNet at all levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Qualitative comparison results of SwiftNet-RGB and EAFNet-RGBD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Glass Car Road Vegetation Sky Pedestrian Bicycle mIoU</figDesc><table><row><cell>Baseline</cell><cell>83.0</cell><cell>73.4 91.6 96.7</cell><cell>94.5</cell><cell>84.7</cell><cell>36.1</cell><cell>82.5</cell><cell>80.3</cell></row><row><cell>SN-A</cell><cell>74.0</cell><cell>66.6 87.1 94.7</cell><cell>91.1</cell><cell>76.1</cell><cell>32.9</cell><cell>65.5</cell><cell>73.5</cell></row><row><cell>SN-RGB/A</cell><cell>83.9</cell><cell>75.6 91.6 96.9</cell><cell>94.4</cell><cell>78.2</cell><cell>41.0</cell><cell>79.9</cell><cell>80.2</cell></row><row><cell>EAF-wo-A</cell><cell>85.2</cell><cell>75.4 92.2 96.8</cell><cell>94.8</cell><cell>84.4</cell><cell>40.6</cell><cell>82.9</cell><cell>81.6</cell></row><row><cell>EAF-A</cell><cell>87.0</cell><cell>79.3 93.6 97.4</cell><cell>95.3</cell><cell>87.1</cell><cell>60.4</cell><cell>85.6</cell><cell>85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Accuracy analysis of the supplement experiment on Lost and Found</head><label>3</label><figDesc>(%).</figDesc><table><row><cell>Group</cell><cell cols="2">SwiftNet-RGB</cell><cell cols="2">EAFNet-RGBD</cell></row><row><cell>Class</cell><cell cols="4">Road Obstacle Road Obstacle</cell></row><row><cell cols="2">Precision 85.4</cell><cell>26.5</cell><cell>88.2</cell><cell>76.2</cell></row><row><cell>Recall</cell><cell>63.9</cell><cell>49.8</cell><cell>75.9</cell><cell>63.0</cell></row><row><cell>IoU</cell><cell>56.2</cell><cell>20.9</cell><cell>68.9</cell><cell>52.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by Hangzhou SurImage Technology Company Ltd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclosures</head><p>The authors declare that there are no conflicts of interest related to this article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting polarization beyond semantics for wearable robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haase-Sch?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intell. Transp. Syst. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?ic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12599" to="12608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bridging the day and night domain gap for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1312" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11169</biblScope>
			<biblScope unit="page">111690</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for semantic image segmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>M?riaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput. p</title>
		<imprint>
			<biblScope unit="page">104042</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Issafe: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08974</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting traversable area and water hazards for the visually impaired with a prgb-d sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A multimodal vision sensor for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05649</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="9373" to="9383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ds-pass: Detail-sensitive panoramic annular semantic segmentation through swaftnet for surrounding sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time fusion network for rgb-d semantic segmentation incorporating unexpected obstacle detection for road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5558" to="5565" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robustifying semantic cognition of traversability across wearable rgb-depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. optics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="3141" to="3155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Palvo: visual odometry based on panoramic annular lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. express</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="24481" to="24497" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="465" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ranus: Rgb and nir urban scene dataset for deep scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1808" to="1815" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segmenting objects in day and night: Edge-conditioned cnn for thermal image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks Learn. Syst. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Heatnet: Bridging the day-night domain gap in semantic segmentation with thermal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Z?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04645</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploration of deep learning-based multimodal fusion for semantic road scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blanchon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Computer Vision</title>
		<meeting>the International Joint Conference on Computer Vision</meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Outdoor scenes pixel-wise semantic segmentation using polarimetry and fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blanchon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crombez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Computer Vision</title>
		<meeting>the International Joint Conference on Computer Vision</meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ev-segnet: Semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1624" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perception framework of water hazards beyond traversability for real-world navigation assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single image water hazard detection using fcn with reflection attention units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Target enhanced 3d reconstruction based on polarization-coded structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. express</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1173" to="1184" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth from stereo polarization in specular scenes for urban robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Voorhies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page" from="1966" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Importance-aware semantic segmentation with efficient pyramidal context network for navigational assistant systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3412" to="3418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A comparative study of high-recall real-time semantic segmentation based on swift factorized network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11169</biblScope>
			<biblScope unit="page">111690</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multimodality semantic segmentation based on polarization and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ainouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bensrhair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Segmenting transparent objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13948</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep polarization cues for transparent object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="8602" to="8611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Road scenes analysis in adverse weather conditions by polarizationencoded images and adapted deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ainouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An end-to-end cnn framework for polarimetric vision tasks based on polarization-parameter-constructing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08740</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A robust monocular depth estimation framework based on light-weight erf-pspnet for day-night driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1518</biblScope>
			<biblScope unit="page">12051</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. journal computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="11534" to="11542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
