<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Multi-Head Channel Self-Attention for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pecoraro</surname></persName>
							<email>-roberto.pecoraro@unito.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<postCode>10124</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<postCode>10124</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viviana</forename><surname>Bono</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<postCode>10124</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Gallo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<postCode>10124</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Multi-Head Channel Self-Attention for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Rev 9 -11/18/2021 First Rev 10/15/2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the Transformer architecture was introduced in 2017 there has been many attempts to bring the selfattention paradigm in the field of computer vision. In this paper we propose a novel self-attention module that can be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the LHC: Local (multi) Head Channel (self-attention). LHC is based on two main ideas: first, we think that in computer vision the best way to leverage the self-attention paradigm is the channel-wise application instead of the more explored spatial attention and that convolution will not be replaced by attention modules like recurrent networks were in NLP; second, a local approach has the potential to better overcome the limitations of convolution than global attention. With LHC-Net we managed to achieve a new state of the art in the famous FER2013 dataset with a significantly lower complexity and impact on the "host" architecture in terms of computational cost when compared with the previous SOTA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The aim of this work is to explore the capabilities of the selfattention paradigm in the context of computer vision, more in particular, in the facial expression recognition. In order to do that we designed a new channel self-attention module, the LHC, which is thought as a processing block to be integrated into a pre-existing convolutional architecture.</p><p>It inherits the basic skeleton of the self-attention module from the very well known Transformer architecture by Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> with a new design thought to improve it and adapt it as an element of a computer vision pipeline. We call the final architecture LHC-Net: Local (multi-)Head Channel (self-attention) Network. In the context of a wider research focused on the recognition of human emotions we tested LHC-Net on the FER2013 dataset, a dataset for facial emotion recognition <ref type="bibr" target="#b1">[2]</ref>. FER2013 was the object of a 2013 Kaggle competition. It is a dataset composed of 35587 grey-scale 48x48 images of faces classified in 7 categories: anger, disgust, fear, happiness, sadness, surprise, neutral. The dataset is divided in a training set (28709 images), a public test set (3589 images), which we used as validation set, and a private test set (3589 images), usually considered the test set for final evaluations. FER2013 is known as a challenging dataset because of its noisy data with a relatively large number of non-face images and misclassifications. It is also strongly unbalanced, with only 436 samples in the less populated category, "Disgust", and 7215 samples in the more populated category, "Happiness": LHC-Net should be generally considered as a family of neural network architectures having a backbone represented by a convolutional neural network in which one or more LHC modules are integrated. More specifically, in this paper, we will refer to LHC-Net as a ResNet34 <ref type="bibr" target="#b2">[3]</ref> having integrated 5 LHC modules as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>For this reason LHC-Net is a general purpose computer vision architecture since it doesn't feature any specific solution designed for facial expression recognition.</p><p>In our experiments, LHC-Net achieved a classification accuracy on the private test set of FER2013 which is, to the best of our knowledge (and accordingly with the paperswithcode's leaderboard at the time this paper is being written), the current single deep learning model state of the art both with and without test time augmentation, with a computational cost which is only a fraction of the previous SOTA architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Related work 1.1 Attention</head><p>The attention paradigm became very popular in the last few years with a large variety of mechanics and implementations in both NLP and computer vision scenarios. There are two main attention paradigms: either we pay attention to the data with the idea of enhancing the most meaningful aspects or we can try to exploit the inner relationships within these aspects in order to produce a more meaningful representation of the data. The latter approach is usually called self-attention because in some way the data pays attention to itself.</p><p>The first approach was introduced in 2014 by Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref> an updated by Luong et al. in 2015 <ref type="bibr" target="#b4">[5]</ref>. The proposed solutions were used in neural machine translation and integrated in a classic "seq to seq" encoder/decoder architecture in which the decoder learns what are the outputs of the encoder where to pay more attention dinamically. Self-attention was introduced in 2017 by Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> (again for neural machine translation) and it is considered by many the greatest breakthrough technology in AI since the backpropagation was introduced in 1986 <ref type="bibr" target="#b5">[6]</ref>. It fully replaced the previous state of the art technologies, the recurrent and convolutional networks, in NLP.</p><p>Since then there has been many attempts to bring selfattention in computer vision but, as of now, with only partial success. As opposite as the NLP case, in computer vision selfattention struggles to outperform the SOTA computer vision architectures like the classical Inception <ref type="bibr" target="#b6">[7]</ref>, ResNet <ref type="bibr" target="#b2">[3]</ref>, VGG, <ref type="bibr" target="#b7">[8]</ref> etc.</p><p>In computer vision there are several type of attentions paradigms, for clarity from now on we will use the following nomenclature:</p><p>? Global Attention: usually it is only a module used before another model with the idea to enhance the important parts of an image and to ignore the rest of the image.</p><p>? Spatial Attention: the attention modules focus on single pixels or areas of the feature maps.</p><p>? Channel Attention: the attention modules focus on entire feature maps.</p><p>? Self-Attention: the attention tries to find relationships between different aspects of the data.</p><p>? Stand-Alone Attention: the architecture is aimed at fully replacing the convolutional blocks and defining a new processing block for computer vision based on some attention mechanism (mostly self-attention).</p><p>Xu et al. proposed a global attention module for medical image classification <ref type="bibr" target="#b8">[9]</ref>, this module pre-processes images enhancing important areas pixel by pixel before feeding them into a standard convolutional neural network. This kind of pre-processing is thought to make more robust the following convolution processing. It could be associated to the one proposed by Jaderberg et al. <ref type="bibr" target="#b9">[10]</ref> which attempts to compensate for the lack of rotation/scaling invariance of the convolution paradigm. The proposed module learns a sample-dependant affine transformation to be applied to images in order to make them centered and properly scaled/rotated.</p><p>The channel approach we propose in this paper, despite being relatively unexplored in our self-attention mode, is instead very popular when associated with vanilla attention. Hu et al. proposed the SE-Net (Squeeze and Excitation) <ref type="bibr" target="#b10">[11]</ref>, a simple and effective module which enhances the more important features of a convolutional block. Squeeze and excitation lately became a key module in the very popular Efficient-Net by Tan et al. <ref type="bibr" target="#b11">[12]</ref> which set a new SOTA on several benchamrk datasets. Similarly Woo et al. proposed the CBAM (Convolutional Block Attention Module), a sequential module composed of a spatial and a channel attention sub-modules <ref type="bibr" target="#b12">[13]</ref>. There are other examples of channel and spatial vanilla attention: ECA-Net (Efficient Channel Attention) by Wang et al. <ref type="bibr" target="#b13">[14]</ref> is a new version of Squeeze and Excitation; SCA-CNN (Spatial and Channel-wise attention) proposed by Chen et al. <ref type="bibr" target="#b14">[15]</ref> combines both spatial and channel vanilla attention for image captioning. URCA-GAN by Nie et al. <ref type="bibr" target="#b15">[16]</ref> is a GAN (Generative Adversarial Network) featuring a residual channel attention mechanism thought for image-to-image translation.</p><p>Channel attention wasn't used only in vanilla approaches; similarly to our architecture Fu et al., Liu et al. and Tian et al. proposed self-attention architectures <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> respectively for scene segmentation, feature matching between pairwise images and video segmentation. The main differences between these modules and ours are the following:</p><p>? in all of them channel attention always has a secondary role and there's always a spatial attention sub-module with a primary role ? in all of them the crucial multi-head structure is lacking ? all of them implement channel attention as a "passive" non-learning module ? none of them integrates our local spatial behavior for channel attention</p><p>? none of them integrates our dynamic scaling which is very specific of our architecture.</p><p>As opposite as channel self-attention, spatial self-attention is widely explored, in most cases with the ambitious goal of totally replacing the convolution in computer vision, just like Vaswani's Transformer made LSTM obsolete. Bello et al. proposed an attention-augmented convolutional network <ref type="bibr" target="#b19">[20]</ref> in which Vaswani's self-attention is straightforwardly applied to pixels representations and integrated in a convolutional neural network.</p><p>Similarly Wu et al. proposed the Visual Transformer <ref type="bibr" target="#b20">[21]</ref>, an architecture in which many "tokens" (i.e., image sections derived from a spatial attention module) are feeded into a transformer. The entire block is integrated in a convolutional network. The Visual Transformer is inspired by ViT, the Vision Transformer by Dosovitskiy et al. <ref type="bibr" target="#b21">[22]</ref>, ViT is a standalone spatial self-attention architecture in which the transformer's inputs are patches extracted from the tensor image. Previous attempts to implement stand-alone spatial attention were done by Ramachandran et al. <ref type="bibr" target="#b22">[23]</ref> and Parmar et al. <ref type="bibr" target="#b23">[24]</ref>. Spatial self-attention was also used in GANs by Zhang et al. with their SAGAN (Self-Attention Generative Adversarial Network) <ref type="bibr" target="#b24">[25]</ref>.</p><p>More recently Liu et al. and Dai et al. proposed other two spatial stand-alone self-attention architectures, respectively the Swin Transformer <ref type="bibr" target="#b25">[26]</ref> and the CoAtNet <ref type="bibr" target="#b26">[27]</ref> (depthwise Convolution and self-Attention). We can think at stand-alone architectures as attempts of rethinking convolution and replace it in a way able to address its limitations. Many improvements of convolution were proposed, mainly to make them invariant for more general transformations than translations, such as the Deep Simmetry Network proposed by Gens et al. <ref type="bibr" target="#b27">[28]</ref> or the Deformable Convolutional Network by Dai et al. <ref type="bibr" target="#b28">[29]</ref>.</p><p>Both ViT and CoAtNet can be considered the current state of the art on Imagenet but they outperform Efficient Net by only a very small margin <ref type="bibr" target="#b29">[30]</ref> and at the price of a complexity up to 30x and of a pre-training on the prohibitive JFT-3B dataset containing 3 billions of images.</p><p>These are good reasons for considering convolution not yet fully replaceable by self-attention in computer vision. But the main reason we didn't pursue the goal of a stand-alone architecture is that we don't believe in the main assumption spatial self-attention is based on in computer vision. Selfattention had a great success in NLP because it eventually exploited the inner relationships between the words in a phrase which sequential approaches were not able to model effectively. Every word in a sentence has a strong well defined relationship with any other word in that phrase, and they finally form a complex structure composed of these relationships. But, for instance, if we take a picture of a landscape we see no reason to believe that such a relationship could exist between a rock on the ground and a cloud in the sky or, even more extremely, between two random pixels, at least not in the same way the subject of a phrase is related to its verb. On the other hand this observation does not hold for the features extracted from a picture, and the best way we know, so far, to extract features from a picture is convolution. These are the main reasons we decided to further explore channel selfattention in synergy with convolution, not as a stand-alone solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">FER2013</head><p>As mentioned before FER2013 is a challenging dataset for facial expressions recognition. As reported by Goodfellow et al. even human accuracy on FER2013 is limited to 65 ? 5% <ref type="bibr" target="#b1">[2]</ref>. Tang et al. <ref type="bibr" target="#b1">[2]</ref> successfully used linear support vector machines reaching 71.16% accuracy. Minaee et al. achieved 70.02% accuracy using a convolutional neural network augmented with a global spatial attention module <ref type="bibr" target="#b30">[31]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LHC-Net</head><p>As already mentioned and shown in <ref type="figure" target="#fig_0">Figure 1</ref> our LHC module can be integrated in virtually any existing convolutional architecture, including of course AlexNet <ref type="bibr" target="#b35">[36]</ref>, VGG <ref type="bibr" target="#b7">[8]</ref>, Inception <ref type="bibr" target="#b6">[7]</ref> and ResNet <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this section we will give a detailed mathematical definition of LHC as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, starting from a generic tensor and forward propagating it through the entire architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>We first need to define the model hyper-parameters: let n ? N + be the number of local heads, s ? N + the kernel size of the convolution we will use to process the value tensor, p ? N + the pool size used in average pooling and max pooling blocks, d ? N + the embedding dimension of every head and g ? R ?0 a constant we will need in the dynamic scaling module. Let x ? R H,W,C be a generic input tensor, where H, W and C are respectively the height, width and number of channels, with the constraint that HxW must be divisible by n.</p><p>We define Q, K and V as follows:</p><formula xml:id="formula_0">Q = AvgPool p,1 (x) ? R H,W,C<label>(1)</label></formula><formula xml:id="formula_1">K = MaxPool p,1 (x) ? R H,W,C<label>(2)</label></formula><formula xml:id="formula_2">V = AvgPool 3,1 (2D-Conv s,1 (x)) ? R H,W,C<label>(3)</label></formula><p>where the pooling operators subscripts are respectively the pool size and the stride and the convolution operator subscripts are respectively the kernel size and the stride. Now we want to split the tensors Q, K and V into n horizontal slices and reshape the resulting tensors as follows:</p><formula xml:id="formula_3">?h = 1, ..., n q h = [SplitHeads(Q)] h ? R C,(HxW )/n<label>(4)</label></formula><formula xml:id="formula_4">k h = [SplitHeads(K)] h ? R C,(HxW )/n (5) v h = [SplitHeads(V)] h ? R C,(HxW )/n<label>(6)</label></formula><p>Every head is deputed to process a triplet (q h , k h , v h ) then we have n separate fully connected layers with linear output and weights/biases:</p><formula xml:id="formula_5">w 1,h ? R (HxW )/n,d , b 1,h ? R d .</formula><p>Queries and keys will share the same dense blocks resulting in n embeddings as follows:</p><formula xml:id="formula_6">q i, j h = (HxW )/n ? t=1 q i,t h w t, j 1,h + b j 1,h ? R (7) k i, j h = (HxW )/n ? t=1 k i,t h w t, j 1,h + b j 1,h ? R (8) ?h = 1, ..., n ?i = 1, ...,C ? j = 1, ..., d</formula><p>Or, more shortly (from now on we will omit the head logic quantifier):q</p><formula xml:id="formula_7">h = q h ? w 1,h + b 1,h ? R C,d<label>(9)</label></formula><formula xml:id="formula_8">k h = k h ? w 1,h + b 1,h ? R C,d<label>(10)</label></formula><p>Now we can compute the attention scores through usual transposition and matrix product:</p><formula xml:id="formula_9">S h =q h ?k T h ? R C,C<label>(11)</label></formula><p>Dynamic scaling produces a channel-wise learned scaling (not dependent from heads) through averaging the scores and passing them through another fully connected layer with sigmoid activation and weights/biases</p><formula xml:id="formula_10">w 2 ? R C,C , b 2 ? R C : S h = Mean dim=2 (S h ) ? R C<label>(12)</label></formula><formula xml:id="formula_11">T i h = Sig C ? t=1S t h w t,i 2 + b i 2 ? R ?i = 1, ...,C<label>(13)</label></formula><formula xml:id="formula_12">N i, j h = S i, j h d (g+T i h ) ? R ?i, j = 1, ...,C<label>(14)</label></formula><formula xml:id="formula_13">W h = Softmax dim=2 (N h ) ? R C,C<label>(15)</label></formula><p>where T h is the tensor of the scaling factors, N h the tensor of the normalized attention scores and W h the final attention weights associated with the head h. Now we can compute the final attention tensor for head h very straightforwardly:</p><formula xml:id="formula_14">A h = W h ? v h ? R C,(HxW )/n<label>(16)</label></formula><p>and using simple transpose, reshape and concatenation operators we can compose the output y by assembling the n heads: </p><formula xml:id="formula_15">y = SplitHeads ?1 ([A 1 , A 2 , ..., A n ]) ? R H,W,C<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation and Analysis Channel Self-Attention</head><p>We already explained the main reasons behind our choice of channel-wise self-attention. We can summarize them as follows:</p><p>? spatial attention in computer vision strongly relies on the main assumption that a relationship between single pixels or areas of an image exists. This assumption is not self-evident or at least not as evident as the relationship between words in a phrase spatial attention is inspired by</p><p>? all attempts to pursue spatial self-attention in computer vision (especially in stand-alone mode) gained only minor improvements over previous state of the art architectures and, most of the times, at the price of an unreasonably higher computational cost and a prohibitive pre-training on enormous datasets</p><p>? much more simple and computationally cheaper approaches, like Squeeze and Excitation in Efficient Net, are already proven to be very effective without the need to replace convolution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Scaling</head><p>In Vaswani's Transformer the scaling is static and constant among the sequence. Equation <ref type="bibr" target="#b13">(14)</ref> becomes:</p><formula xml:id="formula_16">N = S ? d</formula><p>The idea behind our dynamic scaling is to exploit the following behavior of Softmax function. Given a non constant vector x ? R n and a positive constant ? &gt; 0 it results:</p><formula xml:id="formula_17">lim ??+? e ?x i n ? j=1 e ?x j = 1, if x i = max(x) 0, otherwise<label>(18)</label></formula><formula xml:id="formula_18">lim ??0 + e ?x i n ? j=1 e ?x j = 1 n ? j=1 1 = 1 n<label>(19)</label></formula><formula xml:id="formula_19">e x i 1 n ? j=1 e x j &lt; e x i 2 n ? j=1 e x j ? e ?x i 1 n ? j=1 e ?x j &lt; e ?x i 2 n ? j=1 e ?x j<label>(20)</label></formula><p>These equations imply that we can multiply a logits vector x by a positive constant ? without altering its softmax ranking (equation <ref type="formula" target="#formula_1">(20)</ref>) and if ? is small the softmax resulting vector approximates an arithmetic average (equation <ref type="formula" target="#formula_0">(19)</ref>), if it is large it will be close to a one-hot vector valued 1 on the max of x and 0 otherwise (equation <ref type="formula" target="#formula_0">(18)</ref>). In other words the dynamic scaling module learns how complex the new feature maps must be. If the ? associated to a given new feature map is large this feature map will be a strict selection of old feature maps, if it is small the new feature map will be a more complex composition of old feature maps involving a greater number of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Linear Embedding and Convolution</head><p>A shared linear layer was already explored by Woo et al. with their CBAM vanilla attention architecture <ref type="bibr" target="#b12">[13]</ref>. Our idea is exploiting the "self" nature of our attention mechanism. Using Vaswani's terminology self-attention means that the query, key and value originate from the same tensor. We decided to leverage this aspect and save some complexity by first differentiating query and key respectively with average and max pooling in order to enhance different scale aspects of the input and then feeding them into a single shared linear embedding layer. Dense mapping is also helped by the big dimensionality reduction due to head splitting.</p><p>On the other hand we used global convolution for the entire value tensor in order to preserve the bi-dimensional structure of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Multi-Head</head><p>In the original Transformer the multi-head structure is a concatenation of blocks all processing the same input. Voita et al. <ref type="bibr" target="#b36">[37]</ref> analyzed the Transformer and found a surprising redundancy in the representation offered by different heads: pruning 44 out of 48 heads from the Transformer's encoder block results only in a residual performance drop. Only 4 heads (8%) were necessary to maintain a performance very close to the one of the entire architecture. We tried to perform a similar evaluation for the LHC-Net by simply "switching off" LHC blocks and by "de-training" them (i.e., set the weights and biases of LHC blocks at the initialization status, before training). In our case it was feasible to just switch off or de-train the new blocks without any further training because the entire ResNet backbone of the network was pre-trained and already able to deliver a very high performance. With this approach we found that at least 16 heads out of 31 (52%) were necessary, more precisely the first 2 LHC blocks.</p><p>We further analyzed this behavior and in order to make another comparison we trained a standard Transformer encoder block as a simple classifier for a NLP classification problem reaching a very high accuracy, then we evaluated the model by simple correlation between the output of the heads and found a correlation between heads up to 93%. As a comparison our architecture had a correlation between heads of 63%.</p><p>There are many attempts to improve the attention mechanism of the Transformer. Cordonnier et al. tried to address the redundancy issue with a modified multi-head mechanism <ref type="bibr" target="#b37">[38]</ref>. Sukhbaatar et al. proposed a modified Transformer with an adaptive attention span <ref type="bibr" target="#b38">[39]</ref>.</p><p>More similarly to our local approach India et al. proposed a multi-head attention mechanism for speaker recognition in which every head processes a different section of the input <ref type="bibr" target="#b39">[40]</ref>. There are two main differences with our approach (other than application field and implementation details):</p><p>? Their approach is not designed for self-attention ? Their local processing units are used at a later stage.</p><p>They directly calculate local attention weights from embeddings (scalar output with softmax activation). Our local processing units calculate the initial embeddings (high dimension output with linear activation)</p><p>The ideas behind local heads are mainly three:</p><p>? Local heads have the advantage of working at a much lower dimension. Detecting a pattern of few pixels is harder if the input includes the entire feature map</p><p>? Splitting the images in smaller parts gives to local heads the ability to build new feature maps considering only the important parts of the old maps. There's no reason to compose feature maps in their entirety when only a small part is detecting an interesting feature. Local heads are able to add a feature map to a new feature map only if the original map is activated by a pattern and only around that pattern, avoiding then to add not useful informations</p><p>? Local heads seem to be more efficient in terms of parameters allocation</p><p>We experimentally found the performance positively correlated with the number of heads but we also tried to give a qualitative explanation of the third observation by designing a concrete example. Let's say we have n feature maps as output of the previous convolution block and that the optimal composition of those maps includes a combination of 2 of them, the i th and the j th , in the k th target feature map. In order to learn this pattern, using equations (9) and (10) (omitting the biases), a single global head must map:</p><formula xml:id="formula_20">q = q ? w 1 ? R C,d k = k ? w 1 ? R C,d</formula><p>in such a way thatq k andk i must be collinear in order to produce a high attention score in the k th target feature for the i th old feature map by dot product. The same forq k andk j . To summarize we have 3 vectors that need to be mapped in other 3 vectors linked by 2 constraint rules. In total we have 3(HW + d) dimensions or 3(HW d) relationships subject to 2 constraints to be modeled. To do that with the embedding linear layer we have a matrix w ? R HxW,d , equivalent to HW d free parameters. So we have:</p><formula xml:id="formula_21">G1 = HW d 3(HW + d)2 (21) G2 = HW d 3(HW d)2 = 1 6 (22)</formula><p>where G1 is the number of free parameters for dimension for every constraint and G2 is the number of free parameters for relationship for every constraint. We see them as qualitative measures of the efficiency of the global single head. Now we want to calculate them in the case of n local heads. The difference is that local heads works only on fractions of the entire input tensor, so we have to take into account where the i th and the j th filters are eventually activated. For a given section of the input tensor there are 3 cases: only one of them could be activated in that area, both of them or none of them. We call A the number of sections with 1 possible activation, B the number of sections with 2 possible activations and C the number of sections with no possible activations. It results: </p><formula xml:id="formula_22">A + B +C = n</formula><formula xml:id="formula_23">= A 2 + B 6 /(A + B)<label>(24)</label></formula><p>We have immediately:</p><formula xml:id="formula_24">L2 &gt; G2 ? A 2 + B 6 /(A + B) &gt; 1 6 ? A &gt; 0</formula><p>Or more shortly:</p><formula xml:id="formula_25">L2 ? G2 (25) L2 = G2 ? A = 0<label>(26)</label></formula><p>if the i th and the j th filters are possibly activated in every section of the input tensor local multi-head is equivalent to global single head in terms of efficiency and effectiveness, but a single section of the input tensor with only one possible activation is enough to make local multi-head more effective.</p><p>If we decide to consider the dimensions (L1 and G1 measures instead of L2 and G2) the calculation is more complicated; to make it easier let's make some basic assumptions. Let's consider the hyper-parameters settings of the actual first two blocks of our LHC-Net, where d = HW 2n and n = 8. We have: </p><formula xml:id="formula_26">L1 &gt; G1</formula><formula xml:id="formula_27">&gt; A + B 6HW (1 + 1 2n ) ? A 1 2(1 + 1 2 ) + B 1 6(1 + 1 2 ) &gt; A + B 6(1 + 1 16 ) ? A 3 + B 9 &gt; 16A + 16B 102 ? (3A + B) &gt; 144A + 144B 102 ? A &gt; 0.26B</formula><p>In this case the combinations A = 0, B = 8 and A = 1, B = 7 give an advantage to global single head. Every other possible combination do the opposite as shown in this figure:</p><p>It appears clear that local heads have an advantage over global heads in any real-world application. For example in FER2013 it is unlikely that a feature extracted from a face could appear anywhere in the picture. For example eyebrows will be almost always in the upper section of the picture.</p><p>This, of course, has not the ambition to be a rigorous proof of the goodness of local heads over global head, it is only a qualitative analysis giving an encouraging view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>As mentioned we mainly focused on using LHC in conjunction with a pre-trained backbone, the ResNet34v2. The training process consisted in training a ResNet34v2 (with Imagenet pre-training initialization) on FER2013, then adding 5 LHC modules as shown in <ref type="figure" target="#fig_0">Fig.1 and further</ref> training the entire architecture. The Idea was designing modules with a small impact on the "host" network similarly at the approach of the Squeeze and Excitation modules <ref type="bibr" target="#b10">[11]</ref>. In other words our main goal was to test the ability of LHC to give an extra performance boost to an existing good performing model. Secondarily we also tested LHC-Net as a stand-alone model trained (only Imagenet pre-training of the ResNet part) from scratch and we obtained limited but very good results. In this section we will discuss the details of our training protocol and the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>We rescaled the FER2013 images to 224x224 and converted them to RGB in order to match the resolution of Imagenet and make them compatible with the pre-trained ResNet. For rescaling we used bilinear interpolation. Then, in order to save RAM memory, we stored the entire training set as jpeg images accepting some neglectable quality loss and used TensorFlow Image Data Generator to feed the model during training. Saving images in jpeg format implies two different quality losses: the jpeg compression itself and the need to approximate the tensor images to be uint8 (bilinear interpolation in rescaling generate non integer values). To do that the tensors could be rounded or truncated. Considering that truncation is only a rounding with the input shifted of 0.5 and that this shifting makes the training set in FER2013 better matching the validation and test set average pixel value we proceeded with raw truncation.</p><p>The implementation details of ResNet are reported in <ref type="figure" target="#fig_0">Fig.1</ref> and the model parameters of the 5 LHC blocks are the following: <ref type="table">LHC1  8  196  3  1  3  LHC2  8  196  3  1  3  LHC3  7  56  3  1  3  LHC4  7  14  3  1  3  LHC5  1  25  3  1  3</ref> We trained the model in a sequential mode with 3 training stages, using standard crossentropy loss, varying the data augmentation protocol, the batch size and the optimzer at every stage. Early stopping is performed on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block Heads Dim Pool Scale Ker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage1:</head><p>Optimizer We observed in some cases, depending on the LHC initialization, that the added modules are somehow "rejected" by the host network and the training struggles to converge, in one case it totally diverged. It happened in a minority of the total attempts but to perform the following evaluations we kept only the models whose training loss was less than the starting ResNet training loss plus an extra 10% to take into account the augmented complexity of the model. To evaluate LHC we first applied stage 4 to the single best ResNet34 model we managed to achieve (with stages 1, 2 and 3), varying the data generator seed, without LHC modules (set A). Then, starting from the same base network we augmented it with LCH modules and trained it using the same protocol. We tried a small number of trainings with a variety of model parameters (keeping the data generator seed fixed) and clearly detected a neighbourhood of settings appearing to work well (set B). At this point we trained several other models with the best promising parameters setting varying the generator seed (set C). We then compared the set A with the set B ?C.</p><p>We also considered a minor variation of LHC-Net. We tried to exploit the analysis on the 5 modules we discussed in the previous section showing the last modules playing a minor role and trained 5 weights, limited by hyperbolic tangent, for every residual sum shown in <ref type="figure" target="#fig_0">Fig.1</ref>. We manually initialized this 5 weights by setting them as follows: a 1 = tansig(0), a 2 = tansig(0), a 3 = tansig(0), a 4 = tansig(?1), a 5 = tansig(?0.5) with the idea of limiting the impact of the last 2 modules. We call it LHC-NetC. Accordingly with the original Kaggle rules and with almost all evaluation protocols in literature only the private test set was used for final evaluations (public test set performance also appeared to be not well correlated with neither training nor private test performances). For comparison with ResNet we didn't use test time augmentation (TTA). We used TTA only for final evaluation and comparison with other models in literature. Our TTA protocol is totally deterministic; we first used a sequence of transformations involving horizontal flipping, ?10 pixels horizontal/vertical shifts and finally ?0.4 radians rotations, in this order. We use rotation after shifting to combine their effect. Rotating first puts the images in only 9 spots, which becomes 25 if we shift first. At this point we used a second batch of transformations involving horizontal flipping, 10% zoom and again ?0.4 radians rotations. Finally we weighted the no-transformation inference 3 times the weight of others inferences. LHC-Net was able to consistently outperform our best performing ResNet34v2, both on average and on peak result. Note that the average is not dramatically affected by peak result. Removing peak results does not alter the average qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy TTA Params Att BoW Repr. <ref type="bibr" target="#b1">[2]</ref> 67.48% no --Human <ref type="bibr" target="#b1">[2]</ref> 70% no --CNN <ref type="bibr" target="#b30">[31]</ref> 70.02% no --VGG19 <ref type="bibr" target="#b34">[35]</ref> 70 8% * these models are reported in the GitHub repository associated with the referenced paper, not directly into the paper.</p><p>There are some key points emerging from this analysis:</p><p>? ResNet34 is confirmed to be the most effective architecture on FER2013, especially its v2 version. In our experiments raw ResNet34 trained with the multi-stage protocol and inferenced with TTA reaches an accuracy not distant from the previous SOTA (ResMaskingNet)</p><p>? heavy architectures seem not able to outperform more simple models on FER2013</p><p>? LHC-Net has the top accuracy both with and without TTA</p><p>? LHC-NetC outperforms LHC-Net but is outperformed when TTA is used ? more importantly, LHC-Net outperforms the previous SOTA with less than one fourth of its free parameters and the impact of the LHC modules on the base architecture is much lower (less than 15% VS over 80%) and it is close to other attention modules like CBAM/BAM/SE</p><p>As mentioned we limitedly experimented stand-alone training as well with very good results. We trained in parallel, using the same data generator seeds and the same multi-stage protocol, both LHC-Net and ResNet34v2. In both models the ResNet34v2 was initialized with Imagenet pre-trained weights. It resulted that LHC-Net consistently outperformed ResNet34v2 at the end of every training stage. It is a limited but very encouraging result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Developments</head><p>Attention, in its every form and shape, is a powerful idea and, despite its impact on computer vision might be not as revolutionary as on NLP, it is still proven to be an important, sometimes decisive, tool.</p><p>In particular we designed a novel local multi-head channel self-attention module, the LHC, and it contributed proving that channel self-attention, in synergy with convolution, could be a functioning paradigm by setting a new state of the art on the well known FER2013 dataset. We also proved that self-attention works well as a small attention module intended as a booster for pre-existing architectures, like other famous vanilla attention modules as CBAM or Squeeze and Excitation.</p><p>The future research on this architectures will include many aspects:</p><p>? testing LHC on other, more computational intensive, scenarios like the Imagenet dataset ? testing LHC with other backbone architectures and with a larger range of starting performances (not only peak performances)</p><p>? we did not optimize the general topology of LHC-Net and the model hyper-parameters of the attention blocks are hand-selected with only a few attempts. There's evidence that both the 5 blocks topology and hyperparameters might be sub-optimal</p><p>? further research on the stand-alone training mode will be necessary ? normalization blocks before and after the the LHC blocks should be better evaluated in order to mitigate the divergence issue mentioned in the previous section ? a second convolution before the residual connection should be considered to mimic the general structure of the original Transformer</p><p>? a better head splitting technique could be key in the future research. The horizontal splitting we used was only the most obvious way to do it but not necessarily the most effective. Other approaches should be evaluated. For example learning the optimal areas through spatial attention</p><p>The main results of this paper are replicable by cloning the repository and following the instructions available at: https://github.com/Bodhis4ttva/LHC Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgment</head><p>We would like to express our deepest appreciation to Dr. Carmen Frasca for her crucial support to our research. We would also like to extend our sincere thanks to Dr. Luan Pham and Valerio Coderoni for their helpfulness and kindness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Five LHC modules integrated into a ResNet34v2 architecture. Every module features a residual connection to obtain an easier integration, especially when pre-training is used for the backbone architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The LHC module in its more general multi-head form. Image tensors of shape HxW xC are in pale blue, when reshaped/processed they are in dark blue. The processing units are in violet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>but this time w 1,h ? R (HxW )/n,d , hence we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Pramerdorfer et al. experimented several architectures on FER2013 reaching 71.6% accuracy with Inception, 72.4% with ResNet and 72.7% with VGG [32]. Khanzada et al. managed to achieve 72.7% accuracy with SE-ResNet50 and 73.2% with ResNet50 [33]. Khaireddin et al. reached 73.28% accuracy using VGG with a specific hyper-parameters fine tuning [34]. Pham et al. designed the ResMaskingNet which is a ResNet backbone augmented with a spatial attention module based on the popular U-Net, a segmentation network mostly used in medical image processing. ResMaskingNet achieves the remarkable accuracy of 74.14%. Pham et al. also reported that an ensemble of 6 convolutional neural networks, including ResMaskingNet, reaches 76.82% accuracy<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>89% 72.77% 73.02% 72.83% 73.39% LHC-NetC 73.04% 72.79% 73.21% 72.89% 73.53%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top</cell><cell></cell></row><row><cell></cell><cell>Top</cell><cell>40%</cell><cell>Top</cell><cell>25%</cell><cell></cell></row><row><cell>Model</cell><cell>40%</cell><cell>w/o</cell><cell>25%</cell><cell>w/o</cell><cell>Best</cell></row><row><cell></cell><cell></cell><cell>best</cell><cell></cell><cell>best</cell><cell></cell></row><row><cell cols="6">ResNet34v2 72.69% 72.65% 72.75% 72.69% 72.81%</cell></row><row><cell>LHC-Net</cell><cell>72.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">Luc</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on neural information processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A novel global spatial attention mechanism in convolutional neural network for medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Nitanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Asaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamanishi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15897</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banggu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Urca-gan: Upsample residual channelwise attention generative adversarial network for imageto-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manhua</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scsa-net: Presentation of two-view reliable correspondence learning via spatialchannel self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luanyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">431</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Triple attention network for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Gang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruili</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="202" to="211" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2537" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep-emotion: Facial expression recognition using attentional convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Minaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Abdolrashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3046</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Facial expression recognition using convolutional neural networks: state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pramerdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kampel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02903</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amil</forename><surname>Khanzada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhat Turker</forename><surname>Celepcikay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11823</idno>
		<title level="m">Facial expression recognition with deep learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Facial emotion recognition: State of the art performance on fer2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousif</forename><surname>Khaireddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofa</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03588</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial expression recognition using residual masking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><forename type="middle">Huynh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><forename type="middle">Anh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4513" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09418</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-head attention: Collaborate instead of concatenate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Self multi-head attention for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>India</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooyan</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hernando</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09890</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
