<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmenting Transparent Objects in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
							<email>wangwenjia@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@smail.nju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
							<email>mingyuding@hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Segmenting Transparent Objects in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transparent Objects</term>
					<term>Dataset</term>
					<term>Benchmark</term>
					<term>Image Segmen- tation</term>
					<term>Object Boundary</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transparent objects such as windows and bottles made by glass widely exist in the real world. Segmenting transparent objects is challenging because these objects have diverse appearance inherited from the image background, making them had similar appearance with their surroundings. Besides the technical difficulty of this task, only a few previous datasets were specially designed and collected to explore this task and most of the existing datasets have major drawbacks. They either possess limited sample size such as merely a thousand of images without manual annotations, or they generate all images by using computer graphics method (i.e. not real image). To address this important problem, this work proposes a large-scale dataset for transparent object segmentation, named Trans10K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. The transparent objects in Trans10K are extremely challenging due to high diversity in scale, viewpoint and occlusion. To evaluate the effectiveness of Trans10K, we propose a novel boundary-aware segmentation method, termed TransLab, which exploits boundary as the clue to improve segmentation of transparent objects. Extensive experiments and ablation studies demonstrate the effectiveness of Trans10K and validate the practicality of learning object boundary in TransLab. For example, TransLab significantly outperforms 20 recent object segmentation methods based on deep learning, showing that this task is largely unsolved. We believe that both Trans10K and TransLab have important contributions to both the academia and industry, facilitating future researches and applications. The codes and models will be released at: github.com/xieenze/Segment Transparent Objects</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transparent objects widely exist in the real world, such as bottles, vitrines, windows, walls and many others made by glass. Transparent objects have diverse . This work proposes the Trans10K dataset for transparent object segmentation, which has 10,428 manually-labeled images with high degree of variability in terms of scale, pose, contrast, category, occlusion and transparency. Objects are divided into two categories, thing and stuff, where things are small and movable objects (e.g. bottle), while stuff are large and fixed (e.g. vitrine). Example images and annotations are shown, where things and stuff are in blue and brown respectively. appearance inherited from their surrounding image background, making segmenting these objects challenging. The task of transparent object segmentation is important because it has many applications. For example, when a smart robot operates tasks in living rooms or offices, it needs to avoid fragile objects such as glasses, vases, bowls, bottles, and jars. In addition, when a robot navigates in factory, supermarket and hotel, its visual navigation system needs to recognize the glass walls and windows to avoid collision. Although transparent object segmentation is important in computer vision, only a few previous datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> were specially collected to explore this task and they have major drawbacks. For example, TransCut <ref type="bibr" target="#b0">[1]</ref> possesses limited sample size with merely 49 images. Although TOM-Net <ref type="bibr" target="#b1">[2]</ref> has large data size of 178K images, all the images are generated by using computer graphics method by simply overlaying a transparent object on different background images, that is, the images are not real and out of the distribution of natural images. Meanwhile, TOM-Net provides 876 real images for test, but these images do not have manual annotations and evaluation performed by user study.</p><p>To address the above issues, this paper proposes a novel large-scale dataset for transparent object segmentation, named Trans10K, containing 10,428 realworld images of transparent objects, each of which is manually labeled with segmentation mask. All images in Trans10K are selected from complex realworld scenarios that have large variations such as scale, viewpoint, contrast, occlusion, category and transparency. Trans10K has rich real images that are 10 times larger than existing datasets. <ref type="table">Table 1</ref>. Comparisons between Trans10K and previous transparent object datasets, where "Syn" represents synthetic images using computer graphics method, "Thing" represents small and movable objects, "Stuff" are large and fixed objects, and "MCC" denotes Mean Connected Components in each image. Trans10K is much more challenging than prior arts in terms of all characteristics presented in this    <ref type="table">Table 1</ref> and <ref type="figure">Fig.3</ref>, Trans10K has three main advantages compared with existing work. (1) Images in Trans10K are collected from diverse scenes such as living room, office, supermarket, kitchen and desks, which are not covered by the existing datasets. (2) The objects in Trans10K are partitioned into two categories, stuff and things. The transparent stuff are fixed and large object such as wall and window. Segmenting stuff objects are useful for many applications, such as helping robots avoid collision during navigation. In contrast, the transparent things are small and moveable such as bottles. (3) As shown in <ref type="table">Table 1</ref>, images in Trans10K are divided into a training set of 5000 images, a validation set of 1000 images and a test set of 4428 images. Both the validation and the test sets contain two subsets, easy and hard. Although the overall benchmark is challenging, we carefully select a hard subset for validation and test to further evaluate different segmentation algorithms. These hard subsets may expose more flaws of semantic segmentation algorithms so as to improve the performance of transparent object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Besides Trans10K, we carefully design a boundary-aware object segmentation method, termed TransLab, which is able to "Look at boundary" to improve transparent object segmentation. As shown in <ref type="figure" target="#fig_2">Fig.2</ref>, we train and evaluate 20 existing representative segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> on Trans10K, and found that simply applying previous methods to this task is not sufficient. For instance, although DeepLabV3+ <ref type="bibr" target="#b9">[10]</ref> is the state-of-the-art semantic segmentation method, it ignores the boundary information, which might be suitable for common object segmentation but not for transparent object. As a result, its mIoU is suboptimal compared to TransLab (69.0 versus 72.1), where the boundary prediction in TransLab is helpful due to the high contrast at the edges but diverse appearance inside a transparent object.</p><p>Specifically, TransLab has two streams including a regular stream for transparent content segmentation and a boundary stream for boundary prediction. After these two streams, a Boundary Attention Module (BAM) is devised to use the boundary map to attend both high-level and low-level features for transparent object segmentation.</p><p>To summarize, the main contributions of this work are three-fold. (1) A large-scale transparent object segmentation dataset, Trans10K, is collected and labeled. It is 10 times larger and more challenging than previous work. (2) A boundary-aware approach for segmenting transparent objects, TransLab, is proposed to validate the effectiveness of Trans10K and the boundary attention module. TransLab surpasses 20 representative segmentation approaches by training and testing them on Trans10K for fair comparisons. For instance, TransLab outperforms DeeplabV3+, a state-of-the-art semantic segmentation method, by over 3% mIoU. (3) Extensive ablation studies and benchmarking results are presented by using Trans10K and TransLab to encourage more future research efforts on this task. The data of Trans10K and trained models are released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation. Most state-of-the-art algorithms for semantic segmentation are predominantly based on CNNs. Earlier approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> transfer classification networks to fully convolutional networks (FCNs) for semantic segmentation, which is in an end-to-end training manner. Several works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> propose to use structured prediction modules such as conditional random fields (CRFs) on network output for improving the segmentation performance, especially around object boundaries. To avoid costly DenseCRF, the work of <ref type="bibr" target="#b13">[14]</ref> uses fast domain transform filtering on network output while also predicting edge maps from intermediate CNN layers. More recently, dramatic improvements in performance and inference speed have been driven by new architectural designs. For example, PSPNet <ref type="bibr" target="#b2">[3]</ref> and DeepLab <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> proposed a feature pyramid pooling module that incorporates multiscale context by aggregating features at multiples scales. Some works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>  Transparent Object Segmentation. TransCut <ref type="bibr" target="#b0">[1]</ref> propose an energy function based on LF-linearity and occlusion detection from the 4D light-field image is optimized to generate the segmentation result. TOM-Net <ref type="bibr" target="#b1">[2]</ref> formulate transparent object matting as a refractive ow estimation problem. This work proposed a multi-scale encoder-decoder network to generate a coarse input, and then a residual network refines it to a detailed matte. Note that TOM-Net needs a refractive flow map as label during training, which is hard to obtain from the real world, so it can only rely on synthetic training data.</p><p>Transparent Object Datasets. TOM-Net <ref type="bibr" target="#b1">[2]</ref> proposed a dataset containing 876 real images and 178K synthetic images which are generated by POV-Ray. Only 4 and 14 objects are repeatedly used in the synthetic and real data. Moreover, the test set of TOM-Net do not have mask annotation, so one cannot evaluate his algorithm quantitatively on it. TransCut <ref type="bibr" target="#b0">[1]</ref> is proposed for the segmentation of transparent objects. It only contains 49 images. However, only 7 objects, mainly bottles and cups, are repeatedly used. The images are capture by 5?5 camera array in 7 different scenes. So the diversity is very limited.</p><p>Most of the background of synthetic images are chosen randomly, so the background and the objects are not semantically coordinated and reasonable. The transparent objects are usually in a unreasonable scene, e.g. a cup flying with a plane. Furthermore, the real data always lack in scale and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Trans10K Dataset and Annotation</head><p>To tackle the transparent object segmentation problem, we build the first largescale dataset, named Trans10K. It contains more than 10k pairs of images with transparent objects and their corresponding manually annotated masks, which is over 10? larger than existing real transparent object datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data description</head><p>The Trans10K dataset contains 10,428 images, with two categories of transparent objects: (1) Transparent things such as cups, bottles and glass, locating these things can make robots easier to grab objects. (2) Transparent stuff such as windows, glass walls and glass doors. It can make robots learn to avoid obstacles and avoid hitting these stuff. As shown in <ref type="table" target="#tab_0">Table.</ref> 2, 5000, 1000 and 4428 images are used for train, validation and test, respectively. Specifically, we keep the same ratio that these two fine-grained categories in train, validation and test set. The images are manually harvested from the internet, image library like google OpenImage <ref type="bibr" target="#b18">[19]</ref> and our own data captured by phone cameras. As a result, the distribution of the images is various, containing different scales, borndigital, perspective distortion glass, crowded and so on. In summary, to our best knowledge, Trans10K is the largest real dataset focus on transparent object segmentation in the wild. Moreover, due to fine-grained categories and high diversity, it is challenging enough for existing semantic segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation</head><p>The transparent objects are manually labeled by ourselves with our labeling tool. The way of annotation is the same with semantic segmentation datasets such as ADE20K. We set the background with 0, transparent things with 1 and transparent stuff with 2. Here are some principles: (1) Only highly transparent objects are annotated, other semi-transparent objects are ignored. Although most transparent objects are made of glass in our dataset, we also annotate those made of other materials such as plastic if they satisfy the attribute of transparent. <ref type="bibr" target="#b1">(2)</ref> If there are things in front of the transparent objects, we will not annotate the region of the things. Otherwise, if things are behind transparent objects, we will annotate the whole region of transparent objects. <ref type="formula" target="#formula_2">(3)</ref> We further divide the validation set and test set into two parts, easy and hard according to the difficulty. The detail is shown in Section. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Complexity</head><p>Our dataset is diversified in scale, category, shape, color and location. We find that the segmentation difficulty varies due to these factors. So we define the easy and hard attribute of each image. The statistics are shown in <ref type="table" target="#tab_0">Table.</ref> 2. The detailed principles are shown as below:</p><p>Easy ( <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>): (1) Less numbered. e.g. most images contain a single object of the same category. (2) Regular shaped. e.g. there is nearly no occlusion like posters over the transparent objects and their shape is regular such as circle.</p><p>(3) Salient. e.g. the transparent objects are salient and easy to figure out due  to the conspicuous reflection and refraction light. (4) Simply displayed. e.g. the transparent objects are located at a center position and spatially isolated to each other clearly. Hard ( <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>): (1) More numbered. e.g. the images contain multiple transparent objects of different categories. (2) Irregular shaped. e.g. their shape is strange and without a regular pattern. (3) High Transparency. e.g. they are hard to figure out because they are of very high transparency and clean, even hard for people to <ref type="figure">figure.</ref> (4) Complexly displayed. e.g. they are located randomly and heavily occluded in a crowd scene. One transparent objects can cover, or contain part of another. <ref type="figure" target="#fig_3">Fig. 4</ref> (a) and <ref type="figure" target="#fig_3">Fig. 4</ref> (b) shows that our dataset contain abundant category distribution. To our knowledge, our dataset contains at least 20 different cate-gories of objects, including but not limited to: Stuff such as bookshelf, showcase, freezer, window, door, glass table, vivarium, floor window, glass ceiling, glass guardrail, microwave oven and electric roaster. Things such as eyeglass, cup, bow, bottle, kettle, storage box, cosmetics, toys, glass chandelier. The abundant categories contain the most common transparent objects in the real world. More visualization can be found in the supplementary materials.  <ref type="figure">. (d)</ref> is the distribution of the object location of the whole dataset. It shows that the stuff is more uniformly distributed while things tend to cluster near the center of the image. This is reasonable because the stuff tends to occupy the majority of the images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metrics</head><p>For a comprehensive evaluation, we apply four metrics that are widely used in semantic segmentation, salient object detection and shadow detection to benchmark the performance of transparent object segmentation. Our metrics are far more comprehensive than TransCut and TOM-Net, we hope they can expose more flaws of different methods on our dataset. Specifically, Intersection over union (IoU) and pixel accuracy metrics (Acc) are used from the semantic segmentation field as our first and second metrics. Note that we only calculate IoU of the thing and stuff, ignoring the background. Mean absolute error (MAE) metrics are used from the salient object detection field. Finally, Balance error rate (BER) is used from the shadow detection field. It considers the unbalanced areas of transparent and non-transparent regions. BER is used to evaluate binary predictions, here we change it to mean balance error rate (mBER) to evaluate the two fine-grained transparent categories. , it is computed as:</p><formula xml:id="formula_0">mBER = 1 C C i=1 (1 ? 1 2 ( T P i T P i + F N i + T N i T N i + F P i )) ? 100,<label>(1)</label></formula><p>Where C is the category of transparent objects, in this dataset C is 2.   <ref type="bibr" target="#b19">[20]</ref> with dilation is used as the backbone network. The regular stream is for transparent object segmentation while the boundary stream is for boundary prediction. We argue that the boundary is easier than content to observe because it tends to have high contrast in the edge of transparent objects, which is consistent with human visual perception. So we first make the network predict the boundary, then we utilize the predicted boundary map as a clue to attend the regular stream. It is implemented by Boundary Attention Module (BAM). In each stream, we use Atrous Spatial Pyramid Pooling module (ASPP) to enlarge the receive field. Finally, we design a simple decoder to utilize both high-level feature (C4) and low-level feature (C1 and C2). The boundary ground-truth is generated as a binary map with thickness 8. The channel of the predicted boundary map is 1. BAM first takes the feature map of the regular stream and the predicted boundary map as input. Then performs boundary attention with a boundary map. The two feature maps before and after boundary attention are concatenated and followed by a channel attention block. Finally, it outputs the refined feature map. BAM can be repeatedly used on the high-level and low-level features of regular stream such as C1, C2 and C4. We demonstrate that the more times of boundary attention, the better performance of segmentation results. Details are shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Boundary Attention Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoder</head><p>Fig. 6 (c) illustrates the detailed structure of the Decoder. The input of decoder is C1, C2 and C4 (after ASPP). BAM is used to apply boundary attention to three of them. We firstly fuse the C4 and C2 by up-sampling C4 and adding 3?3 convolutional operation. The fused feature map is further up-sampled to fuse with C1 in the same approach. In this way, both high level and low-level feature maps are joint fused, which is beneficial for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loss Function</head><p>We define our training loss function as follows:</p><formula xml:id="formula_1">L = L s + ?L b ,<label>(2)</label></formula><p>where L s and L b represent the losses for the segmentation text and boundary, respectively, and ? balances the importance between L s and L b . The trade-off parameter ? is set to 5 in our experiments. Here L s is the standard Cross-Entropy (CE) Loss. Inspired by <ref type="bibr" target="#b20">[21]</ref>, we adopt Dice Loss in our experiment. The dice coefficient D(S i , G i ) is formulated as in Eqn. <ref type="formula" target="#formula_2">(3)</ref>:</p><formula xml:id="formula_2">D(S i , G i ) = 2 x,y (S i,x,y ? G i,x,y ) x,y S 2 i,x,y + x,y G 2 i,x,y ,<label>(3)</label></formula><p>where S i,x,y and G i,x,y refer to the value of pixel (x, y) in segmentation result S i and ground truth G i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We have implemented TransLab with PyTorch <ref type="bibr" target="#b21">[22]</ref>. We use the pre-trained ResNet50 <ref type="bibr" target="#b19">[20]</ref> as the feature extraction network. In the final stage, we use the dilation convolution to keep the resolution as 1/16. The remaining parts of our network are randomly initialized. For loss optimization, we use the stochastic gradient descent (SGD) optimizer with momentum of 0.9 and a weight decay of 0.0005. Batch size is set to 8 per GPU. The learning rate is initialized to 0.02 and decayed by the poly strategy <ref type="bibr" target="#b22">[23]</ref> with the power of 0.9 for 16 epochs. We use 8 V100 GPU for all experiments. During training and inference, images are resized to a resolution of 512?512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>In this part, we demonstrate the effectiveness of boundary clues for transparent object segmentation. We report the results of the ablation study on the hard set of Trans10K because it is more challenging and can clearly observe the gap of different modules. We simply use the DeeplabV3+ as our baseline. We first show only use boundary loss as auxiliary loss during training can directly improve the segmentation performance. We further show how to use a boundary map as a clue to attend the feature of regular stream. Experiments demonstrate that more boundary attention leading to better results. In summary, locating boundaries is essential for transparent object segmentation. Boundary Loss Selection. Boundary is easier to observe than the content of the transparent object. To obtain high-quality boundary map, the loss function is essential. We choose Binary Cross-Entropy Loss, Focal Loss and Dice Loss to supervise boundary stream. As shown in <ref type="table" target="#tab_0">Table.</ref> 3, Firstly, simply using boundary loss as auxiliary loss can directly improve the performance no matter which loss function is used. We argue this is due to the benefit of multi-task learning. It means the boundary loss can help the backbone focus more on the boundary of transparent objects and extract more discriminative features. Note that under this setting, the boundary stream can be removed during inference so it will not bring computation overhead. Secondly, Focal Loss works better than Binary Cross-Entropy loss because the majority of pixels of a boundary mask are background, and the Focal Loss can mitigate the sample imbalance problem by decreasing the loss of easy samples. However, the Dice Loss achieves the best results without manually adjusting the loss hyper-parameters. Dice Loss views the pixels as a whole object and can establish the right balance between foreground and background pixels automatically. As a result, Dice Loss can improve baseline with 1.25% on mIoU and 2.31% on Acc, which is the best in three loss functions. Meanwhile, the mBer and MAE are also improved (lower is better).</p><p>Boundary Attention Module. After obtaining a high-quality boundary map, boundary attention is another key step in our algorithm. We can fuse boundary information at different levels for the regular stream. In this part, we repeatedly use BAM on C1, C2, C4 feature maps to show how boundary attention module works. To evaluate the relationship between the quantity of boundary attention and the final prediction accuracy, we vary the number of fusion levels from 1 to 3 and report the mIoU, Acc, mBer and MAE in <ref type="table" target="#tab_5">Table 4</ref>. Note that 'BL' indicates baseline with Dice Loss. It can be observed that performance is improved consistently by using boundary attention module at more  levels. Our final model that fuses boundary information in all three levels further improves mIoU from 70.29% to 72.10% and Acc from 80.38% to 83.04%. Meanwhile, the mBer and MAE are also improved (lower is better). By using a high-quality boundary map for attention, the feature maps of regular stream can have higher weights on the boundary region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to the State-of-the-arts</head><p>We select several main-stream semantic segmentation methods to evaluate on our challenging Trans10K dataset. Specifically, we choose FPENet <ref type="bibr" target="#b23">[24]</ref>, Con-textNet <ref type="bibr" target="#b24">[25]</ref>, FastSCNN <ref type="bibr" target="#b25">[26]</ref>, DeeplabV3+ with MobilenetV2 <ref type="bibr" target="#b9">[10]</ref>, CGNet <ref type="bibr" target="#b26">[27]</ref>, HRNet <ref type="bibr" target="#b27">[28]</ref>, HardNet <ref type="bibr" target="#b28">[29]</ref>, DABNet <ref type="bibr" target="#b29">[30]</ref>, LEDNet <ref type="bibr" target="#b30">[31]</ref>, ICNet <ref type="bibr" target="#b3">[4]</ref> and BiSeNet <ref type="bibr" target="#b22">[23]</ref> as real-time methods. DenseASPP <ref type="bibr" target="#b6">[7]</ref>, DeepLabV3+ with Resnet50 <ref type="bibr" target="#b9">[10]</ref>, FCN <ref type="bibr" target="#b8">[9]</ref>, OCNet <ref type="bibr" target="#b31">[32]</ref>, RefineNet <ref type="bibr" target="#b7">[8]</ref>, DeepLabV3+ with Xception65 <ref type="bibr" target="#b9">[10]</ref>, DUNet <ref type="bibr" target="#b4">[5]</ref>, UNet <ref type="bibr" target="#b32">[33]</ref> and PSPNet <ref type="bibr" target="#b2">[3]</ref> as regular methods. We re-train each of the models on the training set of our dataset and evaluate them on our testing set. For a fair comparison, we set the size of the input image as 512?512 with single scale training and testing. <ref type="table" target="#tab_7">Table 5</ref> reports the overall quantitative comparison results on test set, where our TransLab outperforms all other methods in our Trans10K in terms of all four metrics in both easy/hard set and things/stuff categories. Especially, TransLab outperforms DeepLabV3+, the sota semantic segmentation method, in a large gap on all metrics as well as both things and stuff, especially in hard set. For instance, it surpasses DeepLabV3 by 3.97% on 'Acc' (hard set). <ref type="figure" target="#fig_8">Fig 7 also</ref> shows TransLab can predict sharp boundary with high-quality masks when compared with other methods. More analysis are shown in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present the Trans10K dataset, which is the largest real dataset for transparent object segmentation. We also benchmark 20 semantic segmentation algorithms on this novel dataset and shed light on what attributes are especially difficult for current methods. We suggest that transparent object segmentation in the wild is far from being solved. Finally, we propose a boundaryaware algorithm, termed TransLab, to utilize the boundary prediction to improve   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Detailed annotation information.</p><p>Our dataset contains 20 different categories of transparent objects. As shown in <ref type="figure" target="#fig_11">Fig. 8(a)</ref>, we count the number of images for different categories. We see that for things, the "Cup" appears the most frequently while the "Stationery" appears the least. For stuff, the "French Window" appears the most frequently while the " <ref type="table">Table"</ref> appears the least. Furthermore, our dataset also contains 13 scenarios. As shown in <ref type="figure" target="#fig_11">Fig. 8(b)</ref>, we see that the "Desktop" is the most frequently scene while the "Vechile" is the least scene. In summary, our Trans10K contains abundant category distribution of transparent objects and scenarios, which is not available in TOM-Net <ref type="bibr" target="#b1">[2]</ref> and TransCut <ref type="bibr" target="#b0">[1]</ref>.</p><p>A.2 More visual results Analysis.</p><p>Failure Samples Analysis As shown in <ref type="figure" target="#fig_12">Fig. 9</ref>, our method has some limitations. For example, in <ref type="figure" target="#fig_12">Fig. 9 (a)</ref>, when facing highly-transparency objects, our method will fail to segment in some region. <ref type="figure" target="#fig_12">Fig. 9 (b)</ref> shows that some objects with strong reflection will also make our method confuse and lead to wrong classification. In <ref type="figure" target="#fig_12">Fig. 9</ref> (c), we can find our method does not work well when some objects have overlap and occlusion with transparent objects. Finally, in <ref type="figure" target="#fig_12">Fig. 9 (d)</ref>, when semi-transparent objects are adjacent with transparent objects, our method will also confuse.</p><p>Visual results on external data. In this part, we also directly test our TransLab trained on Trans10K dataset to evaluate the generalize ability of Trans10K dataset and the robustness of TransLab. Firstly, we test our method on two prior datasets: TransCut <ref type="bibr" target="#b0">[1]</ref> and TOM-Net <ref type="bibr" target="#b1">[2]</ref>. As shown in <ref type="figure">Fig. 10</ref>, our method can clearly output very high-quality mask. Moreover, we also test our method on some external data randomly captured by our mobile phones or obtained from Internet videos such as YouTube, TikTok and eBay. We see our TransLab can also successfully segment transparent objects in most cases. In summary, we believe our Trans10K dataset contains high-diversity images which can easily generalize to real scene. Also, our boundary-aware algorithm TransLab is robust enough to segment unseen images.</p><p>More comparison of TransLab with other methods. In this part, we demonstrate more test examples produced by our TransLab on Trans10K dataset in <ref type="figure" target="#fig_2">Fig. 12</ref>. From these results, it can be easily observed that with the proposed Boundary Stream and Boundary Attention Module, our method can output high-quality boundary map and better transparent object segmentation mask than other semantic segmentation methods. (a) Histogram of image numbers for different transparent object categories. It is split by things and stuff. Event categories are ranked in an descending order based on the image numbers. Example images for specific transparent object classes are shown. Yaxis denotes for image numbers. X-axis denotes for transparent object categories.   TranCut TOM-Net <ref type="figure">Fig. 10</ref>. Some transparent objects segmentation results on two prior datasets: Tran-sCut <ref type="bibr" target="#b0">[1]</ref> and TOM-Net <ref type="bibr" target="#b1">[2]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. This work proposes the Trans10K dataset for transparent object segmentation, which has 10,428 manually-labeled images with high degree of variability in terms of scale, pose, contrast, category, occlusion and transparency. Objects are divided into two categories, thing and stuff, where things are small and movable objects (e.g. bottle), while stuff are large and fixed (e.g. vitrine). Example images and annotations are shown, where things and stuff are in blue and brown respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Comparisons between TransLab and 20 representative semantic segmentation methods. All methods are trained on Trans10K. mIoU on hard set of Trans10K is chosen as the metric. Deeper color bar indicates methods with larger FLOPs. 'DLv3+' denotes DeepLabV3+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparisons of easy and hard samples in Trans10K. Red represents things and white represents stuff. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>displays the statistic information of the Trans10K dataset. (a) is the distribution of area ratio of connected components in each image, ranging from 0 to 1.0. (b) is the number of connected components of things and stuff in each image. (c) is the distribution of the image resolution of the train and valida-tion+test set. The horizontal axis is the resolution (million pixels) of each image calculated by width ? height</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Statistics of the Trans10K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 6 (</head><label>66</label><figDesc>The architecture of TransLab. a) shows the overall architecture of TransLab, which is composed of two parallel stream: regular stream and boundary stream. ResNet50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 (</head><label>6</label><figDesc>b) illustrates the structure of the Boundary Attention Module (BAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparison of TransLab to other semantic segmentation methods. Our TransLab clearly outperforms others thanks to the boundary attention, especially in yellow dash region. the segmentation performance. Acknowledgement This work is partially supported by the SenseTime Donation for Research, HKU Seed Fund for Basic Research, Startup Fund and General Research Fund No.27208720. Chunhua Shen and his employer received no financial support for the research, authorship, and/or publication of this article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Histogram of image numbers for different scene categories. Event categories are ranked in an descending order based on the image numbers. Example images for specific scene classes are shown. Y-axis denotes for image numbers. X-axis denotes for event scene name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Statistics of Trans10K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Failure cases. Our method fails to segment transparent objects in some complex scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Some transparent objects segmentation results on challenging images captured by our mobile phones and obtained from Internet such as YouTube, TikTok and eBay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>More visual comparison of TransLab to other semantic segmentation methods. Our TransLab clearly outperforms others thanks to the boundary attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Number</cell><cell></cell><cell></cell><cell>Objects</cell><cell></cell><cell cols="3">Height (pixels)</cell><cell></cell><cell>Properties</cell><cell></cell></row><row><cell>Datasets</cell><cell cols="2">Train</cell><cell>Val</cell><cell cols="10">Test Num Thing Stuff &lt;1K 1K-2K &gt;2K MCC Occlusion Contrast</cell></row><row><cell></cell><cell cols="2">Real Syn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransCut [1]</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>49</cell><cell>7</cell><cell>?</cell><cell>?</cell><cell>49</cell><cell>0</cell><cell>0</cell><cell>1.14</cell><cell>?</cell><cell>?</cell></row><row><cell>TOM-Net [2]</cell><cell>0</cell><cell>178K</cell><cell>900</cell><cell>876</cell><cell>18</cell><cell>?</cell><cell>?</cell><cell>178K</cell><cell>876</cell><cell>0</cell><cell>1.33</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">Trans10K 5000</cell><cell>0</cell><cell cols="3">1000 4428 10K+</cell><cell>?</cell><cell>?</cell><cell>1544</cell><cell>593</cell><cell cols="2">8291 3.96</cell><cell>?</cell><cell>?</cell></row><row><cell>72.1</cell><cell cols="2">69.0 68.3 66.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">62.5 60.4 59.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">56.4 56.2 55.5 53.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48.4 46.2 45.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">40.3 39.6 38.8 37.7 37.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>propose modules that use learned pixel affinities for structured information propagation across intermediate CNN representations.</figDesc><table><row><cell></cell><cell></cell><cell>Stuff</cell><cell>Thing</cell><cell></cell><cell>Synthetic Image</cell><cell></cell><cell></cell></row><row><cell>Image</cell><cell>Mask</cell><cell>Image</cell><cell>Mask</cell><cell>Image</cell><cell>Mask</cell><cell>Image</cell><cell>Mask</cell></row><row><cell cols="2">TransCut</cell><cell cols="2">TOM-Net</cell><cell></cell><cell cols="2">Trans10K</cell><cell></cell></row><row><cell cols="8">Fig. 3. Example transparent object images and masks in TransCut [1], TOM-Net [2],</cell></row><row><cell cols="8">and our Trans10K. We see Trans10K has more diverse scene and challenging viewpoint,</cell></row><row><cell cols="5">categories, occlusion than TransCut and TOM-Net.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Image statistics of Trans10K. MCC denotes Mean Connected Components in each image.</figDesc><table><row><cell cols="2">Dataset</cell><cell>All</cell><cell cols="3">Image Number Only things Only stuff Containing both</cell><cell>MCC</cell></row><row><cell>Train</cell><cell></cell><cell>5000</cell><cell>2845</cell><cell>2047</cell><cell>108</cell><cell>3.87</cell></row><row><cell>Validation</cell><cell>easy</cell><cell>788</cell><cell>490</cell><cell>290</cell><cell>8</cell><cell>3.31</cell></row><row><cell></cell><cell>hard</cell><cell>212</cell><cell>82</cell><cell>118</cell><cell>12</cell><cell>5.20</cell></row><row><cell>Test</cell><cell>easy</cell><cell>3491</cell><cell>2255</cell><cell>1222</cell><cell>14</cell><cell>3.15</cell></row><row><cell></cell><cell>hard</cell><cell>937</cell><cell>337</cell><cell>549</cell><cell>51</cell><cell>6.29</cell></row></table><note>(a) Examples of easy cases. With regular shapes, less occlusion and contrast.(b) Examples of hard cases. With irregular shapes, more occlusion and contrast.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for different loss functions of boundary stream.</figDesc><table><row><cell>Lb</cell><cell cols="4">mIoU ? Acc ? mBer ? MAE ?</cell></row><row><cell>-</cell><cell>69.04</cell><cell>78.07</cell><cell>17.27</cell><cell>0.194</cell></row><row><cell>BCE</cell><cell>69.33</cell><cell>78.61</cell><cell>16.89</cell><cell>0.190</cell></row><row><cell>Focal</cell><cell>69.41</cell><cell>78.76</cell><cell>16.27</cell><cell>0.188</cell></row><row><cell>Dice</cell><cell cols="4">70.29 80.38 15.44 0.183</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study for different setting of Boundary Attention Module.</figDesc><table><row><cell>Method</cell><cell cols="4">mIoU ? Acc ? mBer ? MAE ?</cell></row><row><cell>BL</cell><cell>70.29</cell><cell>80.38</cell><cell>15.44</cell><cell>0.183</cell></row><row><cell>BL+C1</cell><cell>71.05</cell><cell>81.80</cell><cell>13.98</cell><cell>0.180</cell></row><row><cell>BL+C1&amp;2</cell><cell>71.32</cell><cell>82.05</cell><cell>13.69</cell><cell>0.178</cell></row><row><cell cols="5">BL+C1&amp;2&amp;4 72.10 83.04 13.30 0.166</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Evaluated Semantic Segmentation methods. Sorted by FLOPs. Note that FLOPs is computed with one 512?512 image. (a) Comparison between things and stuff. .345 69.17 73.83 54.67 64.03 69.11 46.18 18.91 15.58 30.52 DABNet [30] 0.230 0.187 0.391 54.87 59.29 41.07 54.90 59.45 38.77 25.71 22.63 36.15 LEDNet [31] 0.168 0.124 0.331 75.70 80.62 60.37 67.54 73.04 48.38 15.15 11.83 26.58 ICNet [4] 0.244 0.200 0.408 52.65 58.31 35.01 50.65 55.48 33.44 24.63 21.71 35.24 BiSeNet [23] 0.140 0.102 0.282 77.92 82.79 62.72 73.93 78.74 56.37 13.96 10.83 24.85</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">MAE ? ACC ?</cell><cell cols="2">IoU ? Things Stuff</cell><cell cols="3">BER ? Things Stuff</cell><cell cols="4">Computation FLOPs Params</cell></row><row><cell>FPENet [24]</cell><cell></cell><cell>0.339</cell><cell cols="2">24.73</cell><cell>33.96</cell><cell>34.36</cell><cell cols="2">39.59</cell><cell>39.01</cell><cell cols="2">0.78G</cell><cell cols="2">0.11M</cell></row><row><cell cols="2">ContextNet [25]</cell><cell>0.217</cell><cell cols="2">62.09</cell><cell>56.29</cell><cell>56.61</cell><cell cols="2">22.26</cell><cell>22.46</cell><cell cols="2">0.89G</cell><cell cols="2">0.87M</cell></row><row><cell cols="2">FastSCNN [26]</cell><cell>0.206</cell><cell cols="2">64.20</cell><cell>58.62</cell><cell>59.74</cell><cell cols="2">20.59</cell><cell>23.95</cell><cell cols="2">1.03G</cell><cell cols="2">1.20M</cell></row><row><cell cols="3">DeepLabv3+MBv2 [10] 0.130</cell><cell cols="2">80.92</cell><cell>78.55</cell><cell>71.97</cell><cell cols="2">10.38</cell><cell>14.58</cell><cell cols="2">2.70G</cell><cell cols="2">1.96M</cell></row><row><cell>CGNet [27]</cell><cell></cell><cell>0.216</cell><cell cols="2">59.15</cell><cell>58.33</cell><cell>56.28</cell><cell cols="2">21.02</cell><cell>24.88</cell><cell cols="2">3.52G</cell><cell cols="2">0.49M</cell></row><row><cell>HRNet [28]</cell><cell></cell><cell>0.134</cell><cell cols="2">75.82</cell><cell>79.34</cell><cell>69.78</cell><cell cols="2">10.39</cell><cell>16.64</cell><cell cols="2">4.20G</cell><cell cols="2">1.53M</cell></row><row><cell>HardNet [29]</cell><cell></cell><cell>0.184</cell><cell cols="2">69.17</cell><cell>64.91</cell><cell>63.15</cell><cell cols="2">17.18</cell><cell>20.63</cell><cell cols="2">4.43G</cell><cell cols="2">4.11M</cell></row><row><cell>DABNet [30]</cell><cell></cell><cell>0.230</cell><cell cols="2">54.87</cell><cell>54.48</cell><cell>55.32</cell><cell cols="2">25.77</cell><cell>25.64</cell><cell cols="2">5.25G</cell><cell cols="2">0.75M</cell></row><row><cell>LEDNet [31]</cell><cell></cell><cell>0.168</cell><cell cols="2">75.70</cell><cell>70.37</cell><cell>64.68</cell><cell cols="2">12.68</cell><cell>17.62</cell><cell cols="2">6.32G</cell><cell cols="2">2.31M</cell></row><row><cell>ICNet [4]</cell><cell></cell><cell>0.244</cell><cell cols="2">52.65</cell><cell>53.90</cell><cell>47.38</cell><cell cols="2">19.78</cell><cell>29.46</cell><cell cols="4">10.66G 8.46M</cell></row><row><cell>BiSeNet [23]</cell><cell></cell><cell>0.140</cell><cell cols="2">77.92</cell><cell>77.39</cell><cell>70.46</cell><cell cols="2">10.86</cell><cell>17.04</cell><cell cols="4">19.95G 13.30M</cell></row><row><cell cols="2">DenseASPP [7]</cell><cell>0.114</cell><cell cols="2">81.22</cell><cell>81.79</cell><cell>74.41</cell><cell></cell><cell>9.07</cell><cell>15.31</cell><cell cols="4">36.31G 29.09M</cell></row><row><cell cols="2">DeepLabv3+R50 [10]</cell><cell>0.081</cell><cell cols="2">89.54</cell><cell>87.90</cell><cell>81.16</cell><cell></cell><cell>5.31</cell><cell>10.25</cell><cell cols="4">37.98G 28.74M</cell></row><row><cell>FCN [9]</cell><cell></cell><cell>0.108</cell><cell cols="2">83.79</cell><cell>84.40</cell><cell>74.92</cell><cell></cell><cell>7.30</cell><cell>13.36</cell><cell cols="4">42.35G 34.99M</cell></row><row><cell>OCNet [32]</cell><cell></cell><cell>0.122</cell><cell cols="2">80.85</cell><cell>80.55</cell><cell>73.15</cell><cell></cell><cell>8.91</cell><cell>16.38</cell><cell cols="4">43.43G 35.91M</cell></row><row><cell>RefineNet [8]</cell><cell></cell><cell>0.180</cell><cell cols="2">57.97</cell><cell>73.65</cell><cell>58.40</cell><cell cols="2">16.44</cell><cell>27.98</cell><cell cols="4">44.34G 29.36M</cell></row><row><cell cols="3">DeepLabv3+XP65 [10] 0.082</cell><cell cols="2">89.18</cell><cell>87.54</cell><cell>80.98</cell><cell></cell><cell>5.64</cell><cell>10.34</cell><cell cols="4">51.95G 41.05M</cell></row><row><cell>DUNet [5]</cell><cell></cell><cell>0.140</cell><cell cols="2">77.84</cell><cell>79.10</cell><cell>69.00</cell><cell cols="2">10.53</cell><cell cols="5">15.84 123.35G 31.21M</cell></row><row><cell>UNet [33]</cell><cell></cell><cell>0.234</cell><cell cols="2">51.07</cell><cell>54.99</cell><cell>52.96</cell><cell cols="2">27.04</cell><cell cols="5">25.69 124.62G 13.39M</cell></row><row><cell>PSPNet [3]</cell><cell></cell><cell>0.093</cell><cell cols="2">86.25</cell><cell>86.13</cell><cell>78.42</cell><cell></cell><cell>6.68</cell><cell cols="5">12.75 187.27G 50.99M</cell></row><row><cell>TransLab</cell><cell></cell><cell>0.063</cell><cell cols="2">92.69</cell><cell>90.87</cell><cell>84.39</cell><cell></cell><cell>3.63</cell><cell>7.28</cell><cell cols="4">61.60G 42.19M</cell></row><row><cell></cell><cell></cell><cell cols="8">(b) Comparison between Easy and Hard.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>All</cell><cell>MAE ? Easy</cell><cell>Hard</cell><cell>All</cell><cell>Acc ? Easy</cell><cell>Hard</cell><cell>All</cell><cell>mIoU ? Easy</cell><cell>Hard</cell><cell>All</cell><cell cols="2">mBER ? Easy</cell><cell>Hard</cell></row><row><cell>FPENet [24]</cell><cell cols="13">0.339 0.297 0.492 24.73 26.50 19.19 34.17 36.82 24.41 39.31 37.88 44.03</cell></row><row><cell>ContextNet [25]</cell><cell cols="13">0.217 0.171 0.386 62.09 67.14 46.34 56.46 61.73 37.71 22.36 18.77 34.44</cell></row><row><cell>FastSCNN [26]</cell><cell cols="13">0.206 0.161 0.373 64.20 69.42 48.01 59.18 64.63 40.27 22.27 18.74 34.22</cell></row><row><cell cols="11">DeepLabv3+MBv2[10] 0.130 0.091 0.275 80.92 85.90 65.43 75.27 80.55 56.17 12.49</cell><cell></cell><cell>9.08</cell><cell>24.47</cell></row><row><cell>CGNet [27]</cell><cell cols="13">0.216 0.173 0.379 59.15 64.57 42.26 57.31 62.41 39.56 22.95 19.67 34.33</cell></row><row><cell>HRNet [28]</cell><cell cols="10">0.134 0.092 0.291 75.82 82.17 56.04 74.56 80.43 53.42 13.52</cell><cell></cell><cell>9.95</cell><cell>26.17</cell></row><row><cell cols="11">HardNet [29] 0.184 0.141 0DenseAspp [7] 0.114 0.078 0.247 81.22 86.25 66.55 78.11 83.11 60.38 12.19</cell><cell></cell><cell>8.85</cell><cell>23.71</cell></row><row><cell cols="10">DeepLabv3+R50[10] 0.081 0.050 0.194 89.54 93.22 78.07 84.54 89.09 69.04</cell><cell>7.78</cell><cell></cell><cell>4.91</cell><cell>17.27</cell></row><row><cell>FCN [9]</cell><cell cols="10">0.108 0.073 0.239 83.79 88.55 68.93 79.67 84.53 62.51 10.33</cell><cell></cell><cell>7.36</cell><cell>20.47</cell></row><row><cell>OCNet [32]</cell><cell cols="10">0.122 0.087 0.253 80.85 85.63 65.96 76.85 81.53 59.75 12.65</cell><cell></cell><cell>9.43</cell><cell>23.69</cell></row><row><cell>RefineNet [8]</cell><cell cols="13">0.180 0.135 0.345 57.97 64.53 37.53 66.03 71.41 45.71 22.22 19.01 34.06</cell></row><row><cell cols="10">DeepLabv3+XP65 [10] 0.082 0.051 0.195 89.18 92.61 78.51 84.26 88.87 68.34</cell><cell>8.00</cell><cell></cell><cell>5.16</cell><cell>17.44</cell></row><row><cell>DUNet [5]</cell><cell cols="10">0.140 0.100 0.289 77.84 83.41 60.50 74.06 79.19 55.53 13.19</cell><cell></cell><cell>9.93</cell><cell>25.01</cell></row><row><cell>UNet [33]</cell><cell cols="13">0.234 0.191 0.398 51.07 55.44 37.44 53.98 58.60 37.08 26.37 23.40 36.80</cell></row><row><cell>PSPNet [3]</cell><cell cols="9">0.093 0.062 0.211 86.25 90.41 73.28 82.38 86.79 66.35</cell><cell>9.72</cell><cell></cell><cell>6.67</cell><cell>20.08</cell></row><row><cell>TransLab</cell><cell cols="10">0.063 0.036 0.166 92.69 95.77 83.04 87.63 92.23 72.10 5.46</cell><cell></cell><cell cols="2">3.12 13.30</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transcut: Transparent object segmentation from a light-field image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taniguchi</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tom-net: Learning transparent object matting from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dunet: A deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Knowledge-Based Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Feature pyramid encoding network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Contextnet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<title level="m">Fast-scnn: fast semantic segmentation network. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cgnet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Deep high-resolution representation learning for visual recognition. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lednet: A lightweight encoder-decoder network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<editor>ICIP.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Ocnet: Object context network for scene parsing. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MICCAI.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
