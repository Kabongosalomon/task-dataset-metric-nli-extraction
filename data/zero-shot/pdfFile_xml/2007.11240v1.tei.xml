<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge-aware Graph Representation Learning and Reasoning for Face Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gusi</forename><surname>Te</surname></persName>
							<email>tegusi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglu</forename><surname>Liu</surname></persName>
							<email>liuyinglu1@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
							<email>shihailin@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge-aware Graph Representation Learning and Reasoning for Face Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face parsing</term>
					<term>graph representation</term>
					<term>attention mechanism</term>
					<term>graph reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their efficiency in face parsing, which however overlook the correlation among different face regions. The correlation is a critical clue about the facial appearance, pose, expression, etc., and should be taken into account for face parsing. To this end, we propose to model and reason the region-wise relations by learning graph representations, and leverage the edge information between regions for optimized abstraction. Specifically, we encode a facial image onto a global graph representation where a collection of pixels ("regions") with similar features are projected to each vertex. Our model learns and reasons over relations between the regions by propagating information across vertices on the graph. Furthermore, we incorporate the edge information to aggregate the pixel-wise features onto vertices, which emphasizes on the features around edges for fine segmentation along edges. The finally learned graph representation is projected back to pixel grids for parsing. Experiments demonstrate that our model outperforms state-of-the-art methods on the widely used Helen dataset, and also exhibits the superior performance on the large-scale CelebAMask-HQ and LaPa dataset. The code is available at https://github.com/tegusi/EAGRNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The region-based methods have been recently proposed to model the facial components separately <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and achieved state-of-the-art performance on the current benchmarks. However, these methods are based on the individual information within each region, and the correlation among regions is not exploited yet to capture long range dependencies. In fact, facial components present themselves with abundant correlation between each other. For instance, eyes, mouth and eyebrows will generally become more curvy when people smile; facial skin and other components will be dark when the lighting is weak, and so on.</p><p>The correlation between the facial components is the critical clue in face representation, and should be taken into account in the face parsing. To this end, we propose to learn graph representations over facial images, which model the relations between regions and enable reasoning over non-local regions to capture long range dependencies. To bridge the facial image pixels and graph vertices, we project a collection of pixels (a "region") with similar features to each vertex. The pixel-wise features in a region are aggregated to the feature of the corresponding vertex. In particular, to achieve accurate segmentation along the edges between different components, we propose the edge attention in the pixel-to-vertex projection, assigning larger weights to the features of edge pixels during the feature aggregation. Further, the graph representation learns the relations between facial regions, i.e., the graph connectivity between vertices, and reasons over the relations by propagating information across all vertices on the graph, which is able to capture long range correlations in the facial image. The learned graph representation is finally projected back to the pixel grids for face parsing. Since the number of vertices is significantly smaller than that of pixels, the graph representation also reduces redundancy in features as well as computational complexity effectively.</p><p>Specifically, given an input facial image, we first encode the high-level and low-level feature maps by the ResNet backbone <ref type="bibr" target="#b6">[7]</ref>. Then, we build a projection matrix to map a cluster of pixels with similar features to each vertex. The feature of each vertex is taken as the weighted aggregation of pixel-wise features in the cluster, where features of edge pixels are assigned with larger weights via an edge mask. Next, we learn and reason over the relations between vertices (i.e., regions) via graph convolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> to further extract global semantic features. The learned features are finally projected back to a pixel-wise feature map. We test our model on Helen, CelebAMask-HQ and LaPa datasets, and surpass stateof-the-art methods.</p><p>Our main contributions are summarized as follows.</p><p>-We propose to exploit the relations between regions for face parsing by modeling on a region-level graph representation, where we project a collection of pixels with similar features to each vertex and reason over the relations to capture long range dependencies. -We introduce edge attention in the pixel-to-vertex feature projection, which emphasizes on features of edge pixels during the feature aggregation to each vertex and thus enforces accurate segmentation along edges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism</head><p>Attention mechanism has been proposed to capture long-range information <ref type="bibr" target="#b16">[17]</ref>, and applied to many applications such as sentence encoding <ref type="bibr" target="#b17">[18]</ref> and image feature extraction <ref type="bibr" target="#b18">[19]</ref>. Limited by the locality of convolution operators, CNN lacks the ability to model global contextual information. Furthermore, Chen et al. propose Double Attention Model that gathers information spatially and temporally to improve complexity of traditional non-local modules <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr">Zhao</ref>   <ref type="figure">Fig. 1</ref>. The overview of the proposed face parsing framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Reasoning</head><p>Interpreting images from the graph perspective is an interesting idea, since an image could be regarded as regular pixel grids. Chandra et al. propose Conditional Random Field (CRF) based method on image segmentation <ref type="bibr" target="#b25">[26]</ref>. Besides, graph convolution network (GCN) is imported into image segmentation. Li et al. introduce graph convolution to the semantic segmentation, which projects features into vertices in the graph domain and applies graph convolution afterwards <ref type="bibr" target="#b26">[27]</ref>. Furthermore, Lu et al. propose Graph-FCN where semantic segmentation is reduced to vertex classification by directly transforming an image into regular grids <ref type="bibr" target="#b27">[28]</ref>. Pourian et al. propose a method of semi-supervised segmentation <ref type="bibr" target="#b28">[29]</ref>. The image is divided into community graph and different labels are assigned to corresponding communities. Te et al. propose a computation-efficient and posture-invariant face representation with only a few key points on hypergraphs for face anti-spoofing beyond 2D attacks <ref type="bibr" target="#b29">[30]</ref>. Zhang et al. utilize graph convolution both in the coordinate space and feature space <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As illustrated in <ref type="figure">Fig. 1</ref>, given an input facial image, we aim to predict the corresponding parsing label and auxiliary edge map. The overall framework of our method consists of three procedures as follows.</p><p>-Feature and Edge Extraction. We take ResNet as the backbone to extract features at various levels for multi-scale representation. The low-level features contain more details but lack semantic information, while the highlevel features provide rich semantics with global information at the cost of image details. To fully exploit the global information in high-level features, we employ a spatial pyramid pooling operation to learn multi-scale contextual information. Further, we construct an edge perceiving module to acquire an edge map for the subsequent module. -Edge Aware Graph Reasoning. We feed the feature map and edge map into the proposed Edge Aware Graph Reasoning (EAGR) module, aiming to learn intrinsic graph representations for the characterization of the relations between regions. The EAGR module consists of three operations: graph projection, graph reasoning and graph reprojection, which projects the original features onto vertices in an edge-aware fashion, reasons the relations between vertices (regions) over the graph and projects the learned graph representation back to pixel grids, leading to a refined feature map with the same size. -Semantic Decoding. We fuse the refined features into a decoder to predict the final result of face parsing. The high-level feature map is upsampled to the same dimension as the low-level one. We concatenate both feature maps and leverage 1 ? 1 convolution layer to reduce feature channels, predicting the final parsing labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Edge-Aware Graph Reasoning</head><p>Inspired by the non-local module <ref type="bibr" target="#b18">[19]</ref>, we aim to build the long-range interactions between distant regions, which is critical for the description of the facial structure. In particular, we propose edge-aware graph reasoning to model the long-range relations between regions on a graph, which consists of edge-aware graph projection, graph reasoning and graph reprojection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge-Aware Graph Projection</head><p>We first revisit the typical non-local modules. Given a feature map X ? R HW ?C , where H and W refer to the height and width of the input image respectively and C is the number of feature channels. A typical non-local module is formulated as:</p><formula xml:id="formula_0">X = softmax ?(X)? (X) ?(X) = V?(X),<label>(1)</label></formula><p>where ?, ? and ? are convolution operations with 1?1 kernel size. V ? R HW ?HW is regarded as the attention maps to model the long-range dependencies. However, the complexity of computing V is O(H 2 W 2 C), which does not scale well with increasing number of pixels HW . To address this issue, we propose a simple yet effective edge-aware projection operation to eliminate the redundancy in features.</p><p>Given an input feature map X ? R HW ?C and an edge map Y ? R HW ?1 , we construct a projection matrix P by mapping X onto vertices of a graph with Y as a prior. Specifically, we first reduce the dimension of X in the feature space via a convolution operation ? with 1 ? 1 kernel size, leading to ?(X) ? R HW ?T , T &lt; C. Then, we duplicate the edge map Y to the same dimension of ?(X) for ease of computation. We incorporate the edge information into the projection, by taking the Hadamard Product of ?(X) and Y. As the edge map Y encodes the probability of each pixel being an edge pixel, the Hadamard Product operation essentially assigns a weight to the feature of each pixel, with larger weights to features of edge pixels. Further, we introduce an average pooling operation P(?) with stride s to obtain anchors of vertices. These anchors represent the centers of each region of pixels, and we take the multiplication of ?(X) and anchors to capture the similarity between anchors and each pixel. We then apply a softmax function for normalization. Formally, the projection matrix takes the form:</p><formula xml:id="formula_1">P = softmax P(?(X) Y) ? ?(X) ,<label>(2)</label></formula><p>where denotes the Hadamard product, and P ? R HW/s 2 ?HW . In Eq. (2), we have two critical operations: the edge attention and the pooling operation. The edge attention emphasizes the features of edge pixels by assigning larger weights to edge pixels. Further, we propose the pooling operation in the features, whose benefits are in twofold aspects. On one hand, the pooling leads to compact representations by averaging over features to remove the redundancy. On the other hand, by pooling with stride s, the computation complexity is</p><formula xml:id="formula_2">reduced from O(H 2 W 2 C) in non-local modules to O(H 2 W 2 C/s 2 ).</formula><p>With the acquired projection matrix P, we project the pixel-wise features X onto the graph domain, i.e.,</p><formula xml:id="formula_3">X G = P?(X),<label>(3)</label></formula><p>where ? is a a convolution operation with 1 ? 1 kernel size so as to reduce the dimension of X, resulting in ?(X) ? R HW ?K . The projection aggregates pixels with similar features as each anchor to one vertex, thus each vertex essentially represents a region in the facial images. Hence, we bridge the connection between pixels and each region via the proposed edge aware graph projection, leading to the features of the projected vertices on the graph X G ? R HW/s 2 ?K via Eq. <ref type="formula" target="#formula_3">(3)</ref>.</p><p>Graph Reasoning Next, we learn the connectivity between vertices from X G , i.e., the relations between regions. Meanwhile, we reason over the relations by propagating information across vertices to learn higher-level semantic information. This is elegantly realized by a single-layer Graph Convolution Network (GCN). Specifically, we feed the input vertex features X G into a firstorder approximation of spectral graph convolution. The output feature map</p><formula xml:id="formula_4">X G ? R HW/s 2 ?K i? X G = ReLU [(I ? A)X G W G ] = ReLU [(I ? A)P?(X)W G ] ,<label>(4)</label></formula><p>where A denotes the adjacent matrix that encodes the graph connectivity to learn, W G ? R K?K denotes the weights of the GCN, and ReLU is the activation function. The featuresX G are acquired by the vertex-wise interaction (multiplication with (I ? A)) and channel-wise interaction (multiplication with W G ). Different from the original one-layer GCN <ref type="bibr" target="#b31">[32]</ref> in which the graph A is handcrafted, we randomly initialize A and learn from vertex features. Moreover, we add a residual connection to reserve features of raw vertices. Based on the learned graph, the information propagation across all vertices leads to the finally reasoned relations between regions. After graph reasoning, pixels embedded within one vertex share the same context of features modeled by graph convolution. We set the same number of output channels as the input to keep consistency, allowing the module to be compatible with the subsequent process.</p><p>Graph Reprojection In order to fit into existing framework, we reproject the extracted vertex features in the graph domain to the original pixel grids. Given the learned graph representationX G ? R HW/s 2 ?K , we aim to compute a matrix V ? R HW ?HW/s 2 that mapsX G to the pixel space. In theory, V could be taken as the inverse of the projection matrix P. However, it is nontrivial to compute because P is not a square matrix. To tackle this problem, we take the transpose matrix P as the reprojection matrix <ref type="bibr" target="#b26">[27]</ref>, in which P ij reflects the correlation between vertex i and pixel j. The limitation of this operation is that the row vectors in P are not normalized.</p><p>After reprojection, we deploy a 1 ? 1 convolution operation ? to increase the feature channels in consistent with the input features X. Then, we take the summation of the reprojected refined features and the original feature map as the final features. The final pixel-wise feature map Z ? R HW ?C is thus computed by Z = X + ?(P X G ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Loss Function</head><p>To further strengthen the effect of the proposed edge aware graph reasoning, we introduce the boundary-attention loss (BA-Loss) inspired by <ref type="bibr" target="#b32">[33]</ref> besides the traditional cross entropy loss for predicted parsing maps and edge maps. The BA-loss computes the loss between the predicted label and the ground truth only at edge pixels, thus improving the segmentation accuracy of critical edge pixels that are difficult to distinguish. Mathematically, the BA-loss is written as</p><formula xml:id="formula_6">L BA = HW i=1 N j=1 [e i = 1] y ij log p ij ,<label>(6)</label></formula><p>where i is the index of pixels, j is the index of classes and N is the number of classes. e i denotes the edge label, y ij denotes the ground truth label of face parsing, and p ij denotes the predicted parsing label.</p><p>[?] is the Iverson bracket, which denotes a number that is 1 if the condition in the bracket is satisfied, and 0 otherwise. The total loss function is then defined as follows:</p><formula xml:id="formula_7">L = L parsing + ? 1 L edge + ? 2 L BA ,<label>(7)</label></formula><p>where L parsing and L edge are classical cross entropy losses for the parsing and edge maps. ? 1 and ? 2 are two hyper-parameters to strike a balance among the three loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>Since non-local modules and graph-based methods have drawn increasing attention, it is interesting to analyze the similarities and differences between previous works and our method.</p><p>Comparison with non-local modules Typically, a traditional non-local module models pixel-wise correlations by feature similarities. However, the high-order relationship between regions are not captured. In contrast, we exploit the correlation among distinct regions via the proposed graph projection and reasoning. The features of each vertex embed not only local contextual anchor aggregated by average pooling in a certain region but also global features from the overall pixels. We further learn and reason over the relations between regions by graph convolution, which captures high-order semantic relations between different facial regions. Also, the computation complexity of non-local modules is expensive in general as discussed in Section 3.2. Our proposed edge-aware pooling addresses the issue by extracting significant anchors to replace redundant query points. Also, we do not incorporate pixels within each facial region during the sampling process while focusing on edge pixels, thus improving boundary details. The intuition is that pixels within each region tend to share similar features.</p><p>Comparison with graph-based models In comparison with other graphbased models, such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>, we improve the graph projection process by introducing locality in sampling in particular. In previous works, each vertex is simply represented by a weighted sum of image pixels, which does not consider edge information explicitly and brings ambiguity in understanding vertices. Besides, with different inputs of feature maps, the pixel-wise features often vary greatly but the projection matrix is fixed after training. In contrast, we incorporate the edge information into the projection process to emphasize on edge pixels, which preserves boundary details well. Further, we specify vertex anchors locally based on the average pooling, which conforms with the rule that the location of facial components keeps almost unchanged after face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>The Helen dataset includes 2,330 images with 11 categories: background, skin, left/right brow, left/right eye, upper/lower lip, inner mouth and hair. Specifically, we keep the same train/validation/test protocol as in <ref type="bibr" target="#b33">[34]</ref>. The number of the training, validation and test samples are 2,000, 230 and 100, respectively. The CelebAMask-HQ dataset is a large-scale face parsing dataset which consists of 24,183 training images, 2,993 validation images and 2,824 test images. The number of categories in CelebAMask-HQ is 19. In addition to facial components, the accessories such as eyeglass, earring, necklace, neck, and cloth are also annotated in the CelebAMask-HQ dataset. The LaPa dataset is a newly released challenging dataset for face parsing, which contains 11 categories as Helen, covering large variations in facial expression, pose and occlusion. It consists of 18,176 training images, 2,000 validation images and 2,000 test images.</p><p>During training, we use the rotation and scale augmentation. The rotation angle is randomly selected from (?30 ? , 30 ? ) and the scale factor is randomly selected from (0.75, 1.25). The edge mask is extracted according to the semantic label map. If the label of a pixel is different with its 4 neighborhoods, it is regarded as a edge pixel. For the Helen dataset, similar to <ref type="bibr" target="#b3">[4]</ref>, we implement face alignment as a pre-processing step and the results are re-mapped to the original image for evaluation.</p><p>We employ three evaluation metrics to measure the performance of our model: pixel accuracy, mean intersection over union (mIoU) and F1 score. Directly employing the accuracy metric ignores the scale variance amid facial components, while the mean IoU and F1 score are better for evaluation. To keep consistent with the previous methods, we report the overall F1-score on the Helen dataset, which is computed over the merged facial components: brows (left+right), eyes (left+right), nose, mouth (upper lip+lower lip+inner mouth). For the CelebAMask-HQ and LaPa datasets, the mean F1-score over all categories excluding background is employed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our backbone is a modified version of the ResNet-101 <ref type="bibr" target="#b6">[7]</ref> excluding the average pooling layer, and the Conv1 block is changed to three 3 ? 3 convolutional layers. For the pyramid pooling module, we follow the implementation in <ref type="bibr" target="#b34">[35]</ref> to exploit global contextual information. The pooling factors are {1, 2, 3, 6}. Similar to <ref type="bibr" target="#b35">[36]</ref>, the edge perceiving module predicts a two-channel edge map based on the outputs of Conv2, Conv3 and Conv4 in ResNet-101. The outputs of Conv1 and the pyramid pooling serve as the low-level and high-level feature maps, respectively. Both of them are fed into the EAGR module separately for graph representation learning. As for the EAGR module, we set the pooling size to 6 ? 6. To pay more attention on the facial components, we just utilize the central 4 ? 4 anchors for graph construction. The feature dimensions K and T are set to 128 and 64, respectively.</p><p>Stochastic Gradient Descent (SGD) is employed for optimization. We initialize the network with a pretrained model on ImageNet. The input size is 473?473 and the batch size is set to 28. The learning rate starts at 0.001 with the weight decay of 0.0005. The batch normalization is implemented with In-Place Activated Batch Norm <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>On different components We demonstrate the effectiveness of different components in the proposed EAGR module. Specifically, we remove some components and train the model from scratch under the same initialization. The quantitative results are reported in <ref type="table" target="#tab_2">Table 1</ref>. Baseline means the model only utilizes the ResNet backbone, pyramid pooling and multi-scale decoder without any EGAR module, and Edge represents whether edge aware pooling is employed. Graph represents the EAGR module, while Reasoning indicates the graph reasoning excluding graph projection and reprojection. We observe that Edge and Graph lead to improvement over the baseline by 1% in mIoU respectively. When both components are taken into account, we achieve even better performance. The boundary-attention loss (BA-loss) also leads to performance improvement.</p><p>We also provide subjective results of face parsing from different models in <ref type="figure" target="#fig_1">Fig. 3</ref>. Results of incomplete models exhibit varying degrees of deficiency around  edges in particular, such as the edge between the hair and skin in the first row, the upper lips in the second row, and edges around the mouse in the third row.</p><p>In contrast, our complete model produce the best results with accurate edges between face constitutes, which is almost the same as the ground truth. This validates the effectiveness of the proposed edge aware graph reasoning.</p><p>On the deployment of the EAGR module We also conduct experiments on the deployment of the EAGR module with respect to the feature maps as well as pooling sizes. We take the output of Conv2 in the ResNet as the low-level feature map, and that of the pyramid pooling module as the high-level feature map. We compare four deployment schemes: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters (M)</head><p>(a) (b) <ref type="figure">Fig. 4</ref>. Complexity comparison on the Helen dataset. We reset the start value of y-axis for better appearance.</p><p>level and high-level feature maps are fed into one EAGR module respectively; 4) 3-moduels, which combines 2) and 3). As listed in <ref type="table" target="#tab_3">Table 2</ref>, the scheme of 2-modules leads to the best performance, which is the one we finally adopt.</p><p>We also test the influence of the pooling size, where the number of vertices changes along with the pooling size. As presented in <ref type="table" target="#tab_3">Table 2</ref>, the size of 6 ? 6 leads to the best performance, while enlarging the pooling size further does not bring performance improvement. This is because more detailed anchors lead to the loss of integrity, which breaks the holistic semantic representation.</p><p>On the complexity in time and space Further, we study the complexity of different models in time and space in <ref type="figure">Fig. 4</ref>. We compare with three schemes: 1) a simplified version without the EAGR module, which we refer to as the Baseline; 2) a non-local module <ref type="bibr" target="#b18">[19]</ref> employed without edge aware sampling (i.e., pooling) as Without sampling; and 3) a version without graph convolution for reasoning as Without graph. As presented in <ref type="figure">Fig. 4</ref>, compared with the typical non-local module, our proposed method reduces the computation time by more than 4? in terms of flops. We also see that the computation and space complexity of our method is comparable to those of the Baseline, which indicates that most complexity comes from the backbone network. Using Nvidia P40, the time cost of our model for a single image is 89ms in the inference stage. This demonstrates that the proposed EAGR module achieves significant performance improvement with trivial computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>We conduct experiments on the broadly acknowledged Helen dataset to demonstrate the superiority of the proposed model. To keep consistent with the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref>, we employ the overall F1 score to measure the performance, which is computed by combining the merged eyes, brows, nose and mouth categories. As <ref type="table">Table 3</ref> shows, Our model surpasses state-of-the-art methods and achieves 93.2% on this dataset. We also evaluate our model on the newly proposed CelebAMask-HQ <ref type="bibr" target="#b0">[1]</ref> and LaPa <ref type="bibr" target="#b32">[33]</ref> datasets, whose scales are about 10 times larger than the Helen dataset. Different from the Helen dataset, CelebAMask-HQ and LaPa have accurate annotation for hair. Therefore, mean F1-score (over all foreground categories) is employed for better evaluation. <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref> give the comparison results of the related works and our method on these two datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization of Graph Projection</head><p>Further, we visualize the graph projection for intuitive interpretation. As in <ref type="figure">Fig. 5</ref>, given each input image (first row), we visualize the weight of each pixel that contributes to a vertex marked in a blue rectangle in the other rows, which we refer to as the response map. Darker color indicates higher response. We observe that the response areas are consistent with the vertex, which validates that our graph projection maps pixels in the same semantic component to the same vertex. <ref type="table">Table 5</ref>. Experimental comparison on the LaPa dataset (in F1 score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Skin </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel graph representation learning paradigm of edge aware graph reasoning for face parsing, which captures region-wise relations to model longrange contextual information. Edge cues are exploited in order to project significant pixels onto graph vertices on a higher semantic level. We then learn the relation between vertices (regions) and reason over all vertices to characterize the semantic information. Experimental results demonstrate that the proposed method sets the new state-of-the-art with low computation complexity, which efficiently reconstructs boundary details in particular. In future, we will apply the paradigm of edge aware graph reasoning to more segmentation applications, such as scene parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>2 x T HW x T HW x T HW x HW/s 2 HW/s 2 x K HW/s 2 x K HW x HW/s 2 Architecture of the Edge Aware Graph Reasoning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Parsing results of different models on the Helen dataset. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Fig. 5 .</head><label>15</label><figDesc>Hair L-Eye R-Eye U-lip I-mouth L-lip Nose L-Brow R-Brow Mean Zhao et al. [35] 93.5 94.1 86.3 86.0 83.6 86.9 84.7 94.8 86.8 86.9 88.4 Liu et al. [33] 97.2 96.3 88.1 88.0 84.4 87.6 85.7 95.5 87.7 87.6 89.8 Ours 97.3 96.2 89.5 90.0 88.1 90.0 89.0 97.1 86.5 87.0 91.Visualization of graph projection via response maps. The first row shows the input image, and the rest visualize response maps with respect to the vertex marked in a blue rectangle. Darker color indicates higher response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>-</head><label></label><figDesc>We conduct extensive experiments on Helen, CelebAMask-HQ and LaPa datasets. The experimental results show our model outperforms state-ofthe-art methods on almost every category. Warrell et al. describe spatial relationship of facial parts with epitome model [11]. Kae et al. combine Conditional Random Field (CRF) with a Restricted Boltzmann Machine (RBM) to extract local and global features [12]. With the rapid development of machine learning, CNN has been introduced to learn more robust and rich features. Liu et al. import CNN-based features into the CRF framework to model individual pixel labels and neighborhood dependencies [13]. Luo et al. propose a hierarchical deep neural network to extract multi-scale facial features [14]. Zhou et al. adopt adversarial learning approach to train the network and capture high-order inconsistency [15]. Liu et al. design a CNN-RNN hybrid model that benefits from both high quality features of CNN and non-local properties of RNN [6]. Zhou et al. present an interlinked CNN that takes multi-scale images as input and allows bidirectional information passing [16]. Lin et al. propose a novel RoI Tanh-Warping operator preserving central and peripheral information. It contains two branches with the local-based for inner facial components and the global based for outer facial ones. This method shows high performance especially on hair segmentation [4].</figDesc><table><row><cell>2 Related Work</cell></row><row><cell>2.1 Face Parsing</cell></row><row><cell>Face parsing is a division of semantic segmentation, which assigns different labels</cell></row><row><cell>to the corresponding regions on human faces, such as nose, eyes, mouth and etc..</cell></row><row><cell>The methods of face parsing could be classified into global-based and local-based</cell></row><row><cell>methods.</cell></row><row><cell>Traditionally, hand crafted features including SIFT [10] are applied to model</cell></row><row><cell>the facial structure.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. propose a point-wise spatial attention module, relaxing the local neighborhood constraint [21]. Zhu et al. also present an asymmetric module to reduce abundant computation and distillate features [22]. Fu et al. devise a dual attention module that applies both spatial and channel attention in feature maps [23]. To research underlying relationship between different regions, Chen et al. project original features into interactive space and utilize GCN to exploit high order relationship [24]. Li et al. devise a robust attention module that incorporates the Expectation-Maximization algorithm [25].</figDesc><table><row><cell></cell><cell cols="2">Edge Aware</cell></row><row><cell></cell><cell cols="2">Graph Reasoning</cell></row><row><cell>Feature Map</cell><cell></cell><cell></cell></row><row><cell>Pooling Pyramid</cell><cell cols="2">Graph Reasoning Edge Aware</cell><cell>Decoder</cell></row><row><cell>Feature Map</cell><cell></cell><cell></cell></row><row><cell>Input Image</cell><cell></cell><cell></cell></row><row><cell>Edge Perceiving</cell><cell>Projection</cell><cell>Reasoning Reprojection</cell></row><row><cell>Edge Map</cell><cell></cell><cell></cell><cell>Parsing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on the Helen dataset.</figDesc><table><row><cell cols="4">Model Baseline Edge Graph Reasoning BA-loss mIoU F1-score Accuracy</cell></row><row><cell>1</cell><cell>76.5</cell><cell>91.4</cell><cell>85.9</cell></row><row><cell>2</cell><cell>77.5</cell><cell>92.0</cell><cell>86.2</cell></row><row><cell>3</cell><cell>77.3</cell><cell>92.3</cell><cell>85.8</cell></row><row><cell>4</cell><cell>77.8</cell><cell>92.4</cell><cell>84.6</cell></row><row><cell>5</cell><cell>77.3</cell><cell>92.3</cell><cell>86.7</cell></row><row><cell>6</cell><cell>78.2</cell><cell>92.8</cell><cell>87.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison with different deployment of the EAGR module and pooling size.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Deployment</cell><cell></cell><cell cols="3">Pooling Size</cell></row><row><cell cols="8">Model 0-module 1-module 2-modules 3-modules 4 ? 4 6 ? 6 8 ? 8</cell></row><row><cell>mIoU</cell><cell>77.6</cell><cell>77.6</cell><cell>78.2</cell><cell>77.4</cell><cell>77.0</cell><cell>78.2</cell><cell>78.0</cell></row><row><cell>F1-score</cell><cell>92.0</cell><cell>92.5</cell><cell>92.8</cell><cell>92.3</cell><cell>92.1</cell><cell>92.8</cell><cell>92.6</cell></row><row><cell>Accuracy</cell><cell>85.5</cell><cell>86.0</cell><cell>87.3</cell><cell>85.4</cell><cell>87.4</cell><cell>87.3</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison with state-of-the-art methods on the Helen dataset (in F1 score). Wei et al. [38] 95.6 95.2 80.0 86.7 86.4 89.0 82.6 93.6 91.7 Yin et al. [5] -96.3 82.4 85.6 86.6 89.5 84.8 92.8 91.0 Liu et al. [33] 94.9 95.8 83.7 89.1 91.4 89.8 83.5 96.1 93.1 Ours 94.6 96.1 83.6 89.8 91.0 90.2 84.9 95.5 93.2 Experimental comparison on the CelebAMask-HQ dataset (in F1 score).</figDesc><table><row><cell cols="2">Methods</cell><cell cols="5">Skin Nose U-lip I-mouth L-lip Eyes Brows Mouth Overall</cell></row><row><cell cols="6">Liu et al. [6] 92.1 93.0 74.3 79.2 81.7 86.8 77.0 89.1</cell><cell>88.6</cell></row><row><cell cols="6">Lin et al. [4] 94.5 95.6 79.6 86.7 89.8 89.6 83.1 95.0</cell><cell>92.4</cell></row><row><cell>Methods</cell><cell cols="6">Face Nose Glasses L-Eye R-Eye L-Brow R-Brow L-Ear R-Ear Mean I-Mouth U-Lip L-Lip Hair Hat Earring Necklace Neck Cloth</cell></row><row><cell>Zhao et al. [35]</cell><cell></cell><cell>94.8 89.8</cell><cell>90.3 75.8 79.9 80.1 87.1 88.8 90.4 58.2</cell><cell>77.3 65.7</cell><cell>78 19.4</cell><cell>75.6 73.1 76.2 82.7 64.2</cell></row><row><cell>Lee et al. [1]</cell><cell></cell><cell>95.5 63.4</cell><cell cols="2">85.6 92.9 84.3 85.2 88.9 90.1 86.6 91.3 63.2 81.4</cell><cell>81.2 26.1</cell><cell>84.9 83.1 80.3 92.8 68.3</cell></row><row><cell>Ours</cell><cell cols="2">96.2 95</cell><cell cols="2">94 88.9 91.2 94.9 87.6 68.3 92.3 88.6 88.7 85.7</cell><cell>85.2 27.6</cell><cell>88 85.7 85.1 89.4 85.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5549" to="5558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesis of high-quality visible faces from polarimetric thermal faces using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Riggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face parsing with roi tanh-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5654" to="5663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end face parsing via interlinked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04831</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face parsing via recurrent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exemplar-based face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3484" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Labelfaces: Parsing facial features by multiclass labeling with an epitome prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on image processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2481" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmenting crfs with boltzmann machine shape priors for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interlinked convolutional neural networks for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="352" to="361" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graphbased global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5103" to="5112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="9225" to="9235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph-fcn for image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised graph based semantic segmentation by learning communities of image-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1359" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring hypergraph representation on face anti-spoofing beyond 2D attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06121</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings, OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A new dataset and boundaryattention semantic segmentation for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11637" to="11644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4814" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate facial image parsing at real-time speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4659" to="4670" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
