<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Few-shot Learning via Deep Laplacian Eigenmaps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuilin</forename><surname>Chen</surname></persName>
							<email>kuilin.chen@mail.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Guhn</forename><surname>Lee</surname></persName>
							<email>cglee@mie.utoronto.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Few-shot Learning via Deep Laplacian Eigenmaps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning a new task from a handful of examples remains an open challenge in machine learning. Despite the recent progress in few-shot learning, most methods rely on supervised pretraining or meta-learning on labeled meta-training data and cannot be applied to the case where the pretraining data is unlabeled. In this study, we present an unsupervised few-shot learning method via deep Laplacian eigenmaps. Our method learns representation from unlabeled data by grouping similar samples together and can be intuitively interpreted by random walks on augmented training data. We analytically show how deep Laplacian eigenmaps avoid collapsed representation in unsupervised learning without explicit comparison between positive and negative samples. The proposed method significantly closes the performance gap between supervised and unsupervised few-shot learning. Our method also achieves comparable performance to current state-of-the-art self-supervised learning methods under linear evaluation protocol.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning <ref type="bibr" target="#b26">(Fei-Fei et al., 2006)</ref> aims to learn a new classification or regression model on a novel task that is not seen during training, given only a few examples in the novel task. Existing few-shot learning methods either rely on episodic meta-learning  or standard pretraining <ref type="bibr" target="#b68">, Tian et al., 2020b</ref> in a supervised manner to extract transferrable knowledge to a new few-shot task. Unfortunately, these methods require many labeled meta-training samples. Acquiring a lot of labeled data is costly or even impossible in practice. Recently, several unsupervised meta-learning approaches have attempted to address this problem by constructing synthetic tasks on unlabeled meta-training data  or meta-training on self-supervised pretrained features . However, the performance of unsupervised meta-learning approaches is still far from their supervised counterparts. Empirical studies in supervised pretraining show that representation learning via grouping similar samples together <ref type="bibr" target="#b24">, Dhillon et al., 2020</ref><ref type="bibr" target="#b44">, Laenen and Bertinetto, 2021</ref><ref type="bibr" target="#b68">, Tian et al., 2020b</ref>) outperforms a wide range of episodic meta-learning methods, where the definition of similar samples is given by class labels. The motivation of this study is to develop an unsupervised representation learning method by grouping unlabeled meta-training data without episodic training and close the performance gap between supervised and unsupervised few-shot learning.</p><p>Contrastive self-supervised learning has shown remarkable success in learning representation from unlabeled data, which is competitive with supervised learning on multiple visual tasks <ref type="bibr" target="#b34">(H?naff et al., 2020</ref><ref type="bibr" target="#b67">, Tian et al., 2020a</ref>. The common underlying theme behind contrastive learning is to pull together representation of augmented views of the same training sample (positive sam- <ref type="figure">Figure 1</ref>: A graph on augmented views of unlabeled training data. The thickness of the edge indicates the transition probability between vertices, which is proportional to their similarity. We group similar vertices together by minimizing the total transition probability between different groups. ple) and disperse representation of augmented views from different training samples (negative sample) <ref type="bibr">Isola, 2020, Wu et al., 2018)</ref>. Typically, contrastive learning methods require a large size of negative samples to learn high-quality representation from unlabeled data . This inevitably requires a large batch size of samples, demanding significant computing resources. Non-contrastive methods try to overcome the issue by accomplishing self-supervised learning with only positive pairs. However, non-contrastive methods suffer from trivial solutions where the model maps all inputs to the same constant vector, known as the collapsed representation. Various methods have been proposed to avoid collapsed representation on an ad hoc basis, such as asymmetric network architecture , stop gradient <ref type="bibr" target="#b21">(Chen and He, 2021)</ref>, and feature decorrelation <ref type="bibr" target="#b25">(Ermolov et al., 2021</ref><ref type="bibr" target="#b37">, Hua et al., 2021</ref>. However, theoretical understanding about how non-contrastive methods avoid collapsed representation is limited, though some preliminary attempts are made to analyze the training dynamics of non-contrastive methods <ref type="bibr" target="#b69">(Tian et al., 2021)</ref>. Besides, most self-supervised learning methods focus on the linear evaluation task where the training and test data come from the same classes. They do not account for the domain gap between training and test classes, which is the case in few-shot learning and cross-domain few-shot learning.</p><p>We develop a novel unsupervised representation learning method in which a weighted graph is used to capture unlabeled samples as nodes and similarity among samples as the weights of edges. Two samples are deemed similar if they are augmentations of a single sample and clustering of samples is accomplished by partitioning the graph. We provide an intuitive understanding of the graph partition problem from the perspective of random walks on the graph, where the transition probability between two vertices is proportional to their similarity. The optimal partition can be found by minimizing the total transition probability between clusters. It is linked to the well-known Laplacian eigenmaps in spectral analysis <ref type="bibr" target="#b15">(Belkin and Niyogi, 2003</ref><ref type="bibr" target="#b51">, Meila and Shi, 2000</ref><ref type="bibr" target="#b63">, Shi and Malik, 2000</ref>. We replace the locality-preserving projection in Laplacian eigenmaps with deep neural networks for better scalability and flexibility in learning high-dimensional data such as images.</p><p>An additional technique is integrated into deep Laplacian eigenmaps to handle the domain gap between the meta-training and meta-testing sets. Previous studies on word embeddings (e.g. king -man + woman ? queen) <ref type="bibr" target="#b52">(Mikolov et al., 2013)</ref> and disentangled generative models <ref type="bibr" target="#b39">(Karras et al., 2019)</ref> show that interpolation between latent embeddings may correspond to the representation of a realistic sample, which may not be seen in the training data. In contrast, interpolation in the input space does not result in realistic samples. In parallel, interpolation between the distributions of the nearest two meta-training classes in the embedding space can approximate the distribution of one meta-testing class after the feature extractor is trained <ref type="bibr" target="#b77">(Yang et al., 2021)</ref>. To enhance the performance on downstream few-shot learning tasks, we make interpolation of unlabeled metatraining samples on data manifold to mimic unseen meta-test samples and integrate them into unsupervised training of the feature extractor.</p><p>Our contributions are summarized as follows:</p><p>? A new unsupervised few-shot learning method is developed based on deep Laplacian eigenmaps with an intuitive explanation based on random walks.</p><p>? Our loss function is analyzed to show how collapsed representation is avoided without explicit comparison to negative samples, shedding light on existing feature decorrelation based self-supervised learning methods.</p><p>? The proposed method significantly closes the performance gap between unsupervised and supervised few-shot learning methods.</p><p>? Our method achieves comparable performance to current state-of-the-art (SOTA) selfsupervised learning methods under the linear evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph from augmented data</head><p>First, we construct a graph using augmented views of unlabeled data. Letx ? R d be a raw sample without augmentation. For image data, augmented views are created by the commonly used augmentations defined in SimCLR , including horizontal flip, Gaussian blur, color jittering, and random cropping. X denotes the set of all augmented data and N = |X |. We represent the augmented data in the form of a weighted graph G = (X , S), where each x ? X is a vertex of the graph and S denotes the edge weights. The edge between two vertices x i , x j ? X is weighted by the non-negative similarity s ij between them. For unrelated x i and x j , the similarity s ij should be small. On the other hand, the similarity s ij should be large when x i and x j are augmentations from the same image or augmentations from two images within the same latent classes. An illustrative diagram is shown in <ref type="figure">Fig. 1</ref>.</p><p>Let d i = xj?X s ij denote the total weights associated with x i , which is the degree of a vertex x i in a weighted graph. The degree matrix D is defined as the diagonal matrix with the degrees d 1 , ..., d N on the diagonal. The volume of X is Vol(X ) = xi?X d i . Similarly, the volume of a subset C ? X is defined as Vol(C) = xi?C d i . Let P = D ?1 S be the random walk Laplacian of G, where p ij = (P) ij represents the transition probability from x i to x j , and each row of P sums to 1. L = D ? S is the unnormalized Laplacian of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Random walks and graph partition</head><p>X can be grouped into K clusters, where similar vertices should be grouped into the same cluster and dissimilar vertices should be grouped into different clusters. It resembles the supervised pretraining by embedding samples from the same class together. We will show later that K is also the dimension of the embedding. Since we do not know the number of classes in unlabeled training data, we set the embedding dimension K = 2048, which works well on a wide range of datasets. Graph partition into clusters can be done by minimizing the total similarity xi?C,xj?C s ij between two clusters C, C ? X , C ? C = ?. However, minimizing the total inter-cluster similarity is undesirable because it can simply separate one individual vertex from the rest of the graph. Instead, we run random walks on vertices X .</p><p>The random walk Laplacian P defines a Markov chain on the vertices X . The stationary distribution ? of this chain has an explicit form ? i = d i / Vol(X ) for x i ? X <ref type="bibr" target="#b51">(Meila and Shi, 2000)</ref>. The transition probability P (C | C) = P (X 1 ? C | X 0 ? C) is given by</p><formula xml:id="formula_0">P X 1 ? C | X 0 ? C = ? ? 1 Vol(X ) xi?C,xj?C s ij ? ? Vol(C) Vol(X ) ?1 = xi?C,xj?C s ij Vol(C)<label>(1)</label></formula><p>The inter-cluster transition probability is a proper criterion because it has a small value only if xi?C,xj?C s ij is small (low similarity for vertices in different clusters) and all clusters have sufficiently large volumes. As a result, minimization of the inter-cluster transition probability prevents trivial solutions that simply separate one individual vertex from the rest of the graph.</p><p>The inter-cluster transition probability minimizing problem is a constrained optimization problem. For the case of finding K clusters, we define a matrix Z ? R N ?K , where z ik represents the cluster assignment of the vertex x i .</p><formula xml:id="formula_1">z ik = 1/ vol (C k ) if x i ? C k 0 otherwise<label>(2)</label></formula><p>where i = 1, . . . , N and k = 1, . . . , K. Let z (k) be the k-th column in the matrix Z. The connection between unnormalized graph Laplacian and inter-cluster transition probability is given by</p><formula xml:id="formula_2">z (k) Lz (k) = z (k) (D ? S)z (k) = N i=1 d i z 2 ik ? N i=1 N j=1 s ij z ik z jk = 1 2 ? ? N i=1 d i z 2 ik ? 2 N i=1 N j=1 s ij z ik z jk + N j=1 d j z 2 jk ? ? = 1 2 ? ? N i=1 N j=1 s ij z 2 ik ? 2 N i=1 N j=1 s ij z ik z jk + N j=1 N i=1 s ji z 2 jk ? ? = 1 2 N i,j=1 s ij (z ik ? z jk ) 2 = 1 2 i?Ck,j?Ck s ij 1 Vol(C k ) 2 = 1 2 P (C k | C k )<label>(3)</label></formula><p>It is easy to verify that Z DZ = I, and z (k) Dz (k) = 1. We can write the problem of minimizing inter-cluster transition as</p><formula xml:id="formula_3">min C1,...,CK Tr Z LZ subject to Z DZ = I z ik = 1/ vol (C k ) if x i ? C k else 0 (4)</formula><p>This problem is NP-hard due to discreteness. Relaxing the discreteness condition, we obtain the relaxed problem</p><formula xml:id="formula_4">min Z?R N ?K Tr Z LZ subject to Z DZ = I<label>(5)</label></formula><p>This is the standard trace minimization problem which is solved when the column space of Z is the subspace of the K generalized eigenvectors of Lz = ?Dz. The relaxed problem in Eq. (5) leads to the lower bound on the optimal normalized cut of the graph (??) and retains the interpretation of minimizing the transition probability between clusters. In addition, such relaxation has asymptotic behavior when the number of data points tends to infinity (?). As such, rounding Z leads to cluster indicator because the relaxed problem is a good proxy of the original problem in Eq. (4) <ref type="bibr" target="#b14">(Bach and Jordan, 2006)</ref>. We actually would not round the continuous Z as our goal is not clustering. We will learn a linear classifier on Z in the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep representation learning</head><p>Let z i ? R K be the i-th row of the matrix Z. z i can serve as desirable representation of x i as it exhibits the clustering structure of the graph G. However, it is not sensible to obtain z i by generalized eigenvalue decomposition for two reasons. First, computation of eigenvectors may be prohibitively expensive due to the large size of augmented data X . Second, it is non-trivial to compute the embeddings for unseen data points in meta-test classes because eigenvalue decomposition is non-parametric (one K-dimensional vector z is computed for each x in the training data).</p><p>We assume that z is parametrized by f ? (x), where f ? can be deep neural networks with trainable parameters ?. The relaxed problem in Eq. (5) is converted to</p><formula xml:id="formula_5">min ? Tr Z LZ subject to Z DZ = I (6)</formula><p>where Z = f ? (X). We design a proper loss function to learn ? that solves the constrained optimization problem in Eq. <ref type="formula">(6)</ref>. The trace minimizing can be written as</p><formula xml:id="formula_6">Tr Z LZ = xi,xj?X s ij f (x i ) ? f (x j ) 2<label>(7)</label></formula><p>where the summation is taken w.r.t. pairs (x i , x j ) drawn i.i.d. from X . Note that the similarity s ij for an unrelated pair (x i , x j ) should be negligibly small, compared to the similarity of a related pair (x i , x j ). Therefore, the weighted summation in Eq. <ref type="formula" target="#formula_6">(7)</ref> can be approximated by summation of the Euclidean distance between the representation of positive pairs within a mini-batch V batch</p><formula xml:id="formula_7">L trace = E z,z+?Vbatch z ? z + 2<label>(8)</label></formula><p>where z and z + are representation of augmented views from the same image. The constraint Z DZ = I requires the covariance matrix of the representation to be a diagonal matrix. It is equivalent to minimizing the mean squared error on off-diagonal entries of the covariance matrix</p><formula xml:id="formula_8">L const = K k=1 K l =k (c kl ) 2 (9) where c kl = z?Vbatch (z) k (z) l /B</formula><p>is the covariance between the k-th and l-th dimensions of the feature, B is the size of the mini-batch, and (?) k denotes the k-th element of a vector.</p><p>The total loss is L = L trace + ?L const (10) where L trace comes from trace minimization, L const is translated from the constraint on eigenvectors, and ? can be treated as a Lagrange multiplier.</p><p>Our method does not require asymmetric twin neural networks, large batch size, large memory bank, or momentum update. It naturally avoids the trivial solution via the orthogonality constraint, which is realized by decorrelating each dimension of the representation. In Section 3, we will provide a more detailed analysis of how collapsed representation is avoided without explicit comparison between positive and negative samples in the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interpolation between unlabeled training samples</head><p>The representation of the augmented views can be encoded as z = f ? (x) = f L (g L (x)), where g L is part of the feature extractor from the input layer to the hidden layer L and f L is the remaining part of the feature extractor to the output layer.The hidden layer L is randomly selected from a set of eligible layers in the neural network f ? so that we can interpolate two samples on their intermediate representation. Let x i and x i+ be a positive pair. We interpolate intermediate representation g L (x i+ ) and g L (x j+ ) to get the manifold mixup <ref type="bibr" target="#b71">(Verma et al., 2019)</ref> as follows</p><formula xml:id="formula_9">z ij+ = f L (?g L (x i+ ) + (1 ? ?)g L (x j+ ))<label>(11)</label></formula><p>where ? ? [0, 1] is a mixing coefficient drawn from a Beta distribution and z ij+ is the mixed representation of x i+ and x j+ after interpolation on the data manifold. z ij+ should match the interpolated representation of z i and z j in the embedding space with the same mixing coefficient. The trace minimization loss in Eq. (8) can be replaced by </p><formula xml:id="formula_10">L trace = E z,z+?Vbatch (?z i + (1 ? ?)z j ) ? z ij+ 2<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unravel feature decorrelation</head><p>We analyze the feature decorrelation loss in Eq. (9) and show that feature decorrelation is indeed mathematically equivalent to contrasting between positive and negative samples. At first glance, the off-diagonal entries have nothing to do with the inner product between the representation of positive and negative samples. Previous works <ref type="bibr" target="#b37">(Hua et al., 2021</ref> provide qualitative and empirical analysis to show that feature decorrelation avoids collapsed representation. To the best of our knowledge, we are the first to shed light on the equivalence and analyze the gradient of the feature decorrelation loss.</p><p>When each dimension of Z is standardized to zero mean and unit variance (due to the final batch normalization layer), the diagonal entries of the covariance matrix C become constant c ii = 1. Minimizing the square of off-diagonal entries in Eq. (9) is equivalent to minimizing the Frobenius norm of the covariance matrix,</p><formula xml:id="formula_11">C 2 F = 1 N 2 Z Z 2 F = 1 N 2 tr Z Z Z Z = 1 N 2 tr ZZ ZZ = 1 N 2 tr ZZ ZZ = 1 N 2 N i=1 N j=1 ZZ ? ZZ ij = 1 N 2 N i=1 N j=1 f ? (x i ) f ? (x j ) 2<label>(13)</label></formula><p>where ? denotes the Hadamard product. The second line of Eq. <ref type="formula" target="#formula_0">(13)</ref> is based on the cyclic property of the trace operation and the fact that ZZ is symmetric. After rewriting the trace operation via the Hadamard product, we have the final expression, showing that the squared Frobenius norm of the covariance matrix can be expressed as a summation over the squared inner product between pairs of representation. Note that z i = f ? (x i ) is usually projected to a sphere ball with a radius of 1 in self-supervised learning. The inner product between the representation of the same augmented view is constant z i z i = 1. Minimizing the square of off-diagonal entries in Eq. (9) is equivalent to minimizing the total squared cosine similarity between random pairs. After demystifying the feature decorrelation loss, we can analyze the gradient of our loss function to show how it pulls similar samples together and pushes dissimilar samples apart. Let z i be the anchor sample, z i+ be a positive sample, and z j be an unspecified sample in the current batch V batch . The gradient with respect to the anchor sample is given by</p><formula xml:id="formula_12">?L ?z i = 2 B ? ? (1 ? ?) z i ? z i+ + ? zj?Vbatch z i z j B z j ? ?<label>(14)</label></formula><p>where B is the size of the mini-batch. Since ? is a small positive number, the gradient can be further</p><formula xml:id="formula_13">simplified as ?L ?zi = 2 B (z i ? z i+ ) + ? zj?Vbatch ? j z j ,</formula><p>where ? j is a weighting factor for negative samples which is proportional to the similarity between the positive and negative samples. The first term of the gradient pulls positive pairs together while the second term disperses the negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Few-shot learning is cast to optimization-based <ref type="bibr" target="#b12">(Antoniou et al., 2018</ref><ref type="bibr" target="#b16">, Bertinetto et al., 2019</ref><ref type="bibr" target="#b28">, Flennerhag et al., 2020</ref><ref type="bibr" target="#b48">, Lee and Choi, 2018</ref><ref type="bibr" target="#b53">, Nichol et al., 2018</ref><ref type="bibr" target="#b56">, Park and Oliva, 2019</ref><ref type="bibr" target="#b59">, Ravi and Larochelle, 2017</ref><ref type="bibr" target="#b61">, Rusu et al., 2019</ref> or metric-based <ref type="bibr" target="#b42">(Koch et al., 2015</ref><ref type="bibr" target="#b58">, Qi et al., 2018</ref><ref type="bibr" target="#b66">, Sung et al., 2018</ref><ref type="bibr" target="#b72">, Vinyals et al., 2016</ref><ref type="bibr" target="#b79">, Yoon et al., 2019</ref> meta-learning problems through supervised episodic training because it mimics the circumstances encountered in few-shot learning. The model is trained by a series of learning episodes, each of which consists of a limited number of support (training) samples and query (validation) samples.</p><p>Nevertheless, simple baselines can outperform SOTA episodic meta-learning methods by using embeddings pre-trained with standard supervised learning <ref type="bibr" target="#b24">, Dhillon et al., 2020</ref><ref type="bibr" target="#b44">, Laenen and Bertinetto, 2021</ref><ref type="bibr" target="#b49">, Mangla et al., 2020</ref><ref type="bibr" target="#b68">, Tian et al., 2020b</ref>. Although episodic meta-learning methods can be applied to unlabeled meta-training data by constructing synthetic tasks  or modeling the multi-modality within each randomly sampled episode , the performance is much worse than the supervised counterparts. Different from established few-shot learning methods, our method learns useful representation for downstream few-shot learning tasks using unlabeled meta-training data without episodic training.</p><p>Contrastive learning with variants of InfoNCE loss <ref type="bibr" target="#b30">(Gutmann and</ref><ref type="bibr">Hyv?rinen, 2010, Oord et al., 2018)</ref> has been widely used in self-supervised/unsupervised representation learning <ref type="bibr" target="#b34">, H?naff et al., 2020</ref><ref type="bibr" target="#b76">, Wu et al., 2018</ref>. It is derived from the maximization of the mutual information (MI) between related views <ref type="bibr" target="#b57">(Poole et al., 2019)</ref>. However, this interpretation could be inconsistent with some empirical observations in self-supervised learning, such as tighter lower bounds of MI leading to worse performance <ref type="bibr" target="#b50">(McAllester and Stratos, 2020</ref><ref type="bibr" target="#b70">, Tschannen et al., 2020</ref><ref type="bibr" target="#b74">, Wang and Isola, 2020</ref>. The InfoNCE loss can be expressed as</p><formula xml:id="formula_14">L InfoNCE = ? i log exp(z i zi+/? ) z j ?V exp(z i zj/? ) ,</formula><p>where V can be a mini-batch or a memory bank. The core idea of contrastive learning is pulling positive pairs together while pushing negative samples apart <ref type="bibr" target="#b74">(Wang and Isola, 2020)</ref>. It can be easily verified from the gradient ?LInfoNCE ?zi</p><formula xml:id="formula_15">= (?z i+ + zj?V ? j z j )/? , where ? j = exp(z i zj/? ) z j ?V exp(z i zj/? )</formula><p>is a weighting factor that is proportional to the similarity between the anchor sample z i and the negative sample z j . Our method shares a similar form of the gradient with a different weighting scheme on negative samples, though our loss function is derived from a different perspective. Recent studies show that weighting schemes on negative samples affect the learning efficiency with respect to the negative sample size <ref type="bibr" target="#b78">Liu, 2021, Yeh et al., 2021)</ref>. Compared with contrastive learning with InfoNCE loss, our method does not require a large size of negative samples to work well.</p><p>Clustering methods have been employed in self-supervised learning <ref type="bibr" target="#b13">(Asano et al., 2019</ref><ref type="bibr" target="#b17">, Caron et al., 2018</ref> by simultaneously clustering the unlabeled data while enforcing consistent cluster assignments for different augmented views of the same image. These methods do not compare positive and negative samples directly as in contrastive learning, but careful implementation details and large batches are necessary because clustering methods are prone to collapse. The derivation of our method resembles spectral clustering but our method does not perform clustering. Unsupervised representation learning via deep Laplacian eigenmaps can be intuitively interpreted by random walks on augmented views of unlabeled data and use deep neural networks to handle high-dimensional image data. Random walks on image pixels have been developed to solve a supervised image segmentation problem by minimizing the Kullback-Leibler divergence between the learned transition probability and the target transition probability <ref type="bibr" target="#b51">(Meila and Shi, 2000)</ref>. If f ? is a linear function, the linear projection is locality preserving <ref type="bibr" target="#b33">(He and Niyogi, 2004)</ref>.</p><p>Feature decorrelation methods avoid collapsed representation in self-supervised learning without using a large number of negative samples. Feature decorrelation can be achieved by differentiable Cholesky decomposition on each batch of embeddings <ref type="bibr" target="#b25">(Ermolov et al., 2021)</ref>, forcing the cross-correlation matrix of representations close to the identity matrix , or utilization of decorrelated batch normalization with a shuffling operation <ref type="bibr" target="#b37">(Hua et al., 2021)</ref>. Feature decorrelation methods show comparable performance to contrastive learning methods, but the fundamental principle behind it is unclear. Although Barlow Twins  are derived from the information bottleneck principle, it is only valid for Gaussian distributed data. The loss function in our method is similar to those in decorrelation methods. However, our method is derived from the well-known spectral analysis of the Laplacian matrix and requires minimal assumptions about the training data. Existing feature decorrelation methods in self-supervised learning can be unified in our framework -a trace minimization problem with orthogonality constraints, with minor differences in handling orthogonality constraints in practice. We also show the exact reason why feature decorrelation avoids collapsed representation.</p><p>Mixup  and its variants <ref type="bibr" target="#b71">(Verma et al., 2019</ref><ref type="bibr" target="#b82">, Yun et al., 2019</ref> provide effective data augmentation strategies to improve performance in supervised learning. Mixing up the image pixels has been explored in self-supervised learning <ref type="bibr" target="#b46">(Lee et al., 2021b</ref><ref type="bibr">, Shen et al., 2022</ref>. MoChi <ref type="bibr" target="#b38">(Kalantidis et al., 2020)</ref> mixes the final representation of negative samples to create hard negative samples. Our method mixes up the intermediate representation to achieve better empirical performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the performance of our model trained on unlabeled meta-training data on fewshot learning tasks, including in-domain and more challenging cross-domain settings. In addition, our method is also compared with SOTA self-supervised learning method under linear evaluation protocol to show that the proposed method can be applied to a wide range of downstream tasks beyond few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Few-shot classification</head><p>We conduct few-shot classification experiments on three widely used few-shot image recognition benchmarks.</p><p>miniImageNet is a 100-class subset of the original ImageNet dataset <ref type="bibr" target="#b23">(Deng et al., 2009</ref>) for few-shot learning <ref type="bibr" target="#b72">(Vinyals et al., 2016)</ref>. miniImageNet is split into 64 training classes, 16 validation classes, and 20 testing classes, following the widely used data splitting protocol <ref type="bibr" target="#b59">(Ravi and Larochelle, 2017)</ref>.</p><p>FC100 is a derivative of CIFAR-100 with minimized overlapped information between train classes and test classes by grouping the 100 classes into 20 superclasses . They are further split into 60 training classes (12 superclasses), 20 validation classes (4 superclasses), and 20 test classes (4 superclasses).</p><p>miniImageNet to CUB is a cross-domain few-shot classification task, where the models are trained on miniImageNet and tested on CUB <ref type="bibr" target="#b75">(Welinder et al., 2010)</ref>. Cross-domain few-shot classification is more challenging due to the big domain gap between two datasets. We can better evaluate the generalization capability in different algorithms. We follow the experiment setup in <ref type="bibr" target="#b81">(Yue et al., 2020)</ref>.</p><p>The feature extractor f ? contains two components: a backbone network and a projection network. The backbone network can be a variant of ResNet architecture <ref type="bibr" target="#b31">(He et al., 2016)</ref>. The projection network is a 3-layer MLP with batch normalization and ReLU activation. The dimension of each layer in the projection MLP is 2048. We use the same augmentations defined in SimCLR , including horizontal flip, Gaussian blur, color jittering, and random cropping.</p><p>We use ResNet12  and WRN-28-10 (Yue et al., 2020) as the backbone networks for few-shot learning and cross-domain few-shot learning, respectively. Those two backbone networks are selected because they are widely used in SOTA few-shot learning methods. The feature extractor is trained on unlabeled meta-training data by the SGD optimizer (momentum of 0.9 and weight decay of 5e-4) with a mini-batch size of 128. The learning rate starts at 0.05 and decreases to 0 with a cosine schedule. The projection network in the feature extractor is discarded after training on unlabeled data.</p><p>During meta-testing, we train a regularized logistic regression model using 1 ? 5 or 5 ? 5 support samples on frozen representations after the global average pooling layer in the backbone network. Each few-shot task contains 5 classes and 75 query samples. The classification accuracy is evaluated on the query samples. <ref type="table">Table 1</ref>: Few-shot classification results on miniImageNet and FC100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone miniImageNet 5-way FC100 5-way 1-shot 5-shot 1-shot 5-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised</head><p>The results of the proposed method and previous few-shot learning methods using similar backbones are reported in <ref type="table">Table 1</ref>. The proposed method outperforms existing unsupervised fewshot learning methods such as CACTUS  and UMTRA  by a large margin. It demonstrates that high-quality representation for downstream few-shot tasks can be learned from unlabeled meta-training data without episodic training. Our method is in the category of self-supervised representation learning like SimCLR , MoCo v2 , BYOL , and Barlow Twins  as it does not perform episodic learning. SimCLR achieves weaker performance than other self-supervised learning methods because it typically requires very large batch sizes to perform well. Although Meta-GMVAE  performs unsupervised meta-learning on top of the pretrained features from Sim-CLR, the performance gain versus vanilla representation from SimCLR is at most marginal when deep backbones are used in our reproduction. This observation aligns with the empirical results in supervised few-shot learning where the advantage of episodic meta-learning diminishes as the backbone becomes deep <ref type="bibr" target="#b68">, Tian et al., 2020b</ref>. Our method is also compared with some strong baselines in supervised few-shot learning. The performance gap between supervised and unsupervised few-shot learning is significantly reduced by our method, compared with previ- ous results in unsupervised few-shot learning . Our method is also applied to the cross-domain few-shot classification task as summarized in <ref type="table" target="#tab_0">Table 2</ref>. The proposed method outperforms other unsupervised methods in this challenging task, indicating that the learned representation has strong generalization capability. We use the same hyperparameters (training epochs, learning rate, etc.) from in-domain few-shot learning to train the model. The strong results indicate that our method is robust to hyperparameter choice. Although meta-learning methods with adaptive embeddings are expected to perform better than a fixed embedding when the domain gap between base classes and novel classes is large, empirical results show that a fixed embedding from supervised or unsupervised pretraining achieves better performance in both cases. <ref type="bibr" target="#b68">(Tian et al., 2020b)</ref> also reports similar results that a fixed embedding from supervised pretraining shows superior performance on a large-scale cross-domain few-shot classification dataset. We still believe that adaptive embeddings should be helpful when the domain gap between base and novel classes is large. Nevertheless, how to properly train a model on unlabeled meta-training training to obtain useful adaptive embeddings in novel tasks is an open question.</p><p>Ablation studies are conducted to analyze how individual components affect the performance of few-shot learning. We study four variants of our methods: (a) the model is trained by only minimizing the similarity between positive pairs L trace ; (b) the projector network is a 2-layer MLP; (c) manifold mixup is not used in the model; (d) manifold mixup is replaced by input mixup. <ref type="table" target="#tab_1">Table  3</ref> shows the results of our ablation studies on FC100. When the model is trained without feature decorrelation, the accuracy on few-shot learning is close to random guess. It indicates that feature decorrelation is the key to avoiding trivial representation in learning from unlabeled data. After replacing the projector network with a 2-layer MLP, we can see obvious performance loss in the proposed method. Sufficient depth in the projector network is required to achieve optimal performance. The performance deteriorates without manifold mixup, indicating that manifold mixup helps the model to learn task-relevant information for downstream meta-test tasks. Compared with manifold mixup, input mixup is less effective in improving the few-shot learning performance. Ours 39.7 ? 0.7 57.9 ? 0.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Linear evaluation</head><p>To examine the quality of the learned representation, we follow the linear evaluation protocol in self-supervised learning. After the feature extractor is pretrained by unlabeled training data, a linear classifier is trained on top of the frozen backbone network using the labeled training data. The linear evaluation performance is widely used as the proxy for representation quality because it is highly correlated to the performance in downstream tasks, such as transfer learning, objection detection, and image segmentation <ref type="bibr">He, 2021, He et al., 2020)</ref>. Different from few-shot learning, unlabeled training data, labeled training data, and test data are from the same classes under the linear evaluation protocol. We conduct experiments on CIFAR-10/100 and STL-10.</p><p>CIFAR-10/100 are two datasets of tiny natural images with a size 32 ? 32 <ref type="bibr" target="#b43">(Krizhevsky, 2009</ref>). CIFAR-10 and CIFAR-100 have 10 and 100 classes, respectively. Both datasets contain 50,000 training images and 10,000 test images. STL-10 is a 10-class image recognition dataset for unsupervised learning <ref type="bibr" target="#b22">(Coates et al., 2011)</ref>. Each class contains 500 labeled training images and 800 test images. In addition, it also contains 100,000 unlabeled training images. Both labeled and unlabeled training images are used for feature extractor pretraining without using labels. The linear classifier is learned using the labeled training images.</p><p>ResNet18 is adopted as the backbone network in the feature extractor. We train the feature extractor using SGD with momentum of 0.9 and weight decay of 5e-4. The learning rate starts at 0.05 and decreases to 0 with a cosine schedule. The feature extractor is trained for 800 epochs with a batch size of 256.</p><p>After the feature extractor is pretrained by unlabeled data, a linear classifier is trained using SGD with a batch size of 256 and no weight decay for 100 epochs. The learning rate starts at 30.0 and is decayed by 0.1 at the 60th and 80th epochs. The test accuracy is reported in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Our approach achieves comparable performance to SOTA self-supervised learning methods in the linear evaluation under the same training recipe. Considering the good performance in linear evaluation, our method can be used in a wide range of downstream tasks beyond few-shot learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this article, we propose a new unsupervised few-shot learning method via deep Laplacian eigenmaps. Our method learns representation from unlabeled data by grouping similar samples together and can be intuitively interpreted by random walks on augmented training data. We provide a detailed analysis of our loss function derived from constrained trace minimization to show how it avoids collapsed representation analytically and the connection to existing self-supervised learning methods. The few-shot learning performance benefits from the interpolation of unlabeled training samples on the data manifold. Compared with existing unsupervised few-shot learning methods, the performance gap to supervised few-shot learning methods is significantly narrowed. Additional results on linear evaluation suggest that our method can be applied to a wide range of downstream tasks beyond few-shot classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Interpolation of unlabeled training data on data manifold.An illustrative diagram can be found inFig. 2. The pseudo-code is presented in Algorithm 1.Algorithm 1 Pseudo-code of deep Laplacian eigenmaps in a PyTorch-like style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Cross-domain few-shot classification results on miniImageNet to CUB.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">miniImageNet to CUB 5-way 1-shot 5-shot</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell></row><row><cell>MAML (Finn et al., 2017)</cell><cell cols="2">WRN-28-10 39.06 ? 0.47</cell><cell>55.04 ? 0.42</cell></row><row><cell>LEO (Rusu et al., 2019)</cell><cell cols="2">WRN-28-10 41.45 ? 0.54</cell><cell>56.66 ? 0.48</cell></row><row><cell>MTL (Sun et al., 2019)</cell><cell cols="2">WRN-28-10 43.15 ? 0.44</cell><cell>56.89 ? 0.41</cell></row><row><cell cols="3">Matching Net (Vinyals et al., 2016) WRN-28-10 42.04 ? 0.57</cell><cell>53.08 ? 0.45</cell></row><row><cell>SIB (Hu et al., 2020)</cell><cell cols="2">WRN-28-10 43.27 ? 0.44</cell><cell>59.94 ? 0.42</cell></row><row><cell>Baseline (Chen et al., 2019)</cell><cell cols="2">WRN-28-10 42.89 ? 0.41</cell><cell>62.12 ? 0.40</cell></row><row><cell>Unsupervised</cell><cell></cell><cell></cell></row><row><cell cols="3">CACTUs-MAML (Hsu et al., 2019) WRN-28-10 33.48 ? 0.49</cell><cell>49.97 ? 0.41</cell></row><row><cell cols="3">UMTRA (Khodadadeh et al., 2019) WRN-28-10 33.59 ? 0.48</cell><cell>50.21 ? 0.45</cell></row><row><cell>Meta-GMVAE (Lee et al., 2021a)</cell><cell cols="2">WRN-28-10 38.09 ? 0.47</cell><cell>55.65 ? 0.42</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell cols="2">WRN-28-10 38.25 ? 0.49</cell><cell>55.89 ? 0.46</cell></row><row><cell>MoCo v2 (He et al., 2020)</cell><cell cols="2">WRN-28-10 39.29 ? 0.47</cell><cell>56.49 ? 0.44</cell></row><row><cell>BYOL (Grill et al., 2020)</cell><cell cols="2">WRN-28-10 40.63 ? 0.46</cell><cell>56.92 ? 0.43</cell></row><row><cell cols="3">Barlow Twins (Zbontar et al., 2021) WRN-28-10 40.46 ? 0.47</cell><cell>57.16 ? 0.42</cell></row><row><cell>Ours</cell><cell cols="2">WRN-28-10 41.08 ? 0.48</cell><cell>58.86 ? 0.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on FC100.</figDesc><table><row><cell></cell><cell cols="2">FC100 5-way</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Only L trace</cell><cell cols="2">Collapsed Collapsed</cell></row><row><cell>2-layer MLP</cell><cell cols="2">36.1 ? 0.7 50.2 ? 0.7</cell></row><row><cell cols="3">Remove manifold mixup 38.2 ? 0.7 54.4 ? 0.7</cell></row><row><cell>Use input mixup</cell><cell cols="2">38.7 ? 0.7 55.6 ? 0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Accuracy under linear evaluation protocol CIFAR-10 CIFAR-100 STL-10</figDesc><table><row><cell>SimCLR</cell><cell>90.57</cell><cell>63.84</cell><cell>87.52</cell></row><row><cell>MoCo v2</cell><cell>90.67</cell><cell>64.13</cell><cell>87.71</cell></row><row><cell>BYOL</cell><cell>91.74</cell><cell>65.92</cell><cell>88.46</cell></row><row><cell>Barlow Twins</cell><cell>91.58</cell><cell>65.83</cell><cell>88.65</cell></row><row><cell>Ours</cell><cell>92.24</cell><cell>66.16</cell><cell>88.97</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Proto</forename><surname>Net</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maml (finn</surname></persName>
		</author>
		<idno>56.58 ? 1.84 70.85 ? 0.91 36.9 ? 0.6 51.2 ? 0.7</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ResNet-12</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadam (oreshkin</surname></persName>
		</author>
		<idno>ResNet-12 58.50 ? 0.30 76.70 ? 0.30 40.1 ? 0.4 56.1 ? 0.4</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Baseline++</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>ResNet-12 60.83 ? 0.81 77.81 ? 0.76 41.3 ? 0.7 58.7 ? 0.7</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Metaoptnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno>ResNet-12 62.64 ? 0.61 78.63 ? 0.46 41.1 ? 0.6 55.5 ? 0.6</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unsupervised</forename><surname>Cactus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Maml (</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Umtra (khodadadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta-Gmvae (</forename><surname>Lee</surname></persName>
		</author>
		<idno>ResNet-12 55.93 ? 0.85 74.28 ? 0.72 36.3 ? 0.7 49.7 ? 0.7</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Simclr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>ResNet-12 55.76 ? 0.88 75.59 ? 0.69 36.2 ? 0.7 49.9 ? 0.7</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-12 57.73 ? 0.84 77.51 ? 0.63 37.7 ? 0.7 53.2 ? 0.7</idno>
		<title level="m">MoCo v2</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byol (grill</surname></persName>
		</author>
		<idno>ResNet-12 56.17 ? 0.89 76.17 ? 0.66 37.2 ? 0.7 52.8 ? 0.6</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Barlow Twins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zbontar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>References Antreas Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ym Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning spectral clustering, with application to speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Analysis of Single Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Whitening for selfsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3015" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hujun Yin, and Raia Hadsell. Meta-learning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Garcia</forename><surname>Shell Xu Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On feature decorrelation in self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9598" to="9608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>B?lent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for fewshot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning through latent-space interpolation in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharare</forename><surname>Zehtabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Vahidian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>Boloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On episodes, prototypical networks, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steinar</forename><surname>Laenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-gmvae: Mixture of gaussian vae for unsupervised meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Dong Bok Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seanie</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">$i$-mix: A domain-agnostic strategy for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2207" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning segmentation by random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="873" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Meta-curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3314" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJY0-Kcll" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Un-mix: Rethinking image mixtures for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>Marios Savvides, Trevor Darrell, and Eric Xing</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rethinking fewshot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10268" to="10278" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hsiao</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yao</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06848</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled contrastive learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with taskadaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019 (International Conference on Machine Learning). ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Xtarnet: Learning to extract taskadaptive representation for incremental few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Yeon</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10852" to="10860" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Interventional few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
