<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mugs: A Multi-Granular Self-Supervised Learning Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teck</forename><forename type="middle">Khim</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mugs: A Multi-Granular Self-Supervised Learning Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>self-supervised learning, multi-granular representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In self-supervised learning, multi-granular features are heavily desired though rarely investigated, as different downstream tasks (e.g., general and fine-grained classification) often require different or multigranular features, e.g. fine-or coarse-grained one or their mixture. In this work, for the first time, we propose an effective MUlti-Granular Selfsupervised learning (Mugs) framework to explicitly learn multi-granular visual features. Mugs has three complementary granular supervisions: 1) an instance discrimination supervision (IDS), 2) a novel local-group discrimination supervision (LGDS), and 3) a group discrimination supervision (GDS). IDS distinguishes different instances to learn instancelevel fine-grained features.</p><p>LGDS aggregates features of an image and its neighbors into a local-group feature, and pulls local-group features from different crops of the same image together and push them away for others. It provides complementary instance supervision to IDS via an extra alignment on local neighbors, and scatters different local-groups separately to increase discriminability. Accordingly, it helps learn high-level fine-grained features at a local-group level. Finally, to prevent similar local-groups from being scattered randomly or far away, GDS brings similar samples close and thus pulls similar local-groups together, capturing coarse-grained features at a (semantic) group level. Consequently, Mugs can capture three granular features that often enjoy higher generality on diverse downstream tasks over single-granular features, e.g. instancelevel fine-grained features in contrastive learning. By only pretraining on ImageNet-1K, Mugs sets new SoTA linear probing accuracy 82.1% on ImageNet-1K and improves previous SoTA by 1.1%. It also surpasses SoTAs on other tasks, e.g. transfer learning, detection and segmentation. Codes and models are available at https://github.com/sail-sg/mugs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The family of self-supervised learning (SSL) approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> aims to learn highly transferable unsupervised representation for various downstream tasks by training deep models on a large-scale unlabeled dataset. To this end, a pretext task, e.g. jigsaw puzzle <ref type="bibr" target="#b42">[43]</ref> or orientation <ref type="bibr" target="#b33">[34]</ref>, is elaborately designed to generate pseudo labels of unlabeled visual data which are then utilized to train a model without using manual annotations. Since unlabeled visual data are of huger amount and also much cheaper than the manually annotated data, SSL has been very popularly adopted for visual representation learning recently <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b61">62]</ref>, and is showing greater potential than supervised learning approaches for learning highly-qualified and well-transferable representation.</p><p>Motivation. In practice, various downstream tasks in SSL field often require different granular features, such as coarse-or fine-grained features. For instance, general classification downstream tasks distinguish a category from other categories and typically desire coarse-grained features, while fine-grained classification often discriminates subordinate categories and needs more fine-grained features. Actually, many downstream tasks highly desire multi-granular features. Take the classification task on ImageNet-1K <ref type="bibr" target="#b20">[21]</ref> as an example. One needs coarse-grained features to distinguish a big category, e.g. dog, from other categories, e.g. bird and car, and also requires fine-grained features to discriminate different subordinate categories, such as Labrador and poodle in the dog category. However, this important multi-granularity requirement is ignored in the current state-of-the-art SSL approaches, including the representative contrastive learning family <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> and clustering learning family <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. For contrastive learning, its instance discrimination task only aims to distinguish individual instances for learning more instance-level fine-grained features, and does not consider the coarse-grained cluster structure in the data. As a result, it cannot well push semantically similar instances to be close either empirically <ref type="bibr" target="#b61">[62]</ref> or theoretically <ref type="bibr" target="#b49">[50]</ref>, impairing performance, especially for classification downstream tasks. The clustering learning family aims to cluster similar instances into the same cluster and thus learns cluster-level coarse-grained features. In this way, it cannot well handle the downstream tasks that require some fine-grained features. Therefore, in absence of prior feature preference of down stream tasks, one should build an SSL framework to learn multi-granular representation to well handle as many downstream tasks as possible. Contributions. In this work, we propose an effective MUlti-Granular Self-supervised learning (Mugs) framework to explicitly learn multi-granular features for visual data. Specifically, Mugs adopts three complementary granular supervisions: 1) instance discrimination supervision (IDS), 2) localgroup discrimination supervision (LGDS), and 3) group discrimination supervision (GDS). Inspired by contrastive learning, IDS distinguishes instances via scattering different instance features separately, and thus supervises instance-level fine-grained feature learning. To capture the higher-level finegrained feature which is also called the "local-group feature" in this work, Mugs proposes a novel and effective LGDS. LGDS aggregates the features of an instance and its few highly similar neighbors into a local-group feature through a small transformer. Then it brings local-group features of different crops from   the same image together and pushes them far away for others. This supervision enhances Mugs from two aspects: 1) it provides complementary instance supervision to the above IDS, since it enforces different crops of the same image to have highly similar neighbors, which is an extra challenging alignment, and can boost local-group semantic alignment; 2) it encourages highly-similar instances to constitute small local-groups and scatters these groups separately, boosting more discriminative semantic learning. Finally, GDS is designed to avoid the cases that similar local-groups are scattered randomly or far away. To this end, GDS brings similar samples together and thus pulls similar local-groups close, which captures coarse-grained features at a (semantic) group level. With these complementary supervisions, Mugs can well learn multi-granular data features which can accurately capture the data semantics, e.g. the shapes of "mugs" in ImageNet-1K as illustrated in <ref type="figure" target="#fig_6">Fig. 6</ref>, and also often enjoy better generality and transferability on diverse downstream tasks than single-granular features.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>, by only pretraining on ImageNet-1K, our Mugs sets a new state-of-the-art (SoTA) 82.1% linear probing accuracy on ImageNet-1K and surpasses the previous SoTA, i.e. iBOT <ref type="bibr" target="#b60">[61]</ref>, by a large margin 1.1%. Moreover, under different model sizes (see <ref type="figure" target="#fig_2">Fig. 2</ref> (a)) and pretraining epochs (see <ref type="figure" target="#fig_2">Fig. 2</ref> (b)), Mugs consistently improves previous SoTA pretrained on ImageNet 1K by a non-trivial 0.8% linear probing accuracy. Under other three classification settings, i.e. KNN, fine-tuning the whole network and semi-supervised learning, Mugs also beats previous best methods on the same model. In addition, Mugs shows its advantages on transfer learning, object detection, instance and semantic segmentation tasks. These results well testify the high quality, generality and transferability of the learnt features by Mugs. Note that in this work, we evaluate the effectiveness of Mugs through vision transformer (ViT) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>, as ViT often achieves better performance than CNN of the same model size <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b38">39]</ref> and also shows great potential to unify vision and language models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>As an effective family of SSL, contrastive learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b0">1]</ref>, e.g., MoCo <ref type="bibr" target="#b26">[27]</ref> and SimCLR <ref type="bibr" target="#b10">[11]</ref>, has gained much attention recently. Its key is an instance dis-crimination task which aims to train a network so that the positive pair, i.e. crops of the same image, are close but far from the negatives, namely the crops of other images. Later, BYOL <ref type="bibr" target="#b24">[25]</ref> trains a network by only bringing two positives close in the feature space without using any negatives. Though successful, these methods only distinguish individual instances to learn fine-grained feature, and often cannot well push semantically similar instances close, impairing their performance.</p><p>Another line of SSL is clustering learning <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b36">37]</ref>, which assigns pseudo cluster labels for each sample and then trains a network to learn unsupervised representation. For instance, deepcluster <ref type="bibr" target="#b7">[8]</ref> uses k-means to cluster all data features and generates pseudo clustering labels which are then used to train a network. PCL <ref type="bibr" target="#b34">[35]</ref> and SwAV <ref type="bibr" target="#b6">[7]</ref> integrate scalable online clustering approach with contrastive learning via learning cluster prototypes as the positive and negative instances for queries, and achieve promising performance. Recently, DINO <ref type="bibr" target="#b9">[10]</ref> and MST <ref type="bibr" target="#b35">[36]</ref> propose a much simple online labeling framework that generates pseudo-labels via a momentum teacher. Unfortunately, this clustering family often learns cluster-level coarse-grained (semantic) features, and cannot well handle the downstream tasks which desire fine-grained features.</p><p>Finally, the recently proposed masked auto-encoder (MAE) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20</ref>,2] is a new SSL family. It builds on a reconstruction task that randomly masks image patches and then reconstructs the missing pixels or semantic features via an auto-encoder. However, it emphasizes local region reconstruction, and lacks semantic discrimination ability. As a result, for adapting to downstream tasks via only fine-tuning a task head at the top of the pretrained backbone, it performs much worse than contrastive and clustering learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b60">61]</ref>. Indeed, to achieve satisfactory performance, this reconstruction family needs to fine-tune the whole pretrained network for learning global semantics which are necessary for many downstream tasks, e.g. classification. But this requires much higher extra training cost, and also results in very different models for different downstream tasks, which destroys model compatibility and increases practical deployment cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-granular self-supervised learning</head><p>Here we first introduce the overall framework of our MUlti-Granular Self-supervised learning (Mugs), and then elaborate on its three granular supervisions. We evaluate the effectiveness of Mugs through ViT <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> and thus will take ViT as an example to introduce Mugs. This is because 1) with similar model size, ViT shows better performance than CNN <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58]</ref>; 2) ViT shows great potential for unifying the vision and language models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Framework</head><p>Here we first introduce our motivation. As discussed in Sec. 1, different downstream tasks, e.g. general classification and its fine-grained variant, often require different granular features, e.g. coarse-or fine-grained one. More importantly, many real downstream tasks actually highly desire multi-granular feature. For instance, for classification problems, one needs coarse-grained feature to discriminate animals from other big categories, e.g. plants, since one big category often  <ref type="figure">Fig. 3</ref>: Overall framework of Mugs. (a) shows the overall framework. For each image, Mugs respectively performs two random augmentations and feeds two crops into backbones of student and teacher. Next, it adopts three granular supervisions/losses: 1) instance discrimination supervision, 2) local-group discrimination supervision, and 3) group discrimination supervision. Teacher is updated via exponential moving average of student. "sg" denotes stop-gradient.</p><p>(b) shows the pipeline of local-group modules in both student and teacher. This module averages all patch tokens, and then finds top-k neighbors from memory buffer. Next, it uses a transformer to aggregate the average and its k neighbors to obtain a local-group feature (class token) and feeds it into a local-group head.</p><p>share much coarse-grained features, e.g. legs and head in animals, leaves and multi-branches in plants. Then to distinguish subcategories in each big category, e.g. dogs and wolves, a slightly lower-level coarse-grained (high-level fine-grained) feature is necessary. Finally, to further discriminate different species in each subcategory, e.g. Labrador dog and poodle dog, more fine-grained feature is actually required. Unfortunately, this important requirement for multi-granular features is seldom brought to front and even totally ignored in existing SSL methods.</p><p>To alleviate this issue, we propose a simple but effective Mugs framework to learn multi-granular features, which can better satisfy different granular feature requirements of various downstream tasks and thus enjoy higher transferability and generality than single-granular features. As illustrated in <ref type="figure">Fig. 3</ref> (a), given an image x, Mugs independently performers augmentations T t and T s to obtain its two crops x 1 and x 2 . Next, it respectively feeds x 1 and x 2 into the teacher and student backbones, and obtains their corresponding representations y 1 and y 2 which contain class and patch tokens. Finally, Mugs builds three granular supervisions: 1) instance discrimination supervision for instance-level fine-grained features, 2) local-group discrimination supervision for high-level fine-grained features at a local-group level, 3) group discrimination supervision for coarse-grained semantic features at a (semantic) group level. In this way, Mugs can learn multi-granular features, and thus better handles as many downstream tasks as possible, in contrast with SSL methods that only consider single-granular features, e.g. MoCo <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref> for instance discriminative fine-grained features and deepclustering <ref type="bibr" target="#b7">[8]</ref>/DINO <ref type="bibr" target="#b9">[10]</ref> for group-discriminative coarse-grained features. Next, we will introduce our three granular supervisions which help Mugs learn multi-granular features in a complementary manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-granular supervisions</head><p>Instance discrimination supervision. With this supervision, Mugs regards each instance as a unique class of its own which is our finest level of granularity. Accordingly, it pulls the random crops of the same instance together and pushes the crops from different images away in the feature space. In this way, it approximately scatters the instance features separately from the chaos distribution on the spherical surface as shown in <ref type="figure">Fig. 3 (a)</ref>, which also accords with the empirical and theoretical observations in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48]</ref> and our experimental results in <ref type="figure" target="#fig_3">Fig. 4</ref> of Sec. 4. To implement this supervision on the instance x, Mugs respectively feeds two class tokens y c 1 and y c 2 in the two features y 1 and y 2 into their corresponding instance heads h t in and h s in as shown in <ref type="figure">Fig. 3</ref> (a). Next, Mugs additionally passes h s in (y c 2 ) into an extra prediction head p in which could alleviate the side effects of feature alignment upon the generality of the feature learnt by student or teacher backbone. Finally, following MoCo <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, Mugs employs InfoNCE loss <ref type="bibr" target="#b44">[45]</ref> for instance discrimination:</p><formula xml:id="formula_0">L instance (x 1 , x 2 ) = ? log exp(cos(z 1 , z 2 )/? in ) exp(cos(z 1 , z 2 )/? in ) + z?B in exp(cos(z 2 , z)/? in ) ,<label>(1)</label></formula><p>where z 1 = h t in (y c 1 ), z 2 = p in (h s in (y c 2 )), and ? in is a temperature. The buffer B in stores the negative instances of z 2 , and is updated by the historical minibatch features {z 1 } generated by teacher in a first-in and first-out order. In this way, Mugs pushes the crop x 2 away from its negative keys (other instances) in the buffer B in while pulling together its positive crop x 1 . So it can encourage the model to learn fine-grained features, and boost instance-level feature diversity.</p><p>Local-group discrimination supervision. As explained in Sec. 3.1, finegrained features are often insufficient for diverse downstream tasks, e.g. classification, due to lack of sufficient high-level data semantics. In this work, to learn higher-level fine-grained features, also called "local-group features" here, Mugs proposes a novel and effective local-group supervision. Intuitively, as shown in the third sphere of <ref type="figure">Fig. 3</ref> (a), this supervision encourages instance features to have small but separately scattered local-group structures, i.e. small/large distance among highly similar/dissimilar samples. See detailed explanation at the end of this subsection. Accordingly, it helps capture slightly higher-level semantics in the data compared with instance discrimination supervision. Next, we will first introduce its details and then discuss its benefits to Mugs.</p><p>As shown in <ref type="figure">Fig. 3</ref> (a), given the crop x 1 of image x, the teacher backbone outputs y 1 which contains class token y c 1 and patch tokens y p 1 . Similarly, Mugs feeds another crop x 2 of x into student backbone to obtain y 2 consisting of class token y c 2 and patch tokens y p 2 . Next, Mugs respectively averages the patch tokens y p 1 and y p 2 to obtain two average tokens y 1 and y 2 as illustrated in <ref type="figure">Fig. 3</ref> (b). Here we use the average of patch tokens instead of the class token, as we find that the average token often contains more information and finds more accurate neighbors in the next step. Note, for a CNN backbone, we can average the last feature map to obtain y 1 and y 2 . On the other hand, Mugs uses a memory buffer B lg to store the historical minibatch average tokens {y 1 } and {y 2 } in a first-in and first-out order. Then as shown by <ref type="figure">Fig. 3</ref> (b), for both y 1 and y 2 , Mugs respectively finds their corresponding top-k neighbors</p><formula xml:id="formula_1">{y 1,i } k i=1 and {y 2,i } k i=1</formula><p>from the buffer B lg . Next, it respectively uses a transformer to aggregate the average token and its k neighbors as follows:</p><formula xml:id="formula_2">y * 1 = g t transformer (y 1 ; {y 1,i } k i=1 ) and y * 2 = g s transformer (y 2 ; {y 2,i } k i=1 ). (2) Here g t transformer (y 1 ; {y 1,i } k i=1</formula><p>) denotes a 2-layered vanilla ViT without any patch embedding layers, and has input class token y 1 , input patch tokens {y 1,i } k i=1 and output class token y * 1 . Since the new representation y * 1 comes from y 1 and its top-k neighbors {y 1,i } k i=1 which together constitute a local group of y 1 , y * 1 is also called a "local group feature". g s transformer (y 2 ; {y 2,i } k i=1 ) has the same function. Finally, Mugs pulls these two local-group features y * 1 and y * 2 from the same instance x together and pushes away the local-group features of other instances. Similarly, Mugs adopts the InfoNCE loss <ref type="bibr" target="#b44">[45]</ref> to achieve this target:</p><formula xml:id="formula_3">L local-group (x 1 , x 2 ) = ? log exp(cos(z 1 , z 2 )/? lg ) exp(cos(z 1 , z 2 )/? lg )+ z?B lg exp(cos(z 2 , z)/? lg ) ,<label>(3)</label></formula><p>where z 1 = h t lg (y * 1 ) and z 2 = p lg (h s lg (y * 2 )). h t lg and h s lg are two projection heads and p lg is a prediction head. Buffer B lg also stores the historical local-group features {y * 1 } produced by teacher in a first-in first-out order. This supervision benefits Mugs from two aspects. Firstly, it provides complementary instance supervision to the above instance discrimination supervision. It brings two local-group features y * 1 and y * 2 from the same instance x together, where local-group features y * 1 /y * 2 are the aggregation of the crop x 1 /x 2 and its top-k nearest neighbors. So to achieve small local-group supervision loss L local-group (x 1 , x 2 ), the two crops x 1 and x 2 of x should have very similar top-k neighbors. This means besides the crops themselves, their corresponding neighbors should also be well aligned, which is an extra challenging alignment problem compared with instance discrimination supervision and enhances local-group semantic alignment. Secondly, it encourages highly-similar instances to form localgroups and scatters these local-groups separately, increasing the semantic discrimination ability of the learnt feature. This is because 1) this supervision uses a small k (around 10) for neighbors such that the samples in the same local-group are highly similar and have small distance, helping form local-groups; 2) this supervision further pushes current local-group feature away local-group features from other instances, and thus scatters different local-groups separately. With these two aspects, this local-group discrimination supervision boosts higher-level fine-grained feature learning by considering the local-group structures in data.</p><p>Group discrimination supervision. This supervision is the most coarse level supervision in Mugs. Intuitively, as shown in the last sphere in <ref type="figure">Fig. 3 (a)</ref>, it tar-gets at clustering semantically similar instances and local-groups into the same big group/cluster which could reveal more global semantics in data compared with the instance and local-group supervisions. In the following, we first introduce its details and then discuss the co-effects of the three granular supervisions.</p><p>For the instance x, Mugs respectively feeds the class token y c 1 in the feature y 1 from teacher backbone and the class token y c 2 in y 2 from student backbone into two group heads h t g and h s g . Then, it builds a set of learnable cluster prototypes {c i } m i=1 and computes soft pseudo clustering labels in an online manner:</p><formula xml:id="formula_4">p t i = exp(?(h t g (y c 1 )) ? c i /? g ) m i=1 exp(?(h t g (y c 1 )) ? c i /? g ) and p s i = exp(h s g (y c 2 ) ? c i /? g ) m i=1 exp(h s g (y c 2 ) ? c i /? g )</formula><p>. <ref type="formula">(4)</ref> Here the function ?(h t g (y c 1 )) = h t g (y c 1 )?p ema is to increase the diversity of the feature h t g (y c 1 ) and thus sharpens the soft pseudo label p t in Eqn. (4), where p ema denotes the estimated average statistics of all past h t g (y c 1 ) via an exponential moving average of the mini-batch B, namely,</p><formula xml:id="formula_5">p ema ? ? ? p ema + (1 ? ?) ? 1 |B| p t ?B p t with a constant ? ? [0, 1]</formula><p>. Such a technique is shown to be useful in <ref type="bibr" target="#b9">[10]</ref>. Next, similar to a supervised classification task, Mugs employs the cross-entropy loss but with soft labels as its training loss:</p><formula xml:id="formula_6">L group (x 1 , x 2 ) = ? m i=1 p t i log (p s i ) .<label>(5)</label></formula><p>Now we put our three granular supervisions together and discuss their coeffects on representation learning which also distinguishes it from existing methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref>. As aforementioned, instance discrimination supervision is to pull the crops of the same image together and to approximately scatter the instance features separately on the spherical surface as shown in <ref type="figure">Fig. 3</ref> (a). It helps Mugs learn instance-level fine-grained features. Next, the local-group discrimination supervision first provides complementary supervision for instance discrimination supervision by encouraging crops of the same instance to have highly similar neighbors. Then, as shown in the third sphere in <ref type="figure">Fig. 3 (a)</ref>, the local-group supervision scatters different local-groups formed by crops and its neighbors separately to boost the semantic discrimination ability of these local-groups. This supervision mainly learns higher-level fine-grained features at a local-group level. Finally, to avoid similar local-groups to be scattered randomly or far away, the group discrimination supervision brings similar samples together and thus pulls similar local-groups close, as intuitively illustrated by the last sphere in <ref type="figure">Fig. 3</ref> (a). It is responsible to capture the coarse-grained features at a (semantic) group level. With these three granular supervisions, Mugs can well learn three different but complementary granular features, which are characterized by better generality and transferability on the various kinds of downstream tasks compared with single-granular features. Compared with existing methods, e.g. MoCo <ref type="bibr" target="#b17">[18]</ref> and DINO <ref type="bibr" target="#b9">[10]</ref>, the main novelties of Mugs lie in two folds: 1) Mugs learns multi-granular representation via three complementary supervisions and can often better handle diverse downstream tasks than the existing methods that often learn single-granular feature; 2) Mugs proposes a novel and effective local-group supervision which complements both instance and group discrimination supervisions and benefits Mugs from two aspects as discussed above.</p><p>Overall training objective. Now we introduce the overall training loss: <ref type="bibr" target="#b5">(6)</ref> where the three constants ? in , ? lg and ? g are to trade-off the three supervisions. For simplicity, we set ? in = ? lg = ? g = <ref type="bibr">1 3</ref> in all experiments. We then can minimize the objective L(x 1 , x 2 ) to optimize student network. Teacher network is updated via the exponential moving average of corresponding parameters in student.</p><formula xml:id="formula_7">L(x 1 , x 2 ) = ? in L instance (x 1 , x 2 ) + ? lg L local-group (x 1 , x 2 ) + ? g L group (x 1 , x 2 ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here we present the performance evaluation of our Mugs on benchmark classification tasks, transfer learning task, delectation and segmentation tasks, and video segmentation task with comparison against several representative stateof-the-art SSL approaches. The code and models will be released soon.</p><p>Architectures. As aforementioned, we use ViT <ref type="bibr" target="#b22">[23]</ref> to evaluate our Mugs. For the instance and local-group discriminations, their four projection heads, i.e., h t in , h s in , h t lg and h s lg , are all 3-layered MLPs with hidden/output dimension 2,048/256, and their prediction heads p in and p lg are all 2-layered MLPs with hidden/output dimension 4,096/256. For group discrimination, its projection heads h t g and h s g , are 3-layered MLP with hidden/output dimension of 2,048/256. Transformers g t transformer and g s transformer have 2 layers and have a total input token number of 9 as we set k = 8 for the neighbors. For the three buffers (B in , B lg and B lg ) and the prototypes {c i } m i=1 , their sizes are all 65,536. Pretraining setup. We pretrain Mugs on the training data of ImageNet-1K <ref type="bibr" target="#b20">[21]</ref> without labels. Following <ref type="bibr" target="#b60">[61]</ref>, we pretrain ViT-S/16 for 800 epochs, ViT-B/16 for 400 epochs, and ViT-L for 250 epochs. Following DINO and iBOT, we use symmetric training loss, i.e. <ref type="bibr" target="#b0">1</ref> 2 (L(x 1 , x 2 ) + L(x 2 , x 1 )). We use AdamW optimizer <ref type="bibr" target="#b40">[41]</ref> with a momentum of 0.9, a weight decay of 0.1, and a cosine schedule <ref type="bibr" target="#b39">[40]</ref>. For data augmentation, we adopt weak augmentation in DINO <ref type="bibr" target="#b9">[10]</ref> to implement T t in teacher, and use strong augmentation (mainly including Au-toAugment <ref type="bibr" target="#b18">[19]</ref>) in DeiT <ref type="bibr" target="#b48">[49]</ref> as the augmentation T s in student. Following conventional multi-crop setting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b60">61]</ref>, we crop each image into 2 large crops of size 224 and 10 extra small crops of size 96. For both large crops, we feed each of them into teacher and use its output to supervise the student's output from the other 11 crops. For two-crop setting, <ref type="table">Table 9</ref> in Appendix A reports the results and shows the superiority of Mugs over SoTAs.</p><p>For Mugs, we follow MoCo to set ? in = ? lg = 0.2 in the infoNCE loss, and follow DINO to set ? g = 0.1 and linearly warm up ? g from 0.04 to 0.07. We set the neighbor number k = 8, and set ? = 0.9 in group discrimination. Mugs has almost the same training cost with DINO, e.g. about 27 hours with 8 A100 GPUs for 100 pretraining epochs on ViT-S/16, as our projection/prediction heads and transformers g transformer are much smaller than the backbone. See more details of the augmentation, multi-crop loss, and pretraining cost in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on ImageNet-1K</head><p>Here we compare Mugs with SoTAs on ImageNet-1K under four classification settings, i.e., linear probing, KNN, fine-tuning, and semi-supervised learning. <ref type="table" target="#tab_3">Table 1</ref>: Linear probing and k-NN accuracy (%) on ImageNet-1K. "Dataset" denotes which dataset is used to pretrain. "Epo." is the effective pretraining epochs adjusted by number of views processed by the models following <ref type="bibr" target="#b60">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Arch. Linear Probing. It trains a linear classifier on top of frozen features generated by the backbone, e.g. ViT, for 100 epochs on ImageNet-1K. We follow DINO and iBOT, and use SGD with different learning rates for different models. Fine-tuning. Besides linear probing and KNN, fine-tuning is proposed recently to further testify performance of a pretrained model. It allows for optimizing the pretrained backbone with a linear classifier. Following BEiT <ref type="bibr" target="#b2">[3]</ref>, DINO and iBOT, we use AdamW optimizer with layer-wise learning rate decay to train ViT-S/ViT-B/ViT-L for 200/100/50 epochs on ImageNet-1K. <ref type="table" target="#tab_4">Table 2</ref> summarizes the classification results, in which "Supervised" means randomly initializing model parameters instead of using pretrained backbone and its results are quoted  Semi-supervised learning. Here we respectively use 1%/10% training data of ImageNet-1K to fine tune the pretrained backbones, and then evaluate on the test data. Following DINO and iBOT, we consider two settings: 1) training a logistic regression classifier on frozen features; and 2) fine-tuning the whole pretrained backbone. <ref type="table" target="#tab_5">Table 3</ref> shows that for both 1% and 10% training data, our Mugs consistently surpasses previous SoTAs under both two settings. Notably, under fine-tuning setting with 1% labeled data, Mugs improves iBOT by a significant 4.9% accuracy, and shows its advantages for low-shot learning.</p><p>Result Analysis. <ref type="figure" target="#fig_3">Fig. 4</ref> uses T-SNE <ref type="bibr" target="#b41">[42]</ref> to reveal the feature differences among MoCo-v3, DINO, iBOT, and Mugs, in which each color denotes a unique class. From the last subfigure, one can find that for one class, Mugs often divides it into several small clusters in the feature space, such as 6 clusters for brown, 4 for purple, 6 for red, and 5 for blue, and scatters these small clusters in a big class. This partially reveals multi-granular structures in the feature: classes are separately scattered, which corresponds to a group-level coarse granularity; several small scattered clusters in a class show a local-group-level fine granularity; and some separate instances in a cluster reveal an instance-level fine granularity. In contrast, MoCo-v3, DINO and iBOT often do not show this multi-granular feature structure. Hence, for some challenging classes denoted by green, red and purple colors, our Mugs can well distinguish them, while MoCo-v3, DINO and iBOT cannot. This is because instead of regarding the class as a whole, Mugs utilizes its multi-granular supervisions to consider the multi-granular (hierarchical) data semantic structures and divide the whole class into several easilydistinguishable clusters in the pretraining phase. Differently, MoCo-v3, DINO and iBOT ignore the multi-granular semantic structures and only uses one granular supervision which often could not well handle the challenging classes. <ref type="figure" target="#fig_3">Fig. 4</ref> (a) further visualizes the self-attention of ViT-B/16. One can observe Mugs can well capture object shapes and thus their semantics. See more details and examples in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on downstream tasks</head><p>Due to limited space, we defer training and implementation details to Appendix.</p><p>Transfer learning. Here we investigate the transferability of the models pretrained by Mugs. Specifically, we pretrain the model on ImageNet-1K, and then fine-tune the pretrained backbone on various kinds of other datasets with same protocols and optimization settings in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref>. <ref type="table">Table 4</ref> summarizes the classification accuracy, in which "Sup." denotes the setting where we pretrain the backbone on ImageNet-1K in a supervised manner and then fine tune backbone on the corresponding dataset. <ref type="table">Table 4</ref> shows our Mugs surpasses SoTAs on the first five datasets and achieves comparable accuracy on the Car dataset.</p><p>Object detection &amp; Instance segmentation. Now we evaluate Mugs on object detection and instance segmentation on COCO <ref type="bibr" target="#b37">[38]</ref>, in which both tasks emphasize the ability to locate and discriminate objects. For fairness, we use the same protocol in <ref type="bibr" target="#b60">[61]</ref> which builds on Cascade Mask R-CNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref> to produce bounding boxes and instance masks simultaneously. See optimization settings in   Appendix B. Besides SSL approaches, e.g. MoBY <ref type="bibr" target="#b54">[55]</ref>, we also compare supervised baselines, Swin-T/7 <ref type="bibr" target="#b38">[39]</ref> with similar model size as ViT-S/16. <ref type="table" target="#tab_7">Table 5</ref> shows that on detection, Mugs achieves 49.8 AP b and makes 0.4 AP b improvement over the runner-up, i.e. iBOT. <ref type="figure" target="#fig_3">Fig. 4 (b)</ref> shows that Mugs can accurately locate and classify objects in COCO. For instance segmentation, Mugs also surpasses all compared baselines and improves 0.4 AP m over the best baseline.</p><p>Semantic segmentation. We transfer the pretrained model to semantic segmentation task on the ADE20K dataset <ref type="bibr" target="#b59">[60]</ref>. Following <ref type="bibr" target="#b60">[61]</ref>, we stack the task layer in UPerNet <ref type="bibr" target="#b52">[53]</ref> and fine-tune the whole backbone. <ref type="table" target="#tab_7">Table 5</ref> reports the mean intersection over union (mIoU) on all semantic categories. Mugs consistently outperforms the compared SoTAs by making significant 2.0 mIoU improvement. <ref type="figure" target="#fig_3">Fig. 4 (c)</ref> shows that Mugs can capture the object shape accurately.</p><p>Video object segmentation via nearest neighbor retrieval. Besides images, we further evaluate the transferability of the frozen pretrained features on videos. Following DINO, we find nearest neighbors to segment objects in the video, since one can propagate segmentation masks via retrieving nearest neighbor between consecutive video frames <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_8">Table 6</ref> reports the mean region similarity J m and mean contour-based accuracy F m on the DAVIS-2017 video segmentation dataset <ref type="bibr" target="#b46">[47]</ref> by using ViT-B/16. One can see that Mugs enjoys better feature transferability than DINO and iBOT even for video segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Here we investigate the effects of each granular supervision in Mugs. Specifically, we train Mugs for 1,00 epochs on ImageNet-1K and report the linear probing accuracy in <ref type="table" target="#tab_9">Table 7</ref>. One can observe that by independently removing each granular supervision, namely, the instance, local-group and group supervision,  the performance of Mugs degenerates, which shows the benefit of each granular supervision, especially for the local-group supervision. Next, we compare Mugs with DINO and iBOT under different augmentations and also show the effects of augmentations. Specifically, for the augmentation T s in student network of Mugs/DINO/iBOT, we implement it by strong or weak augmentation mentioned at the beginning of Sec. 4; for augmentation T t in teacher, we always use weak augmentation. See more implementation details in Appendix B, especially for iBOT. We pretrain all methods for 1,00 epochs on ImageNet-1K. <ref type="table" target="#tab_10">Table 8</ref> shows four observations. 1) Under weak or strong augmentation, Mugs consistently outperforms DINO and iBOT. 2) For all three methods, strong augmentation slightly improves their performance under weak augmentation, showing the effectiveness of our strong augmentation technique on ViTs. 3) Mugs using weak augmentation surpasses iBOT that adopts both weak augmentation and random mask augmentation. 4) Under weak augmentation, Mugs improves 1.5% over DINO which means it is the multi-granular supervisions of Mugs that contributes this 1.5% improvement. Then by using strong augmentation, Mugs surpasses DINO using weak augmentation by 2.2%, showing strong augmentation only contributes 0.7% improvement over DINO. So compared with the strong augmentation technique, the multi-granular supervision framework of Mugs largely contributes to Mugs and is the key factor to the significant improvement of Mugs over DINO and iBOT.</p><p>Finally, we evaluate Mugs without multi-crop augmentation, i.e. only using two crops of size 224?224 for pretraining. <ref type="table">Table 9</ref> in Appendix A shows that Mugs also surpasses the SoTAs, including DINO and iBOT, on ViTs under the same setting, which also demonstrates the superiority of Mugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose Mugs to learn multi-granular features via three complementary granular supervisions: instance discrimination supervision (IDS), localgroup discrimination supervision (LGDS), and group discrimination supervision (GDS). IDS distinguishes different instances to learn fine-grained features.</p><p>LGDS considers the local-group around an instance and then discriminates different local-groups to extract higher-level fine-grained features. GDS clusters similar samples and local-groups into one cluster to capture coarse-grained global group semantics. Experimental results have testified the advantages of Mugs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experimental Results</head><p>Due to space limitation, we defer more experimental results to this appendix.</p><p>Here we first investigate the performance of Mugs without multi-crop augmentations which is widely used in several representative works, and further compare <ref type="table">Table 9</ref>: Linear probing accuracy (%) and k-NN accuracy (%) on ImageNet-1K without multi-crop augmentation (left) and with multi-crop augmentation (right). "Epo" is the effective pretraining epochs adjusted by number of views processed by the models following <ref type="bibr" target="#b60">[61]</ref>.  <ref type="table" target="#tab_3">Table 10</ref>: Fine-tuning classification accuracy (%) on ImageNet-1K. All methods are pretrained on ImageNet-1K. "Epo." is the effective pretraining epochs adjusted by number of views processed by the models following <ref type="bibr" target="#b60">[61]</ref>. it with other methods, include iBOT and DINO under the same setting. Then we present more visualization results, including T-SNE clustering visualization, attention visualization of multi-heads in ViT, and object detection and segmentation visualization. We hope these visualization results can help readers intuitively understand the learnt features by Mugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Comparison w/o and w/ Multi-Crop Augmentation</head><p>Here we first investigate the performance of Mugs without the multi-crop augmentation which is widely used in several representative works, and further compare it with other SoTA methods, include iBOT and DINO under the same setting. Specifically, for Mugs without multi-crop augmentation, it only uses two 224-sized crops for pretraining. The left table in <ref type="table">Table 9</ref> reports the results of all compared methods without multi-crop augmentation, while the right one summarizes the results under multi-crop augmentation setting. By comparison, one can observe that without multi-crop augmentation, Mugs still consistently achieves the highest accuracy under both linear probing setting and KNN setting. Specifically, Mugs improves the runner-up, namely iBOT, by respectively 0.8% and 0.5% under linear probing and KNN evaluation settings. More importantly, we can observe that Mugs without multi-crop augmentation even achieves very similar results as DINO with multi-crop augmentation. All these results are consistent with those results in <ref type="table" target="#tab_3">Table 1</ref> in the manuscript, and well demonstrate the superiority of Mugs over previous state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison under Fine-tuning Setting</head><p>In the manuscript, we already compare Mugs with state-of-the-art approaches on the ViT-S/16 and ViT-B/16 under the fine-tuning setting. Due to limited space, we defer the comparison among Mugs and others on ViT-L/16 into <ref type="table" target="#tab_3">Table 10</ref>. This setting allows us to optimize the pretrained backbone with a linear classifier. Following BEiT <ref type="bibr" target="#b2">[3]</ref>, DINO and iBOT, we use AdamW optimizer with layer-wise learning rate decay to train ViT-L for 50 epochs on ImageNet-1K. On ViT-L, Mugs achieves 85.2% top-1 accuracy, and surpasses all contrastive learning and clustering learning methods. One can also observe that on ViT-L, most of the reconstruction methods achieves higher accuracy than constrictive or clustering learning approaches, including iBOT and our Mugs. There are two possible reasons. Firstly, the reconstruction methods use much more computations for pretraining than constrictive or clustering learning approaches. Specifically, the reconstruction family always use 224?224-sized images to pretrain the model, while constrictive or clustering learning approaches uses multi-crop augmentations which contains two 224-sized images and ten 96-sized images.</p><p>Since "Epo." in <ref type="table" target="#tab_3">Table 10</ref> is the effective pretraining epochs adjusted by number of views processed by the models <ref type="bibr" target="#b60">[61]</ref> which means each 96-sized image equals to one 224-sized image in terms of the defined "epochs", with the same pretraining epochs, the computation cost of the reconstruction approaches is much more. Actually, from <ref type="table" target="#tab_3">Table 10</ref>, the reconstruction methods have much more effective pretraining epochs than constrictive or clustering learning approaches, e.g. 1600 epochs in data2vec v.s. 1000 epochs in iBOT &amp; Mugs, which further increases the training cost. Secondly, for large models, using small-sized images, e.g. ten 96-sized images in multi-crop augmentations, may lead to overfitting issue in contrastive or clustering learning approaches. Specifically, from <ref type="table" target="#tab_3">Table 1</ref> in manuscript and <ref type="table" target="#tab_3">Table 10</ref> here, once can observe that on relatively small models, such as ViT-S and ViT-B, SoTA contrastive learning or clustering methods, such as Mugs and iBOT, outperform the reconstruction methods, even though the formers have much less pretraining cost as mentioned above. But on large models, e.g. ViT-L, the superiority of SoTA contrastive or clustering learning methods disappears. One possible reason for these inconsistent observation is that large model needs more pretraining epochs for learning semantic features, and could suffer from over-fitting problem when using 96-sized crops, since 1) large model is capable to memory all images as demonstrated in many works; and 2) 96-sized crops may contain incomplete semantics of the vanilla image and lead to over-fitting issue, especially under insufficient pretraining epochs. Note, as explained at the end of Sec. 2 in manuscript, this fine-tuning setting needs much higher extra training cost, and also destroys model compatibility for deployment. Therefore, in this work, we do not further push Mugs's limits on the large models which needs huge training cost as the reconstruction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More T-SNE Visualization Results</head><p>Same with <ref type="figure" target="#fig_3">Fig. 4</ref> in the manuscript, here we use T-SNE <ref type="bibr" target="#b41">[42]</ref> to reveal the feature differences among MoCo-v3, DINO, iBOT, and Mugs in <ref type="figure" target="#fig_5">Fig. 5</ref>. By comparison, Mugs often can scatter the samples from different classes more separately, while keeping the samples in the same class close in the feature space. This could means that our Mugs can better distinguish different classes than MoCo-v3, DINO and iBOT, and thus shows higher performance. The potential reason behind this observation is explained in manuscript. That is, instead of regards the class as a whole, Mugs utilizes its multi-granular supervisions to consider the multigranular (hierarchical) data semantic structures and divides the whole class into several clusters for easily discriminating in the pretraining phase. Differently,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo-v3 DINO iBOT Mugs</head><p>(a) T-SNE visualization of 19 classes of insects, e.g. beetle, butterfly, stick, and cricket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo-v3 DINO iBOT Mugs</head><p>(b) T-SNE visualization of 20 classes of various monkeys, e.g. gibbon, siamang, patas, and gorilla.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo-v3 DINO iBOT Mugs</head><p>(c) T-SNE visualization of 17 classes of various birds, e.g. junco, robin, jay, cock, and ostrich. MoCo-v3, DINO and iBOT ignore the multi-granular semantic structures and only uses one granular supervision which often could not well handle the challenging classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 More Attention Visualization Results</head><p>Here same with <ref type="figure" target="#fig_3">Fig. 4</ref> in the manuscript, we visualize more self-attention map of the 12 self-attention heads in ViT-B/16 pretrained by Mugs in <ref type="figure" target="#fig_6">Fig. 6</ref>. The first column denotes the vanilla images, while each column of the last 12 columns denote the self-attention score maps of each individual head. The second column combines the 12 self-attention score maps from 12 heads into one, and also sets a threshold to remove some noises via only keeping top attention score. From these visualizations, one can observe that by using Mugs for pretraining, the overall self-attention of 12 heads can capture the object shapes very well. For example, from the first bird image, it is even hard for human to get the bird location at the first glance, due to the similar color of the bird and the flowers. But the ViT-B/16 pretrained by Mugs still can well locate the bird and also capture the bird shape. Moreover, one can also compare the attention visualization of Mugs with state-of-the-arts, e.g. iBOT. In iBOT <ref type="bibr" target="#b60">[61]</ref>, <ref type="figure" target="#fig_0">Fig. 18</ref> in their appendix also visualizes the self-attention map. By comparison, the model pretrained by Mugs can better separate the object from background. These results testify that   ViT-B/16 pretrained by Mugs can capture semantics in data even without any manual labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 More Visualization Results on Object Detection and Semantic Segmentation</head><p>In the manuscript, we already provide some object detection and segmentation examples in <ref type="figure" target="#fig_3">Fig. 4</ref>. Here we give more examples. <ref type="figure" target="#fig_7">Fig. 7</ref> shows more object detection examples on the COCO datasets, where we use the ViT-B/16 pretrained by our Mugs. From these results, one can observe that Mugs not only accurately locate the objects in the images but also precisely recognizes these objects. For semantic segmentation on ADE20K, <ref type="figure" target="#fig_8">Fig. 8</ref> visualizes more examples. We also can find that Mugs can capture the object shape accurately and thus well captures the semantics of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental details</head><p>Due to space limitation, we defer more experimental details to this section. Here we first provide more details for pretraining. Then we present the experimental details for various kinds of downstream tasks. Finally, we also give the details for ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 More Details for Pretraining</head><p>Augmentation. Here the weak augmentation in our teacher backbone refers to the augmentations used in many SSL works, e.g. DINO <ref type="bibr" target="#b9">[10]</ref>, which includes random crop, color jitter, Gaussian noise, horizontal flipping and gray scaling. The hyper-parameters in these augmentation operations are the same with those in DINO <ref type="bibr" target="#b9">[10]</ref>. The strong augmentation in our student backbone is the combination of AutoAugment <ref type="bibr" target="#b18">[19]</ref>  Loss for multi-crop setting. Following conventional multi-crop setting [9,10,61], we crop each image into 2 large crops of size 224 and 10 extra small crops of size 96. Then to construct the overall pretraining loss 6 in manuscript, we regard one large crop as x 1 and respectively take the remaining 11 crops as x 2 . Then symmetrically, we view another large crop as x 1 and respectively take the remaining 11 crops as x 2 . Finally, we average these loss to obtain the overall training loss.</p><p>Hyper-parameter settings for Pretraining. For all experiments, we use AdamW optimizer <ref type="bibr" target="#b40">[41]</ref> with a momentum of 0.9 and a cosine learning rate schedule <ref type="bibr" target="#b39">[40]</ref>. We also linearly warm up the learning rate at the first 10 epochs from 10 ?6 to its base value, and then decay it with a cosine schedule <ref type="bibr" target="#b39">[40]</ref>. For ViT-S and ViT-B, we use a minibatch size of 1024, a base learning rate of 8?10 ?4 , and a weight decay of 0.1. For ViT-L, due to our limited computational resource, we use a minibatch size of 640, a base learning rate of 1.5 ? 10 ?4 , and a weight decay of 0.08. For all experiments, the learning rate of the patch embedding layer is 5? smaller than the base learning rate. This strategy is demontrated to be useful for stabling training in MoCo-v3 <ref type="bibr" target="#b17">[18]</ref>. For drop path rate, we set it as 0.1/0.2/0.4 for ViT-S/B/L respectively. We set clip gradient as 3.0 for ViT-S/B and 0.3 for ViT-L. For Mugs, we follow MoCo to set ? in = ? lg = 0.2 in the infoNCE loss, and follow DINO to set ? g = 0.1 and linearly warm up ? g from 0.04 to 0.07. We set the neighbor number k = 8, and set ? = 0.9 to estimate the center p ema in group discrimination. All these settings are almost the same as DINO for simplicity which reduces hyper-parameter tuning and saves computational budget.  Semi-supervised learning. Following DINO and iBOT, we consider two settings: 1) training a logistic regression classifier on frozen features; and 2) finetuning the whole pretrained backbone. For logistic regression classifier, we use AdamW optimizer with total minibatch size 1024 and weight decay 0.05, under both 1% and 10% training data settings. We sweep the learning rate {0.03, 0.06, 0.10, 0.2}. For fine-tuning 1000 epochs on ViT-S/16, we also use AdamW optimizer with total minibatch size 1024 and weight decay 0.05 under both 1% and 10% training data settings. We respectively set learning rate as 2 ? 10 ?6 and 5 ? 10 ?6 for 1% and 10% training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 More Details for Downsteam Tasks</head><p>Transfer learning. We pretrain the model on ImageNet-1K, and then fine-tune the pretrained backbone on various kinds of other datasets with same protocols and optimization settings in DINO and iBOT. Specifically, following DINO and iBOT, for both ViT-S and ViT-B, we always use AdamW optimizer with a minibatch size of 768. We fine-tune the pretrained model 360 epochs on INat 18 and INat 19 , and 1000 epochs on Cif 10 , Cif 100 , Flwrs and Car. For all datasets, we sweep the learning rate {7.5?10 ?6 , 1.5?10 ?5 , 3.0?10 ?5 , 7.5?10 ?5 , 1.5?10 ?4 }. For we set weight decay as 2 ? 10 ?2 for CIFAR10 and CIFAR100 on ViT-B, and use a weight decay of 5 ? 10 ?2 for all remaining experiments. For example, on the INat 18 dataset, we use a learning rate of 3.0 ? 10 ?5 /1.5 ? 10 ?5 for ViT-S/ViT-B; on the INat 19 dataset, we set learning rate as 7.5 ? 10 ?5 /3.0 ? 10 ?5 for ViT-S/ViT-B. For more hyper-parameters, please refer to the hyper-parameter configure file in our released code.</p><p>Object detection &amp; Instance segmentation. For fairness, we follow DINO and iBOT, and fine-tune the pretrained backbone via a multi-scale strategy, namely resizing image at different scales. Please refer to iBOT for more details. We use AdamW optimizer with a learning rate of 2 ? 10 ?4 , a weight decay of 0.05 to fine-tune with 1? schedule, i.e. 12 epochs with the learning rate decayed by 10? at epochs 9 and 11. We sweep a layer decay rate of {0.65, 0.75, 0.8, 0.9} and finally choose 0.8 because of its good performance. For test, we do not use multi-scale strategy.</p><p>Semantic segmentation. For semantic segmentation, we follow DINO and iBOT, and fine-tune the pretrained backbone, and fine-tune the pretrained backbone by using 512 ? 512-sized images for 1.6 ? 10 4 iterations. We use AdamW optimizer with a learning rate of 2 ? 10 ?4 , a weight decay of 0.05 and a layer decay rate of 0.9 to fine-tune. For this task, we do not use any multi-scale strategy for training and test. We sweep the learning rate {2 ? 10 ?5 , 3 ? 10 ?5 , 4 ? 10 ?5 , 5 ? 10 ?5 } and finally choose 3 ? 10 ?5 because of its good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 More details for Ablation Study</head><p>Here we provide the implementation details for DINO and iBOT under different augmentations. As mentioned in Sec. 4.3 in the manuscript, for the augmentation T s in student network of Mugs/DINO/iBOT, we implement it by strong or weak augmentation as mentioned at the beginning of Sec. 4; for augmentation T t in teacher, we always use weak augmentation. We first consider weak augmentation setting. For DINO, there is no any change, since its vanilla version uses the weak augmentation. For Mugs, we only replace the strong augmentation used in the student network with the weak augmentation. For iBOT, it has two losses, the proposed masked image modeling (MIM) loss and the clustering loss in DINO. To construct the MIM loss, iBOT needs to randomly mask the patches of input in the student network. But to build the clustering loss, it actually does not require random masks on input patches, but still uses the random masks in practice which actually increases the data augmentations for clustering loss. In this case, for fair comparison, we remove the random masks and only perform weak augmentation to construct the clustering loss in iBOT. Note, we still preserve the random masks for MIM loss to ensure MIM is the vanilla one in iBOT.</p><p>We then consider strong augmentation setting. For DINO, we only replace the weak augmentation used in the student network with our strong augmentation. For iBOT, same as the above for weak augmentation, we does not change the augmentation in MIM loss. But for building the clustering loss, we also remove the random masks and only perform strong augmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Attention visualization on "mugs" of ViT-B/16 trained by our Mugs. See more examples in Sec. 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of linear probing accuracy on ImageNet-1K. By pretraining on ImageNet-1K, under different model sizes (see (a)) and pretraining epochs (see (b)), Mugs consistently improves previous SoTA (iBOT) by at least 0.8%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of pretrained ViT-B/16 (a) and ViT-S/16 (b) &amp; (c) by Mugs. See more examples in Appendix. Best viewed in 3? sized color pdf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>Sea AI Lab, Singapore 2 National University of Singapore This supplementary document provides more additional experimental results and the pretraining &amp; fine-tuning details for the ECCV'22 submission entitled "Mugs: A Multi-Granular Self-supervised Learning Framework". It is structured as follows. Appendix A provides more extra experimental results, including 1) the comparison among SoTAs without the multi-crop augmentation strategy in Appendix A.1, 2) fine-tuning comparison on ViT-L/16 in Appendix A.2, 3) more T-SNE clustering visualization results in Appendix A.3, 4) more attention visualization results in Appendix A.4, 5) more visualization results on object detection and segmentation in Appendix A.5. Appendix B provides more experimental details for Sec. 4 in manuscript. Specifically, Appendix B.1 gives more pretraining details, including implementations of weak and strong augmentations, loss construction under multi-crop setting, hyper-parameter settings for pretraining and the pretraining cost. Then Appendix B.2 introduces more details for finetuning and semi-supervised learning in Sec. 4.1 in manuscript. Next, in Appendix B.3, we present more details for downsteam tasks, including transfer learning, object detection &amp; instance segmentation, and semantic segmentation. Finally, Appendix B.4 tells us more implementation details of DINO and iBOT under weak and strong agumentation settings to complement Sec. 4.3 in manuscript.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>More T-SNE visualization of the learned features by ViT-B/16 trained by our Mugs. Best viewed in color pdf file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Self-attention visualization of ViT-B/16 pretrained by our Mugs. The images from left to right respectively denote the vanilla image, the overall selfattention score of all 12 heads in ViT-B, and the individual self-attention score of 12 heads. Best viewed in color pdf file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Object detection visualization of ViT-B/16 pretrained by Mugs. Best viewed in color pdf file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Semantic segmentation visualization of ViT-B/16 pretrained by Mugs. Best viewed in color pdf file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>B. 2</head><label>2</label><figDesc>More Training Details for Evaluation on ImageNet-1K Fine-tuning. As mentioned in manuscript, we follow BEiT [3], DINO and iBOT, and use AdamW optimizer with layer-wise learning rate decay to train ViT-S/ViT-B/ViT-L for 200/100/50 epochs on ImageNet-1K. We set layer-wise learning rate decay as 0.55 and learning rate 1.2 ? 10 ?3 for both ViT-S and ViT-B. For ViT-L, we use layer-wise learning rate decay 0.75 and learning rate 8.0 ? 10 ?4 . For drop path rate, we set it as 0.1/0.2/0/3 for ViT-S/ ViT-B/ViT-L respectively. All these hyper-parameters are around at the suggested ones in BEiT and iBOT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3% 77.4% 77.9% Mugs 76.4% 78.2% 78.9%</figDesc><table><row><cell>ViT-S/16 ViT-B/16 ViT-L/16 iBOT 77.9% 79.5% 81.0% Mugs 78.9% 80.6% 82.1%</cell><cell>ImageNet Top-1 Accuracy (%)</cell><cell>74 75 76 77 78 79</cell><cell>100 (b) Pretraining Epochs on ViT-S/16 300 800 DINO DINO iBOT iBOT MST Mugs Mugs Mugs DINO iBOT epochs 100 300 800 MST iBOT 75.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>reports top-1 test accuracy on ImageNet-1K. One can find that by pretraining on ImageNet-1K, Mugs consistently outperforms other methods on different back- bones of various sizes. Specifically, Mugs respectively achieves 78.9% and 80.6% top-1 accuracy on ViT-S and ViT-B, and improves corresponding SoTAs by at least 1.0%. Notably, on ViT-L, by only pretraining on ImageNet-1K, Mugs sets a new SoTA accuracy of 82.1%, which is even comparable to the accuracy 82.3% pretrained on ImageNet-22K.KNN. It is another powerful evaluation protocol to test the quality of repre- sentation learnt by SSL models. Following DINO and iBOT, we also sweep over different numbers (10, 20, 50, 100) of nearest neighbors for each model. From Table 1, one can find that for all backbones, Mugs achieves the highest top-1 accuracy on ImageNet-1K test dataset. Particularly, it respectively makes 0.4%, 0.9%, and 2.3% improvement on ViT-S, ViT-B and ViT-L over the runner-up, showing the advantages of multi-granular representation in Mugs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Fine-tuning classification accuracy (%) on ImageNet-1K. All methods are pretrained on ImageNet-1K. "Epo." is the effective pretraining epochs adjusted by number of views processed by the models following<ref type="bibr" target="#b60">[61]</ref>.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">ViT-S/16 Epo. Acc. (%)</cell><cell cols="2">ViT-B/16 Epo. Acc. (%)</cell></row><row><cell></cell><cell>Supervised [49]</cell><cell>-</cell><cell>79.9</cell><cell>-</cell><cell>81.8</cell></row><row><cell></cell><cell>BEiT [3]</cell><cell>800</cell><cell>81.4</cell><cell>800</cell><cell>83.4</cell></row><row><cell></cell><cell>MAE [28]</cell><cell>-</cell><cell>-</cell><cell>1600</cell><cell>83.6</cell></row><row><cell>reconstruction</cell><cell>SimMIM [56]</cell><cell>-</cell><cell>-</cell><cell>1600</cell><cell>83.8</cell></row><row><cell></cell><cell>MaskFeat [51]</cell><cell>-</cell><cell>-</cell><cell>1600</cell><cell>84.0</cell></row><row><cell></cell><cell>data2vec [2]</cell><cell>-</cell><cell>-</cell><cell>1600</cell><cell>84.2</cell></row><row><cell></cell><cell>MoCo-v3 [18]</cell><cell>600</cell><cell>81.4</cell><cell>600</cell><cell>83.2</cell></row><row><cell>contrastive or</cell><cell>DINO [10]</cell><cell>3200</cell><cell>82.0</cell><cell>1600</cell><cell>83.6</cell></row><row><cell>clustering</cell><cell>iBOT [61]</cell><cell>3200</cell><cell>82.3</cell><cell>1600</cell><cell>83.8</cell></row><row><cell></cell><cell>Mugs (ours)</cell><cell>3200</cell><cell>82.6</cell><cell>1600</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised classification accuracy (%) on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell cols="2">logistic regression 1% 10%</cell><cell cols="2">fine-tuning 1% 10%</cell></row><row><cell>SimCLRv2 [14]</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>57.9</cell><cell>68.1</cell></row><row><cell>BYOL [25]</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>53.2</cell><cell>68.8</cell></row><row><cell>SwAV [9]</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>53.9</cell><cell>70.2</cell></row><row><cell>SimCLRv2+SD [14]</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>60.0</cell><cell>70.5</cell></row><row><cell>DINO [10]</cell><cell>ViT-S/16</cell><cell>64.5</cell><cell>72.2</cell><cell>60.3</cell><cell>74.3</cell></row><row><cell>iBOT [61]</cell><cell>ViT-S/16</cell><cell>65.9</cell><cell>73.4</cell><cell>61.9</cell><cell>75.1</cell></row><row><cell>Mugs (ours)</cell><cell>ViT-S/16</cell><cell>66.9</cell><cell>74.0</cell><cell>66.8</cell><cell>76.8</cell></row></table><note>from DeiT [49] that trains 300 epochs. On ViT-S and ViT-B, Mugs respectively achieves new SoTA results of 82.5% and 84.3%, improving the runner-up, namely, iBOT and data2vec [2], by 0.2% and 0.1% respectively. Note, the reconstruction frameworks, e.g. MAE [28], MaskFeat [51] and data2vec, have unsatisfactory linear probing performance and thus are included in Table 1. Moreover, as ex- plained at the end of Sec. 2, this fine-tuning setting needs much higher extra training cost, and also destroys model compatibility for deployment. So here we do not further push Mugs's limits on the large models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SNE visualization of the learned feature by ViT-B/16. We show the fish classes in ImageNet-1K, i.e., the first six classes, including tench, goldfish, white shark, tiger shark, hammerhead, electric ray. See more examples in Appendix. Table 4: Classification accuracy (%) for transfer learning on six datasets. Cif 10 Cif 100 INat 18 INat 19 Flwrs Car Cif 10 Cif 100 INat 18 INat 19</figDesc><table><row><cell>MoCo-v3 MoCo-v3 MoCo-v3 MoCo-v3</cell><cell></cell><cell></cell><cell>DINO DINO DINO DINO</cell><cell></cell><cell></cell><cell></cell><cell>iBOT MILER iBOT iBOT</cell><cell></cell><cell></cell><cell>MILER iBOT MILER MILER Mugs</cell><cell></cell></row><row><cell>Fig. 4: T-Method</cell><cell></cell><cell></cell><cell cols="2">ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ViT-B/16</cell><cell cols="2">Flwrs Car</cell></row><row><cell>Sup. [10]</cell><cell cols="12">99.0 89.5 70.7 76.6 98.2 92.1 99.0 90.8 73.2 77.7 98.4 92.1</cell></row><row><cell>BEiT [3]</cell><cell cols="12">98.6 87.4 68.5 76.5 96.4 92.1 99.0 90.1 72.3 79.2 98.0 94.2</cell></row><row><cell>MAE [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">75.4 80.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MoCo-v3 [18] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-98.9 90.5</cell><cell>-</cell><cell>-</cell><cell cols="2">97.7 -</cell></row><row><cell>DINO [10]</cell><cell cols="12">99.0 90.5 72.0 78.2 98.5 93.0 99.1 91.7 72.6 78.6 98.8 93.0</cell></row><row><cell>iBOT [61]</cell><cell cols="12">99.1 90.7 73.7 78.5 98.6 94.0 99.2 92.2 74.6 79.6 98.9 94.3</cell></row><row><cell cols="13">Mugs (ours) 99.2 91.8 74.4 79.8 98.8 93.9 99.3 92.8 76.4 80.8 98.9 94.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Object detection (Det.) &amp; instance segmentation (ISeg.) on COCO &amp; semantic seg. (SSeg.) on ADE20K.</figDesc><table><row><cell></cell><cell>Arch.</cell><cell>Param.</cell><cell>Det. ISeg. SSeg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AP b AP m mIoU</cell></row><row><cell>Sup. [61]</cell><cell>Swin-T</cell><cell>29</cell><cell>48.1 41.7 44.5</cell></row><row><cell>MoBY [55]</cell><cell>Swin-T</cell><cell>29</cell><cell>48.1 41.5 44.1</cell></row><row><cell>Sup. [61]</cell><cell>ViT-S/16</cell><cell>21</cell><cell>46.2 40.1 44.5</cell></row><row><cell>iBOT [61]</cell><cell>ViT-S/16</cell><cell>21</cell><cell>49.4 42.6 45.4</cell></row><row><cell cols="2">Mugs (ours) ViT-S/16</cell><cell>21</cell><cell>49.8 43.0 47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Video</figDesc><table><row><cell></cell><cell cols="2">object seg-</cell></row><row><cell cols="3">mentation with ViT-B/16</cell></row><row><cell cols="3">on the DAVIS-2017 video</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(J &amp;F )m Jm Fm</cell></row><row><cell cols="2">DINO [10] 62.3</cell><cell>60.7 63.9</cell></row><row><cell>iBOT [61]</cell><cell>62.4</cell><cell>60.8 64.0</cell></row><row><cell>Mugs</cell><cell cols="2">63.1 61.4 64.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Effects of the three granular supervisions in Mugs to the linear probing accuracy (%) on ImageNet-1K. L instance , L local-group and L group respectively denote the instance, local-group and group discrimination supervision. Mugs Mugs w/o L instance Mugs w/o L local-group Mugs w/o Lgroup</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acc (%)</cell><cell>76.4</cell><cell>75.8</cell><cell>75.3</cell><cell>75.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Augmentation effects to linear probing accuracy (%) on ImageNet-1K. ? denotes that we replace vanilla augmentation in the method and run this variant.</figDesc><table><row><cell></cell><cell>weak aug.</cell><cell></cell><cell></cell><cell>strong aug.</cell><cell></cell><cell>weak aug.+random mask</cell></row><row><cell>DINO</cell><cell>iBOT  ?</cell><cell>Mugs</cell><cell>DINO  ?</cell><cell>iBOT  ?</cell><cell>Mugs</cell><cell>iBOT</cell></row><row><cell>74.2</cell><cell>74.9</cell><cell>75.7</cell><cell>74.7</cell><cell>75.4</cell><cell>76.4</cell><cell>75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>used in DeiT<ref type="bibr" target="#b48">[49]</ref> and the above weak augmentation. Specifically, for each image, with probability 0.5, we use AutoAugment<ref type="bibr" target="#b18">[19]</ref> to augment it; otherwise, we use weak augmentation to crop it. We use this sampling strategy to avoid training collapse while keeping sufficient data diversity.Following DINO and iBOT, we always set the global crop scale as [0.25, 1] and local crop scale as [0.05, 0.25].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Pretraining Cost. Mugs takes about 27 hours with 8 A100 GPUs for 100 pretraining epochs on ViT-S/16. This means that Mugs has almost the same training cost with DINO, since our projection/prediction heads and transformers g transformer are much smaller than the backbone. For ViT-B/16, Mugs needs about 24 hours on 16 A100 GPUs for 100 pretraining epochs. To training 100 epochs on ViT-L/16, Mugs takes about 48 hours on 40 A100 GPUs. For ViT-B and ViT-L, it is hard for us to compare with DINO, since it does not report the training time.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Pan Zhou 1 * Yichen Zhou 1 * Chenyang Si 1 * Weihao Yu 1 Teck Khim Ng 2 Shuicheng Yan 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank NVIDIA AI Tech Center (NVAITC) for the support of partial computational resources, and also thank Terry Jianxiong Yin (NVAITC) and Qingyi Tao (NVAITC) for their some GPU technology supports.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mugs: A Multi-Granular Self-Supervised Learning Framework <ref type="table">(Supplementary File)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>tpami</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<title level="m">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR (13-18</idno>
		<ptr target="https://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>III, H.D., Singh, A.</editor>
		<meeting>the 37th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: iccv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11567</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mst: Masked self-supervised transformer for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09645</idno>
		<title level="m">Prototypical graph contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">eccv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6827" to="6839" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">eccv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<title level="m">Self-supervised learning with swin transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">Simmim: A simple framework for masked image modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6509" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418</idno>
		<title level="m">Metaformer is actually what you need for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">cvpr</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">Ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A theory-driven self-labeling refinement method for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
