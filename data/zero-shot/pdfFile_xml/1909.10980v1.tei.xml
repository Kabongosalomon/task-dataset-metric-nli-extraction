<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PST900: RGB-Thermal Calibration, Dataset and Segmentation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rodrigues</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
						</author>
						<title level="a" type="main">PST900: RGB-Thermal Calibration, Dataset and Segmentation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The ability to parse raw imagery and ascertain pixelwise and region-wise semantic information is desirable for environment perception enabling advanced robot autonomy. Semantic segmentation is a major subject of robotics research and has applications ranging from medicine <ref type="bibr" target="#b0">[1]</ref> and agriculture <ref type="bibr" target="#b1">[2]</ref> to autonomous vehicles. Most popularly, convolutional neural networks (CNNs) have been applied to image classification tasks, where they dramatically outperform their classical counterparts. CNNs have also grown in popularity as being highly effective for extracting semantic information from color images. The recent growth in autonomous vehicle research has driven the design of datasets, benchmarks and network architectures that focus on semantic segmentation from RGB imagery <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Until recently, thermal cameras were primarily used by the military, with their cost and restricted usage making them difficult to acquire <ref type="bibr" target="#b7">[8]</ref>. However, over the last few years thermal cameras have become more easily accessible and their competitive prices have led to an increase in popularity. The primary use case thus far has been in surveillance, and most popular thermal cameras such as FLIR's range of LWIR cameras are specifically designed to identify human temperature signatures. There is a vast body of research in the field of thermal camera based human identification and tracking, which is out of the scope of our work.</p><p>We propose the usage of thermal cameras in addition to RGB cameras in challenging environments. Specifically, we look at environments with visibility and illumination limitations, such as in underground tunnels, mines and caves. We show that the additional information from the long-wave infrared spectrum can help to improve overall segmentation accuracy since it is not dependent on visible spectrum illumination which RGB cameras rely heavily upon. In this work, we also show that the segmentation of objects that do not possess very unique thermal signatures, such as handdrills, also improves with the fusion of thermal information.</p><p>Using thermal imagery in addition to RGB for general purpose semantic segmentation is a growing field of research with methods such as MFNet <ref type="bibr" target="#b8">[9]</ref> and RTFNet <ref type="bibr" target="#b9">[10]</ref>, which are currently the latest and most popular CNN-based approaches. However, for these methods to generalize well and achieve state-of-the-art accuracy, they require large amounts of training data. Unlike for RGB imagery, large datasets of annotated thermal imagery for semantic segmentation are hard to find. Ha et al. present a dataset which is the most recent dataset for RGB and Thermal segmentation <ref type="bibr" target="#b8">[9]</ref>. In our work, we present what we believe is the second dataset containing calibrated RGB and Thermal imagery that has per-pixel annotations across four different classes.</p><p>We additionally propose a dual-stream CNN architecture that combines RGB and Thermal imagery for semantic segmentation. We design the RGB stream to be independently re-usable as it is easier to collect large amounts of RGB data and annotations. We design our Thermal stream to leverage the information learned from the RGB stream to refine and improve class predictions. In contrast to a single or tightly coupled network architecture like MFNet and RTFNet, we are able to leverage both modalities in a way that is able to achieve high accuracy while working in real-time on embedded GPU hardware. In summary, our contributions are as follows:</p><p>tion that uses no heated elements, allowing for faster, portable calibration in the field. ? PST900 -Penn Subterranean Thermal 900 : A dataset of approximately 900 annotated RGB and LWIR (thermal) images in both raw 16-bit and FLIR's AGC 8bit format from a variety of challenging environments <ref type="bibr" target="#b10">[11]</ref>. An additional 3416 annotated RGB images are also provided from these environments. ? A dual-stream CNN architecture that is able to fuse RGB information with thermal information in such a way that allows for RGB stream re-usability and fast, real-time inference on embedded GPU platforms such as the NVIDIA Jetson TX2 and AGX Xavier. ? Extensive experiments comparing our method to similar approaches on both PST900 and the MFNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There has been work in Thermal and RGB interaction in the form of cross modal prediction, where either RGB or Thermal imagery is used to predict the other <ref type="bibr" target="#b11">[12]</ref>. This idea of cross modal learning has also extended to stereo disparity estimation, where matching is done across the two different modalities <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, resulting in the creation of interesting cross modal datasets such as LITIV <ref type="bibr" target="#b14">[15]</ref> and St. Charles <ref type="bibr" target="#b15">[16]</ref>. Since thermal cameras still operate at a lower resolution than similarly priced RGB cameras, Choi et al. and Feras et al. propose learning based enhancement <ref type="bibr" target="#b16">[17]</ref> and superresolution methods <ref type="bibr" target="#b18">[18]</ref> for thermal imagery and use RGB imagery as a guide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation with Thermal Images</head><p>Qiao et al. propose a novel level set method for contour detection using an edge based active contour model designed specifically for thermal imagery <ref type="bibr" target="#b20">[20]</ref>. Luo et al. use semantic information in an egocentric RGB-D-T SLAM pipeline <ref type="bibr" target="#b21">[21]</ref> using a variant of YOLO on their RGB-D-T data <ref type="bibr" target="#b22">[22]</ref>. Their experiments suggest that the model is most heavily benefited by the thermal modality and that thermal residues provide good indicators for action recognition tasks.</p><p>Directly relevant to our work is MFNet <ref type="bibr" target="#b8">[9]</ref>, an RGB-T semantic segmentation network architecture proposed by Ha et al. . They present an RGBT dataset in urban scene settings for autonomous vehicles and a dual encoder architecture for RGB and Thermal image data. They show that this architecture performs better than naively introducing the thermal modality as an extra channel. Additionally, the authors state that with slightly misaligned RGB-T images, introducing thermal as a fourth channel can have detrimental effects to segmentation accuracy, often performing worse than RGB alone.</p><p>Recently, Sun et al. proposed a segmentation architecture that uses a dual ResNet encoder with a small decoder <ref type="bibr" target="#b9">[10]</ref>. The multimodal fusion is performed by an element-wise summation of feature blocks from both the RGB and Thermal encoder pathways. Their decoder architecture makes use of a novel Upception block which alternatingly preserves and increases spatial resolution while reducing channel count.</p><p>They evaluate their network against popular semantic segmentation networks such as U-Net <ref type="bibr" target="#b0">[1]</ref>, SegNet <ref type="bibr" target="#b23">[23]</ref>, PSPNet <ref type="bibr" target="#b24">[24]</ref>, DUC-HDC <ref type="bibr" target="#b25">[25]</ref> and ERFNet <ref type="bibr" target="#b27">[26]</ref>. They also compare their work against MFNet and show that RTFNet outperforms theirs on the MFNet dataset. From here on, we will compare our method to MFNet and RTFNet as they are the most relevant to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Calibration</head><p>Our primary motivation for designing a calibration procedure was for portability and ease of use in resource constrained subterranean environments. Current calibration methods are either active: where the calibration target is a thermal emitter or is externally heated to retain a thermal signature or passive: where no explicit heat source is required. Popular among active methods is a paper calibration target heated by means of an external heat source such as a flood lamp in order to drive the temperature of the black ink higher than the white, resulting in an inverted checkerboard in the thermal camera. We initially used this technique and were able to calibrate RGB-T intrinsics and extrinsics successfully, but the process required significant effort to ensure that the amount of heat imparted to the checkerboard was sufficient to obtain sharp checkerboard corners consistently. This was specially difficult because of the fast cool-down time of the heated elements <ref type="bibr" target="#b28">[27]</ref>. Additionally, this required a large heat source (flood lamp), which was a burden to transport and use in the field. This motivated our search for a calibration target that was completely passive and could be used with off-the-shelf calibration tools.</p><p>Zoetgnande et al. propose an active method to calibrate a low-cost low-resolution stereo LWIR thermal camera system. They design a board with 36 bulbs and develop a sub-pixel corner estimation algorithm to detect these heat signatures against a wooden calibration frame <ref type="bibr" target="#b29">[28]</ref>. Zalud et al. detail a five camera RGB-D(ToF)-T system for robotic telepresence <ref type="bibr" target="#b30">[29]</ref>. Their calibration target was an actively heated aluminum checkerboard with squares cut out, placed in front of a black insulator. Rangel et al. present a detailed comparison of different calibration targets and methods for calibrating Depth and Thermal cameras together <ref type="bibr" target="#b31">[30]</ref>. They compare active and passive methods and pick a calibration target with circular cutouts that requires minimal heating prior to calibration to appear in both depth and thermal imagery.</p><p>Tarek et al. present a visual odometry method that uses stereo thermal cameras <ref type="bibr" target="#b32">[31]</ref>. While this work is not directly relevant to ours, they propose an interesting passive calibration method: they design a calibration board from highly polished aluminum with matte black squares applied to it. The calibration board is placed such that the aluminium reflects the cold sky, leading to a high contrast between the aluminium and the squares. While this is a step in the right direction for portability and ease of use, it is still challenging for us to rely on the cold sky effect in the event of calibration in subterranean environments. Our proposed method draws inspiration from this method to propose a passive method that will work in both environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>A. RGB-D-T(LWIR) Calibration 1) Reflectivity Based Calibration: A characteristic of LWIR reflections from metallic surfaces such as aluminum and copper is that detections in this band have much lower emissivities ( ) compared to other bands such as medium wave infra red (MWIR) <ref type="bibr" target="#b7">[8]</ref>. This leads to strong detections of reflections (1 ? ) even if the material is rough and unpolished. We propose a calibration target that uses thermal reflectivity, specifically the reflections of the thermographer, i.e the person calibrating the system, to illuminate sand blasted aluminum squares mounted on a black acrylic background to form a checkerboard <ref type="figure" target="#fig_1">(Fig 2)</ref>. We decided against polishing either surfaces since we found that highly polished surfaces tended to interfere with corner detection in RGB imagery. In practice the silver aluminum checkers appear to be at a higher temperature than the black acrylic background in the thermal imagery. We also achieve sufficient contrast in the RGB imagery to use existing checkerboard detectors. Note that since in both modalities, thermal and RGB, the silver checkers appear to have higher intensity values the correspondence between the two images is direct and no correspondence inversions were required such as those required when heating black ink on paper. For corner detection, OpenCV's chessboard detector performed poorly in both RGB and Thermal, and we used a C++ implementation of libcbdetect instead <ref type="bibr" target="#b33">[32]</ref>. Once the checkerboards are detected, we use OpenCV's fisheye camera calibration toolbox to first calibrate RGB and Thermal intrinsics followed by extrinsics.</p><p>2) Thermal-RGB Alignment: Let K rgb , K thermal and D rgb , D thermal be the camera matrix and distortion coefficients obtained from intrinsic calibration of the RGB camera and thermal camera respectively. From these parameters, we obtain K rgb and K thermal , i.e the undistorted camera matrices for both cameras. The RGB camera is a Stereolabs Zed Mini and its intrinsics are modeled with a plumb-bob model, whereas the Thermal camera is a FLIR Boson 320 2.3mm and its intrinsics are captured with a fisheye model.</p><p>To register each pixel from the Thermal camera to the frame of the RGB camera, we require a mapping of all image co-ordinates I thermal in the thermal image to image co-ordinates I rgb in the rgb image. We achieve this by first <ref type="figure">Fig. 3</ref>.</p><p>Calibration for RGB-T Alignment: Left -With intrinsics and extrinsics known, projecting thermal (320 ? 256) onto RGB (1280 ? 720) is achieved by projecting 2D co-ordinate locations in RGB into 3D using K rgb and using the stereo depth for Z. These 3D co-ordinates are then projected onto the thermal frame using the calibrated R and t matrices along with K thermal to obtain a mapping I therm ? I rgb . Right -Original undistorted RGB and thermal images, depth image from stereo and thermal projected onto RGB frame.</p><p>projecting all RGB co-ordinates into 3D (P ) using the depth image D depth acquired from stereo depth estimation. We then identify a mapping to the thermal frame by projecting these points back onto the thermal camera frame as seen in <ref type="figure">Fig 3.</ref> For some co-ordinate i, let I i,rgb = (xr, yr, 1) be a point in the undistorted RGB image. Let D i,depth denote the depth provided by the stereo system at that pixel. This point is projected to 3D location P i using (K ) ?1 i,rgb as follows:</p><formula xml:id="formula_0">P i = ((K rgb ) ?1 ? ({I i,rgb }, 1)) ? D i,depth<label>(1)</label></formula><p>This 3D co-ordinate P i = (X, Y, Z) can then be projected onto the thermal frame using the calibrated extrinsics R thermal rgb and t thermal rgb ,</p><formula xml:id="formula_1">I i,thermal = K thermal ? [R thermal rgb , t thermal rgb ] ? P i (2)</formula><p>where I i,thermal = (xt, yt, 1) is the point in the thermal image to which I i,rgb is mapped to. We now have I i,rgb ? I i,thermal from which we find I i,thermal ? I i,rgb . While calculating the inverse mapping, we handle the issue of parallax and the many-to-one mapping by choosing the closest 3D point during re-projection. In our dataset, we provide aligned thermal imagery with holes, but provide a simple interpolation script to perform hole filling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Penn Subterranean Thermal 900 Dataset (PST900):</head><p>In this section, we present our dataset of 894 aligned pairs of RGB and Thermal images with per-pixel human annotations. This dataset was driven by the needs of the DARPA Subterranean Challenge 1 , where a set of 4 visible artifacts (fire-extinguisher, backpack, hand-drill, survivor : thermal mannequin, human) are to be identified in challenging underground environments where there are no guarantees of environmental illumination or visibility. We therefore resort to equipping our robots and data collection platforms with high intensity LEDs. Our sensor head, as shown in   consists of a Stereolabs ZED Mini stereo camera and a FLIR Boson 320 camera. We intentionally opted for a wider field of view thermal camera for a greater overlap between the two sensors. We design a calibration procedure to obtain camera intrinsics and system extrinsics as mentioned in the previous section. We then collect representative data from multiple different environments with varying degrees of lighting, these environments include the Number 9 Coal Mine in Lansford PA, cluttered indoor and outdoor spaces as seen in <ref type="figure" target="#fig_3">Fig 4</ref>.</p><p>In addition to this corpus of labelled and calibrated RGB-Thermal data, we additionally provide a much larger set of similarly annotated RGB-only data, collected over a much larger set of environments. Our proposed method is able to leverage both datasets to produce efficient and accurate predictions.</p><p>Labels are acquired from a pool of human-annotators within our laboratory. The human-annotators are briefed on the different artifacts present, and the difficulty of visible identification of small artifacts such as the handdrill. Annotations are made per-pixel and each set of rgb, thermal and label is verified by the authors for accuracy. Artifacts incorrectly labeled or missed artifacts are sent back for re-labeling and re-verification. With this process in place, we acquired our dataset of 894 aligned and annotated RGB-thermal image pairs and 3416 annotated RGB images. Our dataset is made publicly available to the community along with a basic toolkit here: https://github.com/ShreyasSkandanS/pst900 thermal rgb. <ref type="figure">Fig. 5</ref>. Our proposed network architecture that is essentially two streams, a single independent RGB based network that is based on ResNet-18 and UNet, and a second stream that depends on the output of the first network as well as thermal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation</head><p>Collecting large amounts of RGB data and acquiring accurate per-pixel human annotations is significantly easier and cheaper than collecting calibrated and aligned RGB-T data. Therefore, we designed a network that leverages this fact by having an independent RGB stream that can be trained without thermal data. We introduce the thermal modality to the output of this stream to further improve the initial results. We propose a sequential, dual stream architecture that draws influence from ResNet-18 <ref type="bibr" target="#b34">[33]</ref>, UNet <ref type="bibr" target="#b0">[1]</ref> and ERFNet <ref type="bibr" target="#b27">[26]</ref> and show that our design is efficient allowing real-time inference on embedded hardware, flexible since the early exit provides a coarser prediction quickly, and accurate outperforming other methods on our dataset and showing competitive performance on the MFNet dataset.</p><p>The RGB stream is a ResNet-18 architecture with an encoder-decoder skip-connection scheme similar to UNet. Our network heavily based on the implementation of Usuyama et al. <ref type="bibr" target="#b35">[34]</ref> and is shown in <ref type="figure">Fig 5.</ref> The network is first trained on annotated RGB images only. We use a weighted negative log-likelihood loss during training and select the model with the highest mean Intersection-over-Union (mIoU). The weights for the loss function are calculated using the weighting scheme proposed by Paszke et al. <ref type="bibr" target="#b36">[35]</ref>. On datasets such as ours, a weighting scheme is necessary given the dramatic imbalance between background and foreground classes as seen in <ref type="table" target="#tab_0">Table I</ref>. Once the network is trained, we remove the final softmax operator which results in what is intuitively a per-pixel confidence volume for the different classes in our dataset. We use this volume, along with the thermal modality, as input to our next fusion stream.</p><p>The Fusion stream takes as it's input the confidence volume along with the input thermal imagery and color image. The information is concatenated and passed to an ERFNetbased encoder-decoder architecture <ref type="bibr" target="#b27">[26]</ref>. Our architecture differs in that it has a larger set of initial feature layers to account for the larger input. Additionally, we use fewer layers at the end of the encoder. We then freeze the RGB stream and train this entire architecture as a whole using the same loss function as before. Once again, we select our best model to be the one with the lowest mean IoU value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND ANALYSIS</head><p>In this section, we compare our method against relevant methods MFNet <ref type="bibr" target="#b8">[9]</ref>, and RTFNet <ref type="bibr" target="#b9">[10]</ref>. We also compare our method against na?ve RGB-T fusion implementations on relevant segmentation networks such as ERFNet <ref type="bibr" target="#b27">[26]</ref>, MAVNet <ref type="bibr" target="#b37">[36]</ref>, Fast-SCNN <ref type="bibr" target="#b38">[37]</ref> and UNet <ref type="bibr" target="#b0">[1]</ref>. In our na?ve implementations of RGB-T segmentation networks, we introduce the thermal modality by concatenating the thermal image as a fourth channel to the original RGB input. For all our experiments, we compare both RGB and RGB-T performance as seen in <ref type="table" target="#tab_0">Table II and Table III</ref>. We measure performance using mean Intersection over Union (mIoU) across all classes. We train all the models in PyTorch using an NVIDIA DGX-1. For MFNet and RTFNet, we use authorrecommended batch sizes, loss functions and training scripts where applicable. For the rest of the networks, we use a fixed batch size, learning rate, and loss function across all experiments. We measure inference latency in milliseconds on an NVIDIA AGX Xavier embedded GPU device, which is the central compute unit on-board our mobile robot platforms. To allow for fair comparison between methods, we use PST900 RGBT data and exclude the RGB only data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MFNet Dataset</head><p>As shown in <ref type="table" target="#tab_0">Table II</ref>, the best performing method is RTFNet, with our method following second. MAVNet achieves the lowest scores on this dataset, reaching a maximum mIoU of 22.26% with na?ve fusion of thermal information. Nguyen et al. design this network with performance as a primary objective <ref type="bibr" target="#b37">[36]</ref>, which is supported by the low inference latency of this network, as seen in <ref type="table" target="#tab_0">Table III</ref>. Fast-SCNN achieves an mIoU of 28.88% and 32.83% with RGB and RGB-T modalities respectively. The largest increase in class IoU between these two models is seen in the Person class, which comports with the intuition that humans have a strong unique thermal signature, and the network is able to narrow in on this to improve its overall accuracy. However, Fast-SCNN was originally designed for high resolution data, which could explain the low performance <ref type="bibr" target="#b38">[37]</ref>. We were unable to achieve the results mentioned by Sun et al. with UNet on this dataset and therefore for comparisons, we refer to their experiments since we use the same training and validation split. RTFNet performs better than other methods in both ResNet-50 and ResNet-152 variants. This is followed by our method, which closely outperforms ERFNet with a na?vely added thermal fourth channel. Our network achieves 48.42% mIoU on this dataset and from a performance perspective, is roughly 4 times faster than RTFNet-152 as shown in <ref type="table" target="#tab_0">Table III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PST900 Dataset</head><p>All the above networks are trained and evaluated on our PST900 dataset. Our method achieves the best performance with 68.36% mIoU and RTFNet-152 at 57.61% mIoU; results are shown in <ref type="table" target="#tab_0">Table III</ref>. Qualitative comparisons shown in <ref type="figure">Fig 6.</ref> The measured latency of our proposed method on our embedded GPU hardware is approximately 42ms, which is significantly faster than RTFNet-152. For the training and evaluation of the networks compared here, we use the originally recommended training parameters prescribed in their respective works. Aside from our method, the observed trend is similar to the previous experiments on the MFNet dataset, where RTFNet and ERFNet achieve high accuracy. However, RTFNet is outperformed by ERFNet on this dataset, achieving an mIoU of 62.55% with na?ve thermal fusion. Interesting to note is the na?ve introduction of thermal information to UNet and Fast-SCNN results in lower performance than RGB alone, whereas this is not observed on the MFNet dataset.</p><p>We posit that our dataset is significantly more challenging for thermal fusion networks since there is plenty of information available in RGB alone, making it potentially difficult to learn an informative correlation between both modalities. Additionally, our dataset contains the same object in situations where it is both above and below the ambient air temperature, resulting in an inversion in thermal imagery. This could be potentially challenging when learning RGB-Thermal correlations. Our hypothesis is strengthened by the two cases where performance degrades with the na?ve introduction of thermal. The IoU for the Survivor class increases, while objects that might have stronger color cues than thermal cues perform worse, like the backpack class.</p><p>We are able to achieve very competitive results with RGB alone, and accuracy further improves when the Fusion stream is added. We observe that our RGB Stream performs poorly with a na?ve fusion approach. This also supports our hypothesis that for the task of learning correlated representations for RGB and Thermal, a late fusion strategy is highly beneficial as opposed to fusing both modalities early on in the network architecture. This effect is exacerbated by the fact that our dataset contains objects of interest that have very strongly identifiable cues from RGB alone, such as the red backpack and orange hand-drill. When learning to identify these objects, the network may be prone to more heavily weight these attributes in RGB, and neglect the more subtle cues to be learned from Thermal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In summary, this work explores Thermal (LWIR) as a viable supporting modality for general semantic segmentation in challenging environments. We propose an RGB and  <ref type="figure">6</ref>. Qualitative Results on PST900: Our method is able to perform accurate segmentation of distant objects such as the Survivor in (1). Our method also performs well in blurred and and visually degraded settings such as in <ref type="formula">(2)</ref> where motion blur degrades the RGB image but the thermal image is able to help produce an accurate prediction. Example (3) shows a clearly visible Fire Extinguisher that is mistakenly identified by RTFNet-152, potentially because of weak thermal cues in the dataset and because of insufficient learning of color cues. Example (4) shows our method perform poorly on the Survivor; a portion of the survivor's leg appears to be near ground temperature and this leads to noisy segmentation in that region.</p><p>Thermal camera calibration technique that is both portable and easy to use. To further research in this field, we also present PST900, a collection of 894 aligned and annotated RGB and Thermal image pairs and make this publicly available to the community. We compare various existing methods on this dataset and propose a dual-stream CNN architecture for RGB and Thermal guided semantic segmentation that achieves state of the art performance on this dataset. Our network works in real-time on embedded GPUs and can be use in mobile robotic systems. We also compare this method on the MFNet dataset and show that our method is competitive with existing methods. Additionally, we highlight the need for late fusion in these architectures by noting poor performance with na?ve fusion approaches. We also discuss some of the challenges of RGB-Thermal fusion for object identification, such as when objects of interest may not have easily discernible thermal signatures but have strong cues from RGB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Left -Quadruped mobile robot platform for data collection; sensor head includes a Stereolabs Zed Mini Stereo RGB camera, FLIR Boson 320 and an active illumintation setup. Right -example RGB and Thermal imagery, with a human annotated segmentation label at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Calibration: Left -Undistorted rgb image with detected corners overlaid. Right -Undistorted thermal image with detected corners overlaid, example thermal image containing our proposed calibration target; As this figure suggests, even with no heated elements, sufficient contrast can be achieved between the aluminum checkers and the backboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>PST900 Dataset: Examples from our proposed dataset; The first row contains example RGB image data captured from the Stereolabs Zed Mini camera. The second row contains aligned thermal imagery (visualized in 8-bit color) captured from the FLIR Boson camera. The last row contains human annotated per-pixel labels for each of the four classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PST900</head><label>I</label><figDesc>DATASET CLASS IMBALANCE: THE FIRST TABLE SHOWS THE RATIO OF PIXELS OF A PARTICULAR CLASS TO THE TOTAL PIXELS IN THE DATASET. THE SECOND TABLE SHOWS THE RATIO OF INSTANCES OF A PARTICULAR OBJECT TO THE TOTAL IMAGES IN THE DATASET.</figDesc><table><row><cell></cell><cell cols="4">Class Imbalance -per pixel (in %)</cell><cell></cell></row><row><cell>Dataset</cell><cell>bg</cell><cell>fire-ext</cell><cell>backpack</cell><cell>drill</cell><cell>survivor</cell></row><row><cell>RGBDT</cell><cell>96.9845</cell><cell>0.2829</cell><cell>1.1962</cell><cell>0.1764</cell><cell>1.3597</cell></row><row><cell>RGB</cell><cell>98.5003</cell><cell>0.1835</cell><cell>0.5090</cell><cell>0.0941</cell><cell>0.7129</cell></row><row><cell></cell><cell cols="4">Class Imbalance -per instance (in %)</cell><cell></cell></row><row><cell>Dataset</cell><cell>bg</cell><cell>fire-ext</cell><cell>backpack</cell><cell>drill</cell><cell>survivor</cell></row><row><cell>RGBDT</cell><cell>100.0</cell><cell>28.6353</cell><cell>47.0917</cell><cell>34.0044</cell><cell>38.7024</cell></row><row><cell>RGB</cell><cell>100.0</cell><cell>23.9842</cell><cell>29.0225</cell><cell>20.1996</cell><cell>22.0571</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON MFNET DATASET: WE COMPARE OUR METHOD TO SIX DIFFERENT SEMANTIC SEGMENTATION NETWORKS. ASIDE FROM MFNET AND RTFNET, THE RGB-T MODE INDICATES A NAIVE FUSION OF THERMAL BY ADDING IT AS A FOURTH CHANNEL TO THE NETWORK INPUT. ASIDE FROM MFNET AND RTFNET, THE RGB-T MODE INDICATES A NAIVE FUSION OF THERMAL BY ADDING IT AS A FOURTH CHANNEL TO THE NETWORK INPUT. WE ALSO LIST INFERENCE LATENCY (IN MS) ON AN NVIDIA AGX XAVIER.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dataset: MFNet Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>Mode</cell><cell cols="2">Background</cell><cell>Car</cell><cell>Person</cell><cell>Bike</cell><cell>Curve</cell><cell>Car Stop</cell><cell>Guardrail</cell><cell cols="2">Color Cone</cell><cell>Bump</cell><cell>mIoU</cell></row><row><cell>ERFNet</cell><cell>RGB</cell><cell cols="2">0.9589</cell><cell>0.7051</cell><cell>0.4176</cell><cell>0.5029</cell><cell>0.2627</cell><cell>0.2070</cell><cell>0.0638</cell><cell>0.3643</cell><cell></cell><cell>0.3069</cell><cell>0.4199</cell></row><row><cell>ERFNet</cell><cell>RGB-T</cell><cell cols="2">0.9694</cell><cell>0.7384</cell><cell>0.6465</cell><cell>0.5128</cell><cell>0.3773</cell><cell>0.1833</cell><cell>0.0362</cell><cell>0.4023</cell><cell></cell><cell>0.4592</cell><cell>0.4806</cell></row><row><cell>MAVNet</cell><cell>RGB</cell><cell cols="2">0.8264</cell><cell>0.4156</cell><cell>0.1304</cell><cell>0.2878</cell><cell>0.0530</cell><cell>0.0290</cell><cell>0.0019</cell><cell>0.0560</cell><cell></cell><cell>0.0115</cell><cell>0.2014</cell></row><row><cell>MAVNet</cell><cell>RGB-T</cell><cell cols="2">0.8843</cell><cell>0.3754</cell><cell>0.3975</cell><cell>0.1521</cell><cell>0.0850</cell><cell>0.0240</cell><cell>0.0000</cell><cell>0.0400</cell><cell></cell><cell>0.0440</cell><cell>0.2226</cell></row><row><cell>UNet*</cell><cell>RGB</cell><cell cols="2">0.9620</cell><cell>0.6520</cell><cell>0.4260</cell><cell>0.4780</cell><cell>0.2780</cell><cell>0.2080</cell><cell>0.0000</cell><cell>0.3580</cell><cell></cell><cell>0.3100</cell><cell>0.4080</cell></row><row><cell>UNet*</cell><cell>RGB-T</cell><cell cols="2">0.9690</cell><cell>0.6620</cell><cell>0.6050</cell><cell>0.4620</cell><cell>0.4160</cell><cell>0.1790</cell><cell>0.0180</cell><cell>0.3060</cell><cell></cell><cell>0.4420</cell><cell>0.4510</cell></row><row><cell>Fast-SCNN</cell><cell>RGB</cell><cell cols="2">0.9477</cell><cell>0.4075</cell><cell>0.2334</cell><cell>0.2237</cell><cell>0.2358</cell><cell>0.0945</cell><cell>0.0166</cell><cell>0.2242</cell><cell></cell><cell>0.2165</cell><cell>0.2888</cell></row><row><cell>Fast-SCNN</cell><cell>RGB-T</cell><cell cols="2">0.9616</cell><cell>0.4263</cell><cell>0.5141</cell><cell>0.4735</cell><cell>0.2456</cell><cell>0.1087</cell><cell>0.0000</cell><cell>0.2230</cell><cell></cell><cell>0.3462</cell><cell>0.3283</cell></row><row><cell>MFNet</cell><cell>RGB-T</cell><cell cols="2">0.9635</cell><cell>0.6154</cell><cell>0.5530</cell><cell>0.4341</cell><cell>0.2231</cell><cell>0.0797</cell><cell>0.0028</cell><cell>0.2088</cell><cell></cell><cell>0.2471</cell><cell>0.3697</cell></row><row><cell>RTFNet-50</cell><cell>RGB-T</cell><cell cols="2">0.9789</cell><cell>0.8388</cell><cell>0.6676</cell><cell>0.6010</cell><cell>0.4328</cell><cell>0.1260</cell><cell>0.0551</cell><cell>0.2660</cell><cell></cell><cell>0.5684</cell><cell>0.5038</cell></row><row><cell>RTFNet-152</cell><cell>RGB-T</cell><cell cols="2">0.9797</cell><cell>0.8710</cell><cell>0.6700</cell><cell>0.6100</cell><cell>0.4161</cell><cell>0.2360</cell><cell>0.0300</cell><cell>0.3200</cell><cell></cell><cell>0.4947</cell><cell>0.5141</cell></row><row><cell>Ours: RGB Stream</cell><cell>RGB</cell><cell cols="2">0.9678</cell><cell>0.7673</cell><cell>0.4873</cell><cell>0.5532</cell><cell>0.2917</cell><cell>0.2785</cell><cell>0.1525</cell><cell>0.3580</cell><cell></cell><cell>0.4264</cell><cell>0.4776</cell></row><row><cell>Ours: Na?ve</cell><cell>RGB-T</cell><cell cols="2">0.9690</cell><cell>0.7298</cell><cell>0.6200</cell><cell>0.5166</cell><cell>0.3872</cell><cell>0.2011</cell><cell>0.0150</cell><cell>0.3691</cell><cell></cell><cell>0.4421</cell><cell>0.4700</cell></row><row><cell>Ours: Full</cell><cell>RGB-T</cell><cell cols="2">0.9701</cell><cell>0.7684</cell><cell>0.5257</cell><cell>0.5529</cell><cell>0.2957</cell><cell>0.2509</cell><cell>0.151</cell><cell>0.3936</cell><cell></cell><cell>0.4498</cell><cell>0.4842</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">RESULTS ON PST900 DATASET: Dataset: PST900 Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell></cell><cell>Mode</cell><cell cols="2">Background</cell><cell cols="2">Fire-Extinguisher</cell><cell>Backpack</cell><cell>Hand-Drill</cell><cell>Survivor</cell><cell>mIoU</cell><cell cols="2">ms (Xavier)</cell></row><row><cell>ERFNet</cell><cell></cell><cell>RGB</cell><cell cols="2">0.9869</cell><cell>0.6118</cell><cell></cell><cell>0.6528</cell><cell>0.4240</cell><cell>0.4169</cell><cell>0.6185</cell><cell></cell><cell>30</cell></row><row><cell>ERFNet</cell><cell></cell><cell>RGB-T</cell><cell cols="2">0.9873</cell><cell>0.5879</cell><cell></cell><cell>0.6808</cell><cell>0.5276</cell><cell>0.3438</cell><cell>0.6255</cell><cell></cell><cell>31</cell></row><row><cell>MAVNet</cell><cell></cell><cell>RGB</cell><cell cols="2">0.9822</cell><cell>0.2831</cell><cell></cell><cell>0.5850</cell><cell>0.3367</cell><cell>0.0901</cell><cell>0.4551</cell><cell></cell><cell>16</cell></row><row><cell>MAVNet</cell><cell></cell><cell>RGB-T</cell><cell cols="2">0.9789</cell><cell>0.2258</cell><cell></cell><cell>0.5152</cell><cell>0.3194</cell><cell>0.3473</cell><cell>0.4774</cell><cell></cell><cell>17</cell></row><row><cell>UNet</cell><cell></cell><cell>RGB</cell><cell cols="2">0.9843</cell><cell>0.4928</cell><cell></cell><cell>0.6364</cell><cell>0.4026</cell><cell>0.2337</cell><cell>0.5499</cell><cell></cell><cell>12</cell></row><row><cell>UNet</cell><cell></cell><cell>RGB-T</cell><cell cols="2">0.9795</cell><cell>0.4296</cell><cell></cell><cell>0.5289</cell><cell>0.3827</cell><cell>0.3164</cell><cell>0.5274</cell><cell></cell><cell>12</cell></row><row><cell cols="2">Fast-SCNN</cell><cell>RGB</cell><cell cols="2">0.9857</cell><cell>0.3454</cell><cell></cell><cell>0.6679</cell><cell>0.2063</cell><cell>0.2053</cell><cell>0.4822</cell><cell></cell><cell>18</cell></row><row><cell cols="2">Fast-SCNN</cell><cell>RGB-T</cell><cell cols="2">0.9851</cell><cell>0.3548</cell><cell></cell><cell>0.6460</cell><cell>0.1550</cell><cell>0.2168</cell><cell>0.4715</cell><cell></cell><cell>18</cell></row><row><cell>MFNet</cell><cell></cell><cell>RGB-T</cell><cell cols="2">0.9863</cell><cell>0.6035</cell><cell></cell><cell>0.6427</cell><cell>0.4113</cell><cell>0.2070</cell><cell>0.5702</cell><cell></cell><cell>23</cell></row><row><cell cols="2">RTFNet-50</cell><cell>RGB-T</cell><cell cols="2">0.9884</cell><cell>0.4349</cell><cell></cell><cell>0.7058</cell><cell>0.0100</cell><cell>0.2800</cell><cell>0.4840</cell><cell></cell><cell>42</cell></row><row><cell cols="2">RTFNet-152</cell><cell>RGB-T</cell><cell cols="2">0.9892</cell><cell>0.5203</cell><cell></cell><cell>0.7530</cell><cell>0.2537</cell><cell>0.3643</cell><cell>0.5761</cell><cell></cell><cell>127</cell></row><row><cell cols="2">Ours: RGB Stream</cell><cell>RGB</cell><cell cols="2">0.9883</cell><cell>0.6814</cell><cell></cell><cell>0.6990</cell><cell>0.5151</cell><cell>0.4989</cell><cell>0.6765</cell><cell></cell><cell>18</cell></row><row><cell cols="2">Ours: Na?ve</cell><cell>RGB-T</cell><cell cols="2">0.9852</cell><cell>0.6253</cell><cell></cell><cell>0.5961</cell><cell>0.4708</cell><cell>0.4339</cell><cell>0.6223</cell><cell></cell><cell>20</cell></row><row><cell cols="2">Ours: Full</cell><cell>RGB-T</cell><cell cols="2">0.9885</cell><cell>0.7012</cell><cell></cell><cell>0.6920</cell><cell>0.5360</cell><cell>0.5003</cell><cell>0.6836</cell><cell></cell><cell>42</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.subtchallenge.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting apples and oranges with deep learning: A data-driven approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dcunha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Okon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="781" to="788" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="961" to="972" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Infrared thermal imaging: fundamentals, research and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>M?llmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rtfnet: Rgb-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flir automatic gain control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flir</surname></persName>
		</author>
		<ptr target="https://github.com/FLIR/BosonUSB" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Thermalnet: a deep convolutional network for synthetic thermal image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kniaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gorbatsevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mizginov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cats: A color and thermal stereo benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Treible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saponaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolagunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sherbondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese cnns for rgb-lwir disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Beaupre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An iterative integrated framework for thermal-visible image registration, sensor fusion, and people tracking for video surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mass?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="221" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thermalvisible registration of human silhouettes: A similarity measure performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Riahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Thermal image enhancement using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="223" to="230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multimodal sensor fusion in single thermal image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Almasri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Debeir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09276</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combination of colour and thermal sensors for enhanced object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>St-Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Maldague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pr?vost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 10th International Conference on Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thermal infrared pedestrian image segmentation using level set method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1811</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene semantic reconstruction from egocentric rgb-d-thermal videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1451" to="1460" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A mask-based approach for the geometric calibration of thermal-infrared cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lakemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1625" to="1635" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust low resolution thermal stereo camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W K</forename><surname>Zoetgnand?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-J</forename><surname>Foug?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cormier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dillenseger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Machine Vision (ICMV 2018)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11041</biblScope>
			<biblScope unit="page">110411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fusion of thermal imaging and ccd camera-based data for stereovision visual telepresence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zalud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kocmanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d thermal imaging: Fusion of thermography and depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quantitative InfraRed Thermography</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Thermal stereo odometry for uavs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mouats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chermak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6335" to="6347" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic camera and range sensor calibration using a single shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">pytorch-unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<ptr target="https://github.com/usuyama/pytorch-unet" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mavnet: An effective semantic segmentation micro-network for mav-based tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?zaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loianno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wozencraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3908" to="3915" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fast-scnn: fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
