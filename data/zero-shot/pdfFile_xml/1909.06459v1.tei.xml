<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<email>qichen@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ma</surname></persName>
							<email>xuma@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihai</forename><surname>Tang</surname></persName>
							<email>sihaitang@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingda</forename><surname>Guo</surname></persName>
							<email>jingdagu@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<email>qing.yang@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Fu</surname></persName>
							<email>song.fu@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature Fusion</term>
					<term>Connected Autonomous Vehicle</term>
					<term>Edge Computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous vehicles are heavily reliant upon their sensors to perfect the perception of surrounding environments, however, with the current state of technology, the data which a vehicle uses is confined to that from its own sensors. Data sharing between vehicles and/or edge servers is limited by the available network bandwidth and the stringent real-time constraints of autonomous driving applications. To address these issues, we propose a point cloud feature based cooperative perception framework (F-Cooper) for connected autonomous vehicles to achieve a better object detection precision. Not only will feature based data be sufficient for the training process, we also use the features' intrinsically small size to achieve real-time edge computing, without running the risk of congesting the network. Our experiment results show that by fusing features, we are able to achieve a better object detection result, around 10% improvement for detection within 20 meters and 30% for further distances, as well as achieve faster edge computing with a low communication delay, requiring 71 milliseconds in certain feature selections. To the best of our knowledge, we are the first to introduce feature-level data fusion to connected autonomous vehicles for the purpose of enhancing object detection and making realtime edge computing on inter-vehicle data feasible for autonomous vehicles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Connected autonomous vehicles (CAV) provide a promising solution to improving road safety. This relies on vehicles being able to perceive road conditions and detect objects precisely in realtime. However, accurate and real-time perception is challenging in the field. It involves processing high-volume and continuous data streams from various sensors with strict timing requirements. Moreover, the perception accuracy of a vehicle is often affected by the limited view and scope of the sensors. Edge computing can help CAVs achieve better situational awareness via combining and processing information collected from multiple CAVs with more powerful machine learning technologies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. The ultimate goal of integrating edge computing and CAVs is to efficiently analyze massive amount of data in real time under limited network bandwidth.</p><p>An autonomous vehicle edge computing system consists of three layers: vehicle, edge, and cloud <ref type="bibr" target="#b17">[18]</ref>. Each autonomous vehicle is equipped with onboard edge device(s) that integrates the needed Car1 Car1 Car2 <ref type="figure">Figure 1</ref>: Occlusion and truncation situations naturally occur in point clouds data. In the left LiDAR image, only three vehicles (yellow boxes) are recognized by Car 1. When it cooperatively detects with Car 2, four more vehicles (either occluded or truncated) are detected, as shown in red boxes in the right image, which are not detected using its own data.</p><p>functional modules for autonomous driving, including localization, perception, path planning, and vehicle control. Autonomous vehicles can communicate with roadside edge servers, and eventually reach the cloud through wireless networks, e.g., the dedicated short range communication (DSRC) <ref type="bibr" target="#b6">[7]</ref>, 5G or millimeter-wave communication technologies <ref type="bibr" target="#b33">[34]</ref>. This provides a perfect opportunity to develop a cooperative perception system in which vehicles exchange their data with nearby edge servers. It is here that data are fused and processed to further extend the individual vehicle's perception range; beyond line-of-sight and beyond field-of-view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Having a single source of data input for autonomous vehicles is risky in real-world environments, as sensors are just another component of the vehicle that is susceptible to failure. In addition, sensors are also limited by their physical capabilities such as scan frequency, range, and resolution. Perception gets even worse when sensors are occluded, as shown in <ref type="figure">Fig. 1</ref>.</p><p>Among related works on cooperative perception for autonomous vehicles <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>, we find that their main focus is on improving the individual vehicle's precision, overlooking benefits from cooperative perception. Potential issues involved in cooperative perception, such as accuracy of local perception results, impact on networks, format of data to be exchanged, and data fusion on edge servers, are not addressed. When it comes to 3D object detection, Lidar is one of the most important components of autonomous driving vehicles as it generates 3D point clouds to capture the 3D structures of scenes.</p><p>This data gives precise location in 3D space with respect to the LiDAR, and by extension, the car.</p><p>Based on our best acknowledge, the state of the art 3D object detection precision based on monocular LiDAR (Light Detection and Ranging) data comes from VoxelNet <ref type="bibr" target="#b36">[37]</ref>, SECOND <ref type="bibr" target="#b35">[36]</ref> and PointR-CNN <ref type="bibr" target="#b30">[31]</ref>, etc. For example, PointRCNN achieves 75.76% mAP (mean average precision) on the KITTI moderate benchmark <ref type="bibr" target="#b7">[8]</ref>, and 85.94%, 68.32% on easy and hard benchmarks, respectively. That implies the simple fusion of object detection results from different cars would yield errors. Although fusing raw LiDAR data from two vehicles can improve the car detection precision <ref type="bibr" target="#b2">[3]</ref>, it is challenging to send the huge amount of LiDAR data generated by autonomous vehicles in real time. Solutions that increase vehicle's perception precision as well as maintaining or reducing computational complexity and turnaround time are rare in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Proposed Solution</head><p>We propose a method that improves the autonomous vehicle's detection precision without introducing much computational overhead. An useful insight is that modern object detection techniques for autonomous vehicles, both image based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> and 3D LiDAR data based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, commonly adopt a convolutional neural network (CNN) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref> to process raw data, and leverage a region proposal network (RPN) <ref type="bibr" target="#b26">[27]</ref> to detect objects. We argue that the capacity of feature maps is not fully explored, especially for 3D LiDAR data generated on autonomous vehicles, as the feature maps are used for object detection only by single vehicles.</p><p>To this end, we introduce a feature based cooperative perception (F-Cooper) framework that realizes an end-to-end 3D object detection leveraging feature-level fusion to improve detection precision. Our F-Cooper framework supports two different fusion schemes: voxel feature fusion and spatial feature fusion. While the former achieves almost the same detection precision improvement when compared to the raw-data level fusion solution <ref type="bibr" target="#b2">[3]</ref>, the latter offers the ability to dynamically adjust the size of feature maps to be transmitted. A unique characteristic of F-Cooper is that it can be deployed and executed on in-vehicle and roadside edge systems.</p><p>Aside from being able to improve detection precision, data needed for feature fusion is only one hundredth of the size of the original data. For a typical LiDAR sensor, each LiDAR frame contains about 100,000 points, which is about 4 MB. Such huge amount data would become a severe burden for any existing wireless network infrastructure. In stark contrast to the large volume of raw LiDAR data, the size of features generated by a CNN can be as low as 200 Kb after compression techniques is applied. Empirical evidences from our experiments demonstrate that transmitting these features only takes dozens of milliseconds, which makes real-time edge computing feasible. Such a negligible cost also enables feature-level fusion to become an ideal choice for connected autonomous vehicles to improve detection precision while keeping a reasonable communication time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Main Contributions</head><p>To the best of our knowledge, we are the first to propose feature map fusion based 3D object detection for connected autonomous vehicles on the edge. Through our experimentation and analysis, we have proved that not only does feature fusion provide an enhanced perception, it also allows for data to be compressed without losing detection value.</p><p>With this data compression factor, we are able to state with confidence that our feature fusion strategies are suited for autonomous vehicles On-Edge. Due to the fact that vehicles have a limited amount of computational resources on-board, we look towards the edge for more powerful and reliable computational power. Should an autonomous vehicle require extra perception, it only needs to send its compressed feature data and receive either a detection result or a compressed, fused feature map, or even both. By cutting out the computational step, we are effectively pushing the heavy workload onto the edge and mitigating any downsides to data sharing.</p><p>As proven in our experiments, both the data size and network transmission time are small enough that even in the most congested areas, vehicle data transmission will still be smooth. Both voxel feature fusion and spatial feature fusion perform better than the baseline for single vehicles without fusion, both the fusion and nonfusion baseline are derived from the same detection model. While spatial feature fusion data can be dynamically adjusted for a smaller compression size than voxel feature fusion data, the latter is capable of detection improvement on par with raw-data level fusion <ref type="bibr" target="#b2">[3]</ref>. With each strategy holding its own special advantages, we believe that our F-Cooper framework makes a substantial contribution that allows improvement no matter whether deployed in-vehicle or on roadside edge systems.</p><p>The remainder of this paper is organized as follows. Section 2 analyzes the properties of features to see if features are fit for fusion. Section 3 explains how our feature based methods work and outlines their place in F-Cooper. Section 4 tests our methods in fusion scenarios and evaluates suitability for on-edge deployment. Section 5 and 6 discuss previous works and related studies. Finally, Section 7 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TOWARDS FEATURE BASED FUSION OF VEHICLE DATA 2.1 Convolutional Feature Maps</head><p>With 3D points cloud data, the details for the location of each point are used to calculate the relationship between a car and its surrounding environment. Each frame in 3D points cloud data is processed in the same way, and one common key step in the process is to generate feature maps from points cloud data. Due to the popularity of CNN based solutions to object detection for autonomous driving vehicles <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>, in this paper, we focus on the feature maps generated by the convolutional layers in CNN networks.</p><p>As a CNN network processes raw 3D points cloud data <ref type="bibr" target="#b27">[28]</ref>, we are able to extract the processed feature maps from the CNN. These feature maps provide the essential information for object detection. <ref type="figure" target="#fig_0">Fig. 2</ref> depicts the convolutional layers in a classical CNN. First, we send the original data as input to a convolutional layer which is composed of several filters with each filter generating a feature map. All these generated feature maps are considered as the output of the first layer and will be sent to the second convolutional layer as input data. Recursively, previous layer's outputs are fed as input into the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features for Fusion</head><p>Features are a well established and integrated part of any CNN, and due to the nature of CNN, it is opaque by nature. When working with feature maps, we need to ensure that coincident issues are dealt with and explored. For example, depending on the specifications of the convolutional layers in a CNN network, the resulting voxel features may be located equal-distant from other voxels, making lossless fusion impossible to achieve without additional run-time cost.</p><p>To confirm the usefulness of features for fusion, we must answer the following three essential questions. (1) Do features possess the necessary means for fusion? (2) Are we able to communicate the data between autonomous vehicles effectively through features? (3) If features satisfy both of the two prior requirements, then how hard is it for us to obtain feature maps from autonomous vehicles? To analyse these questions and their implications, we provide an in-depth analysis in the sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Fusion Characteristics.</head><p>Inspired by the works that have been dedicated to fusing feature maps generated by different layers, such as Feature Pyramid Network (FPN) <ref type="bibr" target="#b16">[17]</ref> and Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref>, we find that it is possible to detect objects in different feature maps. For example, FPN adopts a top-down pyramid structure feature maps for detection. These networks are very adept in compounding the efficiency of feature fusion.</p><p>Taking the inspiration from these works, we make the assumption that cars compatible for fusion will use the same detection model. This is important as we see only the most reliable detection model being used for self driving. With this assumption in place, we now look at the fusion characteristics.</p><p>As feature maps are available from the CNN, we are able to ensure that all extracted feature maps are obtained with the same format and data type. Next, as feature maps extracted from 3D points cloud also contain location data, we are able to fuse the feature maps from different autonomous vehicles as long as there exists a single point of overlap in between the two vehicles. However, when we faced the issue of equal-distant location alignment, we needed to adjust our fusion algorithm to accommodate such situations. To achieve this goal, we let each car send its GPS and IMU data to allow for the transformation calculations towards point clouds fusion, i.e., transforming the view seen by a sender to the view seen by a receiver. We are clear that GPS and IMU cannot provide enough accurate details to perspective transformation. However, there are existing methods that allow for accurate alignment of two vehicles into the same 3D space. We will discuss this further in the discussion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Compression and Transmission.</head><p>Another advantage of feature maps over raw data is the transmission process between vehicles. Raw data might come in many different formats, they all achieve a single purpose, and that is to preserve the original state of the data captured. For example, LiDAR data taken from a driving session would store all the points cloud along the path of the driving session. However, this storage format records unnecessary data along with the essential data; feature maps avoid this issue. As the raw data is being processed by the CNN network, all the extraneous data is being filtered out by the network, leaving behind only information that is potentially capable of being used for object detection by the network. These feature maps are stored in sparse matrices, which only store the data deemed useful, with a 0 stored in the matrix for any data filtered out.</p><p>The data size advantage can be further compounded through lossless compression such as the gzip compression method as seen in <ref type="bibr" target="#b13">[14]</ref>. Adding in the nature of sparse matrix, we are able to combine the two to achieve compressed feature data that is no bigger than 1 MB, making features a great option for deploying On-Edge fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Generic and Inherent Properties. All autonomous driving vehicles must base their decisions on the data that their sensors generate. The raw data is generated from the physical sensors on the vehicle before getting forwarded to the onboard computing device. From there, the raw data is fed through a CNN based deep learning network to process the raw data and ultimately make the driving decisions.</p><p>During this process, we are able to pull the extracted features for sharing. By doing so, we are effectively able to obtain the feature maps of the raw data without needing extra computation time or power from the onboard computing device. With the CNN based network being used by almost all known autonomous driving vehicles to date, the feature extraction is generic and does not require further processing before fusion.</p><p>Thanks to the means by which autonomous vehicles process data, we are able to directly take the processed feature maps from the raw LiDAR points cloud data for the purpose of fusion, as this inherently provides location data. As long as the LiDAR sensor has been calibrated to the standards needed for autonomous driving, then we should have a feature map that is capable of retaining the relative locations of all things in relation to the vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">F-COOPER: FEATURE BASED COOPERATIVE PERCEPTION</head><p>Inspired by the advantages of feature map fusion in 2D object detection and the feature maps generated by 3D object detection based on LiDAR data, we propose the Feature based Cooperative Perception (F-Cooper) framework for 3D object detection. Our F-Cooper fuses feature maps generated from two LiDAR data sources oriented in two different aspects. Fusing feature maps (rather than raw data) will not only address privacy issues, but also greatly reduce the network bandwidth requirement. In F-Cooper, we present two schemes for feature fusion: Voxel Feature Fusion (VFF) and Spatial  <ref type="figure">Figure 3</ref>: Architecture of the feature based cooperative perception (F-Cooper). F-Cooper has multiple vehicles' (using two here for illustration) LiDAR data inputs which are processed by the VFE layers respectively to generate voxel features. To fuse 3D features from two cars, two fusion paradigms are designed: voxel features fusion and spatial features fusion. In Paradigm I, two sets of voxel features are fused first and then spatial feature maps are generated. In Paradigm II, spatial features are first obtained locally on individual vehicles and then fused together to generate the ultimate feature maps. Symbol indicates where the fusion takes place in each paradigm. An RPN is employed for object detection on the ultimate feature maps in both paradigms. We use dashed arrows to denote data flow and bold red lines to present fusion connections. Best viewed in color.</p><p>Feature Fusion (SFF). As shown in <ref type="figure">Fig. 3</ref>, the first scheme directly fuses the feature maps generated by the Voxel Feature Encoding (VFE) layer, while the second scheme fuses the output spatial feature maps generated by the Feature Learning Network (FLN) <ref type="bibr" target="#b36">[37]</ref>. SFF can be viewed as an enhanced version of VFE, i.e., SFF extracts spatial features locally from voxel features available on individual vehicles before they are transmitted into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voxel Features Fusion</head><p>As with pixels in a bitmap, a voxel represents a value on a regular cube in three-dimensional space. Within a voxel, there may be zero or several points cloud generated by a LiDAR sensor. For any voxel containing at least one point, a voxel feature can be generated by the VFE layer of VoxelNet <ref type="bibr" target="#b36">[37]</ref>.  <ref type="figure">Figure 4</ref>: A 128-dimensional feature is generated for each non-empty voxel in LiDAR data. For computational efficiency and data balance, we randomly sample 35 points from the voxels containing more than 35 points. The points within a voxel are then provided to the Voxel Feature Encoding (VFE) layer which produces a 128-dimensional vector. An empty voxel containing no points has no feature.</p><p>Suppose the original LiDAR detection area is divided into a voxels grid. Of these voxels, we will obtain a vast majority of empty voxels with the remaining ones containing critical information. All nonempty voxels are transformed by a series of full connection layers and converted into a fixed-size vector, with a length of 128. The fixed-sized vector is often referred to as feature map. An example feature map derived from 3D point cloud data is shown on the right part of <ref type="figure" target="#fig_3">Fig. 6</ref>. For example, as shown in <ref type="figure">Fig. 4</ref>, only four voxels are non-empty amongst the twelve voxels present, and each of the four selected voxels becomes a 128-dimensional vector. Maxout C 3 C 5a C 5b C 5c C 5d <ref type="figure">Figure 5</ref>: Voxel features fusion. When two voxels share the same location, we use maxout function to fuse them.</p><p>For memory/compute efficiency, we save the features of nonempty voxels into a hash table where the voxel coordinates are used as the hash keys. As our focus is primarily on autonomous driving, we only store non-empty voxels into our hash table. Combining the fact that our 3D point clouds are of outside driving scenarios, which yields around a few thousand voxels, searching the hash table for voxel fusion becomes an non-factor in the overall speed of our framework. In VFF, we explicitly combine the features of all voxels from two inputs, as depicted in <ref type="figure">Fig. 5</ref>. Specifically, the Voxel 3 from Car 1 and Voxel 5 from Car 2 share the same calibrated location. While the two cars are located in different locations physically, they share the same calibrated 3D space, with different offsets indicating the relative physical location of each car in said 3D calibrated space. To this end, we employ the element-wise maxout scheme to fuse Voxel 3 and Voxel 5.</p><p>Taking inspiration from convolutional neural networks, using maxout <ref type="bibr" target="#b10">[11]</ref> for latent scale selection, we extract the obvious features while suppressing the features that does not contribute to detection in 3D space, thus achieving lower data size. In our experiments, we use the maxout to decide which feature is most prominent when comparing the data in between vehicles. We denote these two voxel features as V 3 and V 5 , respectively, and V i as the i-th element in the voxel. The fused features V can be presented as follows.</p><formula xml:id="formula_0">V i = max V i 3 , V i 5 , ?i = 1, ? ? ? , 128<label>(1)</label></formula><p>The key insight behind our maxout strategy is that it emphasizes important features and removes trivial ones. Also, as maxout is a simple floating-point operation, it introduces no extra parameters. Such a negligible additional computational overhead can be ignored when compared to the overall improvement on object detection precision.</p><p>Naturally, we expect voxels from two cars are able to be perfectly matched. However, this is impractical for real-world applications, even slight bias between voxels would explicitly lead to mismatches. Here, we showcase four different mismatched situations in <ref type="figure">Fig. 5</ref>. The green dot C 3 indicates the center of the voxel 3 from Car 1 and the diamonds C 5a , C 5b , C 5c , C 5d denote the possible centers of the voxel 5 from Car 2. In case (a), the center of Voxel 5, denoted as C 5a , falls within Voxel 3. In case (b), the center C 5b falls on one side of the voxel 3, meaning Voxel 5 connects with two voxels from Car 1. In case (c), C 5c falls along an edge of Voxel 3, which implies Voxel 5 intersects with four voxels from Car 1. In case (d), C 5d falls on a corner point of Voxel 3 and connects with eight voxels. For case (a), we fuse the voxel 3 and voxel 5 directly using maxout. For cases (b,c,d), we fuse Voxel 5 with all the connected voxels from Car 1, and give the results to the connected voxels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Feature Fusion</head><p>VFF needs to consider the features of all voxels from two cars, which involves a large amount of data exchanged between vehicles. To further reduce the network traffic, as well as keeping the benefits of feature based fusion, we design a spatial feature fusion (SFF) scheme. Compared to VFF, SFF fuses spatial feature maps, which are sparser when compared to voxel features and thus more easily compressed for communication. <ref type="figure">Fig. 3</ref> intuitively showcases the relationship between VFF and SFF. Different from VFF, we pre-process the voxel features on each vehicle to get the spatial features. Next we fuse the two source spatial features together and forward the fused spatial features to a RPN for region proposal and object detection. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, the spatial feature maps of a LiDAR frame is generated by the Feature Learning Network <ref type="bibr" target="#b36">[37]</ref>. The output of the feature learning network is a sparse tensor, which has a shape of 128 ? 10 ? 400 ? 352. In order to integrate all the voxel features, we adopt three 3D convolutional layers, and sequentially obtain smaller feature maps with more semantic information and a size of 64 ? 2 ? 400 ? 352. However, the generated features cannot fit into the required shape of the conventional region proposal network. To this end, we must reshape the outputs to the 3D feature maps of size 128 ? 400 ? 352 before we can forward them to RPN. For SFF, we generate a bigger detection range with size W ? H , where W &gt; W 1 , H &gt; H 1 . Next we fuse the overlapped regions while retaining the original features in the non-overlapped regions. Suppose a GPS records the real-world location of Car 1 as (x 1 , y 1 ) and Car 2 as (x 2 , y 2 ), then we can get the position of the left-top corner. And if x 2 + H 1 , y 2 ? W 1 2 belongs to Car 2's feature maps with the lefttop corner being representative of the feature maps of Car 1, then it is easy for us to determine the overlapped areas. Similar to VFF adopting the maxout strategy, we also employ maxout for SFF to fuse the overlapped spatial features.</p><p>As indicated in <ref type="figure">Fig. 7</ref>, the top left corner of Car 2's feature maps can be presented as x 2 + H 1 , , y 2 ? W 1 2 if the car moves towards left. Suppose the corner point falls in the region of Car 1's feature map, then we can fuse the overlapped features in the same manner as the voxel fusion strategy.</p><p>Finally, we adopt region proposal network to propose potential regions on the fused feature maps. Paradigm II in <ref type="figure">Fig. 3</ref> holistically showcases the pipeline of our SFF.</p><p>Recent work like SENet <ref type="bibr" target="#b12">[13]</ref> indicates that different channels share different weights. That is to say some channels in feature maps contribute more toward classification/detection while other channels being redundant or unneeded. Inspired by this, we opt to select partial channels, out of all 128 channels, to transport. We assume that autonomous vehicles are assembled with the same well-trained detection model as in real-world applications. After extensive experimentation, we demonstrate that transporting part of channels can further reduce the time consumption of transmission while keeping the comparable detection precision in our experimental analysis in Section 4.  <ref type="figure">Figure 7</ref>: For spatial features fusion, we use maxout to fuse the two spatial features. The left-top is the spatial feature maps generated by Car 1, and the left-bottom is the spatial feature maps generated by Car 2. After fusion, the fused feature maps contain the key features (marked in red and green boxes) of both feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object Detection Using Fused Features</head><p>For detecting vehicles, we feed the synthetic feature maps to a Region Propose Network (RPN) for object proposal. Next a loss function is applied for network training. <ref type="figure">Fig. 3</ref>, once we get the spatial feature maps, regardless of whether we adopt voxel fusion paradigm or spatial fusion paradigm, we send it to the region proposal networks (RPN) <ref type="bibr" target="#b36">[37]</ref>. After passing through the RPN network, we will obtain two generated outputs for a loss function (Section 3.3.2): (1) a probability score p ? [0, 1] of the proposed region of interests, and (2) the locations of proposed regions P = (P x , P w , P z , P l , P w , P h , P ? ), where P x , P y , P z indicates the center of the proposed region and (P l , P w , P h , P ? ) means the length, width, height and rotation angle, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Region Proposed Network. As indicated in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Loss Function.</head><p>The loss function is comprised of two parts: classification loss L cl s and regression loss L r e? .</p><p>Suppose a 3D ground-truth bounding box can be presented as</p><formula xml:id="formula_1">G = G x , G y , G z , G l , G w , G h , G ? , where G x , G y , G z represents the central point of the box, and (G l , G w , G h , G ? ) denotes the length,</formula><p>width, height and yaw rotation angle, respectively. Our proposed method will generate a vector P to represent the predicted 3D box. In order to minimize the loss between our prediction and the ground truth, we regress our predicted boxes by minimizing the differences (?x, ?y, ?z, ?l, ?w, ?? ) <ref type="bibr" target="#b9">[10]</ref> as:</p><formula xml:id="formula_2">?x = G x ? P x P d , ?y = G y ? P y P d , ?z = G z ? P z P h ?l = log G l P l , ?w = log G w P w , ?h = log G h P h ?? = G ? ? P ?<label>(2)</label></formula><p>where P d = (P l ) 2 + (P w ) 2 1 2 is the dialog of length and width . Suppose our model proposes N pos positive anchors and N ne? negative anchors, we define the loss function as follows:</p><formula xml:id="formula_3">L = ? 1 N ne? N ne? i=1 L cls p i ne? , 0 + ? 1 N pos N pos i=1 L cls p i pos , 1 + 1 N pos N ne? i=1 L r e? P i , G i<label>(3)</label></formula><p>where p i ne? and p i pos are the probability of positive anchors and negative anchors respectively, and N ne? and N pos denote the number of proposed negative and positive anchors respectively. In regression loss, G i indicates the ith ground truth while P i means the corresponding predicted anchor. We use ? and ? to balance these three losses. We employ a binary cross entropy loss for classification Loss and Smooth-L1 loss function <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE EVALUATION 4.1 Datasets</head><p>KITTI <ref type="bibr" target="#b7">[8]</ref> is a well-known vision benchmark suite project which contains labeled data that allows for autonomous vehicles to train detection models and evaluate detection precision .</p><p>As we focus on 3D object detection, we use the 3D Velodyne point cloud data provided by the KITTI dataset. The cloud point data provides 100K points per frame and is stored in a binary float matrix. The data includes 3D location of each point and associated reflectance information. However, as KITTI data is recorded from single vehicles, we must utilize different time segments from the same recording to emulate data generated from two vehicles. As a result, KITTI data is only suitable for certain test scenarios.</p><p>To address this issue, we equip two vehicles, named Tom &amp; Jerry (T&amp;J), with necessary sensors, such as LiDARs (Velodyne VLP-16), cameras (Allied Vision Mako G-319C), radars (Delphi ESR 2.5), IMU&amp;GPSes (Xsens MTi-G-710 kit), and edge computing devices (Nvidia Drive PX2) to gather desired data on the campus of our institution. Our vehicles have 16-beam Velodyn LiDAR sensors that store data in binary raw Ethernet packets. As our vehicles can move independently of each other, we are able to test the entire gamut of scenarios in a real-world environment with our two vehicles.</p><p>Both datasets provide data that allows 3D object detection. Moreover, the LiDAR data provided contains ample data for us to extract feature maps from the CNN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test Scenarios</head><p>From these two datasets, we are able to fully test or simulate an array of different common scenarios such as those detailed below.</p><p>Road intersections. One of the most common places for cars to congregate and thus cause occlusion is a busy road intersection. As the optical based LiDAR and camera sensors are blocked by vehicles in front of them, the information becomes severely limited. Due to this fact, we included this scenario as one of our test cases. Multi-lane roads. Another common place is a multi-lane road. Such roads feature the combination of high speed driving and Tjunctions, both of which are prone to accidents. To ensure our F-Cooper framework is capable in such extreme situations, we also included this scenario in our experiments.</p><p>Campus parking lots. Last but not least, as our main objective is to enhance perception through fusion, we need to test our framework in a crowded situation with many obstacles. As congested zones are best represented by a crowded parking lot, we choose busy campus parking lots as our main test scenario to evaluate the accuracy of F-Cooper in a real-life environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Setup</head><p>To evaluate F-Cooper, over 200 sets of data were collected and tested in our experiments. We separate our tests into four categories, based on the methods used to process the LiDAR data, methods (1) through (3) are derived from the same detection model: (1) Non-fusion as baseline, (2) F-Cooper with VFF, (3) F-Cooper with SFF, and (4) raw point clouds fusion method -Cooper <ref type="bibr" target="#b2">[3]</ref>. Feature fusion takes place in random cases of the above four categories with a heavier focus on busy campus parking lots as it is the most difficult scenario due to significant occlusions. Within each category, we further divide our experiments by considering the distances between objects and the sensing vehicle. We treat objects that are within 20 meters away from a vehicle as high-priority objects and those beyond 20 meters as low-priority objects in the parking lot environment.</p><p>As our LiDAR sensor has only 16 beams, the resulting point cloud data is relatively sparse, compared to higher-end LiDAR sensors. To mitigate the negative impacts of sparse data, we limit the detection range to [0,70.4] by <ref type="bibr">[-40</ref>,40] by [-3,1] meters along the X, Y, and Z axles. We do not use data beyond the detection ranges. In addition to the vehicle's detection range, we also set the voxel size as v D = 0.4 meter, v H = 0.2 meter, v W = 0.2 meter, and thus D 1 = 10, H 1 = 400, and W 1 = 352. In our experiments, the F-Cooper framework runs on a computer with a GeForce GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Top-Level Evaluation of F-Cooper</head><p>To evaluate F-Cooper, we analyze each component individually as well as against other frameworks. Starting with VFF, we can see the results of fusion from two cars in <ref type="figure">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>, with data receiving vehicle (Car 1) and data transmitting vehicle (Car 2). In the figure, we have the LiDAR representation with the detection results on the top and the right-camera in the middle and the left-camera at the bottom. Both the baseline detection and the fusion detection use 0.5 as a confidence threshold, meaning if the confidence level is above this score, we mark the boundary box for that object. As we can see in column (c) of <ref type="figure">Fig. 8 and Fig. 9</ref>, we have the voxel fusion result on the top and the raw data fusion result on the bottom. Through all of our marked bounding boxes, we have distinguished them in three levels of importance to the receiving car: yellow, green and red. The cars marked in yellow represent those that have already been detected by the receiving car originally. The cars marked in green represent those detected by only the sender and not the receiver. The cars marked in red represent those undetected by neither the sender nor the receiver but detected after feature fusion. Taking a closer look at <ref type="figure">Fig. 8</ref>, which details two cars driving forward in parallel, we can see that Car 1 was able to detect four vehicles while Car 2 was able to detect three vehicles. However, in both cases, neither Car 1 nor Car 2 was able to detect cars further away. This was caused by a combination of factors such as occlusion and distance. Through VFF, we are able to detect cars previously occluded to Car 1 or was completely undetected by either cars.</p><p>Similarly, <ref type="figure">Fig. 9</ref> depicts two cars approaching each other from opposing directions. In this figure, we can see that Car 1 detects three vehicles while Car 2 detects four. However, when SFF was conducted, we can see that spatial fusion only enhanced perception by two detections for Car 1 where as raw data fusion enhanced detection by three. A closer inspection reveals that the right most new detection from SFF was not detected in the raw data fusion. From this comparison, we can see that while VFF is similar in precision to raw data fusion, SFF is able to perform better for near cars when compared to VFF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detection Precision Analysis</head><p>Having taken an overview of the precision of our two feature fusion methods, we dive into the details of how each method performs against each other as well as against the baseline, and the Cooper approaches <ref type="bibr" target="#b2">[3]</ref>.</p><p>The data that we use for this analysis comes from both datasets to test all of our listed scenarios. In all our experiments, we report our results using Intersection over Union (IoU) threshold at 0.7 for vehicles. Then, we calculate the precision by comparing the detected vehicles with the ground truth.</p><p>In <ref type="table">Table 1</ref>, we observe that in our baseline test, baseline without fusion on Car 1 achieve a good "Near" detection precision for the road scenarios but fall off sharply in precision in their "Far" detection. As mentioned before, the "Near" and "Far" cut off is 20 meters from the car as the center. Next, looking at how the baseline performs in parking lot scenarios where the most occlusions take place, we can see that again, the "Near" precision is much better than the "Far". This is understandable as the lasers reach out further, it returns a much sparser point cloud.</p><p>Moving on to our method testing, we compare the precision against both the baseline and raw fusion <ref type="bibr" target="#b2">[3]</ref>. It should be noted that we only compared against fusion methods instead of non-fusion detection methods as the former yields a meaningful comparison whereas the latter is not relatable in the same context. For our road scenarios, we see that VFF achieves a similar precision to Cooper (a raw data fusion solution). This signifies that VFF is as capable as raw data fusion method for near object detection, but without collecting others point clouds.</p><p>Interestingly, as we look at the SFF precision, we can see a drastic difference in between the "Near" and "Far" precision. While SFF does not outperform VFF, it was still able to perform better than the baselines in most scenarios. However, it must be noted that SFF is more sparse than both voxel features and raw data by a considerable margin. When we factor in the fact that spatial features are derived from the voxel features, it is normal to have the better precision in the regions where the data is naturally denser, i.e., "Near" the vehicle where the LiDAR point cloud data is the densest.</p><p>To distinguish the differences in how well VFF and SFFs perform in the "Near" and "Far" categories respectively, <ref type="figure" target="#fig_7">Fig. 10</ref> shows the cumulative distribution functions of increase in detection precision. Additionally, in the "Far" category, VFF was able to achieve a 40% detection precision increase for almost 85% of the time; it is also able to increase detection precision by 60% for 30% of the time. Looking at SFF, we do see an increase in detection precision, however, it is not as great of an increase as VFF shows. When it comes to the "Near" category, however, SFF was able to perform as well as VFF if not better in some cases. In <ref type="figure" target="#fig_7">Fig. 10</ref>, SFF and VFF are both at 50% chance to increase detection precision by 20%. But, as we look deeper, we find that SFF outperforms VFF slightly at 30% chance to increase precision by 30%. We conclude from this test that our detection range is able to be extended with an overall average increase in detection precision due to the extra features being harvested. As our features may target the same object multiple times, the detection confidence scores also see a notable increase. The reason why detection results become more precise after fusion is due to the points from different cars becoming fused, thus making the originally sparse data representation of a 3D object less sparse and more "outlined". This allows for the detection model to have a higher precision. Moreover, as single cars have a limited range on their LiDAR beams, multi-car fusion allows for points in the distance to be registered by the receiving car. Through fusion, the missing points in the distance are provided by the other cars, and thus allowing for the recipient car to enhance its detection results. Our detection precision may increase even further with more vehicles sharing data, solving the issue of missing detection on some of our target cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Sensitivity and Resilience</head><p>As feature fusion relies heavily on location information for fusion, alignment has a big impact on the final detection precision of the fusion. To understand the sensitivity and resilience of F-Cooper, we will not only study missed detections, but also compare the changes of confidence level of each detected target.</p><p>In real world situations, all sensors are built within a specific acceptable error tolerance. However, these small discrepancies in between different sensors may cause the same object in 3D space to be labeled at slightly different locations by different cars. As SFF is by nature sensitive to the position of the features, we need to deal with this phenomenon in our fusion. When we integrate our GPS and IMU data, we observe yields of less than 10 cm of positional error <ref type="bibr" target="#b0">[1]</ref>. Additionally, when we explained the nature of voxel and spatial fusion in Sections 3.1 and 3.2, we noticed the discrepancies in location based data fusion. To test the resilience of our fusion methods against sensor drift, we conducted procedural artificial skewing of our GPS readings as seen in <ref type="figure" target="#fig_8">Fig 11.</ref> In <ref type="figure" target="#fig_8">Fig. 11, we have part (a)</ref> showing the scenarios and part (b) and (c) displaying the effects of GPS drifting on VFF and SFF. First, in both VFF and SFF, we can see that there are two tables, one with a drift of 0 meters and the other with a drift of 0.1 meters to simulate drift. The target cars are then separated into "Far" and "Near" groups with respect to the location of each vehicle, "Far" is shaded dark while "Near" is not shaded.</p><p>When we focus on the missed detections, the experiment results indicate location drifting does not significantly affect the detection accuracy of SFF. On the other hand, if we look at the confidence score of each detected target, we find that VFF strategy is not too sensitive to GPS drifting. Taking all of the changes from all of our target scores of VFF, the average of increase and decrease in our confidence scores balance out, indicating that GPS drifting slightly affects VFF. Considering the same scores of SFF, we see that the average of change becomes worse, when compared to VFF, indicating that SFF is more sensitive to GPS drifting than VFF. During our experimentation, SFF performed worse than VFF in the "Far" category. After careful investigation on our experimental setup and methodology, we concluded the following: Compared to raw point cloud fusion and voxel feature fusion, spatial feature fusion is relatively low in feature map resolution. This factor is exponentially amplified during detection for objects in "Far" category as well as for detection of small objects. In retrospect, we realize that for feature extraction on small object, we are even more susceptible to location distance. Furthermore, smaller objects may suffer from missing features after extraction. In point cloud data, when fusing from different angles and perspectives, we are at a higher risk of merging features from different aspects, therein causing a detection conflict. We believe that to overcome this issue, we need to propose a voxel feature extraction method that allows for surgical extraction of features from point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">F-Cooper On-Edge</head><p>Even though point clouds can be simplified to coordinate values, we still need to consider the gap between data generated by autonomous vehicles and the limited wireless networking resources, such as the limited bandwidth provided by DSRC.</p><p>Due to this limitation, we cannot simply transmit raw data for the purpose of fusion, as that would congest the network as well as consume valuable on-board computing resources. With F-Cooper, we are able to eliminate this limitation. 4.7.1 Transmission. First, both VFF and SFF are fusion methods that allow for enhanced perception, with VFF achieving close to raw data fusion and SFF achieving better "Near" detection results than our baseline. Second, both of our feature fusion methods allow for a final compression size of less than 1 MB, which is well within DSRC limits. Due to the limitations of DSRC, F-Cooper restricts the frequency of data exchange between vehicles to 1Hz (1 fusion per second). Given the nature of 3D detection and the situations that we envision, it is not necessary to have a continuous stream of data of more than 1Hz to achieve enhanced precision. For the majority of cases, only one frame of data is needed to provide crucial supplement to the recipient vehicle. In the case of obstructed views, the feature fusion on a single frame, from different perspectives, will be enough to provide ample warning.</p><p>With VFF achieving better results, why do we still need SFF? To answer this question, we analyze the impact of different spatial feature maps on the detection results. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, derived from <ref type="figure">Fig. 9</ref>, we have the indexes of channels used in SFF as well as their respective detection precision for each of the 5 vehicles in the scene. We have 0-127 channels representing full feature maps usage, while 55-99 channels representing the range of key channels contributing the most to SFF results, 95-99 channels represent a minimal set of required channels to obtain a reasonable detection result. This finding is crucial for deploying fusion strategies on the edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Computation.</head><p>Due to the small number of channels being used to detect vehicles, we are able to reduce the amount of data that needs to be encoded for compression and transmission. Looking at <ref type="figure" target="#fig_11">Fig. 13</ref> and <ref type="figure" target="#fig_12">Fig. 14,</ref> we have the graphs depicting both the data volume and processing time for each of our fusion strategies.  From <ref type="figure" target="#fig_11">Fig. 13</ref> we see that the raw point cloud data is about 2 MB when taken directly from our defined LiDAR range as mentioned in the experiment setup section. Similarly, the original data volume for spatial feature is 72.1 MB and 1 to 1.3 MB for voxel feature. However, both voxel and spatial data is capable of being compressed to less than 1 MB as shown in the figure. When combining the data from <ref type="figure" target="#fig_0">Fig. 12</ref> and <ref type="figure" target="#fig_11">Fig. 13</ref>, we can see that with a 55-99 channel SFF compressed, we achieve the highest compression results for all five cases, the average of which is 250 KB. Additionally, if we are to use 95-99 channel SFF, then the end result will achieve an even higher compression. At the same time, SFF is capable of achieving a similar precision while being capable of a far better compression. With this, we can now analyze in <ref type="figure" target="#fig_12">Fig. 14</ref>  Firstly, it should be noted that as vehicles are communicating with each other for data transmission and computation, they are eating up valuable computational resources, so to achieve the best result when it comes to augmenting their perception based on the data from nearby vehicles, edge computing becomes the most important factor.</p><p>As shown in <ref type="figure" target="#fig_12">Fig. 14,</ref> the total time used for both the raw fusion and SFF strategies are both close to the 1 second mark. Here, the total time we state includes the time for both data processing/transmission and object detection. This can become quite the issues when compounding this factor with the fact that a single vehicle may need to process the same request from other vehicles at the same time, causing a waste of computational resources. However, when we cut down the total time to just the transmission time needed for the vehicle to transmit and receive the result to and from an edge node, then we have a very feasible method of reliably enhancing perception with no downsides, especially since transmitting features to an edge computing node will not compromise any privacy. Hence, our fastest strategies only requiring less than a tenth of a second to send and receive results from an edge computing device; the vehicle will only be responsible for sending the data needed for feature fusion without needing to consume computation resources on decoding, fusing and computing the results from other vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Summary of Experimental Analysis</head><p>We adopted an On-Edge end to end framework, F-Cooper, and achieved a satisfactory collaborative perception towards enhancing detection. Both of our strategies, VFF and SFF, performed better than our single car detection results in almost all of our tests. In addition to better precision, our methods were also lightweight and versatile enough to be deployed in On-Edge systems without adjustments to the current infrastructure of autonomous vehicles.</p><p>We also discover that F-Cooper can be leveraged to achieve a reasonable tradeoff in a vehicular edge computing system, considering not only latency and prediction compensation but also data size and network bandwidth. In our experiments, F-Cooper helps detect more objects that are unclear in the distance. This allows for a less constrained latency range as the fusion allows for distant objects to be detected before the car in question reaches that point in space. In addition, with regards to CNN channel selection and compression, our resulting data sizes make low latency transfers a possibility. We endeavor to continue researching even more powerful methods in future works. Lastly, we are only simulating the latency on DSRC channels as that is the most immediate networking medium. However, there are also 5G and millimeter-wave vehicular communications techniques <ref type="bibr" target="#b33">[34]</ref> coming into play, allowing for much smaller latency. Latency is a massive issue, and we are not able to solve the real time challenges fully with our current methodology, but we will continue to strive in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The exploration of object data fusion has prevailed for years. Usually, data fusion methods can be grouped into 3 categories: low level, feature level and high level data fusion <ref type="bibr" target="#b29">[30]</ref>.</p><p>In the era of high level fusion, several works are conducted to fuse the detection results in pursuit of improving detection precision. The work by <ref type="bibr" target="#b25">[26]</ref> exploits a high level sensor data fusion architecture named Car2X-based perception. Their pioneering work delivers one vehicle's consistent results for fusion with the results generated by the host vehicle. High level fusion on multi-sensors has been well investigated to facilitate the development of 3D object detection. <ref type="bibr" target="#b5">[6]</ref> proposed to detect and track moving objects using fused results from multiple sensors. Recently, Crowd sourcing, which has been learned in an automated manner <ref type="bibr" target="#b23">[24]</ref>, has shown competitive perception precision. Sensors from various vehicles are typically crowd-sourced, as cooperators, to provide wider spatial coverage as well as disambiguation. However, their inability to explore undetected objects and the lack of semantic information communication caused the limited success of cooperative perception system. To this end, Qi et al. presented Cooper <ref type="bibr" target="#b2">[3]</ref>, which fuses original calibrated raw LiDAR data from multiple vehicles to improve 3D detection precision in a low-level data fusion method.</p><p>Though Data fusion has been adopted in many areas, such as object detection and object tracking <ref type="bibr" target="#b20">[21]</ref>, the idea of fusing data from multiple sources data On-Edge has been explored by only a few authors. An inspiring work is <ref type="bibr" target="#b28">[29]</ref>, where the authors developed a shared real-time situational awareness system by aggregating crowd sourcing and edge computing together. Another related work is <ref type="bibr" target="#b19">[20]</ref>, which employ collaborative learning On-Edge computing. However, the challenges that edge computing needs to face in the specific application of object detection are not mentioned in this paper.</p><p>Our fusion strategy is different from previous feature-level data fusion methods. For example, <ref type="bibr" target="#b16">[17]</ref> fuses features from different convolutional layers in one detection model, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> fuse features from different sensors within one veihcle. In pursuit of better representation ability, we fuse processed LiDAR features from multiple vehicles. We argue that fusing features from different perspectives is a better solution to improve detection precision. Similar to our work, AVR <ref type="bibr" target="#b22">[23]</ref> extends vision of multiple vehicles by communicating short range stereo camera data. The method uses metadata for localization in 3D map, allowing for a much more precise calibration. Unlike aforementioned works, we present a feature level data fusion method in pursuit of lightweight On-Edge deployment for connected autonomous vehicles. Our methods are fully suited for On-Edge deployment since the amount of transmitted data is significantly reduced and it effectively takes the advantage of On-Edge computing capacity. Finally, our method is based on intermediate features, which can detect more possible objects than high-level data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSIONS</head><p>While it is faster to implement high-level data fusion, there is a fundamental flaw associated with this action. As high-level fusion is fusing object detection results from individual cars, we cannot avoid the issue of what if no car senses enough information to detect a critical object. An example would be if car A and car B both detect half of an object, but neither can detect the whole object due to missing half of the point cloud data. Because neither detected the object, the high-level fusion result will exclude the object.</p><p>Another issue involved in data fusion is perspective transformation, in which a receiver needs to estimate its position relative to a sender, so the sender's data can be mapped into the receiver's local coordinate system. Existing solutions, e.g., AVR (augmented vehicular reality) <ref type="bibr" target="#b22">[23]</ref>, have been proposed for precise fusion. AVR, with an offline sparse 3D map as the benchmark, can provide an accurate relative localization among vehicles, and thus increase data fusion precision.</p><p>Although it is outside the scope of the paper, the fusion of information from different vehicles at the edge opens the door to security vulnerabilities. A prime example can be a malicious vehicle sending phantom vehicle information. This might benefit the malicious vehicle by making space for itself through sending fake information. However, to the general public, this poses a serious driving hazard as they could potentially incur an accident from trying to avoid the phantom vehicle. In addition, we must acknowledge that a vehicle might be unintentionally malicious due to the potential of faulty sensors. This poses the question of how does a vehicle trust the information provided by another vehicle.</p><p>Towards these two issues, we assume that all sources are valid and trustworthy for experimentation purposes; however, these issues must be addressed. One possible approach is to have the edge perform the fusion and check the past history of how trustworthy of individual vehicles, and to have the edge perform authentication of newly registered vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we proposed F-Cooper, which provides both a new framework for applications On-Edge servicing autonomous vehicles as well as new strategies for 3D fusion detection. Through experiment testing and analysis, we conclude that not only does F-Cooper perform at the same level as Cooper, but it also has the added benefits of being more lightweight and computationally inexpensive. Both voxel features and spatial features have their separate advantages and special uses. Compounded with their great fusion detection enhancing capabilities, both strategies are well suited for autonomous vehicles On-Edge.</p><p>Voxel feature fusion out performs spatial feature fusion, but likewise, spatial feature fusion can be adjusted to be more suited for compression and data transfer. As both methods achieve a high detection perception enhancement over the baseline, both are viable for fusion. When we consider the size difference between raw data generated by each autonomous vehicle and only features from the 3D LiDAR data, it becomes clear that the latter is much more suited towards networks with a limited bandwidth.</p><p>When we apply F-Cooper to real-world scenarios, our experimental results on both the data volume and transmission time fall well within acceptable range for On-Edge computation and communication. Thus, from our evaluation, we believe that our proposed F-Cooper framework will add improvement to connected autonomous vehicle system, no matter where or how it is deployed for either in-vehicle or on roadside edge computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work is supported by National Science Foundation (NSF) grants NSF CNS-1761641 and CNS-1852134. We thank the anonymous reviewers for their many suggestions for improving this paper. In particular we thank our shepherd Ramesh Govindan at the University of Southern California, who have read the previous versions of this paper and provided valuable feedback on our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Convolutional feature maps in a classical CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Example of spatial feature maps. H 1 and W 1 represent the size of the LiDAR bird-eye view for each vehicle's detection range, while C indicates the channels number. It is worth noting that we fuse spatial features in a channel-wise manner, where the channels indicate the corresponding kernel numbers used in CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 8 :</head><label>18</label><figDesc>Comparing detection precision on voxel-feature fusion cases when two cars drive forward in parallel . In (a) and (b), the top line is detection results on LiDAR data, while the middle and bottom lines are left and right camera images respectively. In (c), the top line is the result of our voxel fusion and the bottom line is the result of Cooper [3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 Figure 9 :</head><label>19</label><figDesc>Comparing detection accuracy on spatial-feature fusion cases when two cars approach each other from opposing directions. (a) shows the detection results of car 1, (b) shows the detection results of car 2, while (c) is the results on spatial feature fused data and raw fused data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Cumulative Distribution Function vs. detection precision improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>GPS reading drifting impact on F-Cooper. (a): intuitive detection result. (b): numeric detection results of VFF. (c): numeric detection results of SFF. The table exhaustively showcases the detection confidence value on each car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Detection precision of selective channels on spatial feature fusion. The channels here indicate the corresponding kernel numbers used in CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Comparison (C.) on data volume using different fusion approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Comparison on time consuming using different fusion approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Paradigm II: Spatial Feature Fusion (SFF) Paradigm I: Voxel Feature Fusion (VFF)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Fused Voxel Feature</cell><cell>Sparse Convolutional Layer</cell><cell>Spatial Feature</cell><cell>Region Proposal Network</cell><cell>Detection Results</cell></row><row><cell>Voxel Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoding</cell><cell>Voxel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIDAR data from Car 1</cell><cell></cell><cell>Sparse Convolutional Layer</cell><cell>Spatial Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Voxel Feature Encoding</cell><cell>Voxel Feature</cell><cell>Sparse</cell><cell>Spatial</cell><cell>Fused Spatial Feature</cell><cell>Region Proposal Network</cell><cell>Detection Results</cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIDAR data from Car 2</cell><cell></cell><cell>Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>for how this strategy fares in time consumption.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integration of GPS, Monocular Vision, and High Definition (HD) Map for Accurate Vehicle Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3270</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05265</idno>
		<title level="m">Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan Raj</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page" from="1836" to="1843" />
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performance of the 802.11 p physical layer in vehicleto-vehicle environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Borries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bvk Vijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Stancil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lossless compression of predicted floating-point geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Isenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Snoeyink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="869" to="877" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09738</idno>
		<title level="m">Stereo R-CNN based 3D Object Detection for Autonomous Driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edge Computing for Autonomous Driving: Opportunities and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangkai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collaborative Learning on the Edges: A Case Study on Connected Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd {USENIX} Workshop on Hot Topics in Edge Computing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AVR: Augmented vehicular reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawad</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 16th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="81" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards robust vehicular context sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorkem</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1909" to="1922" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Refining object detectors for autonomous driving</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<editor>Rakesh Nattoji Rajaram, Eshed Ohn-Bar, and Mohan Manubhai Trivedi</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="358" to="368" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Car2x-based perception in a high-level fusion architecture for cooperative perception systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Klanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Rasshofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1476" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Edge computing for situational awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN). IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Leveraging Spatio-Temporal Evidence and Independent Vision Channel to Improve Multi-Sensor Fusion for Vehicle Environmental Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV). IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
	<note>Jingmin Xin, and Nanning Zheng</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge computing: Vision and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhuizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Millimeter wave vehicular communications: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vutha</forename><surname>Va</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Heath</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Networking</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Symposium on Edge Computing (SEC)</title>
		<imprint>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
