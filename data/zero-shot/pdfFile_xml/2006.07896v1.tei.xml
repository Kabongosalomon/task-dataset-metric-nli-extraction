<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Team RUC AI?M 3 Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<email>qjin@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Team RUC AI?M 3 Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting meaningful events in an untrimmed video is essential for dense video captioning. In this work, we propose a novel and simple model for event sequence generation and explore temporal relationships of the event sequence in the video. The proposed model omits inefficient two-stage proposal generation and directly generates event boundaries conditioned on bi-directional temporal dependency in one pass. Experimental results show that the proposed event sequence generation model can generate more accurate and diverse events within a small number of proposals. For the event captioning, we follow our previous work [3] to employ the intra-event captioning models into our pipeline system. The overall system achieves state-of-the-art performance on the dense-captioning events in video task with 9.894 ME-TEOR score on the challenge testing set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of dense video captioning <ref type="bibr" target="#b7">[8]</ref> aims to describe videos with a sequence of sentences rather than a single caption as in traditional video captioning. To generate informative video descriptions, it is important to first detect meaningful events in the untrimmed video.</p><p>Previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref> mainly adopt a two-stage method to detect the events, including the candidate proposal generation stage and proposal selection stage. The sliding windows or neural networks such as SST <ref type="bibr" target="#b0">[1]</ref> are first used to propose a large amount of event candidates. Then event classifiers are designed to predict event confidence for each candidate. The event proposals with confidences higher than a fixed threshold will be selected as the final events. There are two major drawbacks of such two-stage approach: First, a large amount of candidates (about 1000) need to be generated to ensure covering all the possible events, which is not efficient and computationally expensive. Second, it does not consider temporal rela- * Corresponding author. tionships between the events which may lead to candidates with high redundancy. However, the events sequence in the video usually follows temporal orders as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We make statistics on the events sequential orders in Activ-ityNet dataset <ref type="bibr" target="#b7">[8]</ref> based on the start and end timestamp of each event. As shown in <ref type="table" target="#tab_0">Table 1</ref>, about 81.5% of videos in AcvitityNet dataset contain events in a clearly sequential order (one after another), while 16.94% of videos contain events in a "Summary-Details" order. Only 1.16% of videos contain events not in any order. Therefore, the events detection can be viewed as a sequence generation problem to directly generate event boundaries one by one.</p><p>In this work, we propose a novel and simple event sequence generation model, which fully exploits bidirectional temporal dependency of each event to generate event boundaries directly. It takes previous event as the input and predicts next event distribution over the whole video timeline at each decoding step conditioned on the encoded videos. To exploit both the past and future event contexts, we generate the event sequence in both forward and backward directions and then fuse the distribution maps in bidirections to generate final event boundaries. Experiments on ActivityNet Captions dataset demonstrate the proposed event sequence generation model can generate more accurate and diverse events with much less redundancy. With the generated events sequence, the intra-event captioning mod- els with contexts as in our previous work <ref type="bibr" target="#b2">[3]</ref> are further employed to generate descriptions for each event. To take advantages of different captioning models, we utilize a videosemantic matching model to evaluate and choose more relevant captions from different models for each event. The whole dense video captioning pipeline achieves the stateof-the-art performance on the challenge testing set. The paper is organized as follows. In Section 2, we describe the whole dense video captioning system, which contains the event sequence generation module, the event captioning module and the re-ranking module. Section 3 presents the experimental results and analysis. Finally, we conclude the paper in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dense Video Captioning System</head><p>The whole framework of our dense video captioning system in ActivityNet Challenge 2020 consists of four components: 1) segment feature extraction; 2) event sequence generation; 3) event caption generation; and 4) event and caption re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Segment Feature Extraction</head><p>Given an untrimmed video, we divide it into nonoverlapping segments with 64 frames per segment. Then we extract segment-level features as in our previous work <ref type="bibr" target="#b2">[3]</ref>, which includes: 1) Resnet200 <ref type="bibr" target="#b5">[6]</ref> from image modality pretrained on ImageNet dataset; 2) I3D <ref type="bibr" target="#b1">[2]</ref> from motion modality pretrained on Kinetics dataset; and 3) VGGish <ref type="bibr" target="#b6">[7]</ref> from acoustic modality pretrained on Youtube8M dataset. These three features are temporally aligned and are concatenated together as v t for the t-th segment. Therefore, the video is converted into V = {v 1 , v 2 , ? ? ? , v T }, which is then used in the following modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Event Sequence Generation</head><p>To generate the event sequence with bidirectional video contexts, we further encode the segment-level features V into context-aware featuresV . We employ a bidirectional GRU <ref type="bibr" target="#b3">[4]</ref> on the segment-level features sequence V to capture the visual context in both forward and backward directions. The hidden states in two directions at each encoding step ? ? h t and ? ? h t are concatenated and added to the segmentlevel feature v t followed by ReLU. Therefore, the contextaware featurev t for the t-th segment can be expressed as:</p><formula xml:id="formula_0">v t = ReLU([ ? ? h t , ? ? h t ] + v t )<label>(1)</label></formula><p>Conditioned on the context-aware video featuresV , we then generate events by another GRU one by one. We represent the i-th event as a T-dimensional binary feature vector</p><formula xml:id="formula_1">e i = {x i 0 , x i 1 , ? ? ? , x i T },</formula><p>where T is the number of video segments. The value x i t is set as 1 if the t-th segment is included in the i-th event interval, otherwise it is set as 0. We utilize all-zeros vector as the special start event e &lt;sos&gt; and special end event e &lt;eos&gt; .</p><p>We initialize the hidden state of the event decoder GRU e with global video featurev = 1 T T t=0v t . The event decoder uses previous generated event e i?1 as input and predicts the i-th event distribution over the whole video timeline as follows:</p><formula xml:id="formula_2">h i = GRU e (e i?1 , h i?1 ) (2) p i t = ?(MLP([h i ,v t ])) (3) e i = I(p i t &gt;= 0.5)<label>(4)</label></formula><p>where MLP is multilayer perceptron, I is the indicator vector and ? is the sigmoid function. p i t denotes the probability of the t-th segment included in the i-th event. The timestamp of e i is therefore represented as [t 0 , t 1 ], where p i t0 and p i t1 are the first and last probability over 0.5. In such way, the event decoder can generate the event sequence one by one until e i is the special end event e &lt;eos&gt; .</p><p>The binary cross entropy is utilized to optimize the event distribution p i = {p i 0 , p i 1 , ? ? ? , p i T } as follows:</p><formula xml:id="formula_3">L ESG = ? N n=1 K i=1 log P (e n i |e n &lt;i ,V ; ?)<label>(5)</label></formula><p>where K is the number of events in the n-th video, and ? represents all the learnable parameters in event sequence generation module. For faster learning, we utilize the teacher forcing training strategy by feeding the ground truth event in each step. In such forward generating direction, we generate the events sequence &lt; e 0 , e 1 , ? ? ? , e K &gt; one by one only depending on the past events, which ignores the future event contexts. Therefore, we train another event generator with the whole video reversed, and generate the events sequence &lt; e K , e K?1 , ? ? ? , e 0 &gt; in the backward direction, which exploits future events for each event prediction. Finally, we match corresponding events in forward and backward directions with the tiou over 0.5 and average the predicted event distributional vectors in two directions to acquire the final event boundaries. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the framework of event sequence generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Event Caption Generation</head><p>The context also plays an important role in event caption generation. Besides the basic segment-level video features V described in Section 2.1, we also capture the contextual information in the whole video with RNN as in <ref type="bibr" target="#b2">[3]</ref>. We train a LSTM on segment-level feature sequence V and the objective function is to predict concepts for each segment. We take the hidden state of the LSTM as context feature v c t for the t-th segment, and it is further concatenated with v t for the following caption generation.</p><p>As analysed in our previous work <ref type="bibr" target="#b2">[3]</ref>, the intra-event captioning models are faster and perform better than the inter-event captioning models. Therefore, similar to <ref type="bibr" target="#b2">[3]</ref>, we adopt intra-event captioning model with local contexts for the event caption generation. We adopt a two-layer stacked GRU as the decoder. The first GRU layer in the decoder is the attention GRU, which takes the previous generated caption word y t?1 and previous hidden state in the second GRU layer h 2 t?1 as inputs to calculate a query vector h 1 t as follows:</p><formula xml:id="formula_4">h 1 t = GRU 1 ([y t?1 ; h 2 t?1 ], h 1 t?1 )<label>(6)</label></formula><p>where h 1 0 is initialized as the mean pooling of video features in the current event concatenated with local contexts. The query vector h 1 t is utilized to select relevant temporal contexts with attention mechanism. Then the second GRU layer predicts the next caption word with the temporal context c t as follows:</p><formula xml:id="formula_5">h 2 t = GRU 2 ([h 1 t ; c t ], h 2 t?1 ) (7) p(y t |y &lt;t ) = softmax(W d h 2 t )<label>(8)</label></formula><p>where W d is the word embedding matrix. We firstly train the captioning model based on groundtruth events with cross entropy loss and then fine-tune the model with self-critical reinforcement learning algorithms <ref type="bibr" target="#b11">[12]</ref> with rewards from METEOR and CIDEr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Event and Caption Re-ranking</head><p>In order to improve the system robustness and further improve performance, we train different captioning models and propose the following re-ranking approach to ensemble different models.</p><p>Event Re-rank: Since the precision of event proposals is vital to the final dense captioning performance, we combine the events generated by our proposed ESG module with the proposals in our previous work <ref type="bibr" target="#b2">[3]</ref>. We adopt the same proposal re-rank policy in <ref type="bibr" target="#b2">[3]</ref> to get the final events.</p><p>Caption Re-rank: With the fixed event proposals, we further re-rank captions generated by different captioning models for each event. To select more accurate and visual relevant captions, we train a video-semantic matching model <ref type="bibr" target="#b4">[5]</ref> on the ActivityNet caption dataset to evaluate the qualities of generated captions. Finally, we choose the best caption based on the predicted score and the number of unique words for each event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We utilize the ActivityNet Dense Caption dataset <ref type="bibr" target="#b7">[8]</ref> for dense video captioning, which consists of 20k videos in total with 3.65 event proposals per video on average. We follow the official split with 10,009 videos for training, 4,917 videos for validation and 5,044 videos for testing in the experiments except for our final testing submission. In the final submission, we enlarge the training set with 80% of validation set, which results in 14,009 videos for training and 917 videos for validation. The video in training set contains one set of event proposal segmentation while video in <ref type="table">Table 2</ref>. Event detection performances including recall and precision at four thresholds of temporal intersection of unions (@tIoU) on the ActivityNet Captions validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Recall(@tIoU) Precision(@tIoU) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation of Event Proposal Generation</head><p>Implementation Details: We set the hidden units of GRU as 512. There are two hidden layers in MLP with ReLU activation. The maximum length of event sequence is set as 8. Dropout of 0.5 is adopted to avoid the overfitting. We train the event proposal generation module for 30 epochs with the mini-batch size 8 videos and the learning rate 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics:</head><p>We evaluate the performance of predicted events by measuring the recall and precision of proposals which have tiou 0.3, 0.5, 0.7 and 0.9 with the ground-truth.</p><p>Experimental Results: <ref type="table">Table 2</ref> presents our performance for event proposal generation. Our proposed bidirectional event sequence generation model performs better than the two-stage method <ref type="bibr" target="#b9">[10]</ref>. Furthermore, it is simpler and more efficient. It generates 2.89 events per video on average, and the self-tiou of them is 0.07, which is close to the ground-truth events with self-tiou 0.05. Generating the event sequence in forward and backward directions achieve competitive proposal performances. Combining the two directions achieves the best performance on both average recall and precision, which shows the past and future events are both helpful for the current event prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation of Event Captioning</head><p>Implementation Details: For the caption decoder, we set the hidden units of GRU as 1024. The dimensionality of word embedding layer is set to 300. We initialize the embedding matrix with the pre-trained Glove <ref type="bibr" target="#b10">[11]</ref>. We train the captioning model for 30 epochs and select the model with best METEOR score on the validation set.</p><p>For the video-semantic matching model, the dimensionality of video-sentence joint embedding space is set as 1024. Contrastive ranking loss with hard negative mining <ref type="bibr" target="#b4">[5]</ref> is utilized for training.</p><p>Evaluation Metrics: To purely evaluate the event captioning performance, we fix event proposals as the groundtruth proposals. We employ the official evaluation process <ref type="bibr" target="#b7">[8]</ref> with tiou threshold of 0.9 since we utilize the groundtruth proposals, and evaluate on common captioning metrics including BLEU, METEOR and CIDEr. When evaluating the caption performance of generated events, we compute the caption performance for proposals possessing tiou 0.3, 0.5, 0.7 and 0.9 with the ground-truth.</p><p>Experimental Results: <ref type="table">Table 3</ref> shows our dense captioning performances on the ground-truth events and generated events. The intra-event captioning model trained with cross-entropy loss has achieved competitive performance with the METEOR 12.42. Fine-tuning the model with the rewards computed by CIDEr and METEOR metrics in reinforcement learning framework further improves the captioning model significantly. Ensembling various captioning models with caption re-ranking achieves additional improvements over all the single-models. Compared with the performance on ground-truth events, the captioning performances on the generated proposals are much inferior, which infers the importance of event proposals generation.</p><p>The performances of our last two submitted models are presented in <ref type="table" target="#tab_1">Table 4</ref>. In our final submission, we enlarge the training data with 80% of validation set. More training data brings substantial improvement, and our model achieves 9.894 METEOR score on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work, we explore the temporal order of the event sequence in the video. With fully exploiting temporal dependence in two directions, we propose a novel and simple event sequence generation model without traditional twostage process. For event captioning, we adopt the intraevent captioning models as our previous work <ref type="bibr" target="#b2">[3]</ref> and employ a video-semantic matching model to re-rank captions for each event. Our proposed system achieves the stateof-the-art performance on the dense video captioning challenge 2020. In the future, we will further explore the coherence of multiple captions for the event sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of event sequences from ActivityNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed event sequence generation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The statistics of temporal order for events sequences in ActivityNet dataset</figDesc><table><row><cell cols="4">Sequential Sum.-Details Details-Sum. Other</cell></row><row><cell>81.50%</cell><cell>16.94%</cell><cell>0.40%</cell><cell>1.16%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>76.32 42.07 10.74 55.51 97.08 78.59 43.73 10.96 57.59 Backward-direction 91.53 74.49 42.21 10.49 54.68 96.84 79.02 45.84 11.58 58.32 Bi-direction 92.35 75.51 43.49 11.81 55.79 97.17 79.33 45.68 12.40 58.65 The evaluation performance on the testing set of two submissions. The official denotes using official split for training and enlarged denotes enlarging training set.</figDesc><table><row><cell></cell><cell></cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>Avg</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>Avg</cell></row><row><cell></cell><cell>ESGN [10]</cell><cell cols="2">93.41 76.4</cell><cell>42.4</cell><cell cols="6">10.1 55.58 96.71 77.73 44.84 10.99 57.57</cell></row><row><cell cols="11">Forward-direction 92.90 Table 3. Dense video captioning results including Bleu@N (B@N), CIDEr and METEOR on ActivityNet Captions validation set.</cell></row><row><cell>Methods</cell><cell></cell><cell cols="3">with GT proposals</cell><cell></cell><cell></cell><cell></cell><cell cols="3">with generated proposals</cell></row><row><cell></cell><cell cols="10">B@1 B@2 B@3 B@4 CIDEr Meteor B@1 B@2 B@3 B@4 CIDEr Meteor</cell></row><row><cell>SDVC [10]</cell><cell cols="2">28.02 12.05 4.41</cell><cell>1.28</cell><cell cols="2">43.48</cell><cell>13.07</cell><cell cols="2">17.92 7.99</cell><cell>2.94</cell><cell>0.93</cell><cell>30.68</cell><cell>8.82</cell></row><row><cell cols="3">Cross-entropy 24.76 13.05 6.43</cell><cell>3.22</cell><cell cols="2">54.10</cell><cell>12.42</cell><cell cols="2">16.65 9.40</cell><cell>5.35</cell><cell>3.05</cell><cell>20.91</cell><cell>9.39</cell></row><row><cell>CIDEr.SC</cell><cell cols="2">26.94 14.29 6.75</cell><cell>2.94</cell><cell cols="2">53.58</cell><cell>13.75</cell><cell cols="3">17.91 10.18 5.57</cell><cell>2.88</cell><cell>20.44</cell><cell>10.38</cell></row><row><cell cols="3">METEOR.SC 26.16 13.80 6.49</cell><cell>2.85</cell><cell cols="2">47.29</cell><cell>14.53</cell><cell cols="2">15.60 8.82</cell><cell>4.87</cell><cell>2.59</cell><cell>13.28</cell><cell>10.70</cell></row><row><cell>Re-ranking</cell><cell cols="2">25.84 13.77 6.60</cell><cell>2.93</cell><cell cols="2">42.86</cell><cell>15.00</cell><cell cols="2">16.59 9.65</cell><cell>5.32</cell><cell>2.91</cell><cell>14.03</cell><cell>11.28</cell></row><row><cell></cell><cell></cell><cell cols="3">Official Enlarged</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GT events on val</cell><cell>15.00</cell><cell cols="2">16.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Generated events on val</cell><cell>11.28</cell><cell cols="2">12.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Generated events on test</cell><cell>9.336</cell><cell cols="2">9.894</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">validation set contains two sets of proposal segmentation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SST: single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Activitynet 2019 task 3: Exploring contexts for dense captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>abs/1907.05092</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jort</forename><forename type="middle">F</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-03-05" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7492" to="7500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6588" to="6597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7190" to="7198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
