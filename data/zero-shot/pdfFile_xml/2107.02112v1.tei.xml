<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recovering the Unbiased Scene Graphs from the Biased Ones</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiun</forename><surname>Chiou</surname></persName>
							<email>mengjiun.chiou@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
							<email>henghui.ding@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
							<email>hanshu.yan@u.nus.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
							<email>changhu.wang@bytedance.com</email>
							<affiliation key="aff3">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
							<email>rogerz@comp.nus.edu.sg</email>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recovering the Unbiased Scene Graphs from the Biased Ones</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given input images, scene graph generation (SGG) aims to produce comprehensive, graphical representations describing visual relationships among salient objects. Recently, more efforts have been paid to the long tail problem in SGG; however, the imbalance in the fraction of missing labels of different classes, or reporting bias, exacerbating the long tail is rarely considered and cannot be solved by the existing debiasing methods. In this paper we show that, due to the missing labels, SGG can be viewed as a "Learning from Positive and Unlabeled data" (PU learning) problem, where the reporting bias can be removed by recovering the unbiased probabilities from the biased ones by utilizing label frequencies, i.e., the per-class fraction of labeled, positive examples in all the positive examples. To obtain accurate label frequency estimates, we propose Dynamic Label Frequency Estimation (DLFE) to take advantage of training-time data augmentation and average over multiple training iterations to introduce more valid examples. Extensive experiments show that DLFE is more effective in estimating label frequencies than a naive variant of the traditional estimate, and DLFE significantly alleviates the long tail and achieves state-of-the-art debiasing performance on the VG dataset. We also show qualitatively that SGG models with DLFE produce prominently more balanced and unbiased scene graphs. The source code will be publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Scene graph generation (SGG) <ref type="bibr" target="#b27">[27]</ref> aims to predict visual relationships in the form of (subject-predicate-object) among salient objects in images. SGG has been shown to be helpful for image captioning <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>, visual question answering <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b39">39]</ref>, indoor scene understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> and thus has been drawing increasing attention <ref type="bibr">[4-7, 9, 11, 13-15, 17-19, 25, 29, 32, 37, 40-43, 45, 46, 49-53]</ref>.</p><p>The long tail problem is common and challenging in SGG <ref type="bibr" target="#b37">[37]</ref>: since certain predicates (i.e., head classes) occur far more frequently than others (i.e., tail classes) in the most widely-used VG dataset <ref type="bibr" target="#b20">[20]</ref>, a model that trained with this unbalanced dataset would favor predicting heads against tails. For instance, the number of training examples of on is ?830? higher than that of painted on in the VG dataset, and (given ground truth objects) a classical SGG model MOTIFS <ref type="bibr" target="#b53">[53]</ref> achieves 74.3 Recall@20 for on, in sharp contrast to 0.0 for painted on. However, the fact that the head classes are less descriptive than the tail classes makes the generated scene graphs coarse-grained and less informative, which is not ideal.</p><p>Most of the existing efforts in long-tailed SGG <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45]</ref> deal with the skewed class distribution directly. However, 1 https://github.com/coldmanck/recovering-unbiased-scene-graphs Unbiased Prediction <ref type="figure">Figure 1</ref>: An illustrative comparison between the traditional, biased inference and the unbiased PU inference for SGG. (a) Traditionally SGG models are not trained in the PU setting and thus output biased probabilities in favor of conspicuous classes (e.g., on). (b) We remove the reporting bias from the biased probabilities by discounting the difference in the chance of being labeled, i.e., label frequency, so that inconspicuous classes (e.g., parked on) are properly predicted.</p><p>unlike common long-tailed classification tasks where the long tails are mostly caused by the unbalanced class prior distributions, the long tail of SGG with the VG dataset is significantly affected by the imbalance of missing labels, which remains unsolved. The missing label problem arises as it is unrealistic to annotate the overwhelming number of possible visual relations (i.e., ( ? 1) possibilities given predicate classes and objects in an image). Training SGG models by treating all unlabeled pairs as negative examples (which is the default setting for most of the existing SGG works) introduces missing label bias in predictions, i.e., predicted probabilities could be under-estimated. What is worse, reporting bias <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b37">37]</ref> which is prevalent in the VG dataset causes an imbalance in the missing labels of different predicates. That is, the conspicuous classes (e.g., on, in) are more likely to be annotated than the inconspicuous ones (e.g., parked on, covered in). Generally, conspicuous classes are more extensively annotated and have higher label frequencies, i.e., the fraction of labeled, positive examples in all examples of individual classes. The unbalanced label frequency distribution means that the predicted probability of an inconspicuous class could be under-estimated more than that of a conspicuous one, causing a long tail. To produce meaningful scene graphs, the inconspicuous but informative predicates need to be properly predicted. To the best of our knowledge, none of the existing SGG debiasing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45]</ref> effectively solve this reporting bias problem.</p><p>In this paper, we propose to tackle the reporting bias problem by removing the effect of unbalanced label frequency distribution. That is, we aim to recover the unbiased version of per-class predicted probabilities such that they are independent of the per-class missing label bias. To do this, we first show that learning a SGG model with the VG dataset can viewed as a Learning from Positive and Unlabeled data (PU learning) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> problem, where a target PU dataset contains only positive examples and unlabeled data. For clarity, we define that a biased model is trained on a PU dataset by treating the unlabaled data as negatives and outputs biased probabilities, while an unbiased model is trained on a fully-annotated dataset and outputs unbiased probabilities. Under the PU learning setting, the per-class unbiased probabilities are proportional to the biased ones with the per-class label frequencies as the proportionality constants <ref type="bibr" target="#b11">[12]</ref>. Motivated by this fact, we propose to recover the unbiased visual relationship probabilities from the biased ones by dividing by the estimated per-class label frequencies so that the imbalance (i.e., reporting bias) can be offset. Especially, the inconspicuous predicates with their probabilities being under-estimated more could then be predicted with higher confidences so that the scene graphs are more informative. An illustrative comparison of the traditional, biased method and our unbiased one is shown in <ref type="figure">Fig. 1</ref>.</p><p>A traditional estimator of label frequencies is the per-class average of biased probabilities on a training/validation set predicted by a biased model <ref type="bibr" target="#b11">[12]</ref>. While this estimator can work in the easier SGG settings where ground truth bounding boxes are given, i.e., PredCls and SGCls, it is found unable to provide estimates for some classes in the hardest SGG setting where no additional information other than images is provided, i.e. SGDet. The reason is that there are no valid examples (i.e., predicted object pairs that match ground truth boxes and object labels) can be used for label frequency estimation. For instance, by forwarding a trained MOTIFS [53] model on VG training set, 9 out of 50 predicates do not have even a single valid example, making it impossible to estimate. In this paper, we propose to take advantage of the training-time data augmentation such as random flipping to increase the number of valid examples. That is, instead of performing post-training estimations, we propose Dynamic Label Frequency Estimation (DLFE) utilizing augmented training examples by maintaining a moving average of the per-batch biased probability during training. The significant increase in the number of valid examples, especially in SGDet, enables accurate label frequency estimation for unbiased probability recovery.</p><p>Our contribution in this work is three-fold. First, we are among the first to tackle the long tail problem in SGG from the perspective of reporting bias, which we remove by recovering the per-class unbiased probability from the biased one with a PU based approach. Second, to obtain accurate label frequency estimates for recovering unbiased probabilities in SGG, we propose DLFE which takes advantage of training-time data augmentation and averages over multiple training iterations to introduce more valid examples. Third, we show that DLFE provides more reliable label frequency estimates than a naive variant of the traditional estimator, and we demonstrate that SGG models with DLFE effectively alleviates the long tail and achieve state-of-the-art debiasing performance with remarkably more informative scene graphs. We will release the source code to facilitate research towards an unbiased SGG methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Scene Graph Generation (SGG)</head><p>SGG <ref type="bibr" target="#b27">[27]</ref> aims to generate pairwise visual relationships in the form of (subject-predicate-object) among salient objects, and there exists three training and evaluation settings <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b53">53]</ref>: (1) Predicate Classification (PredCls) predicting relationships given ground truth bounding boxes and object labels, (2) Scene Graph Classification (SGCls) predicting relationships and object labels given bounding boxes and (3) Scene Graph Detection (SGDet) predicting relationships, object labels and bounding boxes with only input images.</p><p>Typically, SGG models consist of three main modules: proposal generation, object classification, and relationship prediction. Generally, a pre-trained object detection model (e.g., <ref type="bibr" target="#b30">[30]</ref>) is adopted for generating proposals. For object classification, instead of using the predictions of object detection models directly, the generated proposals and their features are usually refined into object contexts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b53">53]</ref> followed by decoding into object labels. A common way to take object contexts into consideration is to run message passing algorithms (e.g., <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b35">35]</ref>) on a fully-connected <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref>, chained <ref type="bibr" target="#b53">[53]</ref> or tree-structured <ref type="bibr" target="#b38">[38]</ref> graph. For relationship prediction, most approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b53">53]</ref> take in object contexts and bounding box features to compute relation contexts with a similar graphical manner. However, not until the recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">38]</ref> proposed the less biased mean recall metrics did the research communities in SGG pay attention to the class imbalance problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref>. Tang et al. <ref type="bibr" target="#b37">[37]</ref> borrow the counterfactual idea from causal inference to remove the context-specific bias. Yan et al. <ref type="bibr" target="#b45">[45]</ref> propose to perform re-weighting with class relatedness-aware weights. Wang et al. <ref type="bibr" target="#b41">[41]</ref> transfer the less-biased knowledge from the secondary learner to the main one with knowledge distillation.</p><p>Our proposed method can be viewed as a model-agnostic debiasing method <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref>. However, instead of focusing on class relatedness <ref type="bibr" target="#b45">[45]</ref>, context co-occurrence bias <ref type="bibr" target="#b37">[37]</ref> or missing label bias <ref type="bibr" target="#b41">[41]</ref>, we tackle the underlying reporting bias <ref type="bibr" target="#b28">[28]</ref> by dealing with the unbalanced label frequency distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Positive Unlabeled (PU) learning</head><p>While the traditional classification setting aims to learn classifiers with both positive and negative data, Learning from Positive and Unlabeled data, or Positive Unlabeled (PU) learning, is a variant of the traditional setting where a PU dataset contains only positive and unlabeled examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. That is, an unlabeled example can either be truly a negative, or belongs to one or more classes. Learning a biased classifier assuming all unlabeled examples are negative (which is the default setting for most of the existing SGG works) could introduce missing label bias, producing unbalanced predictions. Common PU learning methods can be roughly divided into two categories <ref type="bibr" target="#b1">[2]</ref>: (a) training an unbiased model, and (b) inferencing a biased model in a PU manner. We adopt the latter approach in this paper due to its convenience and favorable flexibility.</p><p>We note that while Chen et al. <ref type="bibr" target="#b3">[4]</ref> also deal with SGG in the PU setting, they do not dive deep into the long tail problem in scene  <ref type="figure">Figure 2</ref>: An illustration of training and inferencing a SGG model in a PU manner with Dynamic Label Frequency Estimation (DLFE). Given an input image, proposals and their features are extracted by an object detector. Object classification is performed via message passing on a (e.g., chained <ref type="bibr" target="#b53">[53]</ref>) graph followed by object contexts decoding. Object contexts together with bounding boxes and features are then fed into another graph to refine into relation contexts, followed by decoding into the biased probabilities?( | ). DLFE dynamically estimates the label frequencies with the moving averages of biased probabilities during training. Finally, the unbiased probability of class is recovered with?( = | ) = 1?( = | ) during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal Generation</head><p>graphs as we do in this paper. They propose a three-stage approach which generates pseudo-labels for the unlabeled examples with a biased trained model, followed by training a less biased model with the additional "positive" examples. However, their approach is time and resource consuming since it requires re-generating pseudo labels if different SGG models are used. Unlike <ref type="bibr" target="#b3">[4]</ref>, our approach not only can be easily adapted for any SGG model with minimal modification, but is superior in terms of debiasing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Scene graph generation aims to generate a graph comprising bounding boxes , object labels , and visual relationships , given an input image . The SGG task ( | ) is usually decomposed into three components for joint training <ref type="bibr" target="#b53">[53]</ref>:</p><formula xml:id="formula_0">( | ) = ( | ) ( | , ) ( | , , ),<label>(1)</label></formula><p>where ( | ) denotes proposal generation, ( | , ) means object classification and ( | , , ) is relationship prediction. We propose to biasedly train a SGG model while we dynamically estimate the label frequencies during training. The estimated label frequencies are then used to recover the unbiased probabilities during inference. We describe our choice of proposal generation, object classification and relationship prediction in Section 3.1. We then explain how we recover the unbiased probabilities from the biased ones from a PU perspective in Section 3.2, followed by presenting our Dynamic Label Frequency Estimation (DLFE) in Section 3.3. <ref type="figure">Figure  2</ref> shows an illustration of DLFE applied to SGG models like <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b53">53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Proposal Generation ( | ).</head><p>Given an image , we adopt a pre-trained object detector <ref type="bibr" target="#b30">[30]</ref> to extract object proposals = { | = 1, ..., }, together with their visual representation { | = 1, ..., } and ( ? 1) union bounding box representations { , | , = 1, ..., } pooled from the output feature map. The visual representations also come with predicted class probabilities: { | = 1, ..., } and { , | , = 1, ..., }.</p><p>3.1.2 Object Classification ( | , ). For object classification, a graphical representation is constructed which takes in object features { } and class probabilities { } and outputs object context { } refined with message passing algorithms. We experiment our methods with either chained-structured graphs <ref type="bibr" target="#b53">[53]</ref> with bi-directional LSTM <ref type="bibr" target="#b16">[16]</ref>, or tree-structured graphs <ref type="bibr" target="#b38">[38]</ref> with TreeLSTM <ref type="bibr" target="#b35">[35]</ref>. The output object contexts are then fed into a linear layer followed with a Softmax layer to decode into predicted object labels  </p><formula xml:id="formula_1">= { | = 1, ..., }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recovering the Unbiased Scene Graphs</head><p>Learning SGG from a dataset with missing labels can be viewed as a PU learning problem, which is different from the traditional classification in that (a) no negative examples are available, and (b) unlabeled examples can either be truly negatives or belong to any class. Learning classifiers from a PU dataset by treating all unlabeled data as negatives could introduce strong missing label bias <ref type="bibr" target="#b11">[12]</ref>, i.e., predicted probabilities could be under-estimated, and reporting bias <ref type="bibr" target="#b28">[28]</ref>, i.e., predicted probability of an inconspicuous class could be under-estimated more than that of a conspicuous one. We propose to avoid the both biases by recovering the unbiased probabilities, marginalizing the effect of uneven label frequencies.</p><p>Given predicate classes, we denote the visual relation examples taken in by the relationship prediction module of a SGG model by a set of tuple ( , , ), with an example (i.e., pairwise object features), ? {0, ..., } the true predicate class (0 means the background class) and ? {0, ..., } the relation label (0 means unannotated). The class cannot be observed from the dataset: while we can derive = if the example is labeled ( ? 0), can be any number ranging from 0 to for an unlabeled example ( = 0).</p><p>For clarity, we now regard , and as random variables. For a target class ? {1, ..., }, a biased SGG model is trained to predict the biased probability ( = | ), which can be derived as follows:</p><formula xml:id="formula_2">( = | ) = ( = , = | ) (by PU definition) (2) = ( = | ) ( = | = , ),<label>(3)</label></formula><p>where ( = | = , ) is the probability of example being selected to be labeled and is called propensity score <ref type="bibr" target="#b1">[2]</ref>. Dividing each side by ( = | = , ) we obtain the unbiased probability ( = | ):</p><formula xml:id="formula_3">( = | ) = ( = | ) ( = | = , )</formula><p>.</p><p>However, as it is unrealistic to obtain the propensity scores of each , the existing works propose to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> bypass the dependence on each by making the Selected Completely At Random (SCAR) assumption <ref type="bibr" target="#b1">[2]</ref>: non-background examples are selected for labeling entirely at random regardless of , i.e., the set of labeled examples is uniformly drawn from the set of positive examples. This means that ( = | = , ) = ( = | = ) and Eqn 4 can be written as </p><formula xml:id="formula_5">( = | ) = ( = | ) ( = | = ) ,<label>(5)</label></formula><formula xml:id="formula_6">B ? ? {( , ) ? B| = }; ? ? 1 | B ? | ( , ) ?B ? ( | ); // Update the exponential moving averag? ? ? ? + (1 ? ) ??; end end ??;</formula><p>where ( = | = ) is the label frequency of class , or , which is the fraction of labeled examples in all the examples of class .</p><p>Notably, discounting the effect of per-class label frequencies in this way also removes the reporting bias. Since label frequencies are usually not provided by annotators, an estimation is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic Label Frequency Estimation</head><p>One of the most common estimators of label frequencies, named Train-Est, is the per-class average of biased probabilities?( | ) predicted by a biased model <ref type="bibr" target="#b11">[12]</ref> (see full derivation in Appendix A):</p><formula xml:id="formula_7">= ( = | = ) ? 1 ?? ( , = ) ??( = | ),<label>(6)</label></formula><p>where denotes a training or validation set and is the cardinality of {( , = )}. However, we find this way of estimation inconvenient and unsuitable for SGG. To understand why, recall that PredCls, SGCls and SGDet are the three SGG training and evaluation settings, and note that re-estimation of label frequencies is required for each setting since the expected biased probabilities could vary depending on the task difficulty 2 . Firstly, the post-training estimation required before inferencing in each SGG setting is inconvenient and unfavorable. Secondly, the absence of ground truth bounding boxes in SGDet mode results in lack of valid examples for label frequency estimation. For a proposal pair to be valid, its both objects must match ground truth boxes (with IoU ? 0.5) and object labels simultaneously. By using Train-Est with MOTIFS <ref type="bibr" target="#b53">[53]</ref>, as revealed in in <ref type="figure" target="#fig_2">Fig. 3</ref> (the blue bars), 9 out of 50 predicates do not have even a valid example, i.e., {( , = )} in Eqn. 6 is empty, making it impossible to compute. In addition, more valid examples are missing for inconspicuous classes: as the examples of those classes are concentrated in a much smaller number of images, not matching a bounding box could invalidate lots of examples. A naive remedy is using a default value for those missing estimates; however, as we show in section 4.3 the performance is sub-optimal.</p><p>To alleviate this problem, we propose to take advantage of the training-time data augmentation to get back more valid examples for tail classes. Concretely, during training we augment input data by horizontal flipping with a probability of 0.5, and meanwhile we estimate label frequencies with per-batch biased probabilities. By doing this, the number of valid examples of tail classes could become more normal (and higher) than that of Train-Est, since averaging over augmented examples and multiple training iterations (with varying object label predictions) essentially introduces more samples, which in turn increases the number of valid examples.</p><p>Based on this idea, we propose Dynamic Label Frequency Estimation (DLFE) where the main steps are shown in Algorithm 1. In detail, we maintain per-class moving averages of the biased probabilities (Eqn. 6) throughout the training. The estimated label frequencies?are dynamically updated by the per-batch averages ? with a momentum so that the estimates that are more recent matter more:?? ? ? + (1 ? ) ??. Note that for each mini-batch we update the estimated?of class only if at least one valid example of presents in the current batch. The estimated values gradually stabilize along with the converging model, and we save the final estimates ? R (as a vector of length ). During inference, the estimated label frequencies are utilized to recover the unbiased probability distribution?( | ) from the biased one?( | ) b?</p><formula xml:id="formula_8">( | ) = 1 ??( | ),<label>(7)</label></formula><p>where ? denotes the Hadamard (element-wise) product. The average per-epoch number of valid examples of this way is shown in <ref type="figure" target="#fig_2">Fig. 3 (the tangerine bars)</ref>, where the inconspicuous classes get remarkably more (4? or more) valid examples. This not only enables accurate estimations for all the classes but makes the estimations easier as no additional, post-training estimation is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Evaluation Settings</head><p>We follow the recent efforts in SGG <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b53">53]</ref> to experiment on a subset of the VG dataset <ref type="bibr" target="#b20">[20]</ref> named VG150 <ref type="bibr" target="#b44">[44]</ref>, which contains 62, 723 images for training, 5, 000 for validation and 26, 446 for testing. As discussed earlier, we train and evaluate in the three SGG settings: PredCls, SGCls and SGDet. We evaluate models with, or without graph constraint: whether only a single relation with the highest confidence is predicted for each object pair. Non-graph constraint is denoted as "ng". For evaluation, we adopt recall-based metrics which measures the fraction of ground truth visual relations appearing in top-confident predictions, where is 20, 50, or 100. However, as the plain recall could be dominated by a biased model predicting mostly head classes, we follow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref> to average over per-class recall and focus on the less biased per-class/mean recall (mR@ ) and non-graph constraint per-class/mean recall (ng-mR@ ) <ref type="bibr" target="#b2">3</ref> . We note that the ng per-class/mean recall should be the fairest measure for debiasing methods since it 1) treats each class equally and 2) reflects the fact that more than one visual relations could exist for an object pair. We follow the long-tailed recognition research <ref type="bibr" target="#b26">[26]</ref> to divide the distribution into three parts, including head (many-shot; top-15 frequent predicates), middle (medium-shot; mid-20) and tail (few-shot; last-15) and compute their ng-mRs. Note <ref type="bibr" target="#b2">3</ref> While results in graph constraint recalls (R) and mean recalls (mR) are less reflective of how unbiased a SGG model is, we provide them for reference in Appendix D.  that by DLFE in this section, we mean the dynamic label frequency estimation along with our unbiased scene graph recovery approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>As DLFE is a model-agnostic strategy, we experiment with two popular SGG backbones: MOTIFS <ref type="bibr" target="#b53">[53]</ref> and VCTree <ref type="bibr" target="#b38">[38]</ref>. Following <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b41">41]</ref>, we adopt a pre-trained and frozen Faster R-CNN <ref type="bibr" target="#b30">[30]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b23">[23]</ref> as the object detector, which achieves 28.14 mAP on VG's testing set <ref type="bibr" target="#b37">[37]</ref>. All the hyperparameters, including the momentum = 0.1, are tuned with the validation set. All models are trained using SGD optimizer with the initial learning rate of 0.01 after the first 500 iterations of warm-up. Random flipping is applied to all the training examples. The learning rate is decayed, for a maximum of twice, by the factor of 10 once the validation performance plateaus twice consecutively. Training can early stop when the maximum decay step (two) is reached before the maximum 50,000 iterations. The final checkpoint is used for evaluation.</p><p>The batch size for all experiments is 48 (images). For SGDet setting, we sample 80 proposals from each image and apply per-class NMS <ref type="bibr" target="#b31">[31]</ref>. Beside ground truth visual relations, we follow <ref type="bibr" target="#b37">[37]</ref> to sample up to 1, 024 pairs with background-to-ground truth ratio being 3:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicate Classification (PredCls)</head><p>Scene Graph Classification (SGCls) Scene Graph Detection (SGDet) Model ng-mR@20 ng-mR@50 ng-mR@100 ng-mR@20 ng-mR@50 ng-mR@100 ng-mR@20 ng-mR@50 ng-mR@100 KERN <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing DLFE to Train-Est</head><p>We aim to answer the question: whether DLFE is more effective in estimating label frequency than Train-Est, by comparing 1) the consistencies of the estimated label frequencies and 2) their debiasing performance. As discussed earlier that label frequencies of the predicates lacking a valid example cannot be estimated by Train-Est, we thus naively assign the median of the other estimated label frequencies to those missing values.   <ref type="table">Table 3</ref>: Non-graph constraint head, middle and tail recalls (PredCls). ? and ? are with the same meanings as in <ref type="table" target="#tab_2">Table 1</ref>. DLFE improves the tail recalls by a large margin.</p><p>A comparison of the estimated label frequencies is presented in <ref type="figure" target="#fig_3">Fig. 4</ref>. It is clear from (a) that, even for the classes that with at least one valid example, the Train-Est estimated values tend to be abnormally high in SGDet setting. Note that while there might be differences in estimated values for different SGG settings, with a same backbone they should still be relatively similar. In contrast, (b) shows DLFE-estimated values are more consistent across the three settings. We also compare their debiasing performance in SGDet 4 with the absolute ng per-class R@100 change in <ref type="figure" target="#fig_4">Fig. 5</ref>. Apparently, Train-Est barely improves the per-class recalls especially for tail classes that lacks enough valid examples, while DLFE achieves higher and more consistent improvement across all the predicates.</p><p>These results verify the claim that, apart from being more convenient (requiring no post-training estimation), DLFE is more effective than naive Train-Est for providing more reliable estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing to other Debiasing Methods</head><p>While we list the results of different SGG backbone model for reference, we mainly compare our approach with the model-agnostic debiasing methods including Focal Loss <ref type="bibr" target="#b24">[24]</ref>, Resampling <ref type="bibr" target="#b2">[3]</ref>, Reweighting, L2+{u,c}KD <ref type="bibr" target="#b41">[41]</ref>, TDE <ref type="bibr" target="#b37">[37]</ref>, PCPL <ref type="bibr" target="#b45">[45]</ref> and STL <ref type="bibr" target="#b3">[4]</ref>. L2+{u,c}KD is a two-learner knowledge distillation framework for reducing dataset biases. TDE is an inference-time debiasing method which <ref type="bibr" target="#b3">4</ref> Results in numbers and the other two SGG settings are provided in Appendix B. ensembles counterfactual thinking by removing context-specific bias. PCPL learns the relatedness scores among predicates which are used as the weights in reweighting, and is the current state-of-theart in terms of mR. STL generates soft pseudo-labels for unlabeled data used for joint training. We re-implement PCPL and STL since their backbone is not directly comparable, and Reweighting since there does not exist its reported performance for VCTree. We report our reproduced results of TDE with the authors' codebase <ref type="bibr" target="#b36">[36]</ref>.</p><p>The results in ng-mR@ are presented in <ref type="table" target="#tab_2">Table 1</ref>, where DLFE significantly improves ng-mRs for both MOTIFS and VCTree and outperforms the existing debiasing methods. Notably, TDE, which was proposed to alleviate the long tail by removing the contextspecific bias, is shown to adversely affect the ability of predicting multiple relations for per object pair. This shows that removing the reporting bias is more beneficial for debiasing SGG models. While the graph-constraint mR metric does not reflect the fact that multiple relations could exist between an object pair, due to its popularity we still present the results in <ref type="table" target="#tab_3">Table 2</ref>. Debiasing MOTIFS with our proposed DLFE still significantly improves its mR, achieving state-of-the-art mR across all the three setting. Large performance boost are also seen in VCTree with DLFE, and new SOTAs are attained for PredCls (mR@20), SGCls and SGDet.</p><p>To better understand how DLFE affects the performance of each class, we also present non-graph constraint per-class recall@20 changes compared to MOTIFS backbone-only (biased classifier), in <ref type="figure" target="#fig_5">Figure 6</ref>. While all the debiasing methods increase the recall of the less frequent, middle-to-tail classes, only DLFE improves the tail (last-15) classes' performance significantly. The other approach that also visibly improves the tail classes' performance is Reweighting; however, their relatively small improvements demonstrate that naively dealing with the unbalanced class frequencies is less effective than tackling the reporting bias.</p><p>We also present the head (many shot), middle (medium shot) and tail (few shot) non-graph constraint recalls in PredCls 5 with the MOTIFS backbone in <ref type="table">Table 3</ref>. Remarkably, DLFE outperforms the others by a significant margin regarding the tail recalls, e.g., Tail R@50 is 31.8 for DLFE, versus 1.8 for TDE, versus 6.0 for PCPL, versus 13.3 for Reweighting, showing that DLFE is especially capable of dealing with the long tail. <ref type="bibr" target="#b4">5</ref> The full results with graph-constraint, or SGCls/SGDet are available in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>The scene graphs of three testing images are visualized in <ref type="figure" target="#fig_6">Figure 7</ref>, where the scene graphs on the left side are generated by MOTIFS and those on the right are by MOTIFS-DLFE. (a) is an apparent example that, while wheel-on-car, car-on-street, hair-onman predicted by MOTIFS are reasonable, wheel-mounted oncar, car-parked on-street, hair-belonging to-man predicted by MOTIFS-DLFE match the ground truth and are also more descriptive (while being inconspicuous). Similarly, treegrowing on-hill in Example (b) and woman-standing onbeach in (c) are also correct and more descriptive; however, due to the missing label issue in the VG dataset, tree-growing onhill can not be correctly recalled (shown as tangerine color). In addition, there are some seemingly incorrect annotations such as tree-standing on-tree in Example (a), where the subject actually indicates a smaller, branch part of a tree. For this object pair, MOTIFS-DLFE predicts growing on which, ironically, seems to be more reasonable than the ground truth label.</p><p>To understand how DLFE changes the probability distribution, we visualize the biased (MOTIFS) and unbiased (MOTIFS-DLFE) probabilities, given a subject-object pair, in <ref type="figure" target="#fig_7">Figure 8</ref>. <ref type="bibr" target="#b5">6</ref> Prediction confidences are shown to be calibrated towards minor but expressive predicates like (a) car-parked on-street, (b) wheelof-train, (d) people-sitting on-bench (while sitting on is not in the ground truth). Notably in (c), fork is actually not on the plate but was mis-predicted by MOTIFS due to the strong bias (i.e., many fork-in/on-plate examples in VG dataset), while MOTIFS-DLFE correctly predicts near. Moreover, (b) shows that the confidences of MOTIFS-DLFE for predicates other than the GT of, such as mounted on and part of, have increased remarkably, presumably because they are also reasonable choices. This demonstrates the effectiveness of DLFE for balanced SGG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we deal with the long tail problem in SGG with the cause (unbalanced missing labels) instead of its superficial effect (long-tailed class distribution). To ward off the reporting bias caused by the imbalance in missing labels, we view SGG as a PU learning problem and we remove the per-class missing label bias by recovering the unbiased probabilities from the biased ones. To obtain reliable label frequencies for unbiased probability recovery, we take advantage of the data augmentation during training and perform Dynamic Label Frequency Estimation (DLFE) which maintains the moving averages of per-class biased probability and effectively introduces more valid samples, especially in SGDet training and evaluation mode. Extensive quantitative and qualitative experiments demonstrate that DLFE is more effective in estimating label frequencies than a naive variant of the traditional estimator, and SGG models with DLFE achieve state-of-the-art debiasing performance on the VG dataset, producing well-balanced scene graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES A DERIVING THE TRAIN-EST ESTIMATOR</head><p>We show that why biased probabilities can be used as a label frequency estimator in this section. Denote an annotated example in a training or validation set with ( , , ), where is the pairwise example, is the relation label and is the true class. Note = as we are only considering the annotated ones. Referring to <ref type="bibr" target="#b11">[12]</ref>, the biased probability ( | ) of class can be derived as follows:</p><formula xml:id="formula_9">( = | ) = ( = | , = ) ( = | ) + ( = | , ? ) ( ? | ) = ( = | , = ) ? 1 + 0 ? 0 (since y=r) = ( = | = ), (the SCAR assumption)</formula><p>where ( = | = ) is the label frequency of class . Thus, we can obtain a reasonable label frequency estimate via averaging the per-class biased probability with a training or validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE RESULTS OF TRAIN-EST AND DLFE</head><p>We present additional results in non-graph constraint mean recall (ng-mR@ ) in <ref type="table" target="#tab_6">Table 4</ref>. Across both the MOTIFS <ref type="bibr" target="#b53">[53]</ref> and VCTree <ref type="bibr" target="#b38">[38]</ref> backbone, our DLFE achieves significantly higher ng-mRs in both PredCls and SGDet setting and is on par with Train-Est in SGCls setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C RESULTS IN HEAD/MIDDLE/TAIL RECALLS</head><p>We compare our proposed DLFE with other debiasing methods, i.e., Reweighting, TDE <ref type="bibr" target="#b37">[37]</ref>, PCPL <ref type="bibr" target="#b45">[45]</ref> and STL <ref type="bibr" target="#b3">[4]</ref>, on the recalls of different part of predicate distribution: (i) head (many-shot), (ii) middle (medium-shot) and (iii) tail (few-shot) recall. The bar plots of head, middle and tail recalls (non-graph constraint) of MOTIFS and VCTree backbones are presented in <ref type="figure">Figure 9</ref>. In all the three SGG tasks, both of MOTIFS-DLFE and VCTree-DLFE remarkably outperform other debiasing methods by a large margin regarding the tail recall, while being on par regarding the head and middle recalls.</p><p>The recalls with/without graph constraint in numbers for Pred-Cls, SGCls and SGDet task is presented in <ref type="table" target="#tab_8">Table 5</ref>, <ref type="table">Table 6</ref> and <ref type="table">Table  7</ref>, respectively. Again, DLFE improves middle and tail recalls more significantly, with the cost of head recall similar to that of other debiasing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE RESULTS IN RECALLS</head><p>We present a comprehensive comparison of recently published (2020-now) SGG models/debiasing methods 7 in graph-constraint recalls and mean recalls, in PredCls mode in <ref type="table">Table 8</ref>, in SGCls mode in <ref type="table">Table 9</ref> and in SGDet mode in <ref type="table" target="#tab_2">Table 10</ref>. We note that the plain recall (R@ ) is biased as it favors the head classes and does not reflect that multiple relations could exist in an object pair. While there are some performance drops in more conventional recalls (R@ ) for the debiasing methods, it is because the predicates are being classified into the more descriptive ones (which do not have been annotated as ground truth). Mean recall (mR@ ) is less biased than the plain recall as it treats all classes equally; however, it still does not consider the multi-relation issue. Remarkably, it is clear from all the three tables that our DLFE still achieves state-of-theart mR@ comparing the other debiasing methods with the same backbone, and either MOTIFS-DLFE or VCTree-DLFE attains the highest mR scores across all the models and backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL QUALITATIVE RESULTS</head><p>We present additional results on the change of confidence distribution after applying DLFE to MOTIFS in <ref type="figure" target="#fig_8">Figure 10</ref>. For table-?-window in example (a), instead of the prediction of near by MOTIFS, MOTIFS-DLFE predicts a more descriptive in front of which matches the ground truth. The same applies to bird-?pole in example (b) where MOTIFS-DLFE's standing on is better than MOTIFS's on. (c) is an interesting example that, while the top prediction on by MOTIFS-DLFE is the same as that of MOTIFS, more descriptive predicates such as mounted on, attached to are assigned with the higher confidence scores by MOTIFS-DLFE. This fact should make it easier for, e.g., non-graph constraint mean recall (ng-mR@ ), to recall these fine-grained predicates Finally, car-?-street is another example that MOTIFS-DLFE produces more descriptive parked on rather than on; however, due to the missing labels parked on is not in the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicate Classification (PredCls) Scene Graph Classification (SGCls)</head><p>Scene Graph Detection (SGDet) Model ng-mR@20 ng-mR@50 ng-mR@100 ng-mR@20 ng-mR@50 ng-mR@100 ng-mR@20 ng-mR@50 ng-mR@100 MOTIFS <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b53">53]</ref> 19     <ref type="table">Table 6</ref>: Head, middle and tail (with/without graph constraint) recalls in the SGCls task on VG150. ? and ? are with the same meaning as in <ref type="table" target="#tab_2">Table 1</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) MOTIFS Backbone (b) VCTree Backbone</head><p>Scene Graph Classification Model R@20 R@50 R@100 mR@20 mR@50 mR@100 IMP+ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">44]</ref>   <ref type="table">Table 9</ref>: Recall and mean recall (with graph constraint) results in SGCls task on VG150. Models in the first section are with VGG backbone <ref type="bibr" target="#b34">[34]</ref>. ?, ? and ? are with the same meaning as in <ref type="table" target="#tab_2">Table 1</ref> of the main paper.</p><p>Scene Graph Detection Model R@20 R@50 R@100 mR@20 mR@50 mR@100 IMP+ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">44]</ref> 14  <ref type="table" target="#tab_2">Table 10</ref>: Recall and mean recall (with graph constraint) results in SGDet task on VG150. Models in the first section are with VGG backbone <ref type="bibr" target="#b34">[34]</ref>. ?, ? and ? are with the same meaning as in <ref type="table" target="#tab_2">Table 1</ref> of the main paper.</p><p>Original Prediction Probability Q:  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 3</head><label>13</label><figDesc>Relationship Prediction ( | , , ). Similar to that of object classification, another graphical representation of the same type is established to propagate contexts between features. The module takes in both the object labels and the object contexts{ } and outputs refined relation contexts { }. For each object pair {( , )| , = 1, ..., , ? }, their relation contexts ( , ), bounding boxes ( , ), union bounding boxes and features , are gathered into a pairwise feature for decoding into a probability vector over the classes with MLPs followed by a Softmax layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The per-class ratio of valid examples in all examples of VG150 training set [44] (SGDet), by inferencing a trained MOTIFS (Train-Est) or dynamically inferencing a training MOTIFS with augmented data (DLFE). Numbers of DLFE are averaged over all epochs. All numbers can exceed 1 as a ground truth pair can match multiple proposal pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The label frequencies estimated by (a) Train-Est or (b) DLFE with MOTIFS<ref type="bibr" target="#b53">[53]</ref>. The classes with higher label frequency are more conspicuous than those with a lower one. Predicates sorted by class frequency in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The absolute ng per-class R@100 changes when recovering MOTIFS's<ref type="bibr" target="#b53">[53]</ref> unbiased probabilities with the label frequencies estimated by Train-Est or DLFE, in SGDet mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Non-graph constraint per-class Recall@20 (PredCls) change w.r.t. MOTIFS baseline. DLFE significantly improves the mid-to-tail recalls (where the other debiasing methods struggle) without compromising much head classes performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Scene graphs generated by MOTIFS (left) and MOTIFS-DLFE (right) in PredCls. Only the top-1 prediction is shown for each object pair. A prediction can be correct (matches GT), incorrect (does not match GT and weird) or acceptable (does not match GT but still reasonable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Probability distributions (normalized to sum 1) over the classes by MOTIFS (top of each example) and MOTIFS-DLFE (bottom). The top-1 predictions can be correct (GT), incorrect (Non-GT and weird) or acceptable (Non-GT but reasonable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Confidence distributions over the predicates, produced by MOTIFS (top of each example) and MOTIFS with DLFE (bottom). Green, red and tangerine predicates denote correct (GT), incorrect (Non-GT and weird) and acceptable (Non-GT but reasonable), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Output: boxes and visual feat {f ! }</figDesc><table><row><cell>: pairwise example : true predicate class : annotated label</cell><cell cols="3">Relation Contexts &amp; Prediction Out: Relation prob. { &amp;( | )} by decoding pairwise examples , , window {</cell><cell cols="3">Dynamic Label Frequency Estimation window window window</cell></row><row><cell>Input Image:</cell><cell>window</cell><cell>window</cell><cell></cell><cell cols="2">r="with"</cell><cell>r="near"</cell><cell>r="in front of"</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>door</cell><cell cols="2">curtain</cell><cell>table</cell></row><row><cell></cell><cell>door</cell><cell>curtain</cell><cell></cell><cell>'( = | )</cell><cell>0.06</cell><cell>0.05</cell><cell>0.02</cell></row><row><cell></cell><cell>table</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Label Frequency</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>1</cell><cell>?</cell><cell>1</cell><cell>?</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"#$%</cell><cell>&amp;'()</cell><cell>#&amp;_+),&amp;$_,+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>'( = | )</cell><cell>0.86</cell><cell>0.83</cell><cell>0.67</cell></row><row><cell></cell><cell cols="2">Object Contexts &amp; Classification</cell><cell>,</cell><cell cols="3">Unbiased Inference</cell><cell>Est. Label Frequency</cell></row><row><cell></cell><cell cols="3">Output: object labels by decoding object contexts { ! }</cell><cell></cell><cell></cell></row></table><note>! } incl. relation contexts { ! }</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: DLFE during training time Input : Training dataset and momentum Output : Biased model (?) and estimated label frequency for each mini batch B = {( , )} ? : do Forward model to obtain the biased probabilities ( ); // in-batch average of biased probabilities for each predicate class ? {1, ..., }: do</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>5]</cell><cell>-</cell><cell>36.3</cell><cell></cell><cell>49.0</cell><cell>-</cell><cell>19.8</cell><cell>26.2</cell><cell>-</cell><cell>11.7</cell><cell>16.0</cell></row><row><cell>GB-Net-? [51]</cell><cell>-</cell><cell>44.5</cell><cell></cell><cell>58.7</cell><cell>-</cell><cell>25.6</cell><cell>32.1</cell><cell>-</cell><cell>11.7</cell><cell>16.6</cell></row><row><cell>MOTIFS  ? [41, 53]</cell><cell>19.9</cell><cell>32.8</cell><cell></cell><cell>44.7</cell><cell>11.3</cell><cell>19.0</cell><cell>25.0</cell><cell>7.5</cell><cell>12.5</cell><cell>16.9</cell></row><row><cell>MOTIFS-Reweight  ?</cell><cell>20.5</cell><cell>33.5</cell><cell></cell><cell>44.4</cell><cell>12.6</cell><cell>19.1</cell><cell>24.3</cell><cell>8.0</cell><cell>12.9</cell><cell>16.8</cell></row><row><cell>MOTIFS-L2+uKD  ? [41]</cell><cell>-</cell><cell>36.9</cell><cell></cell><cell>50.9</cell><cell>-</cell><cell>22.7</cell><cell>30.1</cell><cell>-</cell><cell>14.0</cell><cell>19.5</cell></row><row><cell>MOTIFS-L2+cKD  ? [41]</cell><cell>-</cell><cell>37.2</cell><cell></cell><cell>50.8</cell><cell>-</cell><cell>22.1</cell><cell>29.6</cell><cell>-</cell><cell>14.2</cell><cell>19.8</cell></row><row><cell>MOTIFS-TDE  ? [37]</cell><cell>18.7</cell><cell>29.0</cell><cell></cell><cell>38.2</cell><cell>10.7</cell><cell>16.1</cell><cell>21.1</cell><cell>7.4</cell><cell>11.2</cell><cell>14.9</cell></row><row><cell>MOTIFS-PCPL  ? [45]</cell><cell>25.6</cell><cell>38.5</cell><cell></cell><cell>49.3</cell><cell>13.1</cell><cell>19.9</cell><cell>25.6</cell><cell>9.8</cell><cell>14.8</cell><cell>19.6</cell></row><row><cell>MOTIFS-STL  ? [4]</cell><cell>15.7</cell><cell>29.4</cell><cell></cell><cell>43.2</cell><cell>10.3</cell><cell>18.4</cell><cell>27.2</cell><cell>6.4</cell><cell>10.6</cell><cell>15.0</cell></row><row><cell>MOTIFS-DLFE</cell><cell>30.0</cell><cell>45.8</cell><cell></cell><cell>57.7</cell><cell>17.6</cell><cell>25.6</cell><cell>32.0</cell><cell>11.7</cell><cell>18.1</cell><cell>23.0</cell></row><row><cell>VCTree  ? [38, 41]</cell><cell>21.4</cell><cell>35.6</cell><cell></cell><cell>47.8</cell><cell>14.3</cell><cell>23.3</cell><cell>31.4</cell><cell>7.5</cell><cell>12.5</cell><cell>16.7</cell></row><row><cell>VCTree-Reweight  ?</cell><cell>20.6</cell><cell>32.5</cell><cell></cell><cell>41.6</cell><cell>14.1</cell><cell>21.3</cell><cell>27.8</cell><cell>8.0</cell><cell>12.1</cell><cell>15.9</cell></row><row><cell>VCTree-L2+uKD  ? [41]</cell><cell>-</cell><cell>37.7</cell><cell></cell><cell>51.7</cell><cell>-</cell><cell>26.8</cell><cell>35.2</cell><cell>-</cell><cell>13.8</cell><cell>19.1</cell></row><row><cell>VCTree-L2+cKD  ? [41]</cell><cell>-</cell><cell>38.4</cell><cell></cell><cell>52.4</cell><cell>-</cell><cell>26.8</cell><cell>35.8</cell><cell>-</cell><cell>13.9</cell><cell>19.0</cell></row><row><cell>VCTree-TDE  ? [37]</cell><cell>20.9</cell><cell>32.4</cell><cell></cell><cell>41.5</cell><cell>12.4</cell><cell>19.1</cell><cell>25.5</cell><cell>7.8</cell><cell>11.5</cell><cell>15.2</cell></row><row><cell>VCTree-PCPL  ? [45]</cell><cell>25.1</cell><cell>38.5</cell><cell></cell><cell>49.3</cell><cell>17.2</cell><cell>25.9</cell><cell>32.7</cell><cell>9.9</cell><cell>15.1</cell><cell>19.9</cell></row><row><cell>VCTree-STL  ? [4]</cell><cell>16.8</cell><cell>31.8</cell><cell></cell><cell>45.1</cell><cell>12.7</cell><cell>22.0</cell><cell>32.7</cell><cell>6.0</cell><cell>10.0</cell><cell>14.1</cell></row><row><cell>VCTree-DLFE</cell><cell>29.1</cell><cell>44.6</cell><cell></cell><cell>56.8</cell><cell>21.6</cell><cell>31.4</cell><cell>38.8</cell><cell>11.7</cell><cell>17.5</cell><cell>22.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SGDet)</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">mR@20 mR@50</cell><cell>mR@100</cell><cell cols="2">mR@20 mR@50</cell><cell>mR@100</cell><cell cols="3">mR@20 mR@50 mR@100</cell></row><row><cell>IMP+ [5, 44]</cell><cell></cell><cell>-</cell><cell>9.8</cell><cell>10.5</cell><cell>-</cell><cell>5.8</cell><cell>6.0</cell><cell>-</cell><cell>3.8</cell><cell>4.8</cell></row><row><cell>FREQ [38, 53]</cell><cell></cell><cell>8.3</cell><cell>13.0</cell><cell>16.0</cell><cell>5.1</cell><cell>7.2</cell><cell>8.5</cell><cell>4.5</cell><cell>6.1</cell><cell>7.1</cell></row><row><cell>MOTIFS [38, 53]</cell><cell></cell><cell>10.8</cell><cell>14.0</cell><cell>15.3</cell><cell>6.3</cell><cell>7.7</cell><cell>8.2</cell><cell>4.2</cell><cell>5.7</cell><cell>6.6</cell></row><row><cell>KERN [5]</cell><cell></cell><cell>-</cell><cell>17.7</cell><cell>19.2</cell><cell>-</cell><cell>9.4</cell><cell>10.0</cell><cell>-</cell><cell>6.4</cell><cell>7.3</cell></row><row><cell>VCTree [38]</cell><cell></cell><cell>14.0</cell><cell>17.9</cell><cell>19.4</cell><cell>8.2</cell><cell>10.1</cell><cell>10.8</cell><cell>5.2</cell><cell>6.9</cell><cell>8.0</cell></row><row><cell>GPS-Net [25]</cell><cell></cell><cell>17.4</cell><cell>21.3</cell><cell>22.8</cell><cell>10.0</cell><cell>11.8</cell><cell>12.6</cell><cell>6.9</cell><cell>8.7</cell><cell>9.8</cell></row><row><cell>GB-Net-? [51]</cell><cell></cell><cell>-</cell><cell>22.1</cell><cell>24.0</cell><cell>-</cell><cell>12.7</cell><cell>13.4</cell><cell>-</cell><cell>7.1</cell><cell>8.5</cell></row><row><cell>MOTIFS  ? [37, 53]</cell><cell></cell><cell>13.0</cell><cell>16.5</cell><cell>17.8</cell><cell>7.2</cell><cell>8.9</cell><cell>9.4</cell><cell>5.3</cell><cell>7.3</cell><cell>8.6</cell></row><row><cell cols="2">MOTIFS-Focal  ? [24, 37]</cell><cell>10.9</cell><cell>13.9</cell><cell>15.0</cell><cell>6.3</cell><cell>7.7</cell><cell>8.3</cell><cell>3.9</cell><cell>5.3</cell><cell>6.6</cell></row><row><cell cols="2">MOTIFS-Resample  ? [3, 37]</cell><cell>14.7</cell><cell>18.5</cell><cell>20.0</cell><cell>9.1</cell><cell>11.0</cell><cell>11.8</cell><cell>5.9</cell><cell>8.2</cell><cell>9.7</cell></row><row><cell>MOTIFS-Reweight  ?</cell><cell></cell><cell>14.3</cell><cell>17.3</cell><cell>18.6</cell><cell>9.5</cell><cell>11.2</cell><cell>11.7</cell><cell>6.7</cell><cell>9.2</cell><cell>10.9</cell></row><row><cell cols="2">MOTIFS-L2+uKD  ? [41]</cell><cell>14.2</cell><cell>18.6</cell><cell>20.3</cell><cell>8.6</cell><cell>10.9</cell><cell>11.8</cell><cell>5.7</cell><cell>7.9</cell><cell>9.5</cell></row><row><cell cols="2">MOTIFS-L2+cKD  ? [41]</cell><cell>14.4</cell><cell>18.5</cell><cell>20.2</cell><cell>8.7</cell><cell>10.7</cell><cell>11.4</cell><cell>5.8</cell><cell>8.1</cell><cell>9.6</cell></row><row><cell>MOTIFS-TDE  ? [37]</cell><cell></cell><cell>17.4</cell><cell>24.2</cell><cell>27.9</cell><cell>9.9</cell><cell>13.1</cell><cell>14.9</cell><cell>6.7</cell><cell>9.2</cell><cell>11.1</cell></row><row><cell>MOTIFS-PCPL  ? [45]</cell><cell></cell><cell>19.3</cell><cell>24.3</cell><cell>26.1</cell><cell>9.9</cell><cell>12.0</cell><cell>12.7</cell><cell>8.0</cell><cell>10.7</cell><cell>12.6</cell></row><row><cell>MOTIFS-STL  ? [4]</cell><cell></cell><cell>13.3</cell><cell>20.1</cell><cell>22.3</cell><cell>8.5</cell><cell>12.8</cell><cell>14.1</cell><cell>5.4</cell><cell>7.6</cell><cell>9.1</cell></row><row><cell>MOTIFS-DLFE</cell><cell></cell><cell>22.1</cell><cell>26.9</cell><cell>28.8</cell><cell>12.8</cell><cell>15.2</cell><cell>15.9</cell><cell>8.6</cell><cell>11.7</cell><cell>13.8</cell></row><row><cell>VCTree  ? [37, 38]</cell><cell></cell><cell>14.1</cell><cell>17.7</cell><cell>19.1</cell><cell>9.1</cell><cell>11.3</cell><cell>12.0</cell><cell>5.2</cell><cell>7.1</cell><cell>8.3</cell></row><row><cell>VCTree-Reweight  ?</cell><cell></cell><cell>16.3</cell><cell>19.4</cell><cell>20.4</cell><cell>10.6</cell><cell>12.5</cell><cell>13.1</cell><cell>6.6</cell><cell>8.7</cell><cell>10.1</cell></row><row><cell cols="2">VCTree-L2+uKD  ? [41]</cell><cell>14.2</cell><cell>18.2</cell><cell>19.9</cell><cell>9.9</cell><cell>12.4</cell><cell>13.4</cell><cell>5.7</cell><cell>7.7</cell><cell>9.2</cell></row><row><cell>VCTree-L2+cKD  ? [41]</cell><cell></cell><cell>14.4</cell><cell>18.4</cell><cell>20.0</cell><cell>9.7</cell><cell>12.4</cell><cell>13.1</cell><cell>5.7</cell><cell>7.7</cell><cell>9.1</cell></row><row><cell>VCTree-TDE  ? [37]</cell><cell></cell><cell>19.2</cell><cell>26.2</cell><cell>29.6</cell><cell>11.2</cell><cell>15.2</cell><cell>17.5</cell><cell>6.8</cell><cell>9.5</cell><cell>11.4</cell></row><row><cell>VCTree-PCPL  ? [45]</cell><cell></cell><cell>18.7</cell><cell>22.8</cell><cell>24.5</cell><cell>12.7</cell><cell>15.2</cell><cell>16.1</cell><cell>8.1</cell><cell>10.8</cell><cell>12.6</cell></row><row><cell>VCTree-STL  ? [4]</cell><cell></cell><cell>14.3</cell><cell>21.4</cell><cell>23.5</cell><cell>10.5</cell><cell>14.6</cell><cell>16.6</cell><cell>5.1</cell><cell>7.1</cell><cell>8.4</cell></row><row><cell>VCTree-DLFE</cell><cell></cell><cell>20.8</cell><cell>25.3</cell><cell>27.1</cell><cell>15.8</cell><cell>18.9</cell><cell>20.0</cell><cell>8.6</cell><cell>11.8</cell><cell>13.8</cell></row></table><note>Performance comparison in ng-mR@ on VG150 [20, 44]. Models in the first section are with VGG16 backbone [34]. ? models implemented or reproduced ourselves with ResNeXt-101-FPN [23] backbone. ? models also with the same ResNeXt- 101-FPN backbone while their performance are reported by the respective papers.? model using external knowledge bases. Predicate Classification (PredCls) Scene Graph Classification (SGCls) Scene Graph Detection (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of SGG models in graph-constraint mR@ on VG150<ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b44">44]</ref> testing set. Models in the first section are with VGG16 while the others are with ResNeXt-101-FPN. ?, ? and ? are with the same meanings as inTable 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of non-graph constraint mean recalls (ng-mR@ ) between Train-Est and our DLFE, in PredCls, SGCls and SGDet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Figure 9: Bar plots of head (many shot), middle (medium shot) and tail (few shot) classes based on either MOTIFS (left) and VCTree (right) backbone, evaluated on VG150. From top to down is results in PredCls, SGCls and SGDet, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Predicate Classification (PredCls)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Head Recalls</cell><cell></cell><cell></cell><cell>Middle Recalls</cell><cell></cell><cell></cell><cell>Tail Recalls</cell><cell></cell></row><row><cell>Model</cell><cell cols="9">R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100</cell></row><row><cell>MOTIFS  ? [37, 53]</cell><cell>35.5/45.2</cell><cell>42.9/65.9</cell><cell>45.4/78.6</cell><cell>6.0/15.2</cell><cell>9.0/30.0</cell><cell>10.3/45.4</cell><cell>0.0/0.9</cell><cell>0.0/3.3</cell><cell>0.0/9.7</cell></row><row><cell>MOTIFS-Reweight  ?</cell><cell>32.4/40.0</cell><cell>38.5/57.4</cell><cell>40.6/69.2</cell><cell>10.1/17.3</cell><cell>12.6/30.7</cell><cell>13.9/43.0</cell><cell>1.9/5.3</cell><cell>2.4/13.3</cell><cell>2.9/21.5</cell></row><row><cell>MOTIFS-TDE  ? [37]</cell><cell>29.8/32.0</cell><cell>40.8/48.3</cell><cell>46.3/60.8</cell><cell>21.2/22.7</cell><cell>29.8/34.9</cell><cell>34.9/46.1</cell><cell>0.0/0.2</cell><cell>0.0/1.8</cell><cell>0.1/5.3</cell></row><row><cell cols="2">MOTIFS-PCPL  ? [45] 39.4/47.7</cell><cell>47.0/66.5</cell><cell>49.5/77.6</cell><cell>19.5/27.0</cell><cell>25.3/41.8</cell><cell>27.9/55.2</cell><cell>0.2/1.8</cell><cell>0.2/6.0</cell><cell>0.2/13.2</cell></row><row><cell>MOTIFS-STL  ? [4]</cell><cell>33.6/37.1</cell><cell>43.4/56.4</cell><cell>46.3/70.0</cell><cell>7.7/10.3</cell><cell>13.5/24.1</cell><cell>16.5/39.8</cell><cell>0.5/1.6</cell><cell>5.5/9.6</cell><cell>6.0/21.2</cell></row><row><cell>MOTIFS-DLFE</cell><cell>34.5/44.4</cell><cell>40.7/61.9</cell><cell>42.7/72.4</cell><cell>19.9/27.2</cell><cell>25.3/42.8</cell><cell>27.9/54.2</cell><cell>11.2/18.6</cell><cell>15.1/31.8</cell><cell>15.7/44.6</cell></row><row><cell>VCTree  ? [37, 38]</cell><cell>36.6/47.0</cell><cell>43.8/67.5</cell><cell>46.2/79.8</cell><cell>7.8/17.5</cell><cell>11.4/34.3</cell><cell>13.0/50.0</cell><cell>0.0/1.0</cell><cell>0.0/5.5</cell><cell>0.1/12.7</cell></row><row><cell>VCTree-Reweight  ?</cell><cell>36.6/43.6</cell><cell>43.1/61.6</cell><cell>45.4/73.4</cell><cell>12.2/16.6</cell><cell>14.6/28.3</cell><cell>15.2/38.3</cell><cell>1.3/2.9</cell><cell>2.3/9.0</cell><cell>2.3/14.3</cell></row><row><cell>VCTree-TDE  ? [37]</cell><cell>34.0/36.9</cell><cell>45.1/54.8</cell><cell>50.0/67.5</cell><cell>22.5/24.2</cell><cell>31.6/37.9</cell><cell>36.2/49.1</cell><cell>0.1/0.3</cell><cell>0.3/2.5</cell><cell>0.5/5.4</cell></row><row><cell>VCTree-PCPL  ? [45]</cell><cell>38.0/46.3</cell><cell>45.4/64.5</cell><cell>47.8/75.9</cell><cell>18.1/26.3</cell><cell>23.0/42.6</cell><cell>25.4/54.2</cell><cell>0.1/2.4</cell><cell>0.1/6.9</cell><cell>0.1/16.1</cell></row><row><cell>VCTree-STL  ? [4]</cell><cell>34.0/37.8</cell><cell>43.8/57.6</cell><cell>46.5/71.1</cell><cell>8.4/10.9</cell><cell>14.3/26.1</cell><cell>17.1/41.8</cell><cell>2.4/3.6</cell><cell>8.4/13.8</cell><cell>9.1/23.5</cell></row><row><cell>VCTree-DLFE</cell><cell>30.1/41.1</cell><cell>37.3/57.5</cell><cell>39.6/68.3</cell><cell>14.4/21.9</cell><cell>19.0/36.0</cell><cell>20.8/48.2</cell><cell>6.0/12.0</cell><cell>8.1/26.5</cell><cell>9.2/38.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Head, middle and tail (with/without graph constraint) recalls in the PredCls task on VG150. ? and ? are with the same meaning as inTable 1of the main paper.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Scene Graph Classification (SGCls)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Head Recalls</cell><cell></cell><cell></cell><cell>Middle Recalls</cell><cell></cell><cell></cell><cell>Tail Recalls</cell><cell></cell></row><row><cell>Model</cell><cell cols="9">R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100</cell></row><row><cell>MOTIFS  ? [37, 53]</cell><cell>21.3/27.4</cell><cell>25.1/39.1</cell><cell>26.3/45.6</cell><cell>2.1/7.3</cell><cell>3.5/16.6</cell><cell>3.9/24.3</cell><cell>0.0/0.5</cell><cell>0.0/2.1</cell><cell>0.0/5.4</cell></row><row><cell>MOTIFS-Reweight  ?</cell><cell>21.6/25.7</cell><cell>25.1/35.7</cell><cell>26.3/42.1</cell><cell>6.8/10.2</cell><cell>8.0/16.5</cell><cell>8.3/21.9</cell><cell>0.9/2.6</cell><cell>1.6/6.0</cell><cell>1.7/9.7</cell></row><row><cell>MOTIFS-TDE  ? [37]</cell><cell>18.0/19.6</cell><cell>23.5/28.4</cell><cell>26.1/35.2</cell><cell>11.2/12.0</cell><cell>15.2/18.5</cell><cell>17.8/24.3</cell><cell>0.0/0.2</cell><cell>0.0/0.6</cell><cell>0.0/2.7</cell></row><row><cell cols="2">MOTIFS-PCPL  ? [45] 23.0/28.2</cell><cell>27.1/38.7</cell><cell>28.3/44.9</cell><cell>7.3/10.9</cell><cell>9.5/18.5</cell><cell>10.3/25.2</cell><cell>0.1/0.9</cell><cell>0.1/3.1</cell><cell>0.2/6.6</cell></row><row><cell>MOTIFS-STL  ? [4]</cell><cell>21.3/23.8</cell><cell>26.4/34.8</cell><cell>27.8/42.1</cell><cell>5.2/7.0</cell><cell>8.8/16.2</cell><cell>10.2/25.2</cell><cell>0.2/1.2</cell><cell>4.4/5.2</cell><cell>5.7/15.1</cell></row><row><cell>MOTIFS-DLFE</cell><cell>21.2/27.2</cell><cell>24.6/36.6</cell><cell>25.6/42.0</cell><cell>11.3/15.4</cell><cell>14.2/23.5</cell><cell>15.1/29.5</cell><cell>7.1/11.1</cell><cell>8.3/17.7</cell><cell>8.4/24.8</cell></row><row><cell>VCTree  ? [37, 38]</cell><cell>25.3/32.8</cell><cell>29.9/46.8</cell><cell>31.3/54.7</cell><cell>3.8/10.5</cell><cell>5.8/21.2</cell><cell>6.4/31.5</cell><cell>0.0/1.0</cell><cell>0.0/2.6</cell><cell>0.0/8.0</cell></row><row><cell>VCTree-Reweight  ?</cell><cell>24.2/29.5</cell><cell>28.4/41.2</cell><cell>29.7/49.3</cell><cell>7.1/10.9</cell><cell>8.5/18.0</cell><cell>9.0/25.2</cell><cell>1.7/2.9</cell><cell>1.9/5.9</cell><cell>1.9/9.7</cell></row><row><cell>VCTree-TDE  ? [37]</cell><cell>19.1/21.6</cell><cell>26.2/33.6</cell><cell>30.0/42.7</cell><cell>13.7/14.5</cell><cell>18.2/21.2</cell><cell>21.2/28.2</cell><cell>0.0/0.5</cell><cell>0.1/1.9</cell><cell>0.1/4.6</cell></row><row><cell>VCTree-PCPL  ? [45]</cell><cell>27.2/33.6</cell><cell>32.0/46.2</cell><cell>33.5/53.4</cell><cell>11.3/16.8</cell><cell>13.9/26.3</cell><cell>15.2/34.7</cell><cell>0.1/1.3</cell><cell>0.1/5.1</cell><cell>0.1/9.4</cell></row><row><cell>VCTree-STL  ? [4]</cell><cell>24.8/27.6</cell><cell>30.7/40.8</cell><cell>32.3/49.7</cell><cell>6.6/9.0</cell><cell>10.3/19.1</cell><cell>12.2/30.5</cell><cell>1.6/2.8</cell><cell>4.1/6.9</cell><cell>6.6/18.7</cell></row><row><cell>VCTree-DLFE</cell><cell>22.8/30.2</cell><cell>26.9/41.2</cell><cell>28.3/48.0</cell><cell>15.4/20.4</cell><cell>18.5/29.9</cell><cell>19.8/37.6</cell><cell>9.3/14.7</cell><cell>11.3/23.8</cell><cell>12.0/31.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Using label frequencies estimated in other mode is found to degrade the performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">More visualizations are available in Appendix E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that we do not compare with<ref type="bibr" target="#b19">[19]</ref> as 1) their reported numbers are rather selective and incomplete and 2) their method were not compared with other debiasing methods (e.g.,<ref type="bibr" target="#b37">[37]</ref>) fairly, i.e., with the same backbone.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d scene graph: A structure for unified semantics, 3d space, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5664" to="5673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from positive and unlabeled data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessa</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="719" to="760" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Influence of resampling on accuracy of imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Papanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth international conference on machine vision (ICMV 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9875</biblScope>
			<biblScope unit="page">987521</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft Transfer Learning via Gradient Diagnosis for Visual Relationship Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1118" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledgeembedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene graph prediction with limited labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paroma</forename><surname>Vincent S Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2580" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Jiun</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-Shot Multi-View Indoor Localization via Graph Location Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Jiun</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
	<note>An-An Liu, and Roger Zimmermann</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual Relationship Detection With Visual-Linguistic Knowledge From Multimodal Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Jiun</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="50441" to="50451" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from positive and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Letouzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="70" to="83" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual relationships as functions: Enabling few-shot scene graph prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorva</forename><surname>Dornadula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Narcomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from the Scene and Borrowing from the Rich: Tackling the Long Tail in Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<idno>IJCAI-20. 587-593</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Main track</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7211" to="7221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Contextual translation embedding for visual relationship detection and scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zih-Siou</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Generative Probabilistic Graph Neural Networks for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Schulte</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6783</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i07.6783" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11237" to="11245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?t?lina</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Know more say less: Image captioning based on scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2117" to="2130" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GPS-Net: Graph Property Sensing Network for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinquan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene Graph Generation With Hierarchical Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2979270</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2020.2979270" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="909" to="915" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge and curve detection for visual scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azriel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Classification by Attention: Scene Graph Classification with Prior Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><forename type="middle">Moayed</forename><surname>Baharlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10084</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Scene Graph Generation Codebase in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An-An Liu, and Yongdong Zhang. 2020. Part-Aware Interactive Learning for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshuo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="3155" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Jui Julius</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selen</forename><surname>Pehlivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2020-09-07" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HOSE-Net: Higher Order Structure Embedded Network for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>Seattle, WA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1846" to="1854" />
		</imprint>
	</monogr>
	<note>MM &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaotian</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual Relation of Interest Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>Seattle, WA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1386" to="1394" />
		</imprint>
	</monogr>
	<note>MM &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">NODIS: Neural Ordinary Differential Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno>abs/2001.04735</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bridging Knowledge Graphs to Generate Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02314</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning Visual Commonsense for Robust Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09623</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Scene Graph Detection (SGDet) Head Recalls Middle Recalls Tail Recalls Model R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100 R/ngR@20 R/ngR@50 R/ngR@100</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Table 7: Head, middle and tail (with/without graph constraint) recalls in the SGDet task on VG150. ? and ? are with the same</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Table 8: Recall and mean recall (with graph constraint) results in PredCls task on VG150. Models in the first section are with VGG backbone</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>are with the same meaning as in Table 1 of the main paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
