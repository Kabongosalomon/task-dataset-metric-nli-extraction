<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IN-DOMAIN REPRESENTATION LEARNING FOR REMOTE SENSING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Andr?</roleName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
							<email>maximneumann@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susano</forename><surname>Pinto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<email>neilhoulsby@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">IN-DOMAIN REPRESENTATION LEARNING FOR REMOTE SENSING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to establish baselines and a common evaluation protocol in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. Specifically, we investigate in-domain representation learning to develop generic remote sensing representations and explore which characteristics are important for a dataset to be a good source for remote sensing representation learning. The established baselines achieve state-of-the-art performance on these datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Remote sensing via computer vision and transfer learning is an important domain to address climate change as outlined by <ref type="bibr" target="#b22">Rolnick et al. (2019)</ref>. Among others, research in remote sensing promises to help in solving challenges in food security (precision farming), water sustainability, disaster prevention (floods/landslides/earthquake forecasting), deforestation or wild fire detection, urban planning, and monitoring of carbon stocks and fluxes or the air quality.</p><p>The number of Earth observing satellites is constantly increasing, with currently over 700 satellites monitoring many aspects of the Earth's surface and atmosphere from space, generating terabytes of imagery data every day. However the ground truth data acquisition is costly, usually requiring extensive campaign preparation, people and equipment transportation, and in-field gathering of the characteristics under question.</p><p>While there are remote sensing communities working on applying general deep learning methods to remote sensing problems, this domain has received relatively little attention from the representation learning community. Given its importance, it is still in an early development stage in comparison to the progress made in representation learning on natural and medical images (eg. by <ref type="bibr" target="#b20">Raghu et al. (2019)</ref>).</p><p>Some reasons for this are the diversity of data sources (satellite types, data acquisition modes, resolutions), the need of domain knowledge and special data processing, and the wide and scattered field of applications. The scarcity of standard recognized benchmark datasets and evaluation frameworks is another.</p><p>For a long time there were only small labeled remote sensing datasets available. Only recently new large-scale datasets have been generated in this domain (eg. by ; <ref type="bibr" target="#b25">Sumbul et al. (2019)</ref>). However, a consistent evaluation framework is still missing and the performance is usually reported on non-standard splits and with varying metrics, making reproduction and quick research iteration difficult.</p><p>To address this, we provide five representative and diverse remote sensing datasets in a standardized form for easy reuse. In particular, we explore the importance of in-domain representation learning for remote sensing at various data sizes and establish new state-of-the-art baseline results. The main goal of this work is to develop general remote sensing representations that can be applied by researchers to other unseen remote sensing tasks.</p><p>By providing these standardized datasets, common problem definition and baselines, we hope this work will simplify and enable faster iteration of research on remote sensing and inspire general representation learning experts to test their newest methods in this critical domain.</p><p>In summary, the main contributions of this work are as follows:</p><p>1. Exploring in-domain supervised fine-tuning to train generic remote sensing representations.</p><p>2. Generating 5 existing remote sensing datasets in a standardized format and establishing a common evaluation protocol 1 . Publishing the best trained representation in TensorFlow Hub for easy reuse in transfer learning applications 2 .</p><p>3. Establishing state-of-the-art baselines for the BigEarthNet, EuroSAT, RESISC-45, So2Sat, and UC Merced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK REPRESENTATION AND TRANSFER LEARNING</head><p>Shortly after AlexNet <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref> was trained on ImageNet <ref type="bibr" target="#b4">(Deng et al., 2009</ref>), representations obtained from it were used as off-the-shelf feature extractor networks <ref type="bibr" target="#b21">(Razavian et al., 2014)</ref>. Fine-tuning approaches were also explored and led to better results than random-initialization even when the tasks or domains differ <ref type="bibr" target="#b29">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b11">Kornblith et al., 2019)</ref>. These approaches extend beyond natural images and have been used the in medical domain, leading <ref type="bibr" target="#b20">Raghu et al. (2019)</ref> to question on how they work. Simultaneously there have been many improvements on training representations. <ref type="bibr" target="#b15">Mahajan et al. (2018)</ref> explored training bigger models on larger datasets with weaker labels and <ref type="bibr" target="#b18">(Ngiam et al., 2018;</ref><ref type="bibr" target="#b3">Cui et al., 2018)</ref> showed gains by closer matching the domains. Very promising are the more sample-efficient approaches using semi-or self-supervision <ref type="bibr" target="#b30">(Zhai et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEEP LEARNING IN REMOTE SENSING</head><p>Common remote sensing problems include land-use and land cover (LULC) classification, physical and bio-geo-chemical parameter estimation, target detection, time-series analysis, pan-sharpening and change detection. Many of these tasks can be solved or helped by deep learning approaches related to classification, object detection, semantic segmentation, or super-resolution. Reviews of some of these approaches can be found for instance in <ref type="bibr" target="#b0">(Ball et al., 2017;</ref><ref type="bibr" target="#b33">Zhu et al., 2017;</ref>.</p><p>Remote sensing data is acquired in different modes: optical, multi-and hyper-spectral, synthetic aperture radar (SAR), lidar, spectrometer <ref type="bibr" target="#b5">(Elachi &amp; Van Zyl, 2006)</ref>. Each of these modes has its own acquisition geometry and specific characteristics providing unique and complimentary information about the Earth's surface. In dependence of the instrument, the remote sensing imagery is available from very high resolutions at centimeter scale (aerial optical or radar, eg. for urban monitoring) to very low resolution at kilometer scale (eg. for atmosphere and ocean surface monitoring). Another important satellite data characteristic is the revisit time (how fast does a satellite revisit the same location, and constructs a time series), which can range from daily to multiple months.</p><p>The majority of deep learning approaches in remote sensing are currently based on optical imagery at high (0.3-1 m) to medium (10-30 m) resolution, obtained from aerial imagery (such as seen on Google Earth) or publicly available satellite data (eg. NASA's Landsat or ESA's Sentinel-2 satellites), respectively. Though, multi-spectral, hyper-spectral, and radar imagery is increasingly being used as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRESENTATION/TRANSFER LEARNING IN REMOTE SENSING</head><p>Since labeling remote sensing data is expensive, for a long time there was no equivalent to ImageNet and most benchmark datasets were small. The probably most used remote sensing benchmarking dataset, UC Merced <ref type="bibr" target="#b28">(Yang &amp; Newsam, 2010)</ref>, has only 2100 images with 100 images per class.</p><p>Because it was not possible to train large state-of-the-art network on a small dataset from scratch, remote sensing researchers started to build smaller networks, as for instance in <ref type="bibr" target="#b14">Luus et al. (2015)</ref>.</p><p>Another direction was to use models pre-trained on natural images. For instance, <ref type="bibr" target="#b16">Marmanis et al. (2016)</ref> used the extracted CNN features from the pre-trained Overfeat model <ref type="bibr" target="#b23">(Sermanet et al., 2013)</ref> to feed to another CNN model. <ref type="bibr" target="#b1">Castelluccio et al. (2015)</ref> used GoogLeNet <ref type="bibr" target="#b26">(Szegedy et al., 2015)</ref> pre-trained on ImageNet to fine-tune UC Merced. Similarly, <ref type="bibr" target="#b19">Nogueira et al. (2017)</ref> used pre-trained models on natural imagery to fine-tune five models based of cross-validation splits. Afterwards, they trained an additional SVM on the fine-tuned features to achieve best performance.</p><p>Although there are works in using pre-trained representations, there is hardly any work on training representations specific for solving tasks on this domain. An example is the multi-layer generative adversarial network (GAN) <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref> approach for unsupervised representation learning presented by <ref type="bibr" target="#b13">Lin et al. (2017)</ref>. <ref type="bibr" target="#b27">Xie et al. (2015)</ref> developed a transfer learning pipeline for mapping poverty distribution based on transfering the nighttime lights prediction task. Not specifically addressing representation learning, but a still related application using conditional GANs <ref type="bibr" target="#b17">(Mirza &amp; Osindero, 2014)</ref> was demonstrated for cloud removal from RGB images by fusing RGB and near infrared bands in <ref type="bibr" target="#b6">Enomoto et al. (2017)</ref>, or by fusing multi-spectral and synthetic aperture radar data <ref type="bibr" target="#b8">(Grohnfeldt et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASETS</head><p>For this work, five diverse datasets were selected. We prioritized newer and larger datasets that are quite diverse from each other, address scene classification tasks, and include at least optical imagery.</p><p>Image data comes either from aerial high-resolution imagery or from satellites. Three datasets include imagery from the European Space Agency's (ESA) Sentinel-2 satellite constellation, that provides medium resolution imagery of the Earth's surface every three days. The multi-spectral imager on Sentinel-2 delivers next to the 3 RGB channels additional channels at various frequencies (see Appendix A.1). One dataset includes co-registered imagery from a dual-polarimetric synthetic aperture radar (SAR) instrument of ESA's Sentinel-1.</p><p>Besides the differences in data sources, number of training samples, number of classes, image sizes and pixel resolutions (summarized in <ref type="table" target="#tab_0">Table 1</ref>), the datasets are also quite diverse across:</p><p>? Intra-and inter-class visual diversity: some datasets have high in-class and low betweenclasses diversity and vice versa. ? Label imbalance: some datatasets are perfectly balanced, while others are highly unbalanced. ? Label domain: land-use land cover (LULC), urban structures, fine ecological labels. ? Label quality: from fine human selection to weak labels from auxiliary datasets 3 .</p><p>While having only 5 datasets might not allow us to completely disentangle the confounding factors of remote sensing representation learning, it should still help us in understanding the important factors for good remote sensing representation learning datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INDIVIDUAL DATASETS</head><p>In addition to the descriptions of the individual datasets in this section, Appendix A shows example images for all datasets and the distribution of classes.</p><p>BigEarthNet <ref type="bibr" target="#b25">(Sumbul et al., 2019)</ref> is a challenging large-scale multi-spectral dataset consisting of 590,326 image patches from the Sentinel-2 satellite. 12 frequency channels (including RGB) are provided, each covering an area of 1.2 ? 1.2 km with resolutions of 10 m, 20 m, and 60 m per pixel. This is a multi-label dataset where each image is annotated by multiple land-cover classes. The label distribution is highly unbalanced ranging from 217k images of "mixed forest" label to only 328 images with label "burnt areas". About 12% of the patches are fully covered by seasonal 0.3 m multi-class *Image size varies in dependence of resolution from 120x120 to 60x60 to 20x20. snow, clouds or cloud shadows. The only available public baseline metrics include precision and recall values of 69.93% and 77.1%, respectively, for using a shallow CNN on a reduced dataset, after removing the snow and cloud affected samples.</p><p>EuroSAT <ref type="bibr" target="#b10">(Helber et al., 2019)</ref> is another recently published dataset containing 27,000 images from Sentinel-2 satellites. All 13 frequency bands of the satellite are included. Each image covers an area of 640 ? 640 meters and is assigned to one of 10 LULC classes, with 2000 to 3000 images per class. Because the classes are quite distinctive, very high accuracies can be achieved when using the entire dataset for training.</p><p>NWPU RESISC-45 <ref type="bibr" target="#b2">(Cheng et al., 2017)</ref> dataset is an aerial dataset consisting of 31,500 RGB images divided into 45 scene classes. Each class includes 700 images with a size of 256 ? 256 pixels. This is the only dataset with varying spatial resolution ranging from 20 cm to more than 30 meters. The data covers a wide range of countries and biomes. During the construction, the authors paid special attention to have classes with high same-class diversity and between-class similarity to make it more challenging.</p><p>So2Sat LCZ-42  is a dataset consisting of co-registered SAR and multi-spectral 320 ? 320 m image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) <ref type="bibr">(Stewart &amp; Oke, 2012)</ref> labels. The dataset is distributed over 42 cities across different continents and cultural regions of the world. This is another challenging dataset and it is intended for learning features to distinguish various urban zones. The challenge of this dataset is the relatively small image size (32?32) and the relatively high inter-class visual similarity.</p><p>UC Merced Land-Use Dataset <ref type="bibr" target="#b28">(Yang &amp; Newsam, 2010</ref>) is a high-resolution (30 cm per pixel) dataset that was extracted from aerial imagery from the United States Geological Survey (USGS) National Map over multiple regions in the United States. The 256 ? 256 RGB images cover 21 land-use classes, with 100 images per class. This is a relatively small datasets that has been widely benchmarked for remote sensing scene classification task since 2010 and for which nearly perfect accuracy can be achieved with modern convolutional neural networks <ref type="bibr" target="#b1">(Castelluccio et al., 2015;</ref><ref type="bibr" target="#b16">Marmanis et al., 2016;</ref><ref type="bibr" target="#b19">Nogueira et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RELEASE OF STANDARDIZED DATASETS</head><p>To simplify access to the data and its usage, we imported and published these datasets in Tensorflow Datasets (TFDS) <ref type="bibr" target="#b36">4</ref> .</p><p>For reproducability and a common evaluation framework, standard train, validation, and test splits using the 60%, 20%, and 20% ratios, respectively, were generated for all datasets except So2Sat.</p><p>For the So2Sat dataset, the source already provides train and validation splits. To generate the test split, the original upstream validation is separated into validation and test splits with the 25% and 75% ratios, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REMOTE SENSING DATA PROCESSING</head><p>The remote sensing domain is quite distinctive from natural image domain and requires special attention during pre-processing and model construction. Some characteristics are:</p><p>? Remote sensing input data usually comes at higher precision <ref type="bibr">(16 or 32 bits)</ref>.</p><p>? The number of channels is variable, depending on the satellite instrument. RGB channels are only a subset of a multi-or hyper-spectral imagery dataset. Other data sources might have no optical channels (eg. radar or lidar) and the channels distribution can be determined by polarimetric, interferometric or frequency diversity.</p><p>? The range of values varies largely from dataset to dataset and between channels. The values distribution can be highly skewed.</p><p>? Many quantitative remote sensing tasks rely on the absolute values of the pixels.</p><p>? The images acquired from space are usually rotation invariant.</p><p>? Source data can be delivered at different product levels (for instance w/ or w/o atmospheric correction, co-registration, orthorectification, radiometric calibration, etc.).</p><p>? Especially lower resolution data aggregates a lot of information about the illuminated surface in a single pixel since it covers a large area.</p><p>? Image axes might be non-standard, eg. representing range and azimuth dimensions.</p><p>This sets some requirements on pre-processing and encourages to adjust data augmentation of the input pipeline for remote sensing data.</p><p>Specifically for the problems discussed in this paper, it is recommended to rescale and clip the range of values per channel (accounting for outliers). Data augmentation that affects the intensity of the values should be discarded. On the other hand, one can reuse the rotation invariance and extend the augmentation to perform all rotations and flipping (providing 7 additional images per sample). Given multi-spectral data, such as Sentinel-2 based BigEarthNet, EuroSAT and So2Sat, one can use other subsets of channels instead of RGB including all available ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>The main goal is to develop representations that can be used across a wide field of remote sensing applications on unseen tasks. The training and evaluation protocol follows two main stages: <ref type="bibr" target="#b35">(1)</ref> upstream training of the representations model based on some out-or in-domain data, and (2) downstream evaluation of the representations by transferring the trained representation features to the new downstream tasks. For the upstream training on in-domain proxy datasets the entire data is used, that cannot include any data from the downstream tasks. The quality of the trained representations and their generalizability is often evaluated on reduced downstream training sizes to assess efficiency of the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETUP</head><p>In order to simplify the training procedure and to draw more general conclusions, all experiments use the same ResNet50 V2 architecture <ref type="bibr" target="#b9">(He et al., 2016)</ref> and configuration. However, due to the varied number of classes and training samples in the various datasets, we perform sweeps over a small set of hyper-parameters and augmentations as described in detail in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EVALUATION METRICS</head><p>We report performance results using accuracy metrics commonly used in computer vision tasks: for multi-class problems we use the Top-1 global accuracy metric, which denotes the percentage of correctly labeled samples. For multi-label problems we use the mean average precision (mAP) metric, which denotes the mean over the average precision (AV) values (AV is the integral over the precision-recall curve) of the individual labels. To measure and aggregate relative performance improvement over datasets performing at quite different accuracy levels, the logit transformation of the accuracy is preferred <ref type="bibr" target="#b11">(Kornblith et al., 2019)</ref>:</p><formula xml:id="formula_0">? = logit(?) = log ? 1 ? ? = 1 sigmoid(?)</formula><p>where ? is the accuracy rate (Top-1 or mAP) and ? is the corresponding logit accuracy. It captures the importance of accuracy change at different accuracy levels to provide a more fair evaluation of the achievement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">COMPARING IN-DOMAIN REPRESENTATIONS</head><p>To obtain in-domain representations, first we train models either from scratch or by fine-tuning Im-ageNet on each full dataset. The best of these models are then used as in-domain representations to train models on other remote sensing tasks (excluding the one used to train the in-domain representation).</p><p>For an initial evaluation of the different in-domain representation source data, <ref type="table" target="#tab_1">Table Table 2</ref> shows a cross-table of evaluating each trained in-domain and ImageNet representation on each of the downstream tasks. The representations were trained using full datasets upstream, while the down-stream tasks used only 1000 training examples to better emphasize the differences.</p><p>It can be observed that with 1000 training examples, the best results all come from fine-tuning the in-domain representations.</p><p>Additionally, despite having 2 distinctive groups of high-resolution aerial (RESISC-45, UC Merced) and medium-resolution satellite datasets (BigEarthNet, EuroSAT and So2Sat), the representations trained on RESISC-45 were able to outperform the others in all tasks (BigEarthNet representations tied for the UC Merced dataset) and it was the only representation to consistently outperform ImageNet-based representations.</p><p>That RESISC-45 would perform so good on both aerial and satellite data was unexpected. The reason is most likely related to the fact that RESISC-45 is the only dataset that has images with various resolutions. Combined with the large number of classes that have high within class diversity and high between-class similarity it seems to be able to train good representations for a wide range of remote sensing tasks, despite not being a very big dataset. This learning can be reused to adjust augmentation schemes for remote sensing representation learning by stronger varying the scale of images with randomized zooms and potentially other affine transformations.</p><p>The best representations for the RESISC-45 itself come from the other aerial dataset, UC Merced. The relatively small training size of it was counter-balanced by an aggressive augmentation (crop &amp; rot as described in Appendix B.3).</p><p>Counter to the expectation that bigger datasets should train better representations, the two biggest datasets, BigEarthNet and So2Sat, didn't provide the best representations (except of BigEarthNet representations for UC Merced). We hypothesize that this might be due to the weak labeling and the low training accuracy obtained in these datasets. It is possible that the full potential of these largescale datasets was not yet fully utilized and other self-or semi-supervised representation learning approaches could improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LARGE-SCALE COMPARISON</head><p>Having trained in-domain representations, we can now evaluate and compare the transfer quality of fine-tuning the best in-domain representations with fine-tuning ImageNet and training from scratch at various training data sizes. The results using the default experimental setup are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>In all cases, fine-tuning from ImageNet is better than training from scratch. And in all but one case, fine-tuning from an in-domain representation for transfer is even better.</p><p>The only exception is the BigEarthNet dataset at its full size. It is expected that having a large dataset should reduce the need for pre-training, but the gap between in-domain and ImageNet pre-training is quite big. We don't have an explanation for this yet and this needs to be further investigated.</p><p>Overall, these results establish new state-of-the-art baselines for these datasets, as summarized in <ref type="table" target="#tab_3">Table 4</ref>. Note that some results are not comparable: RESISC-45 has been previously evaluated only on 20% of data, So2Sat has no public benchmarking result to our knowledge, and the only published result of BigEarthNet is based on a cleaner version of the dataset (after removing the noisy images containing clouds and snow) and only precision and recall metrics were reported. <ref type="figure" target="#fig_0">Fig. 1</ref> summarizes the results of <ref type="table" target="#tab_2">Table 3</ref> across all datasets and training sizes. Logit accuracy metric is used, which fits better for aggregation of accuracies across wide ranges. <ref type="figure" target="#fig_0">Fig. 1a</ref> emphasizes that the smaller datasets (EuroSAT, RESISC-45, UC Merced) profit more from in-domain knowledge than the larger datasets (BigEarthNet, So2Sat). As is re-iterated again in <ref type="figure" target="#fig_0">Fig. 1b</ref>, using in-domain representations leads to higher accuracy gain when small number of training examples are available, which is of most interest for remote sensing applications to reduce the ground truth data acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">LIMITED NUMBERS OF TRAINING EXAMPLES</head><p>To look closer into in-domain representation learning for small number of training examples, we trained models with small training sizes ranging from 25 to 2500 (samples were randomly drawn disregarding class distributions). We used a simplified set of hyper-parameters that might not deliver the most optimal performance, but still allows to observe the general trends. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the improvement of using in-domain representations is clearly visible for the EuroSAT, RESISC-45 and UC Merced datasets. These are the 3 smaller datasets with higher quality labels. The results are less conclusive for the BigEarthNet and So2Sat datasets that have more noisy labels.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present a common evaluation benchmark for remote sensing representation learning based on five diverse datasets. The results demonstrate the enhanced performance of in-domain representations, especially for tasks with limited number of training samples, and achieve state-of-the-art performance. The five analyzed datasets and the best trained in-domain representations are published for easy reuse by the public.</p><p>We investigate dataset characteristics to be a good source for remote sensing representation learning. As the experimental results indicate, having a multi-resolution dataset helps to train more generalizable representations. Other factors seem to be label quality, number of classes, visual similarity across the classes and visual diversity within the classes. Surprisingly, we observed that representations trained on the large weakly-supervised datasets were not as successful as that of a smaller and more diverse human-curated dataset.</p><p>However, some results were inconclusive and require more investigation. Understanding the main factors of a good remote sensing dataset for representation learning is a major challenge, solving which could improve performance across a wide range of remote sensing tasks and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL DATASET INFORMATION</head><p>This section provides additional information on the used datasets and data sources.</p><p>A.1 SENTINEL-2 SATELLITE Sentinel-2 is a multi-spectral satellite constellation from the European Space Agency (ESA). Since 2017 two satellites are in operation delivering a 5-days revisit at equator and 2-3 days at high latitudes. The characteristics of the 13 bands are presented in <ref type="table" target="#tab_4">Table 5</ref>.    The following figures show some example images and label distribution for the RESISC-45 dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SO2SAT</head><p>The following figures show some example images and label distribution for the So2Sat dataset.    All the models trained in this work use the ResNet50 v2 architecture <ref type="bibr" target="#b9">(He et al., 2016)</ref> and were trained using SGD with momentum set to 0.9 on large batch sizes 512 or 1024. In total we configure and sweep only a fixed set of hyper-parameters per model, namely:</p><p>? learning rate: {0.1, 0.01},</p><p>? weight decay: {0.01, 0.0001},</p><p>? training schedules: {short, medium, long},</p><p>? preprocessing: {resize, crop &amp; rot} (described in Appendix B.3).</p><p>All the 3 training schedules use a linear warm-up for learning rate over the first w steps/epochs and decrease the learning rate by 10 per each learning phase p in the schedule. The learning rate schedules are given by:</p><p>? short: p = {750, 1500, 2250, 2500} steps, w = 200 steps.</p><p>? medium: p = {3000, 6000, 9000, 10000} steps, w = 500 steps.</p><p>? long: p = {30, 60, 80, 90} epochs, w = 5 epochs.</p><p>These hyperparameter settings follow approximately the setup in <ref type="bibr" target="#b31">(Zhai et al., 2019b)</ref> with minor modifications for the schedule and preprocessing. In in inital phase, more extensive hyperparameter sets were tried out, but with not much effect on the best performance and therefore for the experiments presented in this paper we limit the configurations to the ones described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 DETAILED SWEEPS PER EXPERIMENT</head><p>The models for experiments in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> were trained sweeping over all hyper parameters. The best performing models obtained from fine-tuning ImageNet were used as in-domain representations for all experiments (excluding same up-and down-stream dataset configurations).</p><p>For <ref type="figure" target="#fig_1">Fig. 2</ref>, the models were trained by sweeping over the learning rate and using only the short and medium training schedules. The weight decay was set to 0.0001, and the preprocessing was set to resize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PRE-PROCESSING AND DATA AUGMENTATION</head><p>Data pre-processing and augmentation can have a significant impact on performance. Therefore, in the reported results we used 2 pre-processing settings described below.</p><p>? resize -resize the original RGB input to 224x224 both at training and evaluation time.</p><p>? crop &amp; rot -during training resize the original RGB input to 256x256, perform a random crop of 224x224 and apply one of 8 random rotations (90 degrees and horizontal flip). During evaluation resize to 256x256 and perform a central crop of 224x224. <ref type="table" target="#tab_5">Table 6</ref> shows the difference of each strategy when fine-tune from ImageNet. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Aggregated mean relative improvement in logit accuracy of fine-tuning from ImageNet and in-domain representations in comparison to training from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top-1 accuracy rate or mean average precision (mAP) on validation set after training with a given method over a limited number of training examples on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>BigEarthNet image examples. Some images might be affected by seasonal snow, clouds or cloud shadows, which is not reflected in the land cover labels of this dataset. Note that some of the images (for example samples 4, 6, 21) could be affected by seasonal snow and cloud coverage, which is not reflected in the labels.Labels for the examples inFig. 3: BigEarthNet labels distribution counts.A.3 EUROSATThe following figures show some example images and label distribution for the EuroSAT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>EuroSAT image examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 : 16 A</head><label>616</label><figDesc>EuroSAT class distribution counts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>RESISC-45 image examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>RESISC-45 class distribution counts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>So2Sat image examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>So2Sat class distribution counts.A.6 UC MERCEDThe following figures show some example images and label distribution for the UC Merced dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>UC Merced image examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>UC Merced class distribution counts. B TRAINING SETUP B.1 ARCHITECTURE AND HYPER-PARAMETERS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of considered remote sensing datasets.</figDesc><table><row><cell>Name</cell><cell>year Source</cell><cell cols="4">Size Classes Image size Resolution Problem</cell></row><row><cell cols="2">BigEarthNet 2019 Sentinel-2</cell><cell>590k</cell><cell>43</cell><cell>120x120*</cell><cell>10-60 m multi-label</cell></row><row><cell>EuroSAT</cell><cell>2019 Sentinel-2</cell><cell>27k</cell><cell>10</cell><cell>64x64</cell><cell>10 m multi-class</cell></row><row><cell cols="2">RESISC-45 2017 aerial</cell><cell>31.5k</cell><cell>45</cell><cell cols="2">256x256 0.2-60+ m multi-class</cell></row><row><cell>So2Sat</cell><cell cols="2">2019 Sentinel-1/2 376k</cell><cell>17</cell><cell>32x32</cell><cell>10 m multi-class</cell></row><row><cell>UC Merced</cell><cell>2010 aerial</cell><cell>2.1k</cell><cell>21</cell><cell>256x256</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of trained In-Domain and ImageNet representations (rows) when using 1000 training examples for downstream tasks (columns). Emphasized (bold font) are the best accuracies per downstream task (column).</figDesc><table><row><cell cols="6">Source \Target BigEarthNet EuroSAT RESISC-45 So2Sat UC Merced</cell></row><row><cell>ImageNet</cell><cell>25.10</cell><cell>96.84</cell><cell>84.89</cell><cell>53.69</cell><cell>99.02</cell></row><row><cell>BigEarthNet</cell><cell>-</cell><cell>96.45</cell><cell>78.43</cell><cell>50.91</cell><cell>99.61</cell></row><row><cell>EuroSAT</cell><cell>27.10</cell><cell>-</cell><cell>79.59</cell><cell>52.99</cell><cell>98.05</cell></row><row><cell>RESISC-45</cell><cell>27.59</cell><cell>97.14</cell><cell>-</cell><cell>54.43</cell><cell>99.61</cell></row><row><cell>So2Sat</cell><cell>26.30</cell><cell>96.30</cell><cell>77.70</cell><cell>-</cell><cell>97.27</cell></row><row><cell>UC Merced</cell><cell>26.86</cell><cell>96.73</cell><cell>85.73</cell><cell>53.52</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy over different training methods and number of used training samples.</figDesc><table><row><cell></cell><cell>BigEarthNet</cell><cell>EuroSAT</cell><cell>RESISC-45</cell><cell>So2Sat</cell><cell>UC Merced</cell></row><row><cell></cell><cell cols="5">100 1k Full 100 1k Full 100 1k Full 100 1k Full 100 1k Full</cell></row><row><cell>Scratch</cell><cell cols="5">14.5 21.4 72.4 63.9 91.7 98.5 21.4 56.1 95.6 33.9 47.0 62.1 50.8 91.2 95.7</cell></row><row><cell cols="6">ImageNet 17.8 25.1 75.4 87.3 96.8 99.1 44.9 84.9 96.6 44.9 53.7 63.1 79.9 99.0 99.2</cell></row><row><cell cols="6">InDomain 18.8 27.6 69.7 91.3 97.1 99.2 49.0 85.7 96.8 46.4 54.4 63.2 91.0 99.6 99.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on selected remote sensing datasets. All results use Top-1 accuracy, except for BigEarthNet which uses precision/recall and mean average precision. Our best results were obtained by fine-tuning in-domain representations except for BigEarthNet which was obtained by fine-tuning ImageNet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell cols="3">Reference</cell><cell></cell><cell></cell><cell></cell><cell>Result</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">BigEarthNet</cell><cell cols="5">Sumbul et al. (2019) Ours</cell><cell cols="2">69.93% / 77.1% (P/R) 75.36% (mAP)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell cols="5">Helber et al. (2019) Ours</cell><cell></cell><cell>98.57% 99.20%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RESISC-45</cell><cell cols="5">Cheng et al. (2017) Ours</cell><cell></cell><cell>90.36% 96.83%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>So2Sat</cell><cell></cell><cell cols="3">Ours</cell><cell></cell><cell></cell><cell></cell><cell>63.25%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Marmanis et al. (2016)</cell><cell></cell><cell>92.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UC Merced</cell><cell cols="5">Castelluccio et al. (2015) Nogueira et al. (2017)</cell><cell></cell><cell>97.1% 99.41%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Ours</cell><cell></cell><cell></cell><cell></cell><cell>99.61%</cell></row><row><cell>mAP</cell><cell>0.2 0.3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell>1.0 0.6 0.8</cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.75 0.25 0.50</cell></row><row><cell></cell><cell></cell><cell>25</cell><cell>250 Training samples</cell><cell>2500</cell><cell></cell><cell></cell><cell>0.4</cell><cell>25</cell><cell>250 Training samples</cell><cell>2500</cell><cell>25</cell><cell>250 Training samples</cell><cell>2500</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) BigEarthNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) EuroSAT</cell><cell></cell><cell>(c) RESISC-45</cell></row><row><cell>Accuracy</cell><cell>0.3 0.4 0.5</cell><cell>25</cell><cell>250 Training samples</cell><cell>2500</cell><cell>Accuracy</cell><cell cols="2">1.00 0.25 0.50 0.75</cell><cell>25</cell><cell>250 Training samples</cell><cell>2500</cell><cell>InDomain ImageNet FromScratch</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(d) So2Sat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(e) UC Merced</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The following figures show some example images and label distribution for the BigEarthNet dataset.</figDesc><table><row><cell cols="3">Sentinel-2 channel characteristics (Abbreviations: NIR: Near Infra-Red, SWIR: Short-</cell></row><row><cell>Wavelength Infra-Red).</cell><cell></cell><cell></cell></row><row><cell cols="3">Band and Highest Sensitivity Target Spatial Resolution [m] Central Wavelength [nm]</cell></row><row><cell>B01 -Aerosols</cell><cell>60</cell><cell>443</cell></row><row><cell>B02 -Blue</cell><cell>10</cell><cell>490</cell></row><row><cell>B03 -Green</cell><cell>10</cell><cell>560</cell></row><row><cell>B04 -Red</cell><cell>10</cell><cell>665</cell></row><row><cell>B05 -Red edge 1</cell><cell>20</cell><cell>705</cell></row><row><cell>B06 -Red edge 2</cell><cell>20</cell><cell>740</cell></row><row><cell>B07 -Red edge 3</cell><cell>20</cell><cell>783</cell></row><row><cell>B08 -NIR</cell><cell>10</cell><cell>842</cell></row><row><cell>B08A -Red edge 4</cell><cell>20</cell><cell>865</cell></row><row><cell>B09 -Water vapor</cell><cell>60</cell><cell>945</cell></row><row><cell>B10 -Cirrus</cell><cell>60</cell><cell>1375</cell></row><row><cell>B11 -SWIR 1</cell><cell>20</cell><cell>1610</cell></row><row><cell>B12 -SWIR 2</cell><cell>20</cell><cell>2190</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Accuracy of each pre-processing strategy when fine-tuning an ImageNet representation. Full 100 1k Full 100 1k Full 100 1k Full 100 1k Full resize 17.3 24.5 75.4 85.2 96.0 98.9 37.5 78.6 95.8 44.9 51.8 63.1 69.1 98.2 99.2 crop &amp; rot 17.8 25.1 73.4 87.3 96.8 99.1 44.9 84.9 96.6 43.8 53.7 59.6 79.9 99.0 99.2</figDesc><table><row><cell>BigEarthNet</cell><cell>EuroSAT</cell><cell>RESISC-45</cell><cell>So2Sat</cell><cell>UC Merced</cell></row><row><cell>100 1k</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Published at https://www.tensorflow.org/datasets 2 Published at https://tfhub.dev/google/collections/remote_sensing/1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Some datasets and images are affected by not filtered-out cloud and snow coverage that makes the correct classification of the samples difficult.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Published at https://www.tensorflow.org/datasets</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank No? Lutz for useful comments, Jeremiah Harmsen for inspiration, and the Brain Zurich VTAB team for insightful discussions and developing the benchmarking framework. Finally, we would like to acknowledge Tensorflow Hub (TF-Hub) and Tensorflow Datasets (TFDS) teams for their support on publishing datasets and models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.11.042609</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">04</biblScope>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Land Use Classification in Remote Sensing Images by Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1508.00092" />
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2017.2675998</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale fine-grained categorization and domain-specific transfer learning. CoRR, abs/1806.06193</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.06193" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li Jia Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to the physics and techniques of remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob J Van</forename><surname>Zyl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ryosuke Nakamura, and Nobuo Kawaguchi. Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A conditional generative adversarial network to fuse sar and multispectral optical data for cloud removal from sentinel-2 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grohnfeldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGARSS.2018.8519215</idno>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018 -2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1726" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2019.2918242</idno>
		<ptr target="http://dx.doi.org/10.1109/JSTARS.2019.2918242" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">22172226</biblScope>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.08974" />
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning for Remote Sensing Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangluan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2752750</idno>
		<ptr target="http://dx.doi.org/10.1109/LGRS.2017.2752750" />
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">20922096</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiview deep learning for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maharaj</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2015.2483680</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1805.00932</idno>
		<ptr target="http://arxiv.org/abs/1805.00932" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Learning Earth Observation Classification Using ImageNet Pretrained Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Esch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Stilla</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2015.2499239</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7342907/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="109" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<idno>abs/1811.07056</idno>
		<ptr target="http://arxiv.org/abs/1811.07056" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification. Pattern Recogn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiller</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Otvio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefersson</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.07.001</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.07.001" />
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transfusion: Understanding transfer learning with applications to medical imaging. CoRR, abs/1902.07208</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.07208" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: an astounding baseline for recognition. CoRR, abs/1403</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1403.6382" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><forename type="middle">H</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Waldman-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">D</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Karthik</forename><surname>Sherwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Mukkavilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Creutzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Tackling climate change with machine learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local climate zones for urban temperature studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oke</surname></persName>
		</author>
		<idno type="DOI">10.1175/BAMS-D-11-00019.1</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bigearthnet: A large-scale benchmark archive for remote sensing image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gencer</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Charfuelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Begm</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Markl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1510.00098" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<idno type="DOI">10.1145/1869790.1869829</idno>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1869790.1869829" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems -GIS &apos;10</title>
		<meeting>the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems -GIS &apos;10<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">270</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">S 4 L: Self-supervised semisupervised learning. CoRR, abs/1905.03670</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.03670" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Review of Researches on Deep Learning in Remote Sensing Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongning</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">http:/www.scirp.org/journal/doi.aspx?DOI=10.4236/ijg.2019.101001</idno>
		<ptr target="http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ijg.2019.101001" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Geosciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xiang Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2017.2762307</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8113128/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lloyd</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://mediatum.ub.tum.de/1454690" />
	</analytic>
	<monogr>
		<title level="j">So2Sat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Broad-leaved forest (2 labels) 2. Coniferous forest, Mixed forest, Transitional woodland/shrub (3 labels) 3. Sea and ocean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vineyards</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>label</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Coniferous forest, Mixed forest</title>
		<imprint/>
	</monogr>
	<note>Water bodies (4 labels</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Non</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>irrigated arable land, Mixed forest (2 labels</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Coniferous forest, Mixed forest</title>
		<imprint/>
	</monogr>
	<note>Transitional woodland/shrub (4 labels</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Sea and ocean</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Coniferous forest</title>
		<imprint/>
	</monogr>
	<note>Mixed forest (2 labels</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Discontinuous urban fabric, Industrial or commercial units, Coniferous forest, Mixed forest</title>
		<imprint/>
	</monogr>
	<note>Transitional woodland/shrub (5 labels</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Non</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>irrigated arable land, Pastures (2 labels</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Coniferous forest</title>
		<imprint/>
	</monogr>
	<note>Mixed forest, Water bodies (3 labels</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Complex cultivation patterns, Land principally occupied by agriculture, with significant areas of natural vegetation</title>
		<imprint/>
	</monogr>
	<note>Broad-leaved forest (3 labels</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Broad-leaved forest, Sclerophyllous vegetation</title>
		<imprint/>
	</monogr>
	<note>Transitional woodland/shrub (4 labels</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Agro-forestry areas, Broad-leaved forest</title>
		<imprint/>
	</monogr>
	<note>Transitional woodland/shrub (3 labels</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Coniferous forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Non</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Mixed forest (4 labels</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mixed forest, Transitional woodland/shrub, Water bodies (4 labels) 22. Coniferous forest</title>
	</analytic>
	<monogr>
		<title level="m">Coniferous forest</title>
		<imprint/>
	</monogr>
	<note>Mixed forest (2 labels</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Agro-forestry areas, Sclerophyllous vegetation, Transitional woodland/shrub</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Non</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Water bodies (6 labels</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Discontinuous urban fabric, Non-irrigated arable land</title>
		<imprint/>
	</monogr>
	<note>Inland marshes (3 labels</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Land principally occupied by agriculture, with significant areas of natural vegetation, Broad-leaved forest</title>
		<imprint/>
	</monogr>
	<note>Coniferous forest (3 labels</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
