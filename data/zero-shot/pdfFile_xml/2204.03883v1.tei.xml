<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision Transformers for Single Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuda</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuqing</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Du</surname></persName>
						</author>
						<title level="a" type="main">Vision Transformers for Single Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image Processing</term>
					<term>Image Dehazing</term>
					<term>Deep Learn- ing</term>
					<term>Vision Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25% #Param and 5% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous stateof-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method's capability to remove highly non-homogeneous haze. We share our code and dataset at https://github.com/IDKiro/DehazeFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H AZE is a common atmospheric phenomenon that can impair daily life and machine vision systems. The presence of haze reduces the scene's visibility and affects people's judgment of the object, and thick haze can even affect traffic safety. For computer vision, haze degrades the quality of the captured image in most cases. It can impact the model's reliability in high-level vision tasks, further mislead machine systems, such as autonomous driving. All these make image dehazing a meaningful low-level vision task.</p><p>Image dehazing aims to estimate the latent haze-free image from the observed hazy image. For the single image dehazing problem, there is a popular model <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> to characterize the degradation process for hazy images:</p><formula xml:id="formula_0">I = J(x)t(x) + A(1 ? t(x)),<label>(1)</label></formula><p>where I is the captured hazy image, J is the latent haze-free image, A is the global atmospheric light, and t is the medium transmission map. And the transmission can be expressed as</p><formula xml:id="formula_1">t(x) = e ??d(x) ,<label>(2)</label></formula><p>where ? is the scattering coefficient of the atmosphere, and d is the scene depth. As can be seen, image dehazing is a typically Manuscript received XXXX 00, 0000; accepted XXXX 00, 0000. Date of publication XXXX 00, 0000; date of current version XXXX 00, 0000. The associate editor coordinating the review of this manuscript and approving it for publication was XXXX. ill-posed problem, and early image dehazing methods tend to constrain the solution space with priors <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. They generally estimate A and t(x) separately to lower the complexity of the problem and then use Eq.(1) to derive the results. These prior-based methods can produce images with good visibility. However, these images are often visibly different from hazefree images, and artifacts may be introduced in regions that do not satisfy the priors.</p><p>In recent years, deep learning has made a big hit in computer vision, and researchers have proposed a large number of image dehazing methods based on deep convolutional neural networks (CNNs) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. With a sufficient number of synthetic image pairs, these methods can achieve superior performance over prior-based methods. Earlier CNN-based methods <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> also estimate A and t(x) separately, where t(x) is supervised using the transmission map used in synthesizing the dataset. And current methods <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> prefer to predict the latent hazefree image or the residuals of the haze-free image versus the hazy image since it tends to achieve better performance. Very recently, ViT <ref type="bibr" target="#b21">[22]</ref> outperformed almost all CNN architectures in high-level vision tasks using plain Transformer architecture. Subsequently, many modified architectures <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> have been proposed, and vision Transformer is challenging the dominance of CNNs in high-level vision tasks. So many works have demonstrated the effectiveness of vision Transformers, but there is still no Transformer-based image dehazing method that defeats the state-of-the-art image dehazing networks. In this work, we propose an image dehazing Transformer dubbed DehazeFormer, which is inspired by Swin Transformer <ref type="bibr" target="#b29">[30]</ref>. It dramatically surpasses these CNN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2204.03883v1 [cs.CV] 8 Apr 2022</head><p>We find that the LayerNorm <ref type="bibr" target="#b40">[41]</ref> and GELU <ref type="bibr" target="#b41">[42]</ref> commonly used in vision Transformers harm the image dehazing performance. Specifically, the LayerNorm used in vision Transformer normalizes the tokens corresponding to the image patches separately, resulting in the loss of the relativity between the patches. Hence, we remove the normalization layer preceded by the multi-layer perceptron (MLP) and propose RescaleNorm to replace LayerNorm. RescaleNorm performs normalization on the entire feature map and reintroduces the mean and variance of the feature map lost after normalization. Besides, SiLU / Swish <ref type="bibr" target="#b42">[43]</ref> and GELU work well in high-level vision tasks, but ReLU <ref type="bibr" target="#b43">[44]</ref> works better than them in image dehazing. We believe this is because the nonlinearities they introduce are not easily inverted when decoding. We argue that image dehazing requires not only that the network encodes highly expressive features but also that these features are easily recovered to image domain signals.</p><p>Swin Transformer uses window partitioning with cyclic shift to efficiently aggregate local features. But we find that the cyclic shift is suboptimal for image edge regions in image dehazing. Specifically, the cyclic shift should use masked multihead self-attention (MHSA) to prevent unreasonable spatial aggregation, making the windows in the edge regions smaller. We consider that aggregating information within a small window brings instability, which can bias the network's training. Thus we propose a shifted window partitioning scheme based on reflection padding and cropping, which allows MHSA to discard the mask and achieve a constant window size. We also find that the aggregation weights of MHSA are always positive, which makes it behave like a low-pass filter <ref type="bibr" target="#b28">[29]</ref>. Because the aggregation weights of MHSA are dynamic, allpositive, and normalized, we believe that static, learnable, and unconstrained aggregation weights are helpful to complement the MHSA, while the convolution meets this criterion.</p><p>Furthermore, we propose a prior-based soft reconstruction module that outperforms global residual learning and a multiscale feature map fusion module based on SKNet <ref type="bibr" target="#b44">[45]</ref> to replace concatenation fusion. Finally, we build multiple U-Netlike image dehazing Transformers using the proposed modules. Our experiments show that DehazeFormer can substantially outperform contemporaneous methods with lower overhead. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the comparison of DehazeFormer with other image dehazing methods on the SOTS indoor set. Our small model defeats the FFA-Net <ref type="bibr" target="#b17">[18]</ref> with only 25% #Param and 5% computational cost. Our base model is lower in overhead but better in performance than the previous state-of-the-art method, AECR-Net <ref type="bibr" target="#b18">[19]</ref>. To the best of our knowledge, our large model is the first method over 40 dB, substantially outperforming contemporaneous methods.</p><p>There are non-homogeneous image dehazing datasets collected using professional haze machines <ref type="bibr" target="#b45">[46]</ref>, but they are too small and far from the non-homogeneous haze that would be present in natural scenes. Instead, we tend to collect remote sensing image dehazing datasets since highly nonhomogeneous haze is prevalent in remote sensing images. We take into account the wavelength, etc., on the spatial distribution of the haze and then synthesize a large-scale realistic remote sensing image dehazing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Image Dehazing</head><p>Early single-image dehazing methods were generally based on the handcraft priors, such as dark channel prior (DCP) <ref type="bibr" target="#b3">[4]</ref>, color attenuation prior (CAP) <ref type="bibr" target="#b5">[6]</ref>, color-lines <ref type="bibr" target="#b4">[5]</ref>, and hazelines <ref type="bibr" target="#b6">[7]</ref>. These prior-based methods usually yield images with good visibility. However, because these priors are based on empirical statistics, these dehazing methods tend to output unrealistic results when the scenes do not satisfy these priors. With the rapid development of deep learning, learningbased dehazing methods have dominated in recent years. DehazeNet <ref type="bibr" target="#b7">[8]</ref> and MSCNN <ref type="bibr" target="#b8">[9]</ref> are the pioneers in applying CNNs for image dehazing. They learn to estimate t and obtain the result together with A estimated by the conventional method. After that, DCPDN <ref type="bibr" target="#b9">[10]</ref> uses two sub-networks to estimate t and A respectively, while GFN <ref type="bibr" target="#b11">[12]</ref> estimates the fusion coefficient maps for the three predefined image operations. AOD-Net <ref type="bibr" target="#b10">[11]</ref>, on the other hand, rewrites Eq.(1) so that the network needs to estimate only one component. GridDehazeNet <ref type="bibr" target="#b12">[13]</ref> proposes that learning to restore the image is better than estimating t, because the latter will fall into suboptimal solutions. And most recent works <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> tend to estimate the haze-free image or the residual between the haze-free image and the hazy image.</p><p>Since the dehazing performance of the learning-based methods dramatically depends on the quality and size of the dataset, several datasets have been proposed. These dehazing datasets are divided into two main categories: real datasets <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref> and synthetic datasets <ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. Real datasets use real haze produced by professional haze machines to generate real hazy images. Synthetic datasets generally use Eq.(1) to synthesize the corresponding hazy images with haze-free images and depth maps. Although the real datasets seem to be more attractive, it is difficult to obtain enough image pairs, and the distribution of the haze produced by the haze machine still differs significantly from the real haze. Hence, most methods tend to use synthetic datasets for training and testing. In contrast to these datasets, this paper presents a new synthetic remote sensing image dehazing dataset named RS-Haze for evaluating the method's capability to remove highly nonhomogeneous haze. RS-Haze is larger and more realistic than previous datasets <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref>, taking into account sensor characteristics, haze distribution and particle size, wavelengths of light, and other factors that are overlooked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision Transformers</head><p>CNN has dominated most computer vision tasks for years, while recently, the Vision Transformer (ViT) <ref type="bibr" target="#b56">[57]</ref> architectures show the capability of replacing CNNs. ViT pioneered the direct application of the Transformer architecture <ref type="bibr" target="#b21">[22]</ref>, which projects images into token sequences via patch-wise linear embedding. The shortcomings of the original ViT are its weak inductive bias and the quadratic computational cost. To this end, PVT <ref type="bibr" target="#b22">[23]</ref> uses the pyramid architecture to introduce multi-scale inductive bias and downsamples the key and value to reduce the computational cost. T2T-ViT <ref type="bibr" target="#b23">[24]</ref> uses the unfolding operation just like CNNs for tokenization, and it  uses the Performer <ref type="bibr" target="#b24">[25]</ref> to lower the computational cost. Besides, some works <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> employ convolution in the early stages to introduce the inductive bias. Swin Transformer <ref type="bibr" target="#b29">[30]</ref> partitions tokens into windows and performs self-attention within a window to keep the linear computational cost. It employs the cyclic shift scheme to bridge windows so that adjacent blocks adopt different window partitions. Since then, many follow-ups to Swin Transformer have been proposed. For example, some methods bridge windows by reshaping the tensor <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>; while some methods bridge windows by using tokens with global receptive fields as proxies <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>; and others use modified window partitioning schemes <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. Our DehazeFormer can be considered as a combination of Swin Transformer and U-Net <ref type="bibr" target="#b57">[58]</ref>, but with several critical modifications for image dehazing.</p><p>There are also some variants of Swin Transformer for lowlevel vision tasks. SwinIR <ref type="bibr" target="#b58">[59]</ref> is one of the pioneers to employ Swin Transformer in low-level vision tasks, which builds a large residual block consisting of stacked Swin Transformer layers and a subsequent convolutional layer. Uformer <ref type="bibr" target="#b59">[60]</ref> uses Swin Transformer blocks to build a U-Net-like network and inserted depth-wise convolution (DWConv) <ref type="bibr" target="#b60">[61]</ref> in the feed-forward network (FFN) like LocalViT <ref type="bibr" target="#b61">[62]</ref>. However, we found that they perform very poorly in image dehazing. We attribute this to the fact that they inherit the normalization layer, window partitioning scheme, and activation function from the original Swin Transformer. There are a few ViTbased dehazing networks proposed, such as HyLoG-ViT <ref type="bibr" target="#b62">[63]</ref> and TransWeather <ref type="bibr" target="#b63">[64]</ref>. However, HyLoG-ViT does not show convincing performance, while TransWeather aims to use a DETR-like framework <ref type="bibr" target="#b64">[65]</ref> to handle multiple weather conditions simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEHAZEFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall</head><p>DehazeFormer's network architecture is based on the popular Swin Transformer <ref type="bibr" target="#b29">[30]</ref>, but incorporates several improvements to compensate for the deficiencies of the original Swin Transformer when dealing with image dehazing. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the overall architecture of the DehazeFormer. Given the image pair {I(x), J(x)}, we only compute the L 1 loss to train the DehazeFormers.</p><p>First, we briefly review the Swin Transformer. Given an input feature map X ? R b?h?w?c , we project X to Q, K, V (query, key, value) using linear layers and group tokens using window partitioning. Swin Transformer applies MHSA within the window, and the window partitioning of adjacent blocks is different. For simplicity, the following Q, K, V ? R b?l?d correspond to a single window &amp; header, where l is the tokens number in a window and d is the dimension. Thus the self attention is computed by</p><formula xml:id="formula_2">Attention (Q, K, V ) = Softmax QK T / ? d + B V,<label>(3)</label></formula><p>where B is the relative position bias term. A linear layer follows it to project the output of the attention. Our proposed DehazeFormer block differs from the original Swin Transformer block in the normalization layer, the nonlinear activation function, and the spatial information aggregation scheme, detailed in the subsequent subsections. Besides the DehazeFormer block, the SK fusion and soft reconstruction layers are proposed to replace the concatenation fusion layer and global residual learning.</p><p>The SK fusion layer is inspired by SKNet <ref type="bibr" target="#b44">[45]</ref>, it is designed to fuse multiple branches using channel attention. Let the two feature maps be x 1 and x 2 , we first use a linear layer f (?) to project x 1 tox 1 . We use the global average pooling GAP(?), MLP (Linear-ReLU-Linear) F M LP (?), softmax function and split operation to obtain the fusion weights:</p><formula xml:id="formula_3">{a 1 , a 2 } = Split(Softmax(F M LP (GAP (x 1 + x 2 ))). (4)</formula><p>We use the weights {a 1 , a 2 } to fusex 1 , x 2 with an additional short residual via y = a 1x1 + a 2 x 2 + x 2 .</p><p>Current image dehazing networks generally predict the reconstructed image?(x) or global residual R(x) =?(x) ? I(x). We consider it beneficial to introduce priors, provided that there are no strong constraints since the degradation model is an approximation. We rewrite Eq.(1) as</p><formula xml:id="formula_4">J(x) = K(x)I(x) + B(x) + I(x),<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">K(x) = 1/t(x) ? 1 and B(x) = ? (1/t(x) ? 1) A.</formula><p>We drive the network to predict O ? R h?w?4 , and split O into K ? R h?w?1 and B ? R h?w?3 . As a result, the network architecture softly constrains the relationship between K(x) and B(x). This weak prior allows the network to degenerate to predict global residuals (i.e., K(x) = 0, B(x) = R(x)).</p><p>For convenience, we refer to Eq.(5) as soft reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rescale Layer Normalization</head><p>The normalization layer plays a vital role in neural network architecture since it stabilizes the network's training. However, we find that LayerNorm <ref type="bibr" target="#b40">[41]</ref>, which Transformers commonly use, may be unsuitable for image dehazing.</p><p>We first review the formulation of LayerNorm used by Transformers. Assume that the shape of the feature map x ? R b?n?c , where n = h ? w (i.e., height and width), the normalization process can be expressed as:</p><formula xml:id="formula_6">x i = x i ? ? i ? i ? ? i + ? i .<label>(6)</label></formula><p>Here ? and ? denote the mean and standard deviation, ? and ? are learned scaling factor and bias, and i = (i b , i n , i c ) denotes the index. In LayerNorm, ? and ? are computed along the c-axis, making ?, ? ? R b?n . We believe that the mean and standard deviation are correlated with brightness and contrast for images, so the relative brightness and contrast between image patches are somehow discarded after LayerNorm. To this ned, we compute ? and ? along the (n, c)-axes, leading to ?, ? ? R b . We note this normalization method is the LayerNorm more commonly used in CNNs, referred to as LayerNorm ? in this paper. We conduct a simple experiment to show the negative effects of LayerNorm as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Specifically, we build autoencoders using only patch embedding, normalization, and patch reconstruction layers. We train these autoencoders to reconstruct a single input image. Without global residuals, learning identity mappings is not a trivial task <ref type="bibr" target="#b65">[66]</ref>. When LayerNorm is inserted, we can clearly see the block artifacts appearing in the reconstructed image. Because this autoencoder does not involve interactions between patches, it can only memorize the statistics of the sky region at the expense of the rich-texture region. By changing LayerNorm to LayerNorm ? , we largely overcome its negative effects.  However, LayerNorm ? still discards the mean and standard deviation of the feature map. So we propose the Rescale Layer Normalization (RescaleNorm), which is built based on LayerNorm ? , but the mean and standard deviation computed are saved and introduced at the end of the residual block. Specifically, we first fetch the ?, ? ? R b?1?1 , and normalize the input feature map x tox via Eq. <ref type="bibr" target="#b5">(6)</ref>. We then use the main block F(?) to processx to obtain the output?. We use two linear layers with weights</p><formula xml:id="formula_7">W ? , W ? ? R 1?c and biases B ? , B ? ? R 1?1?c to transform ? and ? via {?, ?} = {?W ? +B ? , ?W ? +B ? }, where ?, ? ? R b?1?c .</formula><p>To accelerate convergence, we initialize B ? and B ? to 1 and 0. We inject ? and ? into? to reintroduce the mean and standard deviation. Therefore, RescaleNorm can be formulated as:</p><formula xml:id="formula_8">y = F x ? ? ? ? ? + ? ? (?W ? + B ? ) + (?W ? + B ? ). (7)</formula><p>Compared to BatchNorm <ref type="bibr" target="#b66">[67]</ref>, LayerNorm is not a cheap operation. It needs to compute the mean and standard deviation during inference instead of using the running estimates tracked on the training set. Therefore, we remove the normalization layer before the MLP, as we find that this hardly worsens the method's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Nonlinear Activation Function with Simple Inversal</head><p>GELU performs better than ReLU in high-level tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. However, GELU is much less used than ReLU <ref type="bibr" target="#b43">[44]</ref> and LeakyReLU <ref type="bibr" target="#b69">[70]</ref> in low-level vision tasks. Although some recent Transformer-based image processing networks inherit GELU <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, in our experiments, ReLU and LeakyReLU still perform better than GELU in image dehazing. We believe that GELU does not work in the image dehazing task because it is not easily inverted. If we consider GELU as an image filter, it causes the gradient reversal problem because of its non-monotonicity. Unlike high-level vision tasks, the feature maps in image dehazing would be decoded into images, resulting in the reversal gradients introduced by GELU to react in the output image.</p><p>Comparing GELU and ReLU, another reason for GELU's inferior performance is its stronger nonlinearity since it is more complicated than piece-wise linear functions. Hence we propose SoftReLU, which is a simple smooth approximation to the ReLU as an excess between GELU and ReLU:</p><formula xml:id="formula_9">SoftReLU(x) = x + ? x 2 + ? 2 ? ? 2 .<label>(8)</label></formula><p>where ? is a shape parameter. In particular, SoftReLU is equivalent to ReLU when we set ? = 0. To mimic GELU, we set ? = 0.1 in our experiments. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates a comparison of the SoftReLU with other nonlinear activation functions. We perform ablation studies on activation functions and find that LeakyReLU performs similarly to the ReLU, better than SoftReLU and GELU, while SoftReLU is better than GELU. Therefore, we believe that the nonlinear activation function's invertibility is essential for image dehazing networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Shifted Window Partitioning with Reflection Padding</head><p>Swin Transformer uses cyclic shift with masked MHSA to implement efficient batch computation for shifted window partitioning. Because of the mask, the window size at the edge of the image is smaller than the set window size. For high-level vision tasks, the object of the image is often in the center of the image, making the edge pixels of the image contribute little to the result. For image dehazing, image edges are as important as image centers. A small window size leads to a smaller number of tokens in the window, which biases the network's training. We consider that the network's performance can be improved by keeping the window size of the image edges the same as the set window size.</p><p>To avoid introducing unreasonable inter-patch interactions, we propose to use reflection padding to achieve efficient batch computation for shifted window partitioning, as <ref type="figure" target="#fig_3">Fig. 4</ref> illustrated. Swin Transformer's original paper mentions how to use padding to implement batch computation. However, its proposed padding-based scheme is equivalent to the cyclic shift since the masked MHSA will still be employed. Unlike Swin Transformer, we use reflection padding and do not perform masking. The drawback of this method is that it does introduce additional computational costs compared to the cyclic shift. Fortunately, image dehazing networks tend to process much larger images than image patches at training time. When the image size becomes larger, the percentage of edge regions will become smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. W-MHSA with Parallel Convolution</head><p>We consider that multiplying MHSA is a low-pass filtering, a similar conclusion was presented in a very recent work <ref type="bibr" target="#b28">[29]</ref>. Although the spatial information aggregation weight of MHSA is dynamic, the weight is always positive, making it work like smoothing. As a counterpart to MHSA's spatial information aggregation style, we perform additional convolution on V . Thus the spatial information aggregation scheme is</p><formula xml:id="formula_10">Aggregation(Q, K, V ) = Softmax QK T / ? d + B V + Conv(V ),<label>(9)</label></formula><p>whereV ? R b?h?w?c denotes the V before window partitioning, and Conv(?) can be either DWConv or a ConvBlock (Conv-ReLU-Conv). In other words, we still use the attention mechanism to aggregate information within the window, but also use convolution to aggregate information in the neighborhood without considering window partitioning. Furthermore, we discard the MHSA in some blocks, especially in the encoder's shallow stages and the decoder, and the revised block is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The components illustrated with dashed boxes in the DehazeFormer block indicate they are optional. Specifically, some blocks do not contain MHSA and RescaleNorm, and reflection padding and cropping are used only when shifted window partitioning is required. Note that a similar idea was proposed in CSwin Transformer <ref type="bibr" target="#b37">[38]</ref>, but we use the convolution to extract high frequency information instead of acting as a positional embedding. In contrast to CSwin Transformer, we use reflection padding instead of zero padding because we do not need it to encode position information implicitly <ref type="bibr" target="#b70">[71]</ref>. Most importantly, DehazeFormer's convolutional layer is performed onV before window partitioning, thus it provides the capability to aggregate information between windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head><p>We provide five DehazeFormer's variants (-T, -S, -B, -M, and -L for tiny, small, basic, middle, and large, respectively).   <ref type="bibr" target="#b72">[73]</ref>, we set the initial learning rate to {4, 2, 2, 2, 1} ? 10 ?4 for {-T, -S, -B, -M, -L}. We use AdamW optimizer <ref type="bibr" target="#b73">[74]</ref> with the cosine annealing strategy <ref type="bibr" target="#b74">[75]</ref> to train the models, where the learning rate gradually decreases from the initial learning rate to {4, 2, 2, 2, 1} ? 10 ?6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RS-HAZE DATASET</head><p>The RESIDE dataset is a large-scale homogeneous image dehazing dataset that advances the image dehazing. However, evaluating the method's capability of non-homogeneous image dehazing still relies on some small, unrealistic datasets <ref type="bibr" target="#b45">[46]</ref>, which use a haze machine to generate the non-homogeneous haze that would hardly exist. In contrast, remote sensing image dehazing is a practical non-homogeneous image dehazing task because the haze in remote sensing images is highly nonhomogeneous. Therefore, we propose a new synthetic remote sensing image dehazing dataset named RS-Haze. Comparing to some remote sensing image dehazing datasets <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>, our proposed dataset is more realistic and larger scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Haze Synthesis Formulation</head><p>For generating remote sensing hazy images, researchers generally set d(x) to d 0 since the remote sensing imaging system has a fixed imaging distance. However, d(x) is not the imaging distance but the thickness of the medium that scatters the light. Further, the haze medium in remote sensing images is non-homogeneous, making d(x) vary spatially but consistent over all channels. Besides, the transmission map t(x) is correlated with wavelength and haze conditions. Inspired by prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we model the scattering coefficient as</p><formula xml:id="formula_11">?(?, ?(x)) = c 0 ? ??(x) ,<label>(10)</label></formula><p>where c 0 is a constant, ? is the channel's center wavelength, and the exponent ?(x) corresponds to the region-wise haze conditions. Combining Eq.(2) and Eq.(10), we can derive</p><formula xml:id="formula_12">t(x) = e ??(?,?(x))d(x) .<label>(11)</label></formula><p>Then the relationship of the transmission map between channel i and channel j can be expressed as</p><formula xml:id="formula_13">ln t i (x)/ ln t j (x) = ? i (? i , ?(x)) /? j (? j , ?(x)) ,<label>(12)</label></formula><p>where t {i,j} (x), ? {i,j} , ? {i,j} are the transmission map, scattering coefficient and center wavelength of channel {i, j}, respectively. If we take channel 1 as the reference channel, and the transmission map t j (x) can be obtained via</p><formula xml:id="formula_14">t j (x) = t 1 (x) ? 1 ? j ?(x) ,<label>(13)</label></formula><p>The final haze imaging model can be formulated as</p><formula xml:id="formula_15">I j (x) = J j (x)t j (x) + A j (1 ? t j (x)) .<label>(14)</label></formula><p>Here we can collect clean images J and set ? j to the center wavelength of the corresponding channel. Thus the problem lies in how to obtain t 1 (x), A j and ?(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synthesis Pipeline</head><p>We first consider how to extract the transmission map t 1 (x) from the real hazy images. The reflectance of the cirrus channel (channel 9) can characterize the spatially nonhomogeneous properties of the natural haze <ref type="bibr" target="#b53">[54]</ref>, so we use it to generate the transmission map t 1 (x) as</p><formula xml:id="formula_16">t 1 (x) = 1 ? ?? 9 (x),<label>(15)</label></formula><p>where ? 9 (x) is the reflectance of the cirrus channel of the real hazy image, and ? is a hyperparameter corresponding to the haze density. We find a large dark level in the cirrus channel, making the pixels over 5000 even in the haze-free image. Thus we apply a linear stretch of 0.1% to the cirrus channel to remove the dark level. If we do not remove the dark level, then the maximum value of t 1 (x) is always smaller than 1, which is equivalent to an additional homogenous haze. After that, we need to estimate the atmospheric light of the scene from the haze-free images. To this end, we regard the mean value of each channel's brightest 0.01% pixels as the atmospheric light <ref type="bibr" target="#b53">[54]</ref>. However, there are still many cases of inaccurate estimation. Since the atmospheric light of each channel is correlated with each other, an additional constraint can be introduced to correct for the incorrectly estimated atmospheric light. Assume that the mean value of the estimated atmospheric light of all remote sensing images in channel i is A i . We set the reference values A r = (A 6 + A 7 )/2 and A r = (A 6 + A 7 )/2, and obtain the corrected atmospheric light of channel i by A i = A r ? A i /A r . <ref type="figure" target="#fig_5">Fig. 6</ref> shows how the correction refines the atmospheric light.  Finally, we need to obtain the ?(x). Because the particle properties of haze can vary depending on the haze density <ref type="bibr" target="#b77">[78]</ref>, the exponent ?(x) should be modeled as a function of haze density. As shown in the Table II, we model the exponent ?(x) related to the haze reflectance ?. We use ?? 9 (x) as the haze reflectance and fit the relationship between ?(x) and ?? 9 (x) with a cubic curve, which can be formulated as ?(x) = a 3 (?? 9 (x)) 3 + a 2 (?? 9 (x)) 2 + a 1 (?? 9 (x)) + a 0 , <ref type="bibr" target="#b15">(16)</ref> where a 0 = 6.537, a 1 = ?27.465, a 2 = 41.224, and a 3 = ?21.547. Note that we clip ?(x) to [0, 4] to avoid outliers. Now we can use Eq. <ref type="bibr" target="#b13">(14)</ref> to synthesize the dataset. However, we found that the network trained with this dataset works well in the dense haze region of the synthetic image, but performs poorly on the dense haze region of the real image. We consider that, when the haze is dense, it is likely to block all the light from the ground <ref type="bibr" target="#b54">[55]</ref>. According to the haze imaging model Eq. <ref type="formula" target="#formula_0">(14)</ref>, even when t j (x) is small and the synthesized haze is dense, there still exists information residuals from haze-free channel J j (x). To this end, we revise Eq. <ref type="formula" target="#formula_0">(14)</ref> as</p><formula xml:id="formula_17">I j (x) = J j (x)t j (x) + A j (1 ? t j (x)) ,<label>(17)</label></formula><p>where t j (x) = 1 ? ?(1 ? t j (x)), and we also clip t j (x) to [0, 1] to avoid outliers. Here t j (x) is consistent with Eq.(13), but we introduce a decay factor ? = 1.25 to attenuate the information of J j (x). When the haze reaches a certain concentration, the synthesized hazy image completely lose the information of the hazy-free image in that region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Details</head><p>We download the multi-spectral (MS) images from the Landsat-8 Level 1 data product on EarthExplorer. We selected 76 remote sensing images containing diverse topography with good weather conditions and performed atmospheric correction using the FLAASH module <ref type="bibr" target="#b78">[79]</ref>. Meanwhile, 108 cloudy remote sensing images are selected to generate transmission maps using their cirrus channels. We crop 512?512 image patches from the original remote sensing image using the GDAL library <ref type="bibr" target="#b79">[80]</ref>. Finally, we obtained 6000 patches of hazefree MS images containing various terrain and 1500 patches of cirrus channels with a distribution similar to natural haze. Each haze-free image generates nine synthetic hazed images containing three different haze densities. The haze density is controlled by setting the range of ?. The values of ? in each range are obtained by sampling from the truncated Gaussian function. The summary of RS-Haze is shown in <ref type="table" target="#tab_2">Table III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Experimental setup</head><p>Our experiments are performed on the RESIDE dataset <ref type="bibr" target="#b51">[52]</ref> and our RS-Haze dataset. The RESIDE dataset is one of the most commonly used datasets for image dehazing, and it contains three versions: RESIDE-V0, RESIDE-Standard, and RESIDE-?. It contains several subsets, of which the most commonly used are: indoor training set (ITS), outdoor training set (OTS), and synthetic objective testing set (SOTS). We found that the existing works use different experimental setups and can be divided into two main categories: training on a combination of the ITS and the OTS and testing on the SOTS; training on the ITS and the OTS separately and testing on the indoor and outdoor scenes of the SOTS separately. For proving the effectiveness of DehazeFormer, we perform experiments on both setups, which we name RESIDE-Full and RESIDE-6K, respectively. We do not train large models under each experimental setup since the small models are good enough.</p><p>1) RESIDE-Full: Models are trained and tested on the indoor and outdoor scenes separately. Following FFA-Net <ref type="bibr" target="#b17">[18]</ref>, we use the full ITS (13,990 image pairs from RESIDE-Standard) and OTS (313,950 image pairs from RESIDE-V0) to train indoor models and outdoor models and test them on indoor scenes (500 image pairs) and outdoor scenes (500 image pairs) of the SOTS, respectively. In this experimental setup, all models are trained using their original training strategies, and we replicate the best results reported in the previous works. We train DehazeFormers on ITS for 300 epochs and on OTS for 30 epochs. Note that a few images in the outdoor subset are smaller than the configured patch size, so we discard these images during training. Besides, because the upper part of the outdoor image is often sky, we use only horizontal flipping for data augmentation. 2) RESIDE-6K: Models are trained and tested on the mixed dataset. We use an experimental setup from DA <ref type="bibr" target="#b17">[18]</ref>, which differs significantly from the RESIDE-Full. Its training set contains 3,000 ITS image pairs and 3,000 OTS image pairs, and all images are resized to 400 ? 400. Its testing set mixes indoor and outdoor image pairs to form a test set of 1,000 image pairs without resizing, here called SOTS-mix. In this experimental setup, we retrain all models using L 1 loss on the RESIDE-6K training set for 1,000 epochs, and the learning rate is adjusted according to the model's mini-batch size. For some methods that estimate t(x), we adapt them to predict the output image. Thus we can compare the architectures' performance, regardless of the impact of the training strategy.</p><p>3) RS-Haze: Models are trained on the RS-Haze-mix. For the default experimental setup, we use 8-bit gamma-corrected RGB images for training and testing. We train all models using L 1 loss for 150 epochs, and other settings are the same as RESIDE-6K. For MS image dehazing, we use 16-bit linear images for training and testing. It aims to analyze the properties of MS and RGB images for image dehazing. Note that we compute PSNR and SSIM on the gamma-corrected RGB images when testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Overhead:</head><p>We use the number of parameters (#Param) and multiply-accumulate operations (MACs) to measure the overhead. MACs are measured on 256 ? 256 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Comparison</head><p>We quantitatively compare the performance of DehazeFormers and baselines, and the results are shown in TABLE IV. Here we underline the best results in baselines and bold the results where DehazeFormers exceed them. Overall, our proposed DehazeFormers outperformed these baselines. We argue that the RESIDE-Full indoor set mainly measures the model's capability to handle high-frequency information, and the outdoor set mainly measures the convergence speed of the model. RESIDE-6K measures the stability of the model and the capability to extract low-frequency information. RS-Haze measures the network's capability to extract semantic features. Notably, DehazeFormer-B sometimes outperforms DehazeFormer-M, indicating that the attention mechanism is more critical than convolution in these experimental setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) RESIDE-Full:</head><p>Training on ITS and testing on SOTS indoor set should be the most widely used experimental setup. Comparing the baseline methods, FFA-Net and AECR-Net are far superior to the previous or contemporaneous methods. The former mainly relies on the large network, and the latter may also benefit from the proposed contrastive loss function. However, our proposed DehazeFormer-B surpasses all baseline methods in terms of PSNR and SSIM. Further, the PSNR of DehazeFormer-L exceeds 40 dB. To the best of our knowledge, this is the first method with the PSNR exceeding 40 dB on the SOTS indoor set, dramatically surpassing previous work. Finally, all variants of DehazeFormer work well, and we believe it is a scalable method. Unfortunately, some baselines do not report results on SOTS outdoor set. Because the training set of outdoor scenes consists of more than 300,000 sample pairs, DehazeFormers and baselines may not have converged. We believe that there is still much scope to improve the performance on SOTS outdoor set, and the current results reflect more the network's convergence speed. In particular, DehazeFormer-M is inferior to DehazeFormer-S on the outdoor set, probably because more nonlinear activation functions slow down the training. We remind that the results of baselines on RESIDE-Full are replicated from previous works, and some of them can achieve higher performance using our codebase.</p><p>2) RESIDE-6K: We found that the performance of all CNN-based networks under the RESIDE-6K experimental setup is worse than that of DehazeFormers, which we sup- pose is due to the different image resolutions of the testing and training sets. Because the images of the training set are resized, its high-frequency information distribution is not consistent with the images of the testing set. As we argued, the convolutional layer is good at filtering high-frequency information, while the attention mechanism is good at filtering low-frequency information, making DehazeFormers perform better. We believe this property of the attention mechanism is important for image dehazing because it is not practical to collect dehazing datasets for each resolution setting.</p><p>3) RS-Haze: Compared with other experimental setups, the methods have higher PSNR on RS-Haze but lower SSIM. The scenes of remote sensing images are more monotonous than natural scenes. Thus, it is easier for methods to estimate images' latent color and brightness, making the PSNR higher. In contrast, the haze of RS-Haze is highly non-homogeneous, making the high-frequency information of the image corrupted and the SSIM accordingly lower. We compare the image dehazing methods on RS-Haze. It can be seen that FFA-Net is the best method in baselines, while our small model surpasses it. It is not only due to the excellent design of De-hazeFormer itself but also because the remote sensing images have more similar regions, which are more favorable for selfattention <ref type="bibr" target="#b80">[81]</ref>. Furthermore, the comparison of DehazeFormer-S on RGB and MS images is shown in TABLE V. As expected, dense haze is more difficult to be removed than light haze. Besides, the additional information provided by more channels and larger bit depths does improve the performance of the method substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Comparison</head><p>We also select some samples of the results to analyze the performance of each method qualitatively. Since we do not retrain the baselines on RESIDE-Full, we only show the test results on RESIDE-6K and RS-Haze. <ref type="figure" target="#fig_6">Fig. 7</ref> and <ref type="figure" target="#fig_7">Fig. 8</ref> illustrate qualitative comparisons of our DehazeFormer-S with some representative learning-based dehazing methods.</p><p>1) RESIDE-6K: We select four samples taken from different scenes in the SOTS mix set to evaluate the network's dehazing performance, including synthetic indoor and outdoor haze. AOD-Net and GCANet produce severe color distortions, which make their indoor and outdoor results too dim or too bright. Though color is restored in most areas of images dehazed by PFDN and FFA-Net, color distortion remains on distant objects and small objects near the edge of images. By comparison, the color is recovered correctly through the haze by our DehazeFormer-S, and the results look natural and realistic. When it comes to the region where haze density varies significantly in some indoor scenes, as shown in the enlarged white boxes in the second row in <ref type="figure" target="#fig_6">Fig. 7</ref>, we can observe that almost all the comparative methods fail to remove the haze effectively. However, our DehazeFormer-S restores clear images well, keeps texture and color information, and contains the least haze residual.</p><p>2) RS-Haze: Three images taken from different scenes with different haze densities in the RS-Haze are selected to evaluate the network's dehazing performance on non-homogeneous haze. AOD-Net can barely handle non-homogeneous haze and produces severe artifacts. GCANet, PFDN, and FFA-Net can remove haze effectively when the haze is thin, as shown in the first two rows in <ref type="figure" target="#fig_7">Fig. 8</ref>, but they are not as good as DehazeFormer-S in color and detail reproduction. Moreover, DehazeFormer-S can remove dense haze, while all other networks produce apparent artifacts. See the water surface area in the third row of <ref type="figure" target="#fig_7">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We perform ablation studies on the RESIDE-Full's indoor scene. However, because not every DehazeFormer block has an MHSA, these blocks degenerate into meaningless linear layers when removing the parallel convolution. So we build DehazeFormer-A for ablation studies only. In particular, we set the attention ratio of DehazeFormer-A to 1 and reduce the depth of the network to keep the computational cost and parameters. TABLE VI lists the difference between DehazeFormer-T and DehazeFormer-A. We can see that DehazeFormer-T has better performance than DehazeFormer-A. In terms of overhead, DehazeFormer-A has fewer parameters but a higher computational cost compared to DehazeFormer-T. Note that we find that the results of ablation studies on different datasets are not always consistent, e.g., RESIDE-6K prefers DehazeFormer with a high attention ratio compared to the RESIDE-Full indoor set. We mark the results in red if there is an improvement compared to the baseline (DehazeFormer-A) and in blue if there is a degradation.</p><p>1) Normalization layer: We study normalization layers and their placements on the performance, and the results are shown in TABLE VII. We can see that avoiding the loss of inter-patch relativity and reintroducing the lost statistics does improve the networks' performance. Besides, the normalization layer is more critical for MHSA than MLP. Considering that the normalization layer before MLP has no significant impact on the performance, removing it makes sense since it is not cheap to obtain the standard deviation of the feature maps. However, the negative impact of LayerNorm is not as evident as expected since the normalization layer showed a severe impact on performance in our early ablation studies on RESIDE-6K. Thus we plan to explore the relationship between the dataset and the normalization layer in our future work.</p><p>2) Shifted window partitioning scheme: We study the schemes of shifted window partitioning, and the results are shown in TABLE VIII. Because masked padding and masked cyclic shift are equivalent in terms of spatial information aggregation, we train only a single network. If we replace the reflection padding with zero padding, the network's performance drops significantly. Zero-padding introduces meaningless tokens, and the attention matrix is all-positive, making  useless information mixed in. In contrast, cyclic shift without mask also introduces unreasonable interactions between tokens but has a less negative impact. Finally, our proposed scheme gives a moderate performance improvement to the network. Considering that it only introduces a negligible additional computational cost on 256 ? 256 images, it is worthwhile.</p><formula xml:id="formula_18">GCANet PFDN FFA-Net DehazeFormer-S Input GT AOD-Net</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Nonlinear activation functions:</head><p>We study the difference in the nonlinear activation functions, and the results are shown in TABLE IX. We replace all nonlinear activation functions in the network, including the nonlinear activation functions in MLP and SK fusion layer. Surprisingly, the nonlinear acti-vation functions dramatically affect the network performance, while our early ablation studies on RESIDE-6K did not show such a huge gap. The networks using ReLU and LeakyReLU perform about the same because they are both piecewise linear functions that can be easily inverted. Although the form of SoftReLU is simple, it is not easily inverted, so the networks with it yield significant performance degradation. Furthermore, GELU is non-monotonic, and it is more difficult to be inverted, making the networks with it perform very poorly. We argue that it is essential to consider the invertibility of the nonlinear activation function when building the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Parallel conv:</head><p>We study to prove the importance of parallel convolution with attention: a) remove the parallel convolution; b) place the convolution parallel with the MHSA, i.e., the input to the convolution is X instead of V ; c) place the convolution before the MLP <ref type="bibr" target="#b68">[69]</ref>, and the results are shown in TABLE X. As can be seen, additional convolutional layers in the Transformer block can dramatically improve the network's performance, but their placement is critical. Inserting DWConv into the FFN brings only minor performance, although the scheme has been widely employed in previous work. We consider that the transformer works somehow because it separates intra-token and inter-tokens interactions into two steps, while inserting DWConv in FFN would break this property. DWConv in parallel with attention is better than DWConv in parallel with MHSA. Although both schemes use DWConv to aggregate spatial information, the former is done in the same feature space as attention, while the latter is done in a different feature space. DWConv provides static learnable aggregation weights, while attention provides dynamic allpositive aggregation weights. Thus the convolution parallel to attention does play a complementary role with attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Other components:</head><p>We verify the impact of the soft reconstruction module and SK fusion module on the network's performance. Although SK fusion brings only a minor performance gain, we consider it a good alternative to concatenation fusion, given its lower overhead. Whereas the soft reconstruction brings more improvement than expected, we believe introducing soft constraints on prior is beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper introduces various improvements for Swin Transformer applied to image dehazing, and the proposed Dehaze-Former achieves superior performance on several datasets. To summarize, we propose to use RescaleNorm and ReLU to replace the commonly used LayerNorm and GELU to avoid some negative effects that are not important for high-level vision tasks but critical for low-level vision tasks. To improve the capability of MHSA, we propose a shifted window partitioning scheme based on reflection padding and a spatial information aggregation scheme using convolution in parallel with attention. We also propose some minor improvements that are applicable to other networks. Finally, we collect a largescale remote sensing image dehazing dataset to evaluate the network's capability to remove highly non-homogeneous haze, and DehazeFormer also achieves an impressive performance. In the future, we plan to work on more lightweight and more straightforward architectures and extend the architecture to other low-level vision tasks. Besides, encoding feature maps on thumbnails and then decoding them on the original image may achieve real-time 4K image dehazing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(Yuda Song and Zhuqing He contributed equally to this work.) (Corresponding authors: Xin Du.) Yuda Song, Zhuqing He, Hui Qian, and Xin Du are with Zhejiang University, Hangzhou 310027, China (e-mail: duxin@zju.edu.cn) Comparison of DehazeFormer with other image dehazing methods on the SOTS indoor set. The size of the dots indicates the #Param of the method, and MACs are shown with logarithmic axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>DehazeFormer is a modified 5-stage U-Net, whose convolutional blocks are replaced by our DehazeFormer blocks. The components illustrated with dashed boxes in the DehazeFormer block indicate they are optional. The SK fusion and soft reconstruction layers are proposed to replace the original concatenation fusion and global residual. The input size is H ? W , and the size of feature maps in each stage is shown below the DehazeFormer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Simple autoencoders for analyzing the normalization methods. From left to right, there are the autoencoders' architectures, output images, and error maps, where the error is scaled by 8? for better viewing. The embedding layer and reconstruction layer are linear layers with patch-wise tensor reshaping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The ReLU, GELU, LeakyReLU (? = 0.1), and SoftReLU (? = 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of our proposed reflection padding scheme with cyclic shift scheme for shifted window partitioning. The actual percentage of the edge area is much smaller than the illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Correction of the estimated atmospheric light. The top is atmospheric lights before and after correction, and the bottom is three synthesis samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative comparison of image dehazing methods on SOTS mix set, where the first two rows are indoor images, and the last two rows are the outdoor images. The first column is the hazy images and the last column is the corresponding ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative comparison of image dehazing methods on RS-Haze. The first column is the hazy images and the last column is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I DETAILED</head><label>I</label><figDesc>ARCHITECTURE SPECIFICATIONS.</figDesc><table><row><cell></cell><cell>Num. of Blocks</cell><cell>Embedding Dims</cell><cell>MLP Ratio</cell><cell>Attention Ratio</cell><cell>Num. of Heads</cell><cell>Conv Type</cell></row><row><cell>DehazeFormer-T</cell><cell cols="4">[ 4, 4, 4, 2, 2] [24, 48, 96, 48, 24] [2, 4, 4, 2, 2] [1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>DWConv</cell></row><row><cell>DehazeFormer-S</cell><cell cols="4">[ 8, 8, 8, 4, 4] [24, 48, 96, 48, 24] [2, 4, 4, 2, 2] [1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>DWConv</cell></row><row><cell>DehazeFormer-B</cell><cell cols="4">[16, 16, 16, 8, 8] [24, 48, 96, 48, 24] [2, 4, 4, 2, 2] [1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>DWConv</cell></row><row><cell cols="5">DehazeFormer-M [12, 12, 12, 6, 6] [24, 48, 96, 48, 24] [2, 4, 4, 2, 2] [1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>ConvBlock</cell></row><row><cell>DehazeFormer-L</cell><cell cols="4">[16, 16, 16, 12, 12] [48, 96, 192, 96, 48] [2, 4, 4, 2, 2] [1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>ConvBlock</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>lists the detailed configurations of these variants. The attention ratio here indicates the percentage of blocks containing MHSA, and we place the blocks containing MHSA at the end of each stage. For the three small models (-T, -S, -B), we use DWConv (K = 5) as the parallel convolutions.</figDesc><table /><note>Because DWConv is an operation with low computational cost but high memory access cost [72], we use ConvBlock (K = 3) for two large models (-M, -L). When training, images are randomly cropped to 256 ? 256 patches. We set different mini-batch sizes for training different variants, i.e., {32, 16, 16, 16, 8} for {-T, -S, -B, -M, -L}. Re- ferring to the linear scaling rule</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">ATMOSPHERIC RELATIVE SCATTERING MODELS</cell></row><row><cell>Reflectance ?</cell><cell>Condition</cell><cell>Exponent ?</cell></row><row><cell cols="2">0.000 &lt; ? ? 0.215 very clear</cell><cell>4.0</cell></row><row><cell>0.215 &lt; ? ? 0.294</cell><cell>clear</cell><cell>2.0</cell></row><row><cell>0.294 &lt; ? ? 0.373</cell><cell>moderate</cell><cell>1.0</cell></row><row><cell>0.373 &lt; ? ? 0.451</cell><cell>hazy</cell><cell>0.7</cell></row><row><cell>0.451 &lt; ? ? 0.529</cell><cell>very hazy</cell><cell>0.5</cell></row><row><cell>0.529 &lt; ? &lt; 1.000</cell><cell>cloudy</cell><cell>0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III SUMMARY</head><label>III</label><figDesc>OF RS-HAZE DATASET</figDesc><table><row><cell>Name</cell><cell>Density</cell><cell>?</cell><cell>Train</cell><cell>Test</cell></row><row><cell>RS-Haze-L</cell><cell>Light</cell><cell cols="2">0.100-0.399 17100</cell><cell>900</cell></row><row><cell>RS-Haze-M</cell><cell cols="3">Moderate 0.400-0.699 17100</cell><cell>900</cell></row><row><cell>RS-Haze-D</cell><cell>Dense</cell><cell cols="2">0.700-0.999 17100</cell><cell>900</cell></row><row><cell>RS-Haze-mix</cell><cell>All</cell><cell cols="3">0.100-0.999 51300 2700</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>COMPARISON OF VARIOUS DEHAZING METHODS TRAINED ON THE RESIDE DATASETS.</figDesc><table><row><cell>Methods</cell><cell cols="2">ITS SOTS-indoor</cell><cell cols="2">OTS SOTS-outdoor</cell><cell cols="2">RESIDE-6K SOTS-mix</cell><cell cols="2">RS-Haze RS-Haze-mix</cell><cell cols="2">Overhead</cell></row><row><cell></cell><cell cols="2">PSNR SSIM</cell><cell cols="2">PSNR SSIM</cell><cell cols="2">PSNR SSIM</cell><cell cols="2">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell></row><row><cell>(TPAMI'10) DCP [4]</cell><cell>16.62</cell><cell>0.818</cell><cell>19.13</cell><cell>0.815</cell><cell>17.88</cell><cell>0.816</cell><cell>17.86</cell><cell>0.734</cell><cell>-</cell><cell>-</cell></row><row><cell>(TIP'16) DehazeNet [8]</cell><cell>19.82</cell><cell>0.821</cell><cell>24.75</cell><cell>0.927</cell><cell>21.02</cell><cell>0.870</cell><cell>23.16</cell><cell>0.816</cell><cell cols="2">0.009M 0.581G</cell></row><row><cell>(ECCV'16) MSCNN [9]</cell><cell>19.84</cell><cell>0.833</cell><cell>22.06</cell><cell>0.908</cell><cell>20.31</cell><cell>0.863</cell><cell>22.80</cell><cell>0.823</cell><cell cols="2">0.008M 0.525G</cell></row><row><cell>(ICCV'17) AOD-Net [11]</cell><cell>20.51</cell><cell>0.816</cell><cell>24.14</cell><cell>0.920</cell><cell>20.27</cell><cell>0.855</cell><cell>24.90</cell><cell>0.830</cell><cell>0.002M</cell><cell>0.115G</cell></row><row><cell>(CVPR'18) GFN [12]</cell><cell>22.30</cell><cell>0.880</cell><cell>21.55</cell><cell>0.844</cell><cell>23.52</cell><cell>0.905</cell><cell>29.24</cell><cell>0.910</cell><cell cols="2">0.499M 14.94G</cell></row><row><cell>(WACV'19) GCANet [14]</cell><cell>30.23</cell><cell>0.980</cell><cell>-</cell><cell>-</cell><cell>25.09</cell><cell>0.923</cell><cell>34.41</cell><cell>0.949</cell><cell cols="2">0.702M 18.41G</cell></row><row><cell>(ICCV'19) GridDehazeNet [13]</cell><cell>32.16</cell><cell>0.984</cell><cell>30.86</cell><cell>0.982</cell><cell>25.86</cell><cell>0.944</cell><cell>36.40</cell><cell>0.960</cell><cell cols="2">0.956M 21.49G</cell></row><row><cell>(CVPR'20) MSBDN [17]</cell><cell>33.67</cell><cell>0.985</cell><cell>33.48</cell><cell>0.982</cell><cell>28.56</cell><cell>0.966</cell><cell>38.57</cell><cell>0.965</cell><cell cols="2">31.35M 41.54G</cell></row><row><cell>(ECCV'20) PFDN [15]</cell><cell>32.68</cell><cell>0.976</cell><cell>-</cell><cell>-</cell><cell>28.15</cell><cell>0.962</cell><cell>36.04</cell><cell>0.955</cell><cell cols="2">11.27M 50.46G</cell></row><row><cell>(AAAI'20) FFA-Net [18]</cell><cell>36.39</cell><cell>0.989</cell><cell>33.57</cell><cell>0.984</cell><cell>29.96</cell><cell>0.973</cell><cell>39.39</cell><cell>0.969</cell><cell cols="2">4.456M 287.8G</cell></row><row><cell>(CVPR'21) AECR-Net [19]</cell><cell>37.17</cell><cell>0.990</cell><cell>-</cell><cell>-</cell><cell>28.52</cell><cell>0.964</cell><cell>35.69</cell><cell>0.959</cell><cell cols="2">2.611M 52.20G</cell></row><row><cell>(ours) DehazeFormer-T</cell><cell>35.15</cell><cell>0.989</cell><cell>33.71</cell><cell>0.982</cell><cell>30.36</cell><cell>0.973</cell><cell>39.11</cell><cell>0.968</cell><cell>0.686M</cell><cell>6.658G</cell></row><row><cell>(ours) DehazeFormer-S</cell><cell>36.82</cell><cell>0.992</cell><cell>34.36</cell><cell>0.983</cell><cell>30.62</cell><cell>0.976</cell><cell>39.57</cell><cell>0.970</cell><cell>1.283M</cell><cell>13.13G</cell></row><row><cell>(ours) DehazeFormer-B</cell><cell>37.84</cell><cell>0.994</cell><cell>34.95</cell><cell>0.984</cell><cell>31.45</cell><cell>0.980</cell><cell>39.87</cell><cell>0.971</cell><cell>2.514M</cell><cell>25.79G</cell></row><row><cell>(ours) DehazeFormer-M</cell><cell>38.46</cell><cell>0.994</cell><cell>34.29</cell><cell>0.983</cell><cell>30.89</cell><cell>0.977</cell><cell>39.71</cell><cell>0.971</cell><cell>4.634M</cell><cell>48.64G</cell></row><row><cell>(ours) DehazeFormer-L</cell><cell>40.05</cell><cell>0.996</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.44M</cell><cell>279.7G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V</head><label>V</label><figDesc>PSNR / SSIM OF DEHAZEFORMER-S ON THE RGB / MS SET.</figDesc><table><row><cell>Setting</cell><cell>RS-Haze-L</cell><cell>RS-Haze-M</cell><cell>RS-Haze-D</cell><cell>RS-Haze-mix</cell></row><row><cell>RGB</cell><cell>43.68/0.993</cell><cell>39.58/0.979</cell><cell>35.46/0.938</cell><cell>39.57/0.970</cell></row><row><cell>MS</cell><cell>55.44/0.999</cell><cell>50.75/0.997</cell><cell>43.66/0.984</cell><cell>49.95/0.993</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>BETWEEN DEHAZEFORMER-T AND DEHAZEFORMER-A.</figDesc><table><row><cell cols="4">Num. of Blocks</cell><cell cols="2">MLP Ratio</cell><cell>Attention Ratio</cell><cell>Num. of Heads</cell><cell cols="2">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell></row><row><cell>DehazeFormer-T</cell><cell cols="3">[4, 4, 4, 2, 2]</cell><cell cols="2">[2, 4, 4, 2, 2]</cell><cell>[1/4, 1/2, 3/4, 0, 0]</cell><cell>[2, 4, 6, 1, 1]</cell><cell>35.15</cell><cell>0.989</cell><cell>0.686M</cell><cell>6.658G</cell></row><row><cell>DehazeFormer-A</cell><cell cols="3">[2, 2, 2, 2, 2]</cell><cell cols="2">[2, 4, 4, 4, 2]</cell><cell>[ 1 , 1 , 1 , 1, 1]</cell><cell>[2, 4, 6, 4, 2]</cell><cell>34.85</cell><cell>0.988</cell><cell>0.671M 7.185G</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ABLATION STUDY ON NORMALIZATION LAYERS.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Setting</cell><cell></cell><cell cols="3">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell><cell></cell><cell></cell></row><row><cell>DehazeFormer-A</cell><cell></cell><cell>34.85</cell><cell cols="2">0.988</cell><cell cols="2">0.671M 7.185G</cell><cell></cell><cell></cell></row><row><cell cols="2">RescaleNorm ? LayerNorm  ?</cell><cell>34.73</cell><cell cols="2">0.988</cell><cell cols="2">0.668M 7.175G</cell><cell></cell><cell></cell></row><row><cell>? LayerNorm</cell><cell></cell><cell>34.45</cell><cell cols="2">0.987</cell><cell cols="2">0.668M 7.175G</cell><cell></cell><cell></cell></row><row><cell>? PreNorm for MHSA</cell><cell></cell><cell>34.17</cell><cell cols="2">0.986</cell><cell cols="2">0.668M 7.163G</cell><cell></cell><cell></cell></row><row><cell>+ PreNorm for MLP</cell><cell></cell><cell>34.86</cell><cell cols="2">0.987</cell><cell cols="2">0.674M 7.207G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ABLATION STUDY ON SHIFTED WINDOW PARTITIONING SCHEMES.</cell><cell></cell><cell></cell></row><row><cell>Setting</cell><cell></cell><cell cols="2">PSNR SSIM</cell><cell></cell><cell>#Param</cell><cell>MACs</cell><cell></cell><cell></cell></row><row><cell>DehazeFormer-A</cell><cell></cell><cell>34.85</cell><cell>0.988</cell><cell></cell><cell cols="2">0.671M 7.185G</cell><cell></cell><cell></cell></row><row><cell>? Zero-Padding</cell><cell></cell><cell>34.00</cell><cell>0.986</cell><cell></cell><cell cols="2">0.671M 7.185G</cell><cell></cell><cell></cell></row><row><cell>? Padding w/ Mask</cell><cell></cell><cell>34.54</cell><cell>0.988</cell><cell></cell><cell cols="2">0.671M 7.185G</cell><cell></cell><cell></cell></row><row><cell>? Cyclic Shift w/ Mask</cell><cell></cell><cell>34.54</cell><cell>0.988</cell><cell></cell><cell cols="2">0.671M 7.106G</cell><cell></cell><cell></cell></row><row><cell>? Cyclic Shift w/o Mask</cell><cell></cell><cell>34.34</cell><cell>0.988</cell><cell></cell><cell cols="2">0.671M 7.106G</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX ABLATION</head><label>IX</label><figDesc>STUDY ON NONLINEAR ACTIVATION FUNCTIONS.</figDesc><table><row><cell>Setting</cell><cell cols="2">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell></row><row><cell>DehazeFormer-A</cell><cell>34.85</cell><cell>0.988</cell><cell cols="2">0.671M 7.185G</cell></row><row><cell>ReLU ? GELU</cell><cell>33.02</cell><cell>0.983</cell><cell cols="2">0.671M 7.279G</cell></row><row><cell>? SoftReLU (0.1)</cell><cell>33.73</cell><cell>0.985</cell><cell cols="2">0.671M 7.229G</cell></row><row><cell>? LeakyReLU (0.1)</cell><cell>34.89</cell><cell>0.988</cell><cell cols="2">0.671M 7.229G</cell></row><row><cell></cell><cell>TABLE X</cell><cell></cell><cell></cell></row><row><cell cols="4">ABLATION STUDY ON PARALLEL CONV LAYERS.</cell></row><row><cell>Setting</cell><cell cols="2">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell></row><row><cell>DehazeFormer-A</cell><cell>34.85</cell><cell>0.988</cell><cell cols="2">0.671M 7.185G</cell></row><row><cell>? Parallel DWConv</cell><cell>32.90</cell><cell>0.983</cell><cell cols="2">0.653M 6.910G</cell></row><row><cell>? Parallel DWConv on X</cell><cell>33.99</cell><cell>0.987</cell><cell cols="2">0.671M 7.185G</cell></row><row><cell>? DWConv before MLP</cell><cell>33.49</cell><cell>0.986</cell><cell cols="2">0.671M 7.185G</cell></row><row><cell></cell><cell>TABLE XI</cell><cell></cell><cell></cell></row><row><cell cols="4">ABLATION STUDY ON OTHER COMPONENTS.</cell></row><row><cell>Setting</cell><cell cols="2">PSNR SSIM</cell><cell>#Param</cell><cell>MACs</cell></row><row><cell>DehazeFormer-A</cell><cell>34.85</cell><cell>0.988</cell><cell cols="2">0.671M 7.185G</cell></row><row><cell>Soft Recon. ? Recon.</cell><cell>34.50</cell><cell>0.987</cell><cell cols="2">0.671M 7.171G</cell></row><row><cell>SK Fusion ? Cat Fusion</cell><cell>34.78</cell><cell>0.988</cell><cell cols="2">0.673M 7.256G</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optics of the atmosphere: scattering by molecules and particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="233" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7314" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gated context aggregation network for image dehazing and deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1375" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Physics-based feature dehazing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hardgan: A haze-aware representation distillation gan for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qili</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="722" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale boosted dehazing network with dense feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2157" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive learning for compact single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eaa-net: A novel edge assisted attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao-Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Jian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">107279</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. Domain adaptation for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CPVR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">and Illia Polosukhin. Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uniformer: Unified transformer for efficient spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How do vision transformers work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songkuk</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glance-and-gaze vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossformer: A versatile vision transformer hinging on cross-scale attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cat: Cross attention in vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05786</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regionvit: Regional-to-local attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02689</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with crossshaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pale transformer: A general vision transformer backbone with paleshaped attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14000</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Focal attention for long-range interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Nhhaze: An image dehazing benchmark with non-homogeneous hazy and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">I-haze: a dehazing benchmark with real hazy and haze-free indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACIVS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="620" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">O-haze: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dense-haze: A benchmark for image dehazing with dense-haze and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1014" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">D-hazy: A dataset to evaluate quantitatively dehazing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hazerd: an outdoor scene dataset and benchmark for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3205" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Benchmarking singleimage dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dehazing for multispectral remote sensing images based on a convolutional neural network with the residual architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjun</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengying</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J-STARS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1645" to="1655" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rsdehazenet: Dehazing network with channel refinement for multispectral remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanjing</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2535" to="2549" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Single satellite optical imagery dehazing using sar image prior based on conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Domain-aware unsupervised hyperspectral reconstruction for aerial image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murari</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Narang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Hybrid local-global transformer for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07100</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Transweather: Transformer-based restoration of images degraded by adverse weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil Db</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A coarse-to-fine two-stage attentive network for haze removal of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE GRSL</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1751" to="1755" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Cloud removal in remote sensing images using generative adversarial networks and sar-to-optical image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Reza</forename><surname>Faramarz Naderi Darbaghshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soryani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">An improved dark-object subtraction technique for atmospheric scattering correction of multispectral data. Remote sensing of environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chavez</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="459" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Flaash, a modtran4-based atmospheric correction algorithm, its application and validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Hoke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Chetwynd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adler-Golden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1414" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The geospatial data abstraction library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Warmerdam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Open source approaches in spatial data handling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="87" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Pyramid attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
