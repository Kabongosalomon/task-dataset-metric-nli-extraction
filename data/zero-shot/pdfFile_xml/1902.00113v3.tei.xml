<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Episodic Training for Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
							<email>da.li1@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SketchX</orgName>
								<orgName type="institution" key="instit2">CVSSP</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SketchX</orgName>
								<orgName type="institution" key="instit2">CVSSP</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
							<email>congliu2@iflytek.com</email>
							<affiliation key="aff3">
								<orgName type="department">iFlytek Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<email>y.song@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SketchX</orgName>
								<orgName type="institution" key="instit2">CVSSP</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SketchX</orgName>
								<orgName type="institution" key="instit2">CVSSP</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Episodic Training for Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalization (DG) is the challenging and topical problem of learning models that generalize to novel testing domains with different statistics than a set of known training domains. The simple approach of aggregating data from all source domains and training a single deep neural network end-to-end on all the data provides a surprisingly strong baseline that surpasses many prior published methods. In this paper we build on this strong baseline by designing an episodic training procedure that trains a single deep network in a way that exposes it to the domain shift that characterises a novel domain at runtime. Specifically, we decompose a deep network into feature extractor and classifier components, and then train each component by simulating it interacting with a partner who is badly tuned for the current domain. This makes both components more robust, ultimately leading to our networks producing state-of-the-art performance on three DG benchmarks. Furthermore, we consider the pervasive workflow of using an ImageNet trained CNN as a fixed feature extractor for downstream recognition tasks. Using the Visual Decathlon benchmark, we demonstrate that our episodic-DG training improves the performance of such a general purpose feature extractor by explicitly training a feature for robustness to novel problems. This shows that DG training can benefit standard practice in computer vision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning methods often degrade rapidly in performance if they are applied to domains with very different statistics to the data used to train them. This is the problem of domain shift, which domain adaptation (DA) aims to address in the case where some labelled or unlabelled data from the target domain is available for adaptation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref>; and domain generalisation (DG) aims to address in the case where no adaptation to the target problem is possible <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> due to lack of data or computation. DG is a particularly challenging problem setting, since explicit training on the target is disallowed; yet it is particularly valuable due to its lack of assumptions. For example, it would be valuable to have a domain-general visual feature extractor that performs well 'out of the box' as a representation for any novel problem, even without fine-tuning.</p><p>The significance of the DG challenge has led to many studies in the literature. These span robust feature space learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>, model architectures that are purpose designed to enable robustness to domain shift <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17]</ref> and specially designed learning algorithms for optimising standard architectures <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref> that aim to fit them to a more robust minima. Among all these efforts, it turns out that the naive approach <ref type="bibr" target="#b16">[17]</ref> of aggregating all the training domains' data together and training a single deep network end-to-end is very competitive with state-of-the-art, and better than many published methods -while simultaneously being much simpler and faster than more elaborate alternatives. In this paper we aim to build on the strength and simplicity of this simple data aggregation strategy, but improve it by designing an episodic training scheme to improve DG.</p><p>The paradigm of episodic training has recently been popularised in the area of few-shot learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. In this problem, the goal is to use a large amount of background source data, to train a model that is capable of few-shot learning when adapting to a novel target problem. However despite the data availability, training on all the source data would not be reflective of the target few-shot learning condition. So in order to train the model in a way that reflects how it will be tested, multiple few-shot learning training episodes are setup among all the source datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>How can an episodic training approach be designed for domain generalisation? Our insight is that, from the perspective of any layer l in a neural network, being exposed to a novel domain at testing-time is experienced as that layer's neighbours l?1 or l+1 being badly tuned for the problem at hand. That is, neighbours provide input to the current layer (or accept output from it) with different statistics to the current layer's expectation. Therefore to design episodes for DG, we should expose layers to neighbours that are untrained for the current domain. If a layer can be trained to perform well in this situation of badly tuned neighbours, then its robustness to domain-shift has increased.</p><p>To realise our episodic training idea, we break networks up into feature extractor and classifier modules and train them with our episodic framework. This leads to more robust modules that together obtain state-of-the-art results on several DG benchmarks. Our approach benefits from end-to-end learning, while being model agnostic (architecture independent), and simple and fast to train; in contrast to most existing DG techniques that rely on non-standard architectures <ref type="bibr" target="#b16">[17]</ref>, auxiliary models <ref type="bibr" target="#b32">[33]</ref>, or non-standard optimizers <ref type="bibr" target="#b17">[18]</ref>.</p><p>Finally, we provide a practical demonstration of the value of explicit DG training, beyond the isolated benchmarks that are common in the literature. Specifically, we consider whether DG can benefit the common practitioner workflow of using an ImageNet <ref type="bibr" target="#b30">[31]</ref> pre-trained CNN as a feature extractor for novel tasks and datasets. The standard (homogeneous) DG problem setting assumes shared label-spaces between source and target domain, thus highly restricting its applicability. To benefit the wider computer vision workflow, we go beyond this to heterogeneous DG <ref type="table" target="#tab_8">(Table 5</ref>). That is, to train a feature extractor specifically to improve its robustness in representing novel downstream tasks without fine-tuning. Using the Visual Decathlon benchmark <ref type="bibr" target="#b28">[29]</ref>, we show that Episodic training provides an improved representation for novel downstream tasks compared to the standard ImageNet pre-trained CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Domain Learning (MDL) MDL aims to learn several domains simultaneously using a single model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. Depending on the problem, how much data is available per domain, and how similar the domains are, multi-domain learning can improve <ref type="bibr" target="#b39">[40]</ref> -or sometimes worsen <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> -performance compared to a single model per domain. MDL is related to DG because the typical setting for DG is to assume a similar setup in that multiple source domains are provided. But that now the goal is to learn how to extract a domain-agnostic or domain-robust model from all those source domains. The most rigorous benchmark for MDL is the Visual Decathlon (VD) <ref type="bibr" target="#b28">[29]</ref>. We repurpose this benchmark for DG by training a CNN on a subset of the VD domains, and then evaluating its performance as a feature extractor on an unseen disjoint subset of them. We are the first to demonstrate DG at this scale, and in the heterogeneous label setting required for VD.</p><p>Domain Generalization Despite different details, previous DG methods can be divided into a few categories by motivating intuition. Domain Invariant Features: These aim to learn a domain-invariant feature representation, typically by minimising the discrepancy between all source domains -and assuming that the resulting source-domain invariant feature will work well for the target as well. To this end <ref type="bibr" target="#b26">[27]</ref> employed maximum mean discrepancy (MMD), while <ref type="bibr" target="#b11">[12]</ref> proposed a multi-domain reconstruction auto-encoder to learn this domain-invariant feature. More recently, <ref type="bibr" target="#b19">[20]</ref> applied MMD constraints within the representation learning of an autoencoder via adversarial training. Hierarchical Models: These learn a hierarchical set of model parameters, so that the model for each domain is parameterised by a combination of a domain-agnostic and a domain-specific parameter <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. After learning such a hierarchical model structure on the source domains the domain agnostic parameter can be extracted as the model with the least domain-specific bias, that is most likely to work on a target problem. This intuition has been exploited in both shallow <ref type="bibr" target="#b15">[16]</ref> and deep <ref type="bibr" target="#b16">[17]</ref> settings. Data Augmentation: A few studies proposed data augmentation strategies to synthesise additional training data to improve the robustness of a model to novel domains. These include the Bayesian network <ref type="bibr" target="#b32">[33]</ref>, which perturbs input data based on the domain classification signal from an auxiliary domain classifier. Meanwhile, <ref type="bibr" target="#b36">[37]</ref> proposed an adversarial data augmentation method to synthesize 'hard' data for the training model to enhance its generalization. Optimisation Algorithms: A final category of approach is to modify a conventional learning algorithm in an attempt to find a more robust minima during training, for example through meta-learning <ref type="bibr" target="#b17">[18]</ref>. Our approach is different to all of these in that it trains a standard deep model, without special data augmentation and with a conventional optimiser. The key idea requires only a simple modification of the training procedure to introduce appropriately constructed episodes. Finally, in contrast to the small datasets considered previously, we demonstrate the impact of DG model training in the large scale VD benchmark.</p><p>Neural Network Meta-Learning Learning-to-learn and meta-learning methods have resurged recently, in particular in few-shot recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref>, and learning-to-optimize <ref type="bibr" target="#b27">[28]</ref> tasks. Despite signifiant other differences in motivation and methodological formalisations, a common feature of these methods is an episodic training strategy. In few-shot learning, the intuition is that while lot of source tasks and data may be available, these should be used for training in a way that closely simulates the testing condition. Therefore at each learning iteration, a random subset of source tasks and instances are sampled to generate a training episode defined by a random few-shot learning task of similar data volume and cardinality as the model is expected to be tested on at runtime. Thus the model eventually 'sees' all the training data in aggregate, but in any given iteration, it is evaluated in a condition similar to a real 'testing' condition. In this paper we aim to develop an episodic training strategy to improve domain-robustness, rather than learning-to-learn. While the high-level idea of an episodic strategy is the same, the DG problem and associated episode construction details are completely different. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we will first introduce the basic dataset aggregation method (AGG) which provides a strong baseline for DG performance, and then subsequently present three episodic training strategies for training it more robustly. Problem Setting In the DG setting, we assume that we are given n source domains D = [D 1 ,...,D n ], where D i is the i th source domain containing data-label pairs (x j i ,y j i ) 1 . The goal is to use these to learn a model f : x ? y that generalises well to a novel testing domain D * with different statistics to the training domains, without assuming any knowledge of the testing domain during model learning.</p><p>For homogeneous DG, we assume that all the source domains and the target domain share the same label space Y i =Y j =Y * , ?i,j ? <ref type="bibr">[1,n]</ref>. For the more challenging heterogeneous setting, the domains can have different, potentially completely disjoint label spaces Y i = Y j = Y * . We will start by introducing the homogeneous case and discuss the heterogeneous case later. Architecture We break neural network classifiers f :x?y into a sequence modules. In practice, we use two: A feature extractor ?(?) and a classifier ?(?), so that f(x)=?(?(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Vanilla Aggregation Method A simple approach to the DG problem is to simply aggregate all the source domains' data together, and train a single CNN end-to-end ignoring the domain label information entirely <ref type="bibr" target="#b16">[17]</ref>. This approach is simple, fast and competitive with more elaborate state-of-the-art alternatives. In terms of neural network modules, it means that both the classifier ? and the feature extractor ? are shared across all domains 2 , as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, leading to the optimisation:</p><formula xml:id="formula_0">argmin ?,? E Di?D E (xi,yi)?Di (y i ,?(?(x i ))<label>(1)</label></formula><p>where (?) is the cross-entropy loss here. Domain Specific Models Our goal is to improve robustness by exposing individual modules to neighbours that are badly calibrated to a given domain. To obtain these 'badly calibrated' components, we also train domain-specific models. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, this means that each domain i has its own model 1 i indicates domain index and j indicates instance number within domain. For simplicity, we will omit j in the following. <ref type="bibr" target="#b1">2</ref> At least in the homogeneous case composed of feature extractor ? i and classifier ? i . Each domainspecific module is only exposed to data of that corresponding domain. To train domain-specific models, we optimise:</p><formula xml:id="formula_1">argmin [?1,...,?n],[?1,...,?n] E Di?D E (xi,yi)?Di (y i ,? i (? i (x i )) (2)</formula><p>Episodic Training Our goal is to train a domain agnostic model, as per ? and ? in the aggregation method in Eq. 1. And we will design an episodic scheme that makes use of the domainspecific modules as per Eq. 2 to help the domain-agnostic model achieve the desired robustness. Specifically, we will generate episodes where each domain agnostic module ? and ? is paired with a domain-specific partner that is mismatched with the current data being input. So module and data combinations of the form (?,? i ,x i ) and (? i ,?,x i ) where i =i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Episodic Training of Feature Extractor</head><p>To train a robust feature extractor ?, we ask it to learn features which are robust enough that data from domain i can be processed by a classifier that has never experienced domain i before as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. To generate episodes according to this criterion, we optimise</p><formula xml:id="formula_2">argmin ? E i,j?[1,n],i =j E (xi,yi)?Di (y i ,? j (?(x i ))<label>(3)</label></formula><p>where i =j and ? j means that ? j is considered constant for the generation of this loss, i.e., it does not receive back-propagated gradients. This gradient-blocking is important, because without it the data x i from domain i would 'pollute' the classifier ? j which we want to retain as being naive to domains outside of j. Thus in this optimisation, only the feature extractor ? is penalized whenever the classifier ? j makes the wrong prediction. That means that, for this loss to be minimised, the shared feature extractor ? must map data x i into a format that a 'naive' classifier ? j can correctly classify. The feature extractor must learn to help a classifier recognize a data point that is from a domain that is novel to that classifier. The shared feature extractor feeds domain specific classifiers. The shared classifier reads domain-specific feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Episodic Training of Classifier</head><p>Analogous to the above, we can also interpret DG as the requirement that a classifier should be robust enough to classify data even if it is encoded by a feature extractor which has never seen this type of data in the past, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Thus to train the robust classifier ? we ask it to classify domain i instances x i fed through a domain j-specific feature extractor ? j . To generate episodes according to this criterion, we do:</p><formula xml:id="formula_3">argmin ? E i,j?[1,n],i =j E (xi,yi)?Di (y i ,?(? j (x i ))<label>(4)</label></formula><p>where i = j and ? j means ? j is considered constant for generation of the loss here. Similar to the training of the feature extractor module, this operation is important to retain the domain-specificity of feature extractor ? j . The result is that only the classifier ? is penalised, and in order to minimise this loss ? must be robust enough to accept data x i that has been encoded by a naive feature extractor ? j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Episodic Training by Random Classifier</head><p>The episodic feature training strategy above is limited to the homogeneous DG setting, since it requires all domains to share label-space in order to create episodes. But in the heterogeneous scenarios, the shared label-space assumption is not met. We next introduce a novel feature training strategy that is suitable for both homogeneous and heterogeneous label-spaces.</p><p>In Section 3.2, we introduced the notion of regularising a deep feature extractor by requiring it to support a classifier inexperienced with data from the current domain. Taking this to an extreme, we consider asking the feature extractor to support the predictions of a classifier with random weights, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. To this end, our optimisation task here is:</p><formula xml:id="formula_4">argmin ? E Di?D E (xi,yi)?Di (y i ,? r (?(x i ))<label>(5)</label></formula><p>where, ? r is a randomly initialised classifier, and ? r means it is a constant not updated in the optimization. This can be seen as an extreme version of our earlier episodic cross-domain  </p><formula xml:id="formula_5">for (? i ,? i )?[(? 1 ,? 1 ),...,(? n ,? n )] do 6: Update ? i :=? i ??? ?i (L ds ) 7: Update ? i :=? i ??? ?i (L ds ) 8: end for 9: Update ? :=???? ? (L agg +? 1 L epif +? 3 L epir ) 10:</formula><p>Update ? :=???? ? (L agg +? 2 L epic ) 11: end while 12: Output: ?,? feature extractor training (not only it has not seen any data from domain x i , but it has not seen any data at all). Moreover, it has the benefit of not requiring a label-space to be shared across all training domains unlike the previous method in Eq. 3.</p><p>Specifically, in Eq. 3, the routing x i ? ? ? ? j requires ? j to have a label-space matching (x i ,y j ). But for Eq. 5, each domain can be equipped with its own random classifier ? r with a cardinality matching its normal label-space. This property makes Eq. 5 suitable for heterogeneous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Algorithm Flow</head><p>Our full algorithm brings together the domain agnostic modules that are our goal to train and the supporting domainspecific modules that help train them (Section 3.1). We generate episodes according to the three strategies introduced above. Referring the losses in Eq. 1, 2, 3, 4, 5 as L agg , L ds , L epif , L epic , L epir , then overall we optimise the objective function:</p><formula xml:id="formula_6">L full =L agg +L ds +? 1 L epif +? 2 L epic +? 3 L epir (6) for parameters ?,?,{? i ,? i } n i=1 .</formula><p>The full pseudocode for the algorithm is given in Algorithm 1. It is noteworthy that, in practice, when training we first warm up the domain-specific branches (sometimes the domain-agnostic one as well) for a few iterations before training both the domain-specific and domain-agnostic modules jointly. After training, only the domain agnostic modules (of AGG) will be deployed for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>Datasets We evaluate our algorithm on three different homogeneous DG benchmarks and introduce a novel and larger scale heterogeneous DG benchmark. The datasets are: IXMAS: <ref type="bibr" target="#b37">[38]</ref> is cross-view action recognition task. Two object recognition benchmarks include: VLCS <ref type="bibr" target="#b7">[8]</ref>, which includes images from four famous datasets PASCAL VOC2007 (V) <ref type="bibr" target="#b6">[7]</ref>, LabelMe (L) <ref type="bibr" target="#b31">[32]</ref>, Caltech (C) <ref type="bibr" target="#b18">[19]</ref> and SUN09 (S) <ref type="bibr" target="#b5">[6]</ref> and the more recent PACS which has a larger cross-domain gap than VLCS <ref type="bibr" target="#b16">[17]</ref>. It contains four domains covering Photo (P), Art Painting (A), Cartoon (C) and Sketch (S) images. VD: For the final benchmark we repurpose the Visual Decathlon <ref type="bibr" target="#b28">[29]</ref>   <ref type="bibr" target="#b16">[17]</ref> learns a domain-agnostic model by factoring out the common component from a set of domain-specific models, as well as tensor factorization to compress the model parameters. CCSA <ref type="bibr" target="#b25">[26]</ref> uses semantic alignment to regularize the learned feature subspace. DANN <ref type="bibr" target="#b10">[11]</ref> Domain Adversarial Neural Networks train a feature extractor with a domain-adversarial loss among the source domains. The source-domain invariant feature extractor is assumed to generalise better to novel target domains. MAML <ref type="bibr" target="#b8">[9]</ref> The model-agnostic meta-learning method for fast adaptation, repurposed for DG. MLDG <ref type="bibr" target="#b17">[18]</ref> A recent meta-learning based optimization method. It mimics the DG setting by splitting source domains into meta-train and meta-test, and modifies the optimisation to improve meta-test performance. Fusion <ref type="bibr" target="#b23">[24]</ref> A method that fuses the predictions from source domain classifiers for the target domain. MMD-AAE <ref type="bibr" target="#b19">[20]</ref> A recent method that learns domain invariant feature autoencoding with adversarial training and ensuring that domains are aligned by the MMD constraint. CrossGrad <ref type="bibr" target="#b32">[33]</ref> A recent method that uses Bayesian networks to perturb the input manifold for DG. MetaReg <ref type="bibr" target="#b0">[1]</ref> A recent DG method that meta-learns the classifier regularizer. We note that DANN (domain adaptation) is not designed for DG. However, DANN learns domain invariant features, which is natural for DG. And we found it effective for this problem. Therefore we repurpose it as a baseline.</p><p>We call our method as Episodic. We use Epi-FCR to denote our full method with (f)eature regularisation, (c)lassifier regularisation and (r)andom classifier regularisation respectively. Ablated variants such as Epi-F denote feature regularisation alone, etc. Episodic is implemented using PyTorch 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on IXMAS dataset</head><p>Settings IXMAS contains 11 different human actions. All actions were video recorded by 5 cameras with different views (referred as 0,...,4). The goal is to train an action recognition model on a set of source views (domains), and recognise the action from a novel target view (domain). We follow <ref type="bibr" target="#b19">[20]</ref> to keep the first 5 actions and use the same Dense trajectory features as input. For our method, we follow <ref type="bibr" target="#b19">[20]</ref> to use a one-hidden layer network with 2000 hidden neurons as our backbone and report the average result of 20 runs. The optimizer is M-SGD with learning rate 1e-4, momentum 0.9, weight decay 5e-5. We use ? 1 =2.0, ? 2 =2.0, and ? 3 =0.5. Results From the results in <ref type="table" target="#tab_2">Table 1</ref>, we can see that: (i) The vanilla aggregation method, AGG is a strong competitor compared to several prior published methods, as is DANN, which is newly identified by us as an effective DG algorithm. (ii) Overall our Epi-FCR performs best, improving 2.4% on AGG, and 1.1% on prior state-of-the-art MMD-AAE. (iii) Particularly in view 1&amp;2 our method achieves new state-of-the art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on VLCS dataset</head><p>Settings VLCS domains share 5 categories: bird, car, chair, dog and person. We use pre-extracted DeCAF6 features and follow <ref type="bibr" target="#b25">[26]</ref> to randomly split each domain into train (70%) and test (30%) and do leave-one-out evaluation. We use a 2 fully connected layer architecture with output size of 1024 and 128 with ReLU activation, as per <ref type="bibr" target="#b25">[26]</ref> and report the average performance of 20 trials. The optimizer is M-SGD with learning rate 1e-3, momentum 0.9 and weight decay 5e-5. We use ? 1 =7.0, ? 2 =5.0, and ? 3 =0.5. Results From the results in <ref type="table" target="#tab_3">Table 2</ref>, we can see that: (i) The simple AGG baseline is again competitive with many published alternatives, so is DANN. (ii) Our Epi-FCR method achieves the best performance, improving on AGG by 1.7% and performing comparably to prior state-of-the-art MMD-AAE and MLDG with 0.6% improvement over both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on PACS dataset</head><p>Settings PACS is a recent dataset with different object style depictions, and a more challenging domain shift than VLCS, as shown in <ref type="bibr" target="#b16">[17]</ref>. This dataset shares 7 object categories across domains, including dog, elephant, giraffe, guitar, house, horse and person. We follow the protocol in <ref type="bibr" target="#b16">[17]</ref> including the recommended train and validation split for fair comparison. We first follow <ref type="bibr" target="#b16">[17]</ref> in using the ImageNet pretrained AlexNet (in <ref type="table" target="#tab_5">Table 3</ref>) and subsequently also use a modern ImageNet pretrained ResNet-18 (in <ref type="table" target="#tab_6">Table 4</ref>  train our network using the M-SGD optimizer (batch size/per domain=32, lr=1e-3, momentum=0.9, weight decay=5e-5) for 45k iterations when using AlexNet and train our network using the same optimizer (weight decay=1e-4) for ResNet-18. We use ? 1 =2.0, ? 2 =0.05, and ? 3 =0.1 for both settings. We use the official PACS protocol and split <ref type="bibr" target="#b16">[17]</ref> and rerun MetaReg <ref type="bibr" target="#b0">[1]</ref> on this split, since MetaReg did not release their protocol.</p><p>Results From the AlexNet results in <ref type="table" target="#tab_5">Table 3</ref>, we can see that: (i) Our episodic method obtained the best performance on held out domains C and S and comparable performance on A, P domains. (ii) It also achieves the best performance overall, with 3.3% improvement on vanilla AGG, and at least 1.7% improvement on prior state-of-the-art methods MLDG <ref type="bibr" target="#b17">[18]</ref>, Fusion <ref type="bibr" target="#b23">[24]</ref> and MetaReg <ref type="bibr" target="#b0">[1]</ref>. Meanwhile in <ref type="table" target="#tab_6">Table 4</ref>, we see that with a modern ResNet-18 architecture, the basic results are improved across the board as expected. However our full episodic method maintains the best performance overall, with a 2.4% improvement on AGG.</p><p>We note here that when using modern architectures like <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref> for DG tasks we need to be careful with batch normalization <ref type="bibr" target="#b13">[14]</ref>. Batchnorm accumulates statistics of the training data during training, for use at testing. In DG, the source and target domains have domain-shift between them, so different ways of employing batch norm produce different results. We tried two ways of coping with batch norm, one is directly using frozen pre-trained ImageNet statistics. Another is to unfreeze and accumulate statistics from the source domains. We observed that when training ResNet-18 on PACS with accumulating the statistics from source domains it produced a worse accuracy than freezing ImageNet statistics (75.7% vs 79.1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Further Analysis and Insights</head><p>Ablation Study To understand the contribution of each component of our model, we perform an ablation study using PACS-AlexNet shown in <ref type="figure" target="#fig_5">Fig. 6a</ref>. Episodic training for the feature extractor, gives a 1.6% boost over the vanilla AGG. Including episodic training of the classifier, further improves 0.5%. Finally, combine all the episodic training components, provides </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3% improvement over vanilla AGG. This confirms that each component of our model contributes to final performance.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Domain Testing Analysis To understand how our</head><p>Epi-FCR method obtains its improved robustness to domain shift, we study its impact on cross-domain testing. Recall that when we activate the episodic training of the agnostic feature extractor and classifier, we benefit from the domain specific branches by routing domain i data across domain j branches. E.g., we feed: x i ? ? ? ? j ? y i to train Eq. 3, and x i ?? j ?? ?y i to train Eq. 4.</p><p>Therefore it is natural to evaluate cross-domain testing after training the models. As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, we can see that the episodic training strategy indeed improves cross-domain testing performance. For example, when we feed domain A data to domain C classifier x A ?? ?? C ?y A , the Episodic-trained agnostic extractor ? improves the performance of the domain-C classifier who has never experienced domain A data <ref type="figure" target="#fig_4">(Fig. 5,  left)</ref>; and similarly for the Episodic-trained agnostic classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Solution Robustness</head><p>In the above experiments we confirmed that our episodic model outperforms the strong AGG baseline in a variety of benchmarks, and that each component of our framework contributes. In terms of analysing the mechanism by which episodic training improves robustness to domain shift, one possible route is through leading the model to find a higher quality minima. Several studies recently have analysed learning algorithm variants in terms of the quality of the minima that they leads a model to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref>.   One intuition is that converging to a 'wide' rather than 'sharp' minima provides a more robust solution, because perturbations (such as domain shift, in our case) are less likely to cause a big hit to accuracy if the model's performance is not dependent on a very precisely calibrated solution. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>, we therefore compare the solutions found by AGG and our Epi-FCR by adding noise to the weights of the converged model, and observing how quickly the testing accuracy decreases with the magnitude of the noise. From <ref type="figure" target="#fig_6">Fig. 7</ref> we can see that both models' performance drops as weights are perturbed, but our Epi-FCR model is more robust to weight perturbations. This suggests that the minima found by Epi-FCR is a more robust one than that found by AGG, which may explain the improved cross domain robustness of Epi-FCR compared to AGG.</p><p>Computational Cost Our Episodic model is comparable in cost overall to many contemporaries. Our Epi-C variant does require training multiple feature extractors for the source domains (as do <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>). However, users are more practically interested in testing performance, where our model is as small, fast and simple as AGG (unlike, e.g., <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>). In terms of training requirements, we note that only the Epi-C variant requires multiple feature extractor training, so Epi-FR can still safely be used if this is an issue. Furthermore if a large number of source domains are present, we can sample a subset of them at each batch.</p><p>Concretely, we compare the training time of different methods in <ref type="figure" target="#fig_5">Fig. 6b</ref>. All the methods were run on PACS (ResNet-18) for 3k iterations with CPU: Intel i7-7820 (@3.60GHz x 16) and GPU: 1080Ti. As expected vanilla AGG is the fastest to train (9.8 mins), so we regard it as the the base unit. The second tier are our Epi-F and Epi-R. As expected without Epi-C, our Epi-F and Epi-R variants run fast. The next tier are MetaReg, Epi-FCR and MLDG. And the most expensive one is CrossGrad. Although the use of 'Epi-C' here requires domain-specific feature extractors, our Epi-FCR is still comparably efficient. This is because our episodic training does not generate multi-step graph unrolling or meta-optimization in gradient updates. As a result, our time cost is on par with MetaReg <ref type="bibr" target="#b0">[1]</ref> and faster than MLDG <ref type="bibr" target="#b17">[18]</ref> and CrossGrad <ref type="bibr" target="#b32">[33]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation on VD-DG dataset</head><p>Heterogeneous Problem Setting Visual Decathlon contains ten datasets and was initially proposed as a multi-domain learning benchmark <ref type="bibr" target="#b28">[29]</ref>. We re-purpose Decathlon for a more ambitious challenge of domain generalisation. As explained earlier, our motivation is find out if DG learning can improve the defacto standard 'ImageNet trained CNN feature extractor' for use as a fixed off-the-shelf representation for new target problems. In this case the feature extractor is trained on the source domain, and used to extract features of the target domain data. Then a target domain-specific classifier (we use SVM) is trained to classify in the target domain. As explained in <ref type="table" target="#tab_8">Table 5</ref> (left), this is quite different from the standard DG setting in that target domain labels are used (for shallow classifier training), but the focus here is on the robustness of the learned feature when generalising to represent new domains and tasks without further fine-tuning. If DG training can improve feature generalisation compared to a vanilla ImageNet CNN, this could be of major practical value given the widespread usage of this workflow by vision practitioners.</p><p>Besides evaluating a potentially more generally useful problem setting compared to standard homogeneous DG, our VD experiment is also a larger scale evaluation compared to existing DG studies. As shown in <ref type="table" target="#tab_8">Table 5</ref> (right), VD-DG has twice the domains of VLCS and PACS and is an order of magnitude larger evaluation in terms of data and category numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>We consider five larger datasets in VD (CIFAR-100, Daimler Ped, GTSRB, Omniglot and SVHN) as our source domains, and the four smallest datasets (Aircraft, D. Textures, VGG-Flowers and UCF101) as our target domains. The goal is to use DG training among the source datasets to learn a feature which outperforms the off-the-shelf ImageNet-trained CNN that we use as an initial condition. We use ResNet-18 <ref type="bibr" target="#b12">[13]</ref> as the backbone model, and resize all images to 64?64 for computational efficiency. To support the VD heterogeneous label space, we assume a shared feature extractor, and a source    In each case the final feature is used to train a linear SVM for the corresponding task, as per common practice. We train the network using the M-SGD optimizer (batch size/per domain=32, lr=1e-3, momentum=0.9, weight decay=1e-4) for 100k iterations where the lr is decayed in 40k, 80k iterations by a factor 10. We set ? 3 = 2.5 t+50 , t is the iteration num. Results From the results in <ref type="table" target="#tab_9">Table 6</ref>, we observed that: (i) All methods use the extra data in VD to improve on the initial features ('ImageNet PT'). (ii) In terms of other DG competitors: Only MLDG, CrossGrad, and DANN were feasible to run on the scale of VD; with others either not supporting heterogeneous label-spaces or scaling to this many domains/examples. (iii) Our Epi-R improves on the strong AGG baseline and DG competitors in both average accuracy, and also the VD score recommended in preference to accuracy in <ref type="bibr" target="#b28">[29]</ref>. This demonstrates the value of our Episodic training in learning a feature that is robust to novel domains. (iv) Our concatenation strategy provided the best overall performance compared to directly including ImageNet as a source domain ('Combine'). This partly due to using a fixed 100k iterations to constrain training time. With enough training, the latter option is likely to be best. Overall this is the first demonstration that any DG method can improve robustness to domain shift in a larger-scale setting, across heterogeneous domains, and make a practical impact in surpassing ImageNet feature performance 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We addressed the domain generalisation problem by proposing a simple episodic training strategy that mimics train-test domain-shift during training, thus improving the trained model's robustness to novel domains. We showed that our method achieves state-of-the-art performance on all the main existing DG benchmarks. We also performed the largest DG evaluation to date, using the Visual Decathlon benchmark. Importantly, we provided the first demonstration of DG's potential value 'in the wild' -by demonstrating our model's potential to improve the performance of the defacto standard ImageNet pre-trained CNN as a fixed feature extractor for novel downstream problems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional analysis</head><p>We conduct some analysis to better understand our episodic training method, and its contributions. Comparison model ensemble In our current implementation of episodic training, besides the AGG model we regularize, n domain-specific branches are used for generating DG episodes. In this way, it increases the total parameters during training to n+1 times that of AGG, although in the end only a single AGG branch is used for testing. To verify that the benefit is not solely due to additional parameters, we compare our episodic-training method with the ensemble of n+1 AGG models on PACS using ResNet-18. The result in <ref type="table" target="#tab_11">Table 7</ref> shows that our episodic training is more effective than the ensemble model. Crucially this is despite the fact that Epi-FCR is 1/(n+1)th the size of the full ensemble during testing. Flexibility of episodic training a) Global sharing parameters: In our current demonstration, we use n + 1 branches to conduct the episodic training. To reduce the total trainable parameters, we can also globally share the bottom feature layers and episodic-train the rest feature layers and classifier. For example, globally sharing the first two blocks and episodic-training the remaining parameters still leads to a 1.8% improvement over the baseline (see <ref type="table" target="#tab_11">Table 7</ref>, S2B). b) Intermediate feature layers: Applying episodic training around a typical feature vs classifier module split is intuitive, but other options are possible. For example, we evaluate splitting the modules at intermediate feature layers: first block, and third block of ResNet-18 on PACS. The results, in <ref type="table" target="#tab_11">Table 7</ref> (First Blk and Third Blk), show that episodic training can also work with intermediate layer splits, but the original design is better. c) Fixed domain-specific branches: Currently, we train the domain-specific classification losses and the episodic training losses jointly. We also evaluate our episodic training using the pre-trained domain-specific branches. From the result in <ref type="table" target="#tab_11">Table 7</ref> (Fixed DSNN), we can see that episodic training using fixed domain-specific branches is still effective with 1.8% performance improvement over AGG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of vanilla domain-aggregation for multi-domain learning. A single model ?(?(?)) classifies data from all domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of domain-specific branches. One classifier and feature extractor are trained per-domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Episodic training for feature and classifier regularisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The architecture of random classifier regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Cross-domain test accuracy on PACS (AlexNet) with shared feature extractor or classifier. A ?C means, feed A data through C-specific module. Eg, left: xA ?? ??C, right: xA ??C ??.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(a) Ablation study on PACS (?). (b) Computational cost comparison on PACS (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Minima quality analysis: Episodic training (Epi-FCR) vs baseline (AGG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Episodic Training for Domain Generalization 1: Input: D =[D 1 ,D 2 ,...,D n ] 2: Initialise hyper parameters: ? 1 ,? 2 ,? 3 ,? 3: Initialise model parameters: domain specific modules ? 1 , ..., ? n and ? 1 , ..., ? n ; AGG modules ?, ?; random classifier ? r 4: while not done training do</figDesc><table><row><cell>5:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>benchmark to evaluate DG. Competitors We evaluate the following competitors: AGG the vanilla aggregation method, introduced in Eq. 1, trains a single model for all source domains. DICA [27] a kernel-based method for learning domain invariant feature representations. LRE-SVM [39] a SVM-based method, that trains different SVM model for each source domain. For a test domain, it uses the SVM model from the most similar source domain. D-MTAE [12] a de-noising multi-task auto encoder method, which learns domain invariant features by cross-domain reconstruction. DSN [4] Domain Separation Networks decompose the sources domains into shared and private spaces and learns them with a reconstruction signal. TF-CNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>) as a base CNN architecture. We Source Target DICA [27] LRE-SVM [39] D-MTAE [12] CCSA [26] MMD-AAE [20] DANN [11] MLDG [18] CrossGrad [33] MetaReg [1] AGG Epi-FCR Cross-view action recognition results (accuracy. %) on IXMAS dataset. Best result in bold.</figDesc><table><row><cell>0,1,2,3</cell><cell>4</cell><cell>61.5</cell><cell>75.8</cell><cell>78.0</cell><cell>75.8</cell><cell>79.1</cell><cell>75.0</cell><cell>70.7</cell><cell>71.6</cell><cell>74.2</cell><cell>73.1</cell><cell>76.9</cell></row><row><cell>0,1,2,4</cell><cell>3</cell><cell>72.5</cell><cell>86.9</cell><cell>92.3</cell><cell>92.3</cell><cell>94.5</cell><cell>94.1</cell><cell>93.6</cell><cell>93.8</cell><cell>94.0</cell><cell>94.2</cell><cell>94.8</cell></row><row><cell>0,1,3,4</cell><cell>2</cell><cell>74.7</cell><cell>84.5</cell><cell>91.2</cell><cell>94.5</cell><cell>95.6</cell><cell>97.3</cell><cell>97.5</cell><cell>95.7</cell><cell>96.9</cell><cell>95.7</cell><cell>99.0</cell></row><row><cell>0,2,3,4</cell><cell>1</cell><cell>67.0</cell><cell>83.4</cell><cell>90.1</cell><cell>91.2</cell><cell>93.4</cell><cell>95.4</cell><cell>95.4</cell><cell>94.2</cell><cell>97.0</cell><cell>95.7</cell><cell>98.0</cell></row><row><cell>1,2,3,4</cell><cell>0</cell><cell>71.4</cell><cell>92.3</cell><cell>93.4</cell><cell>96.7</cell><cell>96.7</cell><cell>95.7</cell><cell>93.6</cell><cell>94.0</cell><cell>94.7</cell><cell>94.4</cell><cell>96.3</cell></row><row><cell>Ave.</cell><cell></cell><cell>69.4</cell><cell>84.6</cell><cell>87.0</cell><cell>90.1</cell><cell>91.9</cell><cell>91.5</cell><cell>90.2</cell><cell>89.9</cell><cell>91.4</cell><cell>90.6</cell><cell>93.0</cell></row><row><cell cols="13">Source Target DICA [27] LRE-SVM [39] D-MTAE [12] CCSA [26] MMD-AAE [20] DANN [11] MLDG [18] CrossGrad [33] MetaReg [1] AGG Epi-FCR</cell></row><row><cell>L,C,S</cell><cell>V</cell><cell>63.7</cell><cell>60.6</cell><cell>63.9</cell><cell>67.1</cell><cell>67.7</cell><cell>66.4</cell><cell>67.7</cell><cell>65.5</cell><cell>65.0</cell><cell>65.4</cell><cell>67.1</cell></row><row><cell>V,C,S</cell><cell>L</cell><cell>58.2</cell><cell>59.7</cell><cell>60.1</cell><cell>62.1</cell><cell>62.6</cell><cell>64.0</cell><cell>61.3</cell><cell>60.0</cell><cell>60.2</cell><cell>60.6</cell><cell>64.3</cell></row><row><cell>V,L,S</cell><cell>C</cell><cell>79.7</cell><cell>88.1</cell><cell>89.1</cell><cell>92.3</cell><cell>94.4</cell><cell>92.6</cell><cell>94.4</cell><cell>92.0</cell><cell>92.3</cell><cell>93.1</cell><cell>94.1</cell></row><row><cell>V,L,C</cell><cell>S</cell><cell>61.0</cell><cell>54.9</cell><cell>61.3</cell><cell>59.1</cell><cell>64.4</cell><cell>63.6</cell><cell>65.9</cell><cell>64.7</cell><cell>64.2</cell><cell>65.8</cell><cell>65.9</cell></row><row><cell>Ave.</cell><cell></cell><cell>65.7</cell><cell>65.8</cell><cell>68.6</cell><cell>70.2</cell><cell>72.3</cell><cell>71.7</cell><cell>72.3</cell><cell>70.5</cell><cell>70.4</cell><cell>71.2</cell><cell>72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Cross-dataset object recognition results (accuracy. %) on VLCS benchmark. Best in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Source Target DICA [27] D-MTAE [12] DSN [4] TF-CNN [17] Fusion [24] DANN [11] MLDG [18] CrossGrad [33] MetaReg [1] AGG Epi-FCR</figDesc><table><row><cell>C,P,S</cell><cell>A</cell><cell>64.6</cell><cell>60.3</cell><cell>61.1</cell><cell>62.9</cell><cell>64.1</cell><cell>63.2</cell><cell>66.2</cell><cell>61.0</cell><cell>63.5</cell><cell>63.4</cell><cell>64.7</cell></row><row><cell>A,P,S</cell><cell>C</cell><cell>64.5</cell><cell>58.7</cell><cell>66.5</cell><cell>67.0</cell><cell>66.8</cell><cell>67.5</cell><cell>66.9</cell><cell>67.2</cell><cell>69.5</cell><cell>66.1</cell><cell>72.3</cell></row><row><cell>A,C,S</cell><cell>P</cell><cell>91.8</cell><cell>91.1</cell><cell>83.3</cell><cell>89.5</cell><cell>90.2</cell><cell>88.1</cell><cell>88.0</cell><cell>87.6</cell><cell>87.4</cell><cell>88.5</cell><cell>86.1</cell></row><row><cell>A,C,P</cell><cell>S</cell><cell>51.1</cell><cell>47.9</cell><cell>58.6</cell><cell>57.5</cell><cell>60.1</cell><cell>57.0</cell><cell>59.0</cell><cell>55.9</cell><cell>59.1</cell><cell>56.6</cell><cell>65.0</cell></row><row><cell>Ave.</cell><cell></cell><cell>68.0</cell><cell>64.5</cell><cell>67.4</cell><cell>69.2</cell><cell>70.3</cell><cell>69.0</cell><cell>70.0</cell><cell>67.9</cell><cell>69.9</cell><cell>68.7</cell><cell>72.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Cross-domain object recognition results (accuracy. %) of different methods on PACS using pretrained AlexNet. Best in bold.</figDesc><table><row><cell cols="9">Source Target AGG DANN [11] MAML [9] MLDG [18] CrossGrad [33] MetaReg [1] Epi-FCR</cell></row><row><cell>C,P,S</cell><cell>A</cell><cell>77.6</cell><cell>81.3</cell><cell>78.3</cell><cell>79.5</cell><cell>78.7</cell><cell>79.5</cell><cell>82.1</cell></row><row><cell>A,P,S</cell><cell>C</cell><cell>73.9</cell><cell>73.8</cell><cell>76.5</cell><cell>77.3</cell><cell>73.3</cell><cell>75.4</cell><cell>77.0</cell></row><row><cell>A,C,S</cell><cell>P</cell><cell>94.4</cell><cell>94.0</cell><cell>95.1</cell><cell>94.3</cell><cell>94.0</cell><cell>94.3</cell><cell>93.9</cell></row><row><cell>A,C,P</cell><cell>S</cell><cell>70.3</cell><cell>74.3</cell><cell>72.6</cell><cell>71.5</cell><cell>65.1</cell><cell>72.2</cell><cell>73.0</cell></row><row><cell>Ave.</cell><cell></cell><cell>79.1</cell><cell>80.8</cell><cell>80.6</cell><cell>80.7</cell><cell>77.8</cell><cell>80.4</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Cross-domain object recognition results (accuracy. %) of different methods on PACS using ResNet-18. Best in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Left: Difference between conventional homogeneous DG setting and new heterogeneous DG setting. Right: Contrasting the larger scale of our VD-DG (excluding ImageNet) vs previous DG benchmarks.</figDesc><table><row><cell>Target</cell><cell>ImageNet PT</cell><cell></cell><cell>MLDG [18]</cell><cell></cell><cell></cell><cell cols="2">CrossGrad [33]</cell><cell></cell><cell>AGG</cell><cell></cell><cell></cell><cell>DANN [11]</cell><cell></cell><cell></cell><cell>Epi-R</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="15">Concat Mean Combine Concat Mean Combine Concat Mean Combine Concat Mean Combine Concat Mean Combine</cell></row><row><cell>Aircraft</cell><cell>12.7</cell><cell>17.4</cell><cell>14.2</cell><cell>15.7</cell><cell>17.2</cell><cell>13.7</cell><cell>15.9</cell><cell>17.4</cell><cell>14.6</cell><cell>15.7</cell><cell>17.4</cell><cell>15.0</cell><cell>16.0</cell><cell>17.7</cell><cell>13.9</cell><cell>15.5</cell></row><row><cell>D. Textures</cell><cell>35.2</cell><cell>38.3</cell><cell>34.6</cell><cell>32.5</cell><cell>34.6</cell><cell>31.4</cell><cell>32.2</cell><cell>37.7</cell><cell>35.1</cell><cell>31.5</cell><cell>37.9</cell><cell>36.6</cell><cell>33.0</cell><cell>40.2</cell><cell>37.8</cell><cell>33.9</cell></row><row><cell>VGG-Flowers</cell><cell>48.1</cell><cell>54.0</cell><cell>53.2</cell><cell>54.4</cell><cell>49.2</cell><cell>49.3</cell><cell>54.9</cell><cell>56.3</cell><cell>52.0</cell><cell>57.0</cell><cell>55.5</cell><cell>52.2</cell><cell>53.7</cell><cell>55.4</cell><cell>53.0</cell><cell>55.9</cell></row><row><cell>UCF101</cell><cell>35.0</cell><cell>44.4</cell><cell>36.7</cell><cell>34.9</cell><cell>42.7</cell><cell>35.7</cell><cell>35.2</cell><cell>43.3</cell><cell>35.0</cell><cell>36.1</cell><cell>44.5</cell><cell>36.1</cell><cell>33.9</cell><cell>45.7</cell><cell>37.1</cell><cell>37.3</cell></row><row><cell>Ave.</cell><cell>32.8</cell><cell>38.5</cell><cell>34.7</cell><cell>34.4</cell><cell>35.9</cell><cell>32.5</cell><cell>34.6</cell><cell>38.7</cell><cell>34.2</cell><cell>35.1</cell><cell>38.8</cell><cell>35.0</cell><cell>34.1</cell><cell>39.7</cell><cell>35.5</cell><cell>35.7</cell></row><row><cell>VD-Score</cell><cell>185</cell><cell>279</cell><cell>194</cell><cell>169</cell><cell>241</cell><cell>169</cell><cell>169</cell><cell>265</cell><cell>185</cell><cell>172</cell><cell>277</cell><cell>202</cell><cell>165</cell><cell>304</cell><cell>217</cell><cell>194</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results of top-1 accuracy (%) and visual decathlon overall scores of different methods on VD-DG. Train on CIFAR-100, Daimler Ped, GTSRB, Omniglot, SVHN, and optionally ImageNet (Combine). Test on Aircraft, D. Textures, VGG-Flowers, UCF101.domain-specific classifier. We perform episodic DG training among the source domains, using our (R)andom classifier model variant, which supports heterogeneous label-spaces. After DG training, the model will then be used as a fixed feature extractor for the held out target domains. With regards to use of ImageNet during training, we consider two settings: (i) Use ImageNet CNN as initial condition, but exclude ImageNet data from DG training, (ii) Include ImageNet as a sixth source domain for DG training. The former helps to constrain training cost, but loses some performance due to the forgetting effect. Therefore we combine (concatenation and mean-pooling) the original ImageNet pre-trained features with the VD-DG trained features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Target AGG Ensemble Epi-FCR(S2B) First Blk Third Blk Fixed DSNN Epi-FCR</figDesc><table><row><cell>A.</cell><cell>77.6</cell><cell>79.3</cell><cell>80.4</cell><cell>78.8</cell><cell>81.0</cell><cell>79.1</cell><cell>82.1</cell></row><row><cell>C.</cell><cell>73.9</cell><cell>75.9</cell><cell>75.3</cell><cell>75.5</cell><cell>75.6</cell><cell>76.6</cell><cell>77.0</cell></row><row><cell>P.</cell><cell>94.4</cell><cell>95.4</cell><cell>94.4</cell><cell>93.7</cell><cell>92.2</cell><cell>93.5</cell><cell>93.9</cell></row><row><cell>S.</cell><cell>70.3</cell><cell>71.2</cell><cell>73.4</cell><cell>74.5</cell><cell>74.1</cell><cell>74.7</cell><cell>73.0</cell></row><row><cell>Ave.</cell><cell>79.1</cell><cell>80.4</cell><cell>80.9</cell><cell>80.6</cell><cell>80.7</cell><cell>80.9</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Further evaluation on PACS using ResNet-18.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/HAHA-DL/Episodic-DG</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We note one concurrent study of the heterogeneous DG setting<ref type="bibr" target="#b20">[21]</ref> considered the VD-DG benchmark that we propose here. Their results are slightly higher due do use of a larger image size and cross-validation of SVM parameters (we use sklearn defaults).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by EPSRC grant EP/R026173/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metareg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromansk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><forename type="middle">Jin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Yi-Zhe Song, and Timothy Hospedales. Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perona</forename><surname>Pietro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Generative-Model Based Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-critic networks for heterogeneous domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Best sources forward: Domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<title level="m">Meta-learning with temporal convolutions. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Labelme: A database and web-based tool for image annotation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical networks for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
