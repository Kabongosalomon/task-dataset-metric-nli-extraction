<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Using Transformers for Speech-Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">On Using Transformers for Speech-Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech separation</term>
					<term>source separation</term>
					<term>trans- former</term>
					<term>attention</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have enabled major improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we have proposed SepFormer, which uses self-attention and obtains state-of-the art results on WSJ0-2/3 Mix datasets for speech separation. In this paper, we extend our previous work by providing results on more datasets including LibriMix, and WHAM!, WHAMR! which include noisy and noisy-reverberant conditions. Moreover we provide denoising, and denoising+dereverberation results in the context of speech enhancement, respectively on WHAM! and WHAMR! datasets. We also investigate incorporating recently proposed efficient selfattention mechanisms inside the SepFormer model, and show that by using efficient self-attention mechanisms it is possible to reduce the memory requirements significantly while performing better than the popular convtasnet model on WSJ0-2Mix dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T TRANSFORMERS are playing a pivotal role in modern deep learning <ref type="bibr" target="#b0">[1]</ref>. They contributed to a paradigm shift in sequence learning and made it possible to achieve unprecedented performance in many Natural Language Processing (NLP) tasks, such as language modeling, machine translation, and many other applications <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Transformers are becoming increasingly dominant in speech processing as well. For instance, they have been recently adopted for speech recognition, speaker verification, speech enhancement, and more recently for speech separation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Transformers enable more accurate modeling of longer term dependencies which render them suitable for audio processing, especially for speech separation, where long-term modeling has been shown to impact performance significantly <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Motivated by this reason, we proposed SepFormer <ref type="bibr" target="#b5">[6]</ref>, a transformer based model which obtains state of-the-art results on speech separation.</p><p>In this paper, building on top of this first work, we study to what extent transformers architectures are a viable choice for speech separation and speech enhancement applications. In particular, in our previous work <ref type="bibr" target="#b5">[6]</ref> we focused on the standard WSJ0-2/3Mix benchmark datasets. Here, we expand the study performed in our previous work <ref type="bibr" target="#b5">[6]</ref>, by providing additional experiments and insights on more realistic and challenging datasets such as Libri2/3-Mix <ref type="bibr" target="#b8">[9]</ref>, which include longer mixtures, WHAM! <ref type="bibr" target="#b9">[10]</ref> and WHAMR! <ref type="bibr" target="#b10">[11]</ref>, featuring noisy and noisy &amp; reverberant conditions respectively. Moreover, on WHAM! and on WHAMR! datasets we also provide results for speech enhancement. The training recipes for these experiments are all available in the Speechbrain <ref type="bibr" target="#b11">[12]</ref> toolkit.</p><p>Another contribution of this paper is investigating different types of self-attention for speech separation. Even though transformers consist only of feed-forward layers and therefore are able to leverage parallel processing capabilities of GPUs, the main building block of transformers, the attention mechanism, comes with a O(N 2 ) computation cost that presents itself as a major memory bottleneck. That is, for a sequence of length N , we need to compare N 2 elements, leading to a computational bottleneck that emerges more clearly when processing long signals. In addition, the large number of parameters usually required by Transformers further aggravate this memory problem.</p><p>Mitigating the memory bottleneck of Transformers has been the object of intense research in the last years <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. To address this issue, these methods assume that attending to a subset of elements is sufficient in most applications and propose more constrained, memory efficient self-attention mechanisms. For instance, Sparse Transformer <ref type="bibr" target="#b12">[13]</ref> employs local and dilated sliding windows over a fixed number of elements to learn short and long-term dependencies. Long-Former <ref type="bibr" target="#b13">[14]</ref> augments the Sparse Transformer by adding global attention heads. Linformers <ref type="bibr" target="#b14">[15]</ref>, approximate the full sparse attention with low-rank matrix multiplication, while Reformers <ref type="bibr" target="#b15">[16]</ref> clusters the elements to attend through an efficient locality-sensitive hashing function.</p><p>In this paper, we explore the use of three popular efficient self-attention mechanisms with and without the dual-path processing pipeline <ref type="bibr" target="#b6">[7]</ref>. Namely, we compare the regular selfattention <ref type="bibr" target="#b0">[1]</ref> with Longformer <ref type="bibr" target="#b13">[14]</ref>, Linformer <ref type="bibr" target="#b14">[15]</ref>, and Reformer <ref type="bibr" target="#b15">[16]</ref> attention. We observe that using the full attention mechanism, as used in our previously proposed Sepformer <ref type="bibr" target="#b5">[6]</ref>, provides the best performance overall. However, Reformer and Longformer attention mechanisms, present favourable tradeoffs in terms of performance versus memory requirements. Namely, we observe that using reformer without the dual-path pipeline results in 16.7 dB SI-SNR improvement. We compare the efficiency of this model with the popular convtasnet model, and observe that the proposed model is slightly more efficient while yielding better performance.</p><p>The paper is organized as follows. First, in Section III we introduce the building blocks of transformers and also describe about the three different efficient attention mechanisms (Longformer, Linformer and Reformer) we explore in this work. Next, in Section IV we describe the SepFormer model in detail with special focus on the dual-path processing pipeline employed by the model. Finally, in Section V, we present the results on source separation, speech enhancement, and an experimental analysis of the different types of self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK FOR TRANSFORMERS IN SOURCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEPARATION</head><p>As of the submission time of this paper, to the best of our knowledge there exist few papers on using transformers for source separation. These works include DPTNet <ref type="bibr" target="#b16">[17]</ref>, which uses a similar architecture compared with SepFormer, but employs RNNs before the self-attention layer. Our results in Section V indicate that SepFormer outperforms DPTNet, and is faster on inference time due to the absence of recurrent operations.</p><p>In <ref type="bibr" target="#b17">[18]</ref>, the authors provide a transformer based system to deal with a meeting-like settings with multi-microphone input where the amount of overlapping speech is low and a powerful separation might not be needed. The system's main feature is adapting the number of transformer layers according to the complexity of the input mixture.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, a multi-scale transformer approach is proposed. The results on WSJ0-2Mix and WHAM! datasets are provided. As can be seen on the <ref type="table" target="#tab_1">Tables I, IV</ref>, SepFormer outperforms this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRANSFORMERS</head><p>In this paper, we utilize the encoder part of the Transformer architecture, which is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The encoder turns an input sequence X = {x 1 , . . . , x T } ? R F ?T into an output sequence Y = {y 0 , . . . , y T } ? R F ?T using a pipeline of computations that involve positional embedding, multi-head self-attention, normalization, feed-forward layers, and residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-head self-attention</head><p>The multi-head self-attention mechanism allows the Transformer to model dependencies across all the elements of the sequence. The first step is to calculate the Query, Key, and Value matrices from the input sequence X of length T . This operation is performed by multiplying the input vector by weight matrices:</p><formula xml:id="formula_0">Q = W Q X, K = W K X, V = W V X, where W Q , W K , W V ? R T ?F .</formula><p>The attention layer consists of the following operations:</p><formula xml:id="formula_1">Attention(Q, K, V ) = softmax QK ? d k V,<label>(1)</label></formula><p>where d k represents the latent dimensionality of the Q and K matrices. We observe that the attention weights derive from a scaled dot-product between queries and keys. Closer query and key vectors will have higher dot products. The softmax function ensures that the attention weights range between 0 and 1. The attention mechanism produces T weights for each input element (i.e., T 2 attention weights). All in all, the softmax part of the attention layer computes relative importance weights, and then we multiply this attention map with the input value sequence V . In practice a multi-head-attention mechanism is employed consisting of several parallel attention layers. More in detail, the multi-head-attention is computed as follows:</p><formula xml:id="formula_2">MultiHeadAttention(Q, K, V ) (2) = Concat(head 1 , . . . , head h )W O , where head i = Attention(Q i , K i , V i ),</formula><p>where h is the number of parallel attention heads and W O ? R hF ?dmodel is the matrix that combines the parallel attention heads, and d model denotes the latent dimensionality of this combined model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feedforward Layers</head><p>The feed-forward component of the transformer architecture simply consists of a two-layer perceptron. The exact definition of which is as follows,</p><formula xml:id="formula_3">F F N (x) = ReLU(xW 1 + b 1 )W 2 + b 2 ,<label>(3)</label></formula><p>where x is the input. In the context of sequences, this feedforward transformation is applied to each time point separately. W 1 , and W 2 are learnable matrices, and b 1 , and b 2 are learnable bias vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Positional encoding</head><p>As the self-attention layer and feed-forward layers do not contain any notion of ordering, the transformer makes use of a positional encoding scheme used to inject sequence ordering information. The positional encoding is defined as follows <ref type="bibr" target="#b0">[1]</ref>,</p><formula xml:id="formula_4">P E t,2i =sin(t/10000 2i/dmodel ) (4) P E t,2i+1 =cos(t/10000 2i/dmodel )<label>(5)</label></formula><p>This encoding therefore relies on sinusoids of different frequencies in each latent dimension to encode positional information.</p><p>D. Reducing the memory bottleneck 1) Longformer: Longformer <ref type="bibr" target="#b13">[14]</ref>, aims to reduce the quadratic complexity by replacing the full self-attention structure with a combination of local and global attention. Specifically, Longformer relies on a local attention mechanism that capture dependencies only from nearby elements, and a global attention mechanism that globally captures dependencies from all the elements. To keep the computational requirements manageable, the global attention is only performed for few special elements in the sequence.</p><p>2) Linformer: Linformer <ref type="bibr" target="#b14">[15]</ref>, avoids the quadratic complexity by reducing the size of the time dimension of the matrices K, V ? R C?F . This is done by projecting the time dimension C to a smaller dimensionality k. This is done by using projection matrices P, F ? k ? C. The multi-head attention equation therefore becomes the following:</p><formula xml:id="formula_5">LFSelfAttention (Q, K, V ) = softmax Q(P K) ? d k (F V ),<label>(6)</label></formula><p>which effectively limits the complexity of the matrix product between the softmax output and the V matrix.</p><p>3) Reformer: Reformer <ref type="bibr" target="#b15">[16]</ref> uses locality sensitive hashing (LSH) to reduce the complexity of the self attention. In fact, LSH is used to find the vector pairs (q, k), q ? Q, k ? K that are closer. The intuition is that those pairs, being closer, will have a bigger dot product and will contribute the most to the attention matrix. Because of this, the authors limit the attention computation to the close pairs (q, k), while ignoring the others (saving time and memory). In addition to this, the Reformer, inspired by <ref type="bibr" target="#b19">[20]</ref>, implements reversible Transformer layers that avoid the linear memory complexity scaling with respect to the amount of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SEPARATION TRANSFORMER (SEPFORMER)</head><p>In this section we explain the components of the neural network that constitute the SepFormer. We first of all define the masking-based source separation framework on which SepFormer is based.</p><p>In the mask-based end-to-end source separation pipeline, popularized by <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr">[22]</ref>, an input mixture x ? R T is given as the input to the architecture. A learnable encoder obtains a representation h ? R F ?T . Afterwards a masking network estimates the masks m 1 , m 2 ? R F ?T . The separated time domain signals for source 1 and source 2 are obtained with passing the h * m 1 , and h * m 2 through the decoder, where * denotes element-wise multiplication. Next, we explain each block separately.</p><p>x</p><p>Encoder h Masking Net Decoder?</p><formula xml:id="formula_6">1 s 2 m 1 m 2 Fig. 2.</formula><p>The high-level description of the masking based source separation pipeline: The encoder block estimates a learned-representation for the input signal, while the masking network estimates optimal masks to separate the sources present in the mixtures. The decoder finally reconstructs the estimated sources in the time domain using the masks provided by the masking network. The self-attention based modeling ideas are applied inside the masking network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder</head><p>The encoder takes in the time-domain mixture-signal x ? R T as input. It learns an Short-Time Fourier Transform (STFT) like representation h ? R F ?T using a single convolutional layer:</p><formula xml:id="formula_7">h = ReLU(conv1d(x)).<label>(7)</label></formula><p>As we will describe in Section V-C, the stride factor of this convolution impacts significantly the performance, speed, and memory of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoder</head><p>The decoder simply uses a transposed convolutional layer, with the same stride and kernel size of the encoder. The input to the decoder is the element-wise multiplication between the mask m k relative to the k-th source, and the output of the encoder h. The transformation of the decoder can therefore be expressed as follows:</p><formula xml:id="formula_8">s k = conv1d-transpose(m k * h),<label>(8)</label></formula><p>where s k ? R T denotes the separated source k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Masking Network</head><p>Figure 4 (top) shows the detailed architecture of the masking network (Masking Net). The masking network is fed by the encoded representations h ? R F ?T and estimates a mask {m 1 , . . . , m N s } for each of the N s speakers in the mixture.</p><p>As in [22], the encoded input h is normalized with layer normalization <ref type="bibr" target="#b21">[23]</ref> and processed by a linear layer (with dimensionality F ). Following the dual-path framework introduced in <ref type="bibr" target="#b6">[7]</ref>, we create overlapping chunks of size C by chopping up h on the time axis with an overlap factor of 50%. We denote the output of the chunking operation with h ? R F ?C?N c , where C is the length of each chunk, and N c is the resulting number of chunks.</p><p>The representation h feeds the SepFormer separator block, which is the main component of the masking network. This block, which will be described in detail in Sec. IV, employs a pipeline composed of two transformers able to learn short and long-term dependencies.</p><p>The output of the SepFormer h ? R F ?C?N c is processed by PReLU activations followed by a linear layer. We denote the output of this module h ? R (F ?N s)?C?N c , where N s is the number of speakers. Afterwards we apply the overlap-add scheme described in <ref type="bibr" target="#b6">[7]</ref> and obtain h ? R F ?N s?T . We pass this representation through two feed-forward layers and a ReLU activation at the end to obtain a mask m k for each one of the speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dual-Path Processing Pipeline and SepFormer</head><p>The dual-path processing pipeline <ref type="bibr" target="#b6">[7]</ref> (shown in <ref type="figure">Fig. 3</ref>) enables a combination of short and long term modeling, while making long sequence processing feasible with a selfattention block. In the context of the masking based end-toend source separation shown in <ref type="figure">Fig. 2</ref>, the latent representation h ? R F ?T is divided into overlapping chunks.</p><p>A transformer encoder is applied on each chunk separately. We call this block IntraTransformer (IntraT) and denote it with f intra (.). This block effectively operates as a block diagonal attention structure, modeling the interaction between all elements in each chunk separately so that the attention memory requirements are bounded. Another transformer encoder is applied to model the inter-chunk interactions, i.e. between each chunk element. We call this block InterTransformer (InterT) and denote with f inter (.). Because this block attends to all elements that occupy the same position in each chunk, it effectively operates as a strided attention structure. Mathematically, the overall transformation can be summarized as follows:</p><formula xml:id="formula_9">h = f inter (f intra (h )).<label>(9)</label></formula><p>In the standard SepFormer model defined in <ref type="bibr" target="#b5">[6]</ref>, f inter and f intra are chosen to be full self-attention blocks. In Section IV-E, we describe the full self-attention block employed in SepFormer in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Full Self-Attention Transformer Encoder</head><p>The architecture for the full self-attention transformer encoder layers follows the original Transformer architecture <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_2">Figure 4</ref> (Bottom) shows the architecture of the employed fullattention transformer encoder layers used both for IntraTransformer and InterTransformer blocks.</p><p>We use the variable z to denote the input to the Transformer. First of all, sinusoidal positional encoding e is added to the input z, such that,</p><formula xml:id="formula_10">z = z + e.<label>(10)</label></formula><p>Positional encoding injects information on the order of the various elements composing the sequence, thus improving the separation performance. We follow the positional encoding definition in <ref type="bibr" target="#b0">[1]</ref>. We then apply multiple Transformer layers. Inside each Transformer layer g(.), we first apply layer normalization, followed by multi-head attention (MHA):</p><formula xml:id="formula_11">z = MultiHeadAttention(LayerNorm(z )).<label>(11)</label></formula><p>As proposed in <ref type="bibr" target="#b0">[1]</ref>, each attention head computes the scaled dot-product attention between all the elements of the sequence.</p><p>The Transformer finally employs a feed-forward network (FFW), which is applied to each position independently:</p><formula xml:id="formula_12">z = FeedForward(LayerNorm(z + z )) + z + z . (12)</formula><p>The overall transformer block is therefore defined as follows:</p><formula xml:id="formula_13">f (z) = g K (z + e) + z,<label>(13)</label></formula><p>where g K (.) denotes K layers of transformer layer g(.). We use K = N intra layers for the IntraT, and K = N inter layers for the InterT. As shown in <ref type="figure" target="#fig_2">Figure 4</ref> (Bottom) and Eq. <ref type="formula" target="#formula_1">(13)</ref>, we add residual connections across the transformer layers, and across the transformer architecture to improve gradient backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Datasets</head><p>In our experiments, we use the popular WSJ0-2mix and WSJ0-3mix datasets <ref type="bibr" target="#b22">[24]</ref>, where mixtures of two speakers and three speakers are created by randomly mixing utterances in the WSJ0 corpus. The relative levels for the sources are sampled uniformly between 0 dB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for training, validation, and test. The training and test sets are created with different sets of speakers. We use the 8kHz version of the dataset, with the 'min' version where the added waveforms are clipped to the shorter signal's length. These datasets have become the defacto standard benchmark for source separation algorithms.</p><p>In addition to the WSJ0-2/3 Mix, in this paper we also provide experimental data on WHAM! <ref type="bibr" target="#b9">[10]</ref>, and WHAMR! datasets <ref type="bibr" target="#b10">[11]</ref> which are essentially derived from WSJ0-2Mix dataset by adding environmental noise and environmental noise plus reverberation respectively. We also provide experimental results on the LibriMix dataset <ref type="bibr" target="#b8">[9]</ref>, which contains longer and more challenging mixtures than the WSJ0-Mix dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture and Training Details of SepFormer</head><p>The encoder of the SepFormer is based on 256 convolutional filters with a kernel size of 16 samples and a stride factor of 8 samples. The decoder uses the same kernel size and the stride factors of the encoder.</p><p>In our best models, the SepFormer masking network processes chunks of size C = 250 with a 50 % overlap between them and employs 8 layers of transformers in both Intra-Transformer and InterTransformer. The dual-path processing pipeline is repeated N = 2 times. We used 8 parallel attention heads, and 1024-dimensional positional feed-forward networks within each Transformer layer. The model has a total of 25.7 million parameters.</p><p>We explored the use of dynamic mixing (DM) data augmentation <ref type="bibr" target="#b23">[25]</ref> which consists in on-the-fly creation of new mixtures from single speaker sources. In this work we expanded this powerful technique by applying also speed perturbation on the sources before mixing them. The speed randomly changes between 95 % slow-down and 105 % speed-up.</p><p>We used the Adam algorithm <ref type="bibr" target="#b24">[26]</ref> as optimizer, with a learning rate of 1.5e Then the chunks are concatenated on another dimension (chunk dimension). Afterwards the IntraTransformer is applied along the time dimension of each chunk which amounts to applying a block diagonal attention mechanism, and then the InterTransformer is applied along the chunk dimension which amounts to applying a diagonal attention mechanism shown on the figure.  DM), the learning rate is annealed by halving it if we do not observe any improvement of the validation performance for 3 successive epochs (5 epoch for DM). Gradient clipping is employed to limit the l 2 -norm of the gradients to 5. During training, we used a batch size of 1, and used the scaleinvariant signal-to-noise Ratio (SI-SNR) <ref type="bibr" target="#b25">[27]</ref> via utterancelevel permutation invariant loss <ref type="bibr" target="#b26">[28]</ref>, with clipping at 30 dB <ref type="bibr" target="#b23">[25]</ref>. We used automatic mixed-precision to speed up training. The system is trained for a maximum of 200 epochs. Each epoch takes approximately 1.5 hours on a single NVIDIA V100 GPU with 32 GB of memory with automatic mixed precision.</p><p>C. Results on WSJ0-2/3 Mix datasets WSJ0-2/3 Mix datasets are commonly used as benchmarking datasets in the source-separation literature. In <ref type="table" target="#tab_1">Table   I</ref>, we compare the performance achieved by the proposed SepFormer with the best results reported in the literature on the WSJ0-2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-SNRi) of 22.3 dB and a Signal-to-Distortion Ratio <ref type="bibr" target="#b33">[35]</ref> (SDRi) improvement of 22.4 dB on the testset with dynamic mixing. When using dynamic mixing, the proposed architecture achieves state-of-the-art performance. The SepFormer outperforms previous systems without using dynamic mixing except Wavesplit, which however achieves such impressive performance by leveraging speaker identity as additional information.</p><p>We also study the effect of various hyperparameters and data augmentations strategies on the performance of the SepFormer using the WSJ0-2mix dataset. The results are summarized in <ref type="table" target="#tab_1">Table II</ref>. The reported performance in this table is calculated on the validation set.  We observe that the number of InterT and IntraT blocks has an important impact on the performance. The best results are achieved with 8 layers for both blocks replicated two times. We also would like to point out that a respectable performance of 19.2 dB is obtained even when we use a single layer transformer for the InterTransformer. This suggests that the IntraTransformer, and thus local processing, has a greater influence on the performance. It also emerges that positional encoding is helpful (e.g. see lines 3 and 5 of <ref type="table" target="#tab_1">Table II)</ref>. A similar outcome has been observed in <ref type="bibr" target="#b34">[36]</ref> for speech enhancement. As for the number of attention heads, we observe a slight performance difference between 8 and 16 heads. Finally, it can be observed that dynamic mixing helps the performance significantly. <ref type="table" target="#tab_1">Table III</ref> showcases the best performing models on the WSJ0-3mix dataset. SepFormer obtains the state-of-the-art performance with an SI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best architecture as found for the WSJ0-2mix dataset in <ref type="table" target="#tab_1">Table II</ref>. The only difference is that the decoder has now three outputs. It is worth noting that on this corpus the SepFormer outperforms all previously proposed systems.</p><p>Our results on WSJ0-2mix and WSJ0-3mix show that it is possible to achieve state-of-the-art performance in separation with an RNN-free Transformer-based model. The big advantage of SepFormer over RNN-based systems like <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[34]</ref> is the possibility to parallelize the computations over different time steps. This leads to faster training and inference, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speed and Memory Comparison</head><p>We now compare the training and inference speed of our model with DPRNN <ref type="bibr" target="#b6">[7]</ref> and DPTNet <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure" target="#fig_3">Figure 5 (left)</ref> shows the training curves of the aforementioned models on the WSJ0-2mix dataset. We plot the performance achieved on the validation set in the first 48 hours of training versus the wallclock time. For a fair comparison, we used the same machine with the same GPU (a single NVIDIA V100-32GB) for all the models. Moreover, all the systems are trained with a batch size of 1 and employ automatic mixed precision. We observe that the SepFormer is faster than DPRNN and DPTNeT. <ref type="figure" target="#fig_3">Figure  5</ref> (left), highlights that SepFormer reaches above 17dB levels only after a full day of training, whereas the DPRNN model requires two days of training to achieve the same level of performance. <ref type="figure" target="#fig_3">Figure 5</ref> (middle&amp;right) compares the average computation time (in ms) and the total memory allocation (in GB) during inference when single precision is used. We analyze the speed of our best model for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare our models against DP-RNN, DPTNeT, and Wavesplit. All the models are stored in the same NVIDIA RTX8000-48GB GPU and we performed this analysis using the PyTorch profiler <ref type="bibr" target="#b35">[37]</ref>.</p><p>From this analysis, it emerges that the SepFormer is not only faster but also less memory demanding than DPTNet, DPRNN, and Wavesplit. We observed the same behavior using the CPU for inference. Such a level of computational efficiency is achieved even though the proposed SepFormer employs more parameters than the other RNN-based methods (see <ref type="table" target="#tab_1">Table I</ref>). This is not only due to the superior parallelization capabilities of the proposed model, but also because the best performance is achieved with a stride factor of 8 samples, against a stride of 1 for DPRNN and DPTNet. Increasing the stride of the encoder results in downsampling the input sequence, and therefore the model processes less data. In <ref type="bibr" target="#b6">[7]</ref>, the authors showed that the DPRNN performance degrades when increasing the stride factor. The SepFormer, instead, reaches competitive results even with a relatively large stride, leading to the aforementioned speed and memory advantages.   <ref type="bibr" target="#b10">[11]</ref> datasets are respectively versions of the WSJ0-2Mix dataset with environmental noise and environmental noise and reverberation. We use the same model configuration as the WSJ0-2/3Mix dataset both for WHAM! and WHAMR!. For both datasets, we train the model so that it does source-separation and speech enhancement at the same time. In the WHAM! dataset, the model learns to de-noise while also separating, and in WHAMR! dataset, the model learns to de-noise and de-reverberate in addition to separating. In WHAMR! when using speed augment and or dynamic mixing, we also randomly choose a room-impulse response from the training set of the WHAMR! dataset when creating new mixtures.</p><p>In <ref type="table" target="#tab_1">Tables IV, V</ref> we provide results with SepFormer on the WHAM! and WHAMR! datasets compared to the other methods in the literature. We observe that on both WHAM! and WHAMR! datasets SepFormer outperforms the previously proposed methods even without the use of DM, which further improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on Libri-2/3Mix datasets</head><p>LibriMix <ref type="bibr" target="#b8">[9]</ref> is proposed as an alternative to WSJ0-2/3Mix with more diverse and longer utterances. In <ref type="table" target="#tab_1">Table VI</ref>, we provide results from the Libri-2/3Mix datasets with SepFormer, both for clean any noisy versions, using the same model configuration as found on WSJ0-2mix in <ref type="table" target="#tab_1">Table II</ref>. Similar to the WHAM! dataset, for Libri-2/3 Mix datasets the network is trained to separate and denoise at the same time. We train the models on the train-360 set, both with and without dynamic mixing. In addition providing the performance of the models that were trained on LibriMix, we also provide the performance of the model that was pre-trained on the WSJ0-Mix dataset. One common criticism of the WSJ0-Mix is that the models trained on it do not generalize well to other tasks. Here, we showcase that SepFormer is able to obtain respectable performance on Libri 2/3 Mix clean versions even when trained on WSJ0-2/3Mix, surpassing the performance of a Convtasnet model trained on LibriMix as shown in row Sepformer PT of <ref type="table" target="#tab_1">Table VI</ref>. Also for the noisy version of Libri2Mix, we report the performance obtained with a model pretrained on WHAM! dataset. We do not do this for the noisy version of Libri3Mix as WHAM! dataset does not contain a noisy version for three speakers.</p><p>When trained directly on Libri2/3-Mix with dynamic mixing, SepFormer is able to obtain state-of-the art performance as shown in row Sepformer + DM of Table VI. We also show the case where we fine-tune a pretrained SepFormer model on LibriMix in the row SepFormer + DM PT+FT of Table VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Speech Enhancement</head><p>We trained the SepFormer for speech enhancement on the WHAM! and WHAMR! datasets for noisy an noisyreverberant mixtures with one speaker. The obtained enhancement performances in terms of SI-SNR, SDR, and PESQ <ref type="bibr" target="#b36">[38]</ref> are presented in <ref type="table" target="#tab_1">Table VII, and Table VIII</ref> respectively.</p><p>Note that in WHAM! dataset the model is trained to denoise and on the WHAMR! dataset the model is trained to both dereverberate and denoise at the same time. The SepFormer models are trained to maximize the output SI-SNR between the estimated and the ground truth signals in the time domain.</p><p>In addition to training SepFormer, we also trained a Bidirectional LSTM, CNNTransformer, CFDN models from Speechbrain <ref type="bibr" target="#b11">[12]</ref>, which are trained to minimize the Euclidean distance of the estimated magnitude spectrogram and the spectrogram of the clean signals. We observe that SepFormer is able to outperform these methods with a large margin in terms of SI-SNR, SDR and PESQ. H. Ablations on the type of Self-Attention</p><p>In <ref type="table" target="#tab_1">Table IX</ref>, we compare the WSJ0-2Mix test set performance of four different architectural choices for Full (regular), LongFormer, Linformer and Reformer self-attention previously described in Section III-D. We train all models using only speed augment (no dynamic mixing), using 4 second long training segments. The architectural choices in this ablation experiment given in <ref type="table" target="#tab_1">Table IX</ref> are summarized as follows:</p><p>? (second column, C=250v1) in the IntraTransformer, and</p><p>InterTransformer blocks, we use the same type of attention mechanism, indicated on the first column. The chunk size for the IntraTransformer is set to C = 250. ? (third column, C=250v2) in the IntraTransformer we use the self attention mechanism indicated on the first column, and in the InterTransformer we use the full attention mechanism. The chunk size for the IntraTransformer is set to C = 250.</p><p>? (fourth column, C=1000) in the IntraTransformer we use the self-attention mechanism indicated on the first column, and in the InterTransformer we use the full attention mechanism. The chunk size for the IntraTransformer is set to C = 1000. ? (fifth column, No Chunking) we do not apply chunking, and therefore only the IntraTransformer block is applied.</p><p>We have observed out-of-memory (O.O.M.) behavior on the test-set for the full self-attention due to long sequences, and therefore we denote O.O.M. for the corresponding entry.</p><p>We observe from <ref type="table" target="#tab_1">Table IX</ref> that the architectural choices adopted in the original SepFormer paper lead to the best overall performance in terms of SI-SNR and SDR improvement. We also show the memory usage and forward pass time (in CPU using Pytorch profiler <ref type="bibr" target="#b35">[37]</ref>) for some of the relevant models in <ref type="figure" target="#fig_4">Figure 6</ref> as well as SpeechBrain implementation of convtasnet <ref type="bibr">[22]</ref>, for inputs of varying lengths between 1 second and 8 seconds. The chunking/dual-path mechanism in the SepFormer architecture provides a significant reduction in memory demand compared to the architecture where no chunking is applied. We also see from <ref type="figure" target="#fig_4">Figure 6</ref> that using the Longformer or Reformer blocks on the whole sequence without applying chunking is an efficient alternative which also yields close performance as can be seen from <ref type="table" target="#tab_1">Table  IX</ref>. Furthermore from <ref type="figure" target="#fig_4">Figure 6</ref>, we observe that compared to convtasnet, Reformer without chunking results in almost equivalent memory usage, and faster forward pass time, while yielding slightly better performance, namely 16.7 dB SI-SNRi on the test set of WSJ0-2Mix, compared to 15.3 dB SI-SNRi obtained with convtasnet <ref type="bibr">[22]</ref>.</p><p>We also observe that the full attention is more efficient than using reformer inside the dual-path pipeline in terms of forward pass time and memory usage, and full-attention is not significantly more expensive than longformer in terms of memory. Another observation is the fact that Linformer does not yield competitive performance. We suspect that this stems from the fact that Linformer has a reduced modeling capacity since it projects the time dimension inside the attention mechanism to a lower dimensionality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have explored the application of Transformers for speech separation and speech enhancement, expanding upon our previous work where we introduced Sep-Former, an attention-only masking network that is able to achieve state-of-the-art performance on WSJ0-2/3 Mix speech separation datasets. Here, we expanded this work performing additional experiments for speech separation on LibriMix, WHAM!, WHAMR! datasets and speech enhancement on noisy WHAM! and WHAMR! datasets which respectively capture noisy and noisy-reverberant conditions.</p><p>As another major contribution we investigate the use of more efficient self-attention mechanisms such as Longformer, Linformer and Reformer. Our results suggest that Longformer and especially Reformer self-attention are suitable for speech separation applications and obtain an highly favourable tradeoff between performance and computational requirements. Namely, we observe that using Reformer self-attention without applying chunking results in being computationally more efficient than the popular convtasnet model, and consequently yielding 1.4 dB better performance than convtasnet in terms of SI-SNR improvement on the test of WSJ0-2Mix dataset.</p><p>Finally we would like to indicate that the the code for the training recipes for SepFormer on the WSJ0-2/3Mix, Libri2/3Mix, WHAM! and WHAMR! datasets are all available on Speechbrain toolkit <ref type="bibr" target="#b11">[12]</ref>, as well as the pretrained models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The encoder of a standard Transformer. Positional embeddings are added to the input sequence. Then, N encoding layers based on multihead self-attention, normalization, feed-forward transformations, and residual connections process it to generate an output sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>? 4 .Fig. 3 .</head><label>43</label><figDesc>After epoch 65 (after epoch 85 with h The dual-path processing pipeline employed in SepFormer. The input representationh is first of all chunked to get the chunksh 0 ,h 1 , . . . ,h 6 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer architecture f (.) that is used both in the IntraTransformer block and in the InterTransformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(Left) The training curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle &amp; Right) The comparison of forward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of memory usage and forward-pass time on the SepFormer architecture with several different self-attention types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I BEST</head><label>I</label><figDesc>RESULTS ON THE WSJ0-2MIX DATASET (TEST-SET). DM STANDS FOR DYNAMIC MIXING.</figDesc><table><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell># Param</cell><cell>Stride</cell></row><row><cell>Tasnet [21]</cell><cell>10.8</cell><cell>11.1</cell><cell>n.a.</cell><cell>20</cell></row><row><cell>SignPredictionNet [29]</cell><cell>15.3</cell><cell>15.6</cell><cell>55.2M</cell><cell>8</cell></row><row><cell>ConvTasnet [22]</cell><cell>15.3</cell><cell>15.6</cell><cell>5.1M</cell><cell>10</cell></row><row><cell>Two-Step CTN [30]</cell><cell>16.1</cell><cell>n.a.</cell><cell>8.6M</cell><cell>10</cell></row><row><cell>MGST [19]</cell><cell>17.0</cell><cell>17.3</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>DeepCASA [31]</cell><cell>17.7</cell><cell>18.0</cell><cell>12.8M</cell><cell>1</cell></row><row><cell>FurcaNeXt [32]</cell><cell>n.a.</cell><cell>18.4</cell><cell>51.4M</cell><cell>n.a.</cell></row><row><cell>DualPathRNN [7]</cell><cell>18.8</cell><cell>19.0</cell><cell>2.6M</cell><cell>1</cell></row><row><cell>sudo rm -rf [33]</cell><cell>18.9</cell><cell>n.a.</cell><cell>2.6M</cell><cell>10</cell></row><row><cell>VSUNOS [34]</cell><cell>20.1</cell><cell>20.4</cell><cell>7.5M</cell><cell>2</cell></row><row><cell>DPTNet* [8]</cell><cell>20.2</cell><cell>20.6</cell><cell>2.6M</cell><cell>1</cell></row><row><cell>Wavesplit** [25]</cell><cell>21.0</cell><cell>21.2</cell><cell>29M</cell><cell>1</cell></row><row><cell>Wavesplit** + DM [25]</cell><cell>22.2</cell><cell>22.3</cell><cell>29M</cell><cell>1</cell></row><row><cell>SepFormer</cell><cell>20.4</cell><cell>20.5</cell><cell>26M</cell><cell>8</cell></row><row><cell>SepFormer + DM</cell><cell>22.3</cell><cell>22.4</cell><cell>26M</cell><cell>8</cell></row><row><cell cols="4">*only SI-SNR and SDR (without improvement) are reported.</cell><cell></cell></row></table><note>**uses speaker-ids as additional info.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ABLATION</head><label>II</label><figDesc>OF THE SEPFORMER ON WSJ0-2MIX (VALIDATION SET).</figDesc><table><row><cell>SI-SNRi</cell><cell>N</cell><cell>N intra</cell><cell>N inter</cell><cell># Heads</cell><cell>DFF</cell><cell>PosEnc</cell><cell>DM</cell></row><row><cell>22.3</cell><cell>2</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>1024</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>20.5</cell><cell>2</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>1024</cell><cell>Yes</cell><cell>No</cell></row><row><cell>20.4</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>16</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>20.2</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.9</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.8</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.4</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>No</cell><cell>No</cell></row><row><cell>19.2</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.1</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.0</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>2048</cell><cell>No</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III BEST</head><label>III</label><figDesc>RESULTS ON THE WSJ0-3MIX DATASET.</figDesc><table><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell># Param</cell></row><row><cell>ConvTasnet [22]</cell><cell>12.7</cell><cell>13.1</cell><cell>5.1M</cell></row><row><cell>DualPathRNN [7]</cell><cell>14.7</cell><cell>n.a</cell><cell>2.6M</cell></row><row><cell>VSUNOS [34]</cell><cell>16.9</cell><cell>n.a</cell><cell>7.5M</cell></row><row><cell>Wavesplit [25]</cell><cell>17.3</cell><cell>17.6</cell><cell>29M</cell></row><row><cell>Wavesplit [25] + DM</cell><cell>17.8</cell><cell>18.1</cell><cell>29M</cell></row><row><cell>Sepformer</cell><cell>17.6</cell><cell>17.9</cell><cell>26M</cell></row><row><cell>Sepformer + DM</cell><cell>19.5</cell><cell>19.7</cell><cell>26M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV BEST</head><label>IV</label><figDesc>RESULTS ON THE WHAM DATASET.</figDesc><table><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell></row><row><cell>ConvTasnet</cell><cell>12.7</cell><cell>-</cell></row><row><cell>Learnable fbank</cell><cell>12.9</cell><cell>-</cell></row><row><cell>MGST [19]</cell><cell>13.1</cell><cell>-</cell></row><row><cell>Wavesplit + DM [25]</cell><cell>16.0</cell><cell>16.5</cell></row><row><cell>Sepformer + SpeedA.</cell><cell>16.3</cell><cell>16.7</cell></row><row><cell>Sepformer + DM</cell><cell>16.4</cell><cell>16.7</cell></row><row><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="2">BEST RESULTS ON THE WHAMR DATASET.</cell><cell></cell></row><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell></row><row><cell>ConvTasnet</cell><cell>8.3</cell><cell>-</cell></row><row><cell>BiLSTM Tasnet</cell><cell>9.2</cell><cell>-</cell></row><row><cell>Wavesplit + DM [25]</cell><cell>13.2</cell><cell>12.2</cell></row><row><cell>SepFormer + SpeedA.</cell><cell>13.7</cell><cell>12.7</cell></row><row><cell>SepFormer + DM</cell><cell>14.0</cell><cell>13.0</cell></row></table><note>E. Results on WHAM! and WHAMR! datasets WHAM! [10] and WHAMR!</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>ON LIBRI2MIX AND LIBRI3MIX DATASETS. SEPFORMER + PT INDICATES PRETRAINING ON WSJ0-2/3 MIX, OR WHAM! DATASET ACCORDING TO THE CASE. SEPFORMER + DM PT+FT IS THE CASE WHERE THE PRETRAINED SEPFORMER MODEL IS FINE TUNED ON LIBRIMIX.</figDesc><table><row><cell></cell><cell cols="2">Libri2Mix-Clean</cell><cell cols="2">Libri2Mix-Noisy</cell><cell cols="2">Libri3Mix-Clean</cell><cell cols="2">Libri3Mix-Noisy</cell></row><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell></row><row><cell>ConvTasnet</cell><cell>14.7</cell><cell>-</cell><cell>12.0</cell><cell>-</cell><cell>10.4</cell><cell>-</cell><cell>10.4</cell><cell>-</cell></row><row><cell>SepFormer PT</cell><cell>17.0</cell><cell>17.5</cell><cell>11.2</cell><cell>13.1</cell><cell>15.0</cell><cell>15.6</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>Wavesplit [25]</cell><cell>19.5</cell><cell>20.0</cell><cell>15.1</cell><cell>15.8</cell><cell>15.8</cell><cell>16.3</cell><cell>13.1</cell><cell>13.8</cell></row><row><cell>Wavesplit + DM [25]</cell><cell>20.5</cell><cell>20.0</cell><cell>15.2</cell><cell>15.9</cell><cell>17.5</cell><cell>18.0</cell><cell>13.4</cell><cell>14.1</cell></row><row><cell>SepFormer</cell><cell>19.2</cell><cell>19.4</cell><cell>14.9</cell><cell>15.4</cell><cell>16.9</cell><cell>17.3</cell><cell>14.3</cell><cell>14.8</cell></row><row><cell>SepFormer + DM</cell><cell>20.2</cell><cell>20.5</cell><cell>15.9</cell><cell>16.5</cell><cell>18.2</cell><cell>18.6</cell><cell>15.0</cell><cell>15.5</cell></row><row><cell>SepFormer + DM PT+FT</cell><cell>20.6</cell><cell>20.8</cell><cell>15.9</cell><cell>16.5</cell><cell>18.7</cell><cell>19.0</cell><cell>n.a.</cell><cell>n.a.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="4">SPEECH ENHANCEMENT RESULTS ON WHAM! DATASET (DENOISING)</cell></row><row><cell>Model</cell><cell>SI-SNR</cell><cell>SDR</cell><cell>PESQ</cell></row><row><cell>2DFCN</cell><cell>7.90</cell><cell>8.70</cell><cell>2.36</cell></row><row><cell>2DFCN-BLSTM</cell><cell>7.09</cell><cell>7.89</cell><cell>2.32</cell></row><row><cell>BLSTM</cell><cell>5.15</cell><cell>6.2</cell><cell>2.11</cell></row><row><cell>CNNTransformer</cell><cell>8.3</cell><cell>8.9</cell><cell>2.52</cell></row><row><cell>SepFormer</cell><cell>14.35</cell><cell>15.04</cell><cell>3.07</cell></row><row><cell></cell><cell>TABLE VIII</cell><cell></cell><cell></cell></row><row><cell cols="4">SPEECH ENHANCEMENT RESULTS ON WHAMR! DATASET (DENOISING +</cell></row><row><cell cols="2">DEREVERBERATION)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>SI-SNR</cell><cell>SDR</cell><cell>PESQ</cell></row><row><cell>2DFCN</cell><cell>6.84</cell><cell>7.75</cell><cell>2.18</cell></row><row><cell>2DFCN-BLSTM</cell><cell>5.87</cell><cell>6.82</cell><cell>2.15</cell></row><row><cell>BLSTM</cell><cell>5.58</cell><cell>6.49</cell><cell>2.11</cell></row><row><cell>CNNTransformer</cell><cell>7.38</cell><cell>8.21</cell><cell>2.27</cell></row><row><cell>SepFormer</cell><cell>10.58</cell><cell>12.29</cell><cell>2.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF DIFFERENT TYPES OF SELF-ATTENTION. THE PERFORMANCE NUMBERS ARE GIVEN ON THE TEST SET OF WSJ0-2MIX. THE TRAINING FOR ALL MODELS ARE DONE WITH SPEED AUGMENT ONLY USING 4 SECONDS LONG TRAINING SEGMENTS. THE COLUMN C = 250v1 INDICATES THE CASE WHERE CHUNKSIZE IS 250, INTRAT AND INTERT USE THE SAME ATTENTION MECHANISM INDICATED ON THE FIRST COLUMN. THE COLUMN C = 250v2 INDICATES THE CASE WHERE CHUNKSIZE IS 250, INTRAT USES THE ATTENTION MECHANISM INDICATED ON FIRST COLUMN AND INTERT USES THE FULL ATTENTION MECHANISM. THE COLUMN C = 1000 INDICATES THE CASE WHERE THE CHUNKSIZE IS 1000 AND INTRAT USES THE ATTENTION MECHANISM INDICATED ON THE FIRST COLUMN. THE COLUMN NO CHUNKING INDICATES THE CASE WHERE WE DO NOT HAVE INTERT</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>BLOCK.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">C=250v1</cell><cell cols="2">C=250v2</cell><cell cols="2">C=1000</cell><cell cols="2">No Chunking</cell></row><row><cell>Model</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell><cell>SI-SNRi</cell><cell>SDRi</cell></row><row><cell>Longformer</cell><cell>19.44</cell><cell>19.66</cell><cell>19.32</cell><cell>19.54</cell><cell>18.17</cell><cell>18.39</cell><cell>13.11</cell><cell>13.36</cell></row><row><cell>Linformer</cell><cell>2.75</cell><cell>3.01</cell><cell>6.04</cell><cell>6.30</cell><cell>4.74</cell><cell>5.02</cell><cell>6.39</cell><cell>6.68</cell></row><row><cell>Reformer</cell><cell>20.09</cell><cell>20.28</cell><cell>20.34</cell><cell>20.52</cell><cell>19.42</cell><cell>19.61</cell><cell>16.66</cell><cell>16.86</cell></row><row><cell>Transformer (SepFormer)</cell><cell>21.61</cell><cell>21.79</cell><cell>21.61</cell><cell>21.79</cell><cell>21.48</cell><cell>21.66</cell><cell>O.O.M.</cell><cell>O.O.M.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vision transformers with patch diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech 2020</title>
		<meeting>of Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2642" to="2646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wham!: Extending speech separation to noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Whamr!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SpeechBrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Don&apos;t shoot butterfly with rifles: Multi-channel continuous speech separation with early exit transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale group transformer for long sequence modeling in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<idno>abs/1707.04585</idno>
		<ptr target="http://arxiv.org/abs/1707.04585" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TasNet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno>abs/1711.00541</idno>
		<ptr target="http://arxiv.org/abs/1711.00541" />
	</analytic>
	<monogr>
		<title level="m">Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Furcanext: Endto-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling, 2020</title>
		<imprint>
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sudo rm -rf: Efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSP</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7164" to="7175" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussianweighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6649" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Profiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/tutorials/recipes/recipes/profiler.html" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
