<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odellia</forename><surname>Boni</surname></persName>
							<email>odelliab@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Feigenblat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
							<email>guylev@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Shmueli-Scheuer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sznajder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -AI ?</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present HOWSUMM, a novel large-scale dataset for the task of query-focused multidocument summarization (qMDS), which targets the use-case of generating actionable instructions from a set of sources. This use-case is different from the use-cases covered in existing multi-document summarization (MDS) datasets and is applicable to educational and industrial scenarios. We employed automatic methods, and leveraged statistics from existing human-crafted qMDS datasets, to create HOW-SUMM from wikiHow website articles and the sources they cite. We describe the creation of the dataset and discuss the unique features that distinguish it from other summarization corpora. Automatic and human evaluations of both extractive and abstractive summarization models on the dataset reveal that there is room for improvement. We propose that HOW-SUMM can be leveraged to advance summarization research. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corresponding author ? Work was done while author in IBM</head><p>Get a suede brush and make sure your shoes are dry. Suede has a soft grain that is best cleaned with a special brush, which you can purchase with a suede cleaning kit?..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Brush gently to remove dirt. Use the suede cleaning brush to lightly brush away dust or dirt that has accumulated on your shoes. Don't go back and forth: brush repeatedly towards the same direction. Once you get off this layer of grime, your shoes will already look newer.</p><p>2 Brush vigorously to remove scuff marks. When you scuff your shoes, the suede's grain can get pressed down in one direction? 3 Method 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing Water Stains</head><p>Wet the entire outside of the shoe. Apply a light coat of water with your brush. Water can discolor suede, but properly applied water can also remove those stains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Summarization has become a popular NLP task both in extractive and abstractive settings <ref type="bibr" target="#b16">(Nenkova and McKeown, 2011)</ref>. In recent years, progress has been very fast mainly due to the use of deep learning models starting with <ref type="bibr" target="#b15">Nallapati et al. (2016)</ref>. One major obstacle for improvement in summarization tasks, especially if supervised methods are considered, is the lack of large, high-quality datasets for training summarization systems. Since summaries are difficult and expensive to produce by experts, human crafted summarization datasets contain only a few dozen instances.</p><p>Therefore, the main technique for obtaining large summarization datasets has been to exploit texts written by humans as part of some editorial process that can be considered as summaries even if not created explicitly as such. For single document <ref type="figure">Figure 1</ref>: "How to Clean Suede Shoes" article with 2 methods. Method 1 with 3 steps, method 2 with 2 steps. summarization, the CNN/Dailymail dataset <ref type="bibr" target="#b15">(Nallapati et al., 2016)</ref> takes advantage of key points associated with news articles as part of the article publication process. Similarly, Gigaword considers article titles as summaries of the corresponding article's first sentences <ref type="bibr" target="#b21">(Rush et al., 2015)</ref>.</p><p>Clearly, multi-document summarization datasets (MDS), in which one's task is to summarize several source documents, are even harder to obtain.</p><p>To tackle this challenge, several recent works have created large-scale datasets for MDS by considering existing texts to be summaries of several other sources, as described in section 2. Inspired by these works, we created a dataset based on howto articles. How-to articles aim at providing information about how to do something, and can cover a variety of topics (e.g. education, medicine, or cleaning). Such articles range from a simple, ordered list of instructions to reach a goal, to less linear texts, outlining different ways to realize something, with arguments, conditions, etc.</p><p>wikiHow.com is a very popular website 1 consisting of how-to articles written by a community of authors on a wide variety of topics. The articles follow a well-defined structure that presents readers with instructions organized into methods and steps.</p><p>An article contains several methods, each comprised of a title and several steps, where a step contains a title and a text. In addition, each article cites a set of sources (i.e., references), which are used by the article authors to synthesize the wikiHow article. <ref type="figure">Figure 1</ref> shows a "How to Clean Suede Shoes" 2 article, with methods, and steps. Method 1 is titled "Cleaning Dirt and Scuff Marks" in which the first step is titled "Get a suede brush and make sure your shoes are dry".</p><p>We leverage wikiHow to create a query-focused MDS (qMDS) dataset, named HOWSUMM, by defining the references of a wikiHow article as the set of document sources to summarize, the titles as queries, and corresponding parts of the article content as target summaries. Specifically, we extract two types of target summaries from each article -summaries originating from steps (HOWSUMM-STEP), which are typically short, and those originating from methods (HOWSUMM-METHOD), which are longer. Consider the example in <ref type="figure">Figure 1</ref>. The text of step 2 of method 1 (in blue) is considered as the target-summary of the source documents when using as the query the title of the step and potentially also the titles of method 1 and the wikiHow article (the three titles in red boxes). Note that HOWSUMM-STEP includes five such target summaries from this specific article, corresponding to the 5 (3+2) steps. Similarly, we define the concatenation of the text of all 3 steps of method 1 as a target-summary of the source documents when using as the query the title of the method and potentially also the titles of all steps under method 1, and the wikiHow article title. HOWSUMM-METHOD includes two such target summaries from this specific article, corresponding to the two methods.</p><p>The resulting dataset is suitable for a use-case that is significantly different than the ones considered in other summarization datasets ; distilling how-to instructions from the sources differs from extracting the most salient relevant information as done in News summarization, generating wikipedia paragraphs or scientific abstracts. Such a use-case, we believe, is applicable in educational and industrial applications. For example, in the technical support domain, a major task is composing troubleshooting documents that aim at solving some problem. This requires extracting the relevant problem resolution alternatives from potential sources (including, webpages, knowledge bases, white papers, etc.), and creating a coherent, actionable, and easy to follow (e.g., by using lists) instructions.</p><p>It should be noted that two large-scale summarization datasets based on wikiHow articles already exist <ref type="bibr" target="#b6">(Koupaee and Wang, 2018)</ref>. One uses step title as the summary of the step's text, the other treats a concatenation of all titles in the article as summary of all other text in the article. These datasets target the task of generic summarization using only article's text, while we target queryfocused summarization using the article sources. Thus, HOWSUMM dataset is the first qMDS dataset based on wikiHow.</p><p>Our contributions are as follows: <ref type="formula">(1)</ref> We propose a new, large qMDS dataset, containing 11,121 long summaries, and 84,348 short summaries (available at https://ibm.biz/BdfhzH) based on the wiki-How website. This dataset focuses on new usecase of generating instructions from sources. <ref type="formula">(2)</ref> We analyze the particularities of HOWSUMM's target summaries and sources as opposed to some other MDS datasets. (3) We evaluate abstractive and extractive baselines on this new dataset. (4) Additionally, we propose a new methodology to ensure dataset quality leveraging previously existing human-crafted datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large MDS Datasets</head><p>In recent years, several large-scale datasets for the task of MDS emerged . Multi-News <ref type="bibr" target="#b2">(Fabbri et al., 2019)</ref> and WCEP (Gholipour Ghalandari et al., 2020) utilize professionally edited summaries of news events. As for other domains such human summaries are hard to obtain, an approach to consider existing texts as summaries (even if they were not written with that intention in mind) was used. WikiSum <ref type="bibr" target="#b11">(Liu et al., 2018)</ref> and auto-hMDS <ref type="bibr" target="#b23">(Zopf, 2018)</ref> use lead paragraphs of Wikipedia articles as summaries of their cited reference sources. Multi-XScience <ref type="bibr" target="#b13">(Lu et al., 2020)</ref> treats Related Work sections in scientific papers as summaries of cited articles' abstracts. For query focused setting, in the AQUAMUSE system <ref type="bibr" target="#b7">(Kulkarni et al., 2020)</ref>, the authors propose a general method for build-ing qMDS datasets based on question answering datasets where the answer serves as summary of relevant documents extracted from a corpus. They establish this approach by extracting answers that are Wikipedia paragraphs from Google Natural Questions <ref type="bibr" target="#b8">(Kwiatkowski et al., 2019)</ref> and documents from Common Crawl <ref type="bibr" target="#b19">(Raffel et al., 2020)</ref> corpus.</p><p>In some cases, such as Multi-XScience and Multi-news, the documents to summarize contain only cited sources. In other cases, those are extended by search results to increase coverage.</p><p>An evident challenge when creating summarization datasets in such a way, is ensuring that the information in the suggested summary indeed exists in the suggested source documents. To this end, several methods were suggested: from using only top search results on a certain topic (WikiSum), to training a relevance classifier (WCEP), ending with using text from ground truth summary in the search for sources(AQUAMUSE, auto-hmds).</p><p>3 The HOWSUMM Dataset 3.1 wikiHow.com wikiHow.com is a community website that contains more than 200K how-to articles. The average wikiHow article has been edited by 23 people and reviewed by 16 people 3 . Each article aims to provide a reliabe answer a reader's "question" ("How to do something"). The question is expressed in the article's title. The article itself begins with an introductory paragraph followed by a list of methods. Each method has a title, and is divided into several steps. Each step begins usually with a text in bold (which we treat as the step's title), followed by a regular text (which we consider as the step's text). The article ends with a list of references, related articles and Q&amp;A. In addition, each article includes metadata such as creation date, last edition date, # of authors, category (out of 20) and subcategories.</p><p>When writing a wikiHow article, one should follow some guidelines 4 . Specifically, two important aspects are emphasized, the content of the article, and its sources (i.e., references). The content should includes facts, ideas, methodologies, and examples. Short background information is also acknowledged. In order to create an accurate and authoritative article, authors are encouraged to use sources. The sources should be reliable, and should not include websites like Wikipedia, web forums, blogs, advertisements, ask.com, etc. However, to avoid legal issues, authors should use their own words to express information from the sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Instances Creation</head><p>Our goal is to use wikiHow article's content as query-focused summaries of the sources. For each wikiHow article, we consider several target summaries, one per method (HOWSUMM-METHOD), and one per step (HOWSUMM-STEP). We exclude the option of using entire wikiHow articles as target summaries because of the complex structure and length of target summaries in this case.</p><p>We crawled wikiHow for its articles in English, and, using boilerpy3 5 , extracted their structured content including titles, and their sources URLs. We use regular expressions to remove misleading terms from article title such as "With Pictures" or "In 10 Steps". We divide each step text into a title (bold text at the start) and a target summary (the plain text that follows). We omitted steps where a title or a target summary could not be found. To create target summaries for a method, we concatenate target summaries of its steps.</p><p>Each instance of the dataset contains the sources URLs, the target summary and corresponding article title, method title and step titles (single for HOWSUMM-STEP, multiple for HOWSUMM-METHOD). This setting allows to experiment with several titles combinations as query. For example, for a step, the most obvious choice for a query is the step title, but the combination of the article title, the respective method title and the step title can also serve as the query. Similarly, for methods, in addition to method title, the combination of method title and the titles of the steps comprising the method can also be used as a query.</p><p>To overcome the problem of sources changing over time, we provide archived sources URLs obtained by using the article's "published date" in Archive.org API request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Instances Filtering</head><p>For some articles the content was updated and is no longer consistent with the sources or the sources are only partially accessible. Therefore, we need further filtering to get high-quality instances, suitable for summarization. To this end, we employ metrics of coverage, density, and novel n-grams. These metrics were first proposed in <ref type="bibr" target="#b5">Grusky et al. (2018)</ref> to characterise the level of extractiveness in generic single document summarization datasets. Coverage evaluates how much of the summary text originates from the source, while density indicates the length of shared texts between the summary and the source. Novel n-grams measures level of variance between the summary and the source's text. <ref type="bibr" target="#b2">Fabbri et al. (2019)</ref> applied the same metrics for generic MDS datasets by concatenating source documents. Since these metrics rely solely on common text fragments between a summary and the corresponding sources (and on summary length), we can adopt them also for query-focused summarization.</p><p>We first studied these metrics for existing, human created qMDS datasets, namely, <ref type="bibr">DUC 2005</ref><ref type="bibr" target="#b18">-2007</ref><ref type="bibr" target="#b18">(Over et al., 2007</ref> and TD-QFS <ref type="bibr" target="#b0">(Baumel et al., 2016)</ref>. Both datasets introduce multiple summaries per query, and we treated each of these summaries as an additional instance during metrics calculation. Thus, we ended up with 580 and 120 data points for DUC 2005-2007 and TD-QFS, respectively. Figures 2(a) and 2(b) show density and coverage distributions, and table 1 presents novel ngrams percentage across the human created qMDS datasets. Both datasets have similarly high coverage values, while TD-QFS has higher density and lower percentage of novel n-grams, implying a more extractive nature of its summaries.</p><p>Assuming these human created datasets are representative of high quality instances, we use their metrics bounds as thresholds for filtering instances for the HOWSUMM dataset. Specifically, we require that each instance from our dataset will attain coverage &gt; 0.9, density &gt; 2, and novel bi-grams &lt; 0.6. This filtering process yielded 84,348 instances for HOWSUMM-STEP, 11,121 instances for HOWSUMM-METHOD. The average number of target summaries stemming from the same wiki-How article in HOWSUMM-STEP and HOWSUMM-METHOD is 3.7 and 1.7 , respectively.</p><p>Figures 2(c) and 2(d) and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Human Evaluation</head><p>We performed a human evaluation study to validate the quality of the HOWSUMM dataset. First, one expert identified for each sentence in a target summary, a similar sentence in the sources, where similar is defined as "sentences conveying the same idea not necessarily in the same words". Then, a second expert reviewed the resulting pairs. Only pairs confirmed by the second expert were considered for the coverage calculation below. In total, 31 HOWSUMM-STEP summaries and 15 HOWSUMM-METHOD summaries (with a total of 58 steps) were annotated. 88% of the pairs identified by the first expert were verified by the second one. We define coverage of a target summary as the number of sentences in the summary having similar sentences in the sources, divided by the number of summary sentences. For both HOWSUMM-STEP and HOWSUMM-METHOD the average coverage was 0.62 , similar to that reported for AQUAMUSE, another automatic qMDS dataset.  HOWSUMM was created automatically. Clearly, HOWSUMM is the largest dataset. Unlike the human created datasets which contain many short sources per instance, the sources in the automatically created datasets are fewer but much longer. Next, we look deeper into these sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dataset Statistics and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Sources</head><p>As described in the authoring guidelines, the sources of a wikiHow article should support the article content and increase its authority and credibility. Since the sources describe procedures, methodologies and explanations, they are likely to be rich in headers to organize the concepts, Q&amp;A content that provides authoritative information, in addition to lists, which are often used to record instructions 6 . To confirm this assumption, we randomly selected 10K sources from our dataset, and, for each one, extracted from its HTML format both Q&amp;As and lists (as described in <ref type="bibr" target="#b9">(Lev et al., 2020)</ref>). The average number of Q&amp;As for a source, and the average number of lists, are 2.3, and 10.3, respectively.</p><p>We also counted HTML headers per source, and number of different headers levels 7 used in sources that contain headers. The average number of headers for a source, and the average number of header levels, are 3.1, and 3.3, respectively.</p><p>As a comparison, we applied the same analysis to sources from other summarization datasets, Newsroom <ref type="bibr" target="#b5">(Grusky et al., 2018)</ref> and auto-hMDS <ref type="bibr" target="#b23">(Zopf, 2018)</ref> where the source documents are news articles and Wikipedia references, respectively. Both datasets provide URLs of their sources, so sources content and their HTML format can be reconstructed. News sources have an average of 0.5 Q&amp;As , 0.2 lists, 0.3 headers per page, and use on average 1.1 different header levels. Wikipedia cited references contain an average of 0.9 Q&amp;As , 1.5 lists, 7.4 headers per page, and 1.6 different header levels. This confirms that HOWSUMM sources are significantly richer in content organization aids. 6 https://pressbooks.bccampus.ca/ technicalwriting/chapter/lists/ 7 HTML supports 6 levels of headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Summaries</head><p>Summaries in our dataset are how-to guides. Hence, we expect them to include actionable statements.</p><p>In addition, following authoring guidelines, we expect summaries to include many examples, which demonstrate readers how to apply a certain guideline. To capture the actionable aspects of the summaries, we used a rule-based approach based on POS analysis 8 . In total, 35.4% of the sentences in our dataset summaries are actionable sentences. To capture the examples, we matched a list of phrases which are synonyms of "for example". Here, 10.3% of the summaries include at least one example. We run the same analysis on two other summarization datasets having summary lengths similar to ours. News summaries from Multi-News <ref type="table">(Fabbri</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>All wikiHow articles producing HOWSUMM were split into training, validation and testing sets (80%, 10%, 10%, respectively) inducing a similar split to HOWSUMM-STEP and HOWSUMM-METHOD. In the following section, we describe the performance of different summarization models on the testing sets. Based on the average length of steps and methods texts, we defined length limits to be 90 and 500 words, respectively. As evaluation measures we use ROUGE scores <ref type="bibr" target="#b10">(Lin, 2004)</ref>  <ref type="bibr">9</ref> . We follow the recommendation in <ref type="bibr" target="#b4">(Gholipour Ghalandari et al., 2020)</ref> to evaluate models with a dynamic output length on full-length outputs , while forcing controlled length models (e.g. extractive) to return untruncated sentences up to the given length limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>We utilized different extractive baselines to serve as lower and upper bounds to models performance, as well as some state-of-the-art (SOTA) models. Lower bound. (i) RANDOM: random sources sentences are chosen for the output until length limit is reached. (ii) GREEDYREL: the source sentences with the highest Jaccard similarity to the query's bag-of-words representation comprise the output. Upper bound. We use two oracles. In both, each target summary sentence is matched with the most similar source sentence (from unmatched sentences pool). The resulting set of matched source sentences serves as the output. Note that the oracles do not impose a length limit. ORACLE-BOW where we use Jaccard smilarity between sentences' bag-of-words representations, and ORACLE-BERT where we apply cosine similarity between sentences' BERT embeddings 10 . SOTA-extractive. (i) LEXRANK: an algorithm that computes graph-based centrality score of sources sentences <ref type="bibr" target="#b1">(Erkan and Radev, 2004)</ref>. A query-bias of 0.7 adjusts the algorithm to queryfocused summarization <ref type="bibr" target="#b17">(Otterbacher et al., 2005)</ref>. (ii) CES: an unsupervised model optimizing a set of features of the selected output using crossentropy method <ref type="bibr" target="#b3">(Feigenblat et al., 2017)</ref>. SOTA-abstractive. We use models derived from HIERSUMM <ref type="bibr" target="#b12">(Liu and Lapata, 2019)</ref>, a neural encoder-decoder model which aims to capture interparagraph relations by leveraging a hierarchical Transformer <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> architecture. The input for this model are the query and several dozens top ranked paragraphs taken from the source documents. The authors in <ref type="bibr" target="#b12">(Liu and Lapata, 2019</ref>) split source documents into paragraphs using line-breaks, and train a neural regression model to rank them, using their ROUGE-2 recall against the target summary as ground-truth scores. As this paragraph ranker is not publicly available, we used two alternatives: a ranker which uses a BM25 algorithm <ref type="bibr" target="#b20">(Robertson and Zaragoza, 2009)</ref> to rank the source paragraphs given a query; and an oracle ranker which ranks the paragraphs according to their ROUGE-2 recall against the target summary. The corresponding summarization models are BM25-HIERSUMM and ORACLE-HIERSUMM, respectively.</p><p>In each experiment, the HIERSUMM model was initialized with the parameters of the publiclyavailable pretrained model of <ref type="bibr" target="#b12">(Liu and Lapata, 2019)</ref>.We used the default training hyperparameters and fine-tuned our models for 150,000 steps. We utilize those abstractive models only for HOWSUMM-STEP, as HOWSUMM-METHOD requires producing quite long outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query Choice</head><p>As mentioned before, HOWSUMM allows to define several queries for the same target text. In table 3 we show ROUGE-1 Recall results of various models ran with different query choices for HOWSUMM-STEP and HOWSUMM-METHOD. We see that adding titles of lower level (steps titles for HOWSUMM-METHOD) improves output score while adding title of higher level (article title for HOWSUMM-METHOD, article and method titles for HOWSUMM-STEP) has no or negative impact. A possible explanation is that lower level titles act as a detailed description of required content and help focus the output, while higher level titles are more general and may cause output to drift. Therefore, we use step title as query for HOWSUMM-STEP, and method title together with its comprising steps titles as query for HOWSUMM-METHOD. Appendix A contains examples of models output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic Evaluation</head><p>Table 4 details ROUGE scores of the different models outputs. As expected, selecting random sentences yields the lowest scores. GREEDYREL's simple model that prioritizes similar-to-query sentences improves significantly over the random model. LEXRANK algorithm performs even better since it considers the pool of all sentences when selecting the central ones, thus avoiding redundancy. CES outperforms LexRank as it optimizes also additional features such as output focus. The big gap between extractive models' and extractive oracles' scores implies that there is room for improvement. BERT-based oracle's output is characterized by higher recall but lower precision than that of BOWbased oracle. The reason for that is that while the two oracles' outputs have the same number of sentences, BERT-based oracle tends to select longer sentences (by approximately 20%). This is because BERT embeddings are richer for longer sentences, while Jaccard similarity penalizes sentences pairs of different lengths.</p><p>The HIERSUMM model, with a BM25 paragraph ranker, exhibits scores similar to or lower than  the extractive models. With Oracle ranker, HI-ERSUMM achieves scores much higher than the extractive models, but still lower than those of the extractive oracles. HIERSUMM's results manifest the crucial role of ranking source paragraphs in this model, and show that once this ranking is adequate, HIERSUMM outperforms SOTA extractive models. Note that HIERSUMM's outputs are well under the length limit, with BM25-HIERSUMM creating shorter outputs than ORACLE-HIERSUMM (average of 46.3 words opposed to 64.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Manual Evaluation</head><p>In addition, we evaluated models outputs manually for randomly chosen 20 instances from HOWSUMM-STEP in the following way. First, an expert provided a set of questions for each instance (based on target text). Then, for every instance and for every model, 3 annotators determined whether the questions corresponding to the instance can be answered using the model's output. Majority vote determined answerability of each question. The ratio of answerable questions is the output's information score. The annotators also rated each output on a scale of 1(poor) to 5(good) for several linguistic quality aspects using a set of questions developed in DUC <ref type="bibr" target="#b18">(Over et al., 2007)</ref>: Is the text ungrammatical? Does it contain redundant information? Are the references to different entities clear? Is the text coherent? An average of annotators ratings determines output score for each aspect. Human evaluation results appear in table 5. Agreement between annotators on information questions, measured by Fleiss's Kappa, was 0.49, indicating a moderate agreement. <ref type="table" target="#tab_7">Table 5</ref> reveals that abstractive model outputs are more fluent compared to extractive models as higher focus, coherence and referential clarity indicate, but suffer from repetitions and low informativeness. LEXRANK and CES outputs achieve similar scores in most aspects. LEXRANK outputs were judged to be more informative, but with higher redundancy than those of CES. All models achieved low coherence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Actionability Effect</head><p>We observed in section 3.5.2 that HOWSUMM target summaries contain many actionable sentences. We tried to utilize this feature; In GREEDYREL model, we experimented with several ways to prioritize actionable sentences when creating outputs. However, all experiments resulted in a decrease of ROUGE scores. To further investigate the issue we analyzed actionability of similar sentences in target summary and sources. To determine such pairs, we used Jaccard similarity of sentences' bag-of-words representation. Out of 2400 pairs of such similar sentences, in 504(21%) target sentence was actionable, while matching source sentence was not. For example, source sentence "People with any of these symptoms should see their physician" became "See an eye doctor for certain symptoms." in target text. This indicates that humans do not consider actionability of sentence during content selection, but rather transform some selected sentences into actionable ones during summary construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Examples effect</head><p>Another observation in section 3.5.2 was that HOW-SUMM target summaries are rich with examples. In order to study the potential effect of this phenomenon on extractive models performance we group HOWSUMM-STEP instances by their article's category. In <ref type="figure" target="#fig_2">figure 3</ref> we show average ROUGE-2 F-1 score of ORACLE-BOW , representing the best extractive output, on these categories. Clearly, the summarization "potential" varies between different categories. For example, "Computers and Electronics" has an average score of 26.85, which is 50% higher than the average score on HOWSUMM-STEP. On the other hand, "Relationships" or "Youth" categories did worse than this total average score. This variation is attributed to authors' original sentences in the target summaries, as these match source sentences poorly. Since, according to wikiHow guidelines, examples should   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sources Structure Effect</head><p>The first step in all extractive models is segmenting the sources text into sentences (using Spacy package). Similarly, for abstractive models we split sources' text into paragraphs (using line breaks). These preliminary segmentations of HOWSUMM sources ignore the fact that they contain many headers of different levels, lists, and Q&amp;As (see section 3.5.1).</p><p>Indeed, the outputs of the extractive models, contain segmentation errors. For example, both header and text are included in "Raw eggs: There is a risk of food poisoning from raw or partly cooked eggs". Another example is mashing up all list items in "Examples of compelling ties include: A residence abroad which you do not intend to abandon Your family relationships Your long term plans". Such errors are also reflected in the Grammaticality score in table 5.</p><p>Using tools like in <ref type="bibr" target="#b14">(Manabe and Tajima, 2015)</ref> along with strategies to deal with lists, may improve segmentation and headers handling. Thus, allowing to provide higher-quality input to the different models and hopefully better their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduced a large-scale qMDS summarization dataset based on wikiHow articles and stressed its uniqueness with regard to other datasets. In addition, we present ROUGE evaluation results of several extractive and abstractive models on this dataset. The large gap between results of SOTA models and oracles suggests there is room for models improvement using various features of the dataset. Human evaluation hints on the need to improve coherence in the automatic outputs. We believe that this dataset will promote the development of new qMDS algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Output Examples</head><p>We show in figure 4 some examples of different summarizers outputs, generated for some instances of HOWSUMM-STEP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to Choose a Medical Alert System for Seniors</head><p>Method "Deciding What Level of Monitoring Your Senior Needs"</p><p>Step "Consider automatic detection features" Target summary Many systems offer a simple push button that allows seniors to call for help whenever they need it. There are also newer products on the market, however, that offer a higher level of protection. Many companies now offer units with automatic fall detection capabilities, which means your senior will get emergency help even if she is unconscious or otherwise unable to press the emergency button. Some units are also capable of automatically calling for emergency help if they detect fire or carbon monoxide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarizers output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HierSumm-BM25</head><p>Look for automatic features like push-button and automatic fall, emergency call center assistance, and medical monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HierSumm-Oracle</head><p>Medical alert systems allow senior citizens to live on their own even, since they always have access to help whenever they need it. For seniors who worry that they may need assistance if they fall down or meet an unexpected emergency, it provides an easy way to get access to help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CES</head><p>Some new technologies are addressing the issue by detecting falls automatically.Compared to Monitored Systems If budget allows, it is worthwhile to consider monitored medical alert systems . The system should have the features the senior needs. For an extra monthly fee, customers can get an add-on falldetection device that will automatically call for help when needed, as long as you have a landline. I would recommend choosing a system that has the ability to upgrade features in the future and that does not have a high cancellation policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LexRank</head><p>Some new technologies are addressing the issue by detecting falls automatically. To appeal to younger seniors and people with active lifestyles, several companies now offer more advanced systems featuring varying combinations of push-button and automatic fall detection, emergency call center assistance, and medical monitoring.A similar product is a pendant-style help button that can automatically place a call for help if it detects a fall. The system should have the features the senior needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to Adopt As a Single Woman</head><p>Method "Submitting Your Application"</p><p>Step "Meet with your caseworker." Target summary Once your application is accepted by the agency, you'll be assigned a caseworker who will be your point person throughout the adoption process. They will describe to you the next steps of the process. You need to maintain a good relationship with your caseworker. They are on your side, and want to help you adopt a child. They can answer any questions you have, as well as give you tips and advice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarizers output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HierSumm-BM25</head><p>Once you have completed your application, you will need to meet with a caseworker who will help you through the adoption process. In some caseworkers, family workers who work with the children in care will work with the children in care. you will also need to take time during the application process to get to know them as they will be your guide throughout your adoption journey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HierSumm-Oracle</head><p>Once you have completed your application, you will need to meet with your caseworker who will help you through the rest of the adoption process. In child welfare, generally there are two types of caseworkers : family workers who work with families such as yours and child workers who work with children. It is important to develop and maintain a good relationship with your caseworker, so take time during the application process to get to know them as they will be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CES</head><p>Complete Your Application to Adopt This is where the official paperwork begins and where you will meet the caseworker who will help you through the rest of the adoption process.It helps your caseworker understand your family better and assists them with writing your home study.3. Single parent adoptions are also one of the groups that adopts the most special needs children who need families. In this section you will find resources on single-parent adoption from the Child Welfare Information Gateway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LexRank</head><p>Complete Your Application to Adopt This is where the official paperwork begins and where you will meet the caseworker who will help you through the rest of the adoption process.It helps your caseworker understand your family better and assists them with writing your home study. It's important to develop and maintain a good relationship with your caseworker, so take time during the application process to get to know them as they will be your guide throughout your adoption journey. <ref type="figure">Figure 4</ref>: Ground-truth with corresponding automated summaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Coverage and density in qMDS datasets. "c": average compression ratio; "n": number of instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>et al., 2019) include only a small fraction of actionable sentences (5.12%), and 4.19% contain examples. Wikipedia paragraphs which serve as target summaries in auto-hMDS (Zopf, 2018) contain 18% of actionable sentences, and only 1.7% of them include examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples percentage and ROUGE-2 F-1 of ORACLE-BOW per category (HOWSUMM-STEP). not be copied from article's references, we expect categories with low examples percentage to perform better, and vice-versa. Indeed, the Pearson correlation between category scores and examples percentage (also depicted in figure 3) is -0.64 showing a moderate negative correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table 1 contain the metrics for the resulting HOWSUMM dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">uni-grams bi-grams tri-grams</cell></row><row><cell>DUC 2005-2007 (Human-Crafted)</cell><cell>10.1</cell><cell>46.7</cell><cell>70.9</cell></row><row><cell>TD-QFS (Human-Crafted)</cell><cell>2.8</cell><cell>13.8</cell><cell>26.6</cell></row><row><cell>HOWSUMM-STEP</cell><cell>9.8</cell><cell>47.3</cell><cell>78.9</cell></row><row><cell>HOWSUMM-METHOD</cell><cell>15.2</cell><cell>52.7</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Novel n-grams percentage in qMDS datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 compares</head><label>2</label><figDesc>HOWSUMM with other qMDS datasets. Notice that, in addition toDUC 2005DUC  - 2007 and TD-QFS which are human annotated datasets, we also report on AQUAMUSE which like</figDesc><table><row><cell>Dataset</cell><cell cols="2">Size #Sources (mean)</cell><cell></cell><cell>Source</cell><cell></cell><cell></cell><cell>Summary</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">#words #sentences vocab size #words #sentences vocab size</cell></row><row><cell>DUC 2005-2007 (HUMAN-CRAFTED)*</cell><cell>145</cell><cell>27.37</cell><cell>-</cell><cell>26.41</cell><cell>-</cell><cell>250</cell><cell>-</cell><cell>-</cell></row><row><cell>TD-QFS (HUMAN-CRAFTED)*</cell><cell>40</cell><cell>190.15</cell><cell>98.67</cell><cell>8.14</cell><cell>-</cell><cell>117.48</cell><cell>-</cell><cell>-</cell></row><row><cell>AQUAMUSE*</cell><cell>5519</cell><cell>6</cell><cell>1597.1</cell><cell>66.4</cell><cell>-</cell><cell>105.9</cell><cell>3.8</cell><cell>-</cell></row><row><cell>HOWSUMM-STEP</cell><cell>84348</cell><cell>9.98</cell><cell>1357.37</cell><cell>66.47</cell><cell>1175198</cell><cell>98.98</cell><cell>5.23</cell><cell>70153</cell></row><row><cell>HOWSUMM-METHOD</cell><cell>11121</cell><cell>11.19</cell><cell>1455.52</cell><cell>71</cell><cell>653509</cell><cell>539.11</cell><cell>31.33</cell><cell>58967</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Descriptive statistics of HOWSUMM and existing qMDS datasets. *As reported in the original papers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>ROUGE-1 Recall results of HOWSUMM-STEP and HOWSUMM-METHOD for different query choices.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>ROUGE results of HOWSUMM-STEP and HOWSUMM-METHOD for different models.</figDesc><table><row><cell>Model</cell><cell cols="6">Information Grammaticality Non-redundancy Referential Clarity Focus Coherence</cell></row><row><cell>LEXRANK</cell><cell>31.08%</cell><cell>3.93</cell><cell>3.65</cell><cell>4.15</cell><cell>3.45</cell><cell>2.65</cell></row><row><cell>CES</cell><cell>27.58%</cell><cell>3.70</cell><cell>4.25</cell><cell>4.43</cell><cell>3.37</cell><cell>2.68</cell></row><row><cell>BM25-HIERSUMM</cell><cell>25.44%</cell><cell>3.88</cell><cell>3.00</cell><cell>4.53</cell><cell>4.05</cell><cell>3.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation of a sample of HOWSUMM-STEP for different models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to the site itself, Comscore ranks wikiHow in the top 150 most visited publishers in the world.2 https://www.wikihow.com/ Clean-Suede-Shoes</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.wikihow.com/wikiHow: About-wikiHow 4 https://www.wikihow.com/ Write-a-New-Article-on-wikiHow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://pypi.org/project/boilerpy3/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/ vaibhavsanjaylalka/Action-Item-Detection 9 with flags -f A -a -m -n 4 -t 0 -2 4 -c 95 -u -r 1000 -p 0.5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://huggingface.co/sentence-transformers/bert-basenli-mean-tokens</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic concentration in query focused summarization datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the 30th Conference of the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2573" to="2579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised queryfocused multi-document summarization using the cross entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Roitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odellia</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080690</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="961" to="964" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large-scale multi-document summarization dataset from the Wikipedia current events portal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Demian Gholipour Ghalandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ifrim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1302" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1065</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wikihow: A large scale text summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahnaz</forename><surname>Koupaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1810.09305</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aquamuse: Automatically generating datasets for query-based multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheide</forename><surname>Chammas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achiya</forename><surname>Jerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno>abs/2009.01460</idno>
		<title level="m">orgfaq: A new dataset and analysis on organizational faqs and user questions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-xscience: A large-scale dataset for extreme multi-document summarization of scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<idno>abs/2010.14235</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Extracting logical hierarchical structure of html documents based on headings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Manabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keishi</forename><surname>Tajima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1606" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Aglar Gu? ?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000015</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using random walks for question-focused sentence retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?ne?</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Duc in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2007.01.019</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Pergamon Press, Inc</publisher>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>text transformer</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-hMDS: Automatic construction of a large heterogeneous multilingual multidocument summarization corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
