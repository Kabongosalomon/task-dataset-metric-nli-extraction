<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-world Video Deblurring: A Benchmark Dataset and An Efficient Recurrent Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Gao</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
							<email>yqzheng@ai.u-tokyo.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imari</forename><surname>Sato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tokyo Research Center</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Tokyo Research Center</orgName>
								<orgName type="department" key="dep2">Sato National Institute of Informatics</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country>Japan Imari, Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-world Video Deblurring: A Benchmark Dataset and An Efficient Recurrent Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video deblurring ? Network efficiency ? RNN ? Real-world dataset ? Beam-splitter acquisition system</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world video deblurring in real time still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue that needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a coaxis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https://github.com/zzh-tech/ESTRNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, video recording usually suffers from the quality issues caused by motion blur. This is especially true in poorly illuminated environment, where one has to lengthen the exposure time for sufficient brightness. A great variety of video deblurring methods have been proposed, which have to deal with two competing goals, i.e., to improve the deblurring quality and to reduce the computational cost. Reducing computational cost while ensuring deblurring quality is of critical importance for low-power mobile devices, such as smartphones.</p><p>To properly make use of the spatio-temporal correlation of the video signal is the key to achieve better performance on video deblurring. Deep learningbased methods have brought great advances in the field of video deblurring. The CNN-based methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref> make an inference of the deblurred frame by stacking neighboring frames with current frame as input to the CNN framework. The RNN-based methods, like <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>, employ recurrent neural network architecture to transfer the effective information frame by frame for deblurring. However, how to utilize spatio-temporal dependency of video for deblurring more efficiently still needs to be explored. The CNN-based methods are usually cumbersome in dealing with spatio-temporal dependency of concatenated neighboring frames, and the existing RNN-based methods have limited capacity to transfer the effective information temporally. Thus, they either suffer from huge computational cost, or ineffectiveness of deblurring.  <ref type="figure">Fig. 1</ref> A comparison of network efficiency on video deblurring. SRN <ref type="bibr" target="#b40">[41]</ref>, DeepDeblur <ref type="bibr" target="#b25">[26]</ref> are methods for image deblurring, and STRCNN <ref type="bibr" target="#b15">[16]</ref>, DBN <ref type="bibr" target="#b36">[37]</ref>, IFI-RNN <ref type="bibr" target="#b26">[27]</ref> are methods for video deblurring. (a) shows the computational cost required for processing a frame of 720P(1280 ? 720) video and the corresponding performance of each model on GOPRO <ref type="bibr" target="#b25">[26]</ref> dataset in terms of GMACs and PSNR, respectively. (b) shows the deblurred image generated by SoTA video deblurring methods and ours.</p><p>The quality of the benchmark dataset is also critical to the deblurring performance of the data-driven methods. It is worth noting that all mainstream deblurring datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> are synthesized by averaging highfps videos. Unnatural data distributions and artifacts in synthetic blur will inevitably lead to poor generalization of the models that are trained on synthetic datasets, especially for real blurry images/videos. The community is currently in urgent need of a real-world deblurring dataset. However, obtaining the corresponding ground truth for the captured blurry image/video remains an unsolved challenge.</p><p>To optimize the trade-off between computational cost and deblurring performance, we propose an efficient spatio-temporal recurrent neural network, denoted by ESTRNN. In this work, we mainly focus on the network efficiency, which directly reflects on the deblurring performance of the method with limited computational resources. To make a more computationally efficient video deblurring method, we develop our method through amelioration of basic RNN architecture from three aspects: (i) In temporal domain, the high-level features generated by RNN cell are more informative, which are more suitable for temporal feature fusion (see <ref type="figure" target="#fig_1">Fig. 2</ref>) than using channel-concatenated neighboring frames as input. Reusing high-level features of neighboring frames can also help to improve the overall network efficiency; (ii) It is obvious that not all high-level features from neighboring frames are beneficial to deblurring of the current frame. Thus, it is worth designing an attention module <ref type="bibr" target="#b0">[1]</ref> that allows the method to focus on more informative parts of the high-level features from other frames. To this end, we propose a novel global spatio-temporal atten-tion module (see Sec. 3.3) for efficient temporal feature fusion; (iii) Regarding spatial domain, how to extract the spatial features from the current frame will affect the quality of information transmitted in temporal domain. In other words, well generated spatial features of each frame are a prerequisite for ensuring good temporal feature fusion. Therefore, we integrate the residual dense block <ref type="bibr" target="#b50">[51]</ref> as backbone into RNN cell to construct RDB cell (see Sec. 3.2). The highlevel hierarchical features generated by RDB cell are more computationally efficient with richer spatial information. With the above improvement, the proposed method can achieve better performance with less computational cost against SoTA deblurring methods, as illustrated in <ref type="figure">Fig. 1(a)</ref>. Due to making full use of spatiotemporal dependency of video signal, our method is exceptionally good at restoring high-frequency details of the blurry frame compared with SoTA video deblurring methods, as shown in <ref type="figure">Fig. 1(b)</ref>.</p><p>Furthermore, to get out of the predicament of lacking real-world deblurring dataset, we design a co-axis beam splitter acquisition system for data sample collection. Two cameras with distinct exposure schemes are co-axially aligned via a beam splitter to capture both blurry and sharp video of the same scene simultaneously. Empowered by the proposed beam-splitter acquisition system, we contribute the first real-world video deblurring dataset (BSD) to this field. BSD includes three different blur intensity configurations in dynamic scenes and contains various ego-motion and object-motion types. The advantages of the collected real-world dataset are verified by cross-validation experiments with synthetic datasets. Specifically, models trained on BSD can obtain migratory deblurring ca-pabilities for other datasets; in contrast, the models trained on synthetic datasets have very limited generality, even on other synthetic datasets.</p><p>Our major contributions can be summarized as follows:</p><p>-To better exploit the spatio-temporal correlation of video signal for deblurring, we propose a novel RNN-based model that uses a recurrent cell based on residual dense blocks and a global spatiotemporal attention module to generate hierarchical spatial features and fuse the high-level features of neighboring frames, respectively. -We design a beam splitter acquisition system to capture realistic blurry/sharp video pairs. To the best of our knowledge, the proposed BSD is the first realworld video deblurring dataset. -The experimental results demonstrate that our method achieves better deblurring performance both quantitatively and qualitatively than SoTA video deblurring methods with less computational cost. -We have thoroughly done cross-validation between the proposed BSD and the synthetic datasets. The experimental results demonstrate that real-world dataset BSD outperforms the synthetic datasets in terms of generality of the trained models.</p><p>A short version of this paper was published in <ref type="bibr" target="#b52">[53]</ref>. Compared to <ref type="bibr" target="#b52">[53]</ref>, there are two main extensions: (i) We upgraded the beam-splitter acquisition system with center-aligned scheme and collected a larger real-world dataset for experiments with more scenes, more motion patterns and more blur intensity settings; (ii) We conducted cross-validation experiments between our realworld dataset BSD and the synthetic datasets, including existing dataset synthesized from 240 fps videos and a self-made high-fps dataset synthesized from 2000 fps videos, to validate the effectiveness of BSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Deblurring</head><p>In recent years, video deblurring techniques become significant for daily life media editing and for advanced processing such as SLAM <ref type="bibr" target="#b19">[20]</ref>, 3D reconstruction <ref type="bibr" target="#b20">[21]</ref> and visual tracking <ref type="bibr" target="#b44">[45]</ref>. Research focus starts to shift from early single non-blind image deblurring <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b57">58]</ref> and single blind image deblurring <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref> to the more challenging tasks such as blur decomposition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref>, and video deblurring <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Typically, the blur in a video has varying size and magnitude at different positions in each frame. In the early work of video deblurring, <ref type="bibr" target="#b1">[2]</ref> attempts to automatically segment moving blurred objects from the background and assumes a uniform blur model for them. Then, in view of different kinds of blur in different regions of an image, <ref type="bibr" target="#b45">[46]</ref> tries to segment an image into two layers and generate segment-wise blur kernels for deblurring. More recently, there are some researches that estimate pixel-wise blur kernel with segmentation <ref type="bibr" target="#b31">[32]</ref>, or without segmentation <ref type="bibr" target="#b14">[15]</ref>. However, these kernel based methods are quite expensive in computation and usually rely on human knowledge. An inaccurate blur kernel estimation will result in severe artifacts in the deblurred image. Alternatively, <ref type="bibr" target="#b5">[6]</ref> uses homography as the underlying motion model. Instead of deconvolution with kernel estimation, it searches for the luckier pixels in adjacent frames and uses them to replace the less lucky pixels in the current frame. However, the homography assumption can only be applied to the blur caused by camera shake.</p><p>To overcome the above issues, researchers started to work on deep learning methods for video deblurring. CNN-based methods are used to handle the inter-frame relationship of video signal, such as <ref type="bibr" target="#b36">[37]</ref>, which makes the estimation of deblurred frame by using channelconcatenated neighboring frames with optional alignment using homography or optical flow. To better utilizing information from neighboring frames, EDVR <ref type="bibr" target="#b41">[42]</ref> uses deformable convolutional operation <ref type="bibr" target="#b56">[57]</ref> in the encoder stage. Recently, on the basis of alignment using off-the-shelf optical flow estimators <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>, CDVD-TSP <ref type="bibr" target="#b30">[31]</ref> develops a temporal sharpness prior and a cascaded training approach to jointly optimize the network; while PVDNet <ref type="bibr" target="#b35">[36]</ref> retrains a blur-invariant flow estimator and uses a pixel volume containing candidate sharp pixels to address motion estimation errors. However, additional flow estimator branch usually makes the model more computationally expensive. On the other hand, some researchers tend to focus on RNNbased methods because of their excellent performance for handling time-series signal. RNN-based methods can manage alignment implicitly through hidden states. For example, <ref type="bibr" target="#b43">[44]</ref> employs RNN architecture to reuse the features extracted from the past frame, and <ref type="bibr" target="#b15">[16]</ref> improves the deblurring performance by blending the hidden states in temporal domain. Then, <ref type="bibr" target="#b26">[27]</ref> is proposed to iteratively update the hidden state via reusing RNN cell parameters and achieves impressive video deblurring performance while operating in real time. <ref type="bibr" target="#b55">[56]</ref> proposes a unified RNN framework to generate spatially adaptive filters for alignment and deblurring. <ref type="bibr" target="#b42">[43]</ref> introduces a novel framework that utilizes the motion magnitude prior as guidance for efficient deep video deblurring. In addition, <ref type="bibr" target="#b54">[55]</ref> considers the problem of joint rolling shutter correction and video deblurring in the case of using rolling shutter cameras.</p><p>In this paper, we adopt a RNN framework similar to <ref type="bibr" target="#b26">[27]</ref>. Our method is different from <ref type="bibr" target="#b26">[27]</ref> in that we integrate RDB into the RNN cell in order to exploit the potential of the RNN cell through feature reusing and generating hierarchical features for the current frame. Furthermore, we propose a GSA module to selectively merge effective hierarchical features from both past and future frames, which enables our model to utilize the spatio-temporal information more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deblurring Dataset</head><p>As data-driven methods become dominant in the field of deblurring techniques, high-quality deblurring datasets with training image/video pairs become increasingly important. Since obtaining ground truth for real blurry image/video has been a challenge for a while, researchers thereby turned to synthetic datasets to circumvent this problem. A common approach used to create blurry images is to convolve clean natural images with synthetic blur kernels, just as was done in these works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>. A more typical approach is to average consecutive sharp frames in a high-fps video to create visually realistic blur that forms due to relatively long exposure, such as DVD <ref type="bibr" target="#b36">[37]</ref>, GOPRO <ref type="bibr" target="#b25">[26]</ref> and REDS <ref type="bibr" target="#b24">[25]</ref>. Research on the synthesis of blur on RAW space <ref type="bibr" target="#b2">[3]</ref> has also been carried out. However, the unnatural data distribution and artifacts in the synthetic deblurring datasets will inevitably affect the performance of the model in real-world situation.</p><p>Recently, researchers have resorted to designing customized hardware platforms to collect real-world dataset <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> for low-level vision tasks. Take the image super-resolution task as an example, a zoom lens is used in <ref type="bibr" target="#b49">[50]</ref> to collect high-resolution and lowresolution image pairs in static scenes. In this work, we design a beam-splitter acquisition system for the video deblurring task and contribute the first real-world video deblurring dataset to the community. We found another work <ref type="bibr" target="#b32">[33]</ref> that proposed a similar system to ours in the same period for collecting deblurring dataset. However, unlike <ref type="bibr" target="#b32">[33]</ref>, we adopt the single object lens scheme to ensure precise alignment of the two cameras without receiving the effects of lens inter-reflection. Our carefully designed acquisition system is sufficient to capture blurry/sharp video pairs for both image and video deblurring, while the acquisition system in <ref type="bibr" target="#b32">[33]</ref> can only be used to capture single blurry/sharp image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we will first give an overview of the proposed method in Sec. 3.1. Then we will go into details of RDB cell and GSA module in Sec. 3.2 and Sec. 3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>According to the characteristics of blur in the video, it may keep varying temporally and spatially, which makes deblurring problem intractable. In turn, it is possible that the blurred information in the current frame is relatively clear and complete in the past frames and future frames. When using RNN-based method to implement video deblurring, high-level features of the current frame will be generated to make the inference of deblurred image. Actually, some parts of the high-level <ref type="figure">Fig. 3</ref> The structure of RDB-based RNN cell. h t and h t?1 refer to the hidden state of past frame and current frame, respectively; I t refers to the input blurry frame; f D t refers to the features after downsampling module; f R t refers to the feature set generated by series of RDB modules; f t refers to the hierarchical features generated by the RDB cell; As for the details of each layer and RDB module, k, s, c and g denote kernel size, stride, channels and growth rate, respectively. <ref type="figure">Fig. 4</ref> The structure of global spatio-temporal attention module. f t?2 , f t?1 , f t+1 , f t+2 and f t refer to the hierarchical features of corresponding neighboring frames in the past and future and the current frame, respectively; Linear refers to fully convolutional layer; GAP refers to global average pooling fusion module; F t refers to the output of GSA module, integrating the effective components of hierarchical features from neighboring frames generated by GAP fusion module.</p><p>features are worth saving and reusing for making up the loss information for other frames. Therefore, distributing part of computational resources to fuse informative features in past and future frames could be a method to effectively improve the efficiency of the neural network. Furthermore, how to improve RNN cell itself to extract high-level features with better spatial structure is critical to enhance the efficiency of the neural network. Starting from the above viewpoints, we integrate multiple residual dense blocks into RNN cell to generate hierarchical features and propose a global spatiotemporal attention module for effective feature fusion of neighboring frames.</p><p>The whole video deblurring process of our method is shown as <ref type="figure" target="#fig_1">Fig. 2</ref>. We denote the input frames of blurry video and corresponding output frames as {I t } and {O t } respectively, where t ? {1 ? ? ? T }. Through RDB-based RNN cell, the model could get hierarchical features for each frame as {f t }. To get the inference of latent frame O t , the global spatio-temporal attention module takes current hierarchical feature f t with two past and two future features (f t?2 , f t?1 , f t+1 , f t+2 ) as input to perform feature fusion and generate F t as output. Finally, through re-constructor module, the model can obtain the latent frame O t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RDB Cell: RDB-based RNN Cell</head><p>We adopt residual dense block <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> into the RNN cell, named as RDB cell. The dense connections of RDB inherited from dense block (DB) <ref type="bibr" target="#b11">[12]</ref> let each layer receive feature maps from all previous layers by concatenating them together in channel dimension. The output channels of each layer in RDB will keep the same size. This allows collective features to be reused and save the computational resources. Moreover, through local feature fusion, RDB could generate hierarchical features from convolutional layers in different depth with different size of receptive fields, which could provide better spatial information for image reconstruction.</p><p>The structure of RDB-based RNN cell is illustrated in <ref type="figure">Fig. 3</ref>. First, the current input frame I t will be downsampled and concatenated with last hidden state h t?1 to get shallow feature maps f D t as follows:</p><formula xml:id="formula_0">f D t = CAT (DS (I t ), h t?1 ),<label>(1)</label></formula><p>where CAT (?) refers to concatenation operation; DS (?) refers to downsampling operation in the cell which consists of 5 ? 5 convolutional layers and RDB module.</p><formula xml:id="formula_1">Then, f D</formula><p>where Conv (?) refers to convolutional operation. Then, the hidden state h t could be updated as follows:</p><formula xml:id="formula_2">h t = H(f t ),<label>(3)</label></formula><p>where H refers to the hidden state generation function, consisting of 3?3 convolutional layer and RDB module. In short, while processing each frame in the video, the inputs of RDB cell are current blurry frame and previous hidden state. Then, RDB cell will generate the hierarchical features of this frame and update the hidden state as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GSA: Global Spatio-temporal Attention Module</head><p>The structure of GSA module is illustrated in <ref type="figure">Fig. 4</ref>. This module aims to extract and fuse the effective components of hierarchical features from future and past frames. Intuitively, the frames which are closer to the current frame in time domain are more likely to have useful information for deblurring of current frame. In the situation of real-time video deblurring, considering that the requirement of low computational cost for each output frame, the number of neighboring hierarchical features that will be fused into current frame should be limited. Furthermore, considering that delaying output by only several frames is usually acceptable, the hierarchical features from the future frames are available for the feature fusion. Therefore, the input to GSA will be hierarchical features of two frames before and after the current frame and the current frame itself as {f t?2 , f t?1 , f t , f t+1 , f t+2 }. Inspired by Squeezeand-Excitation (SE) block in <ref type="bibr" target="#b10">[11]</ref>, a submodule named global averaging pooling fusion is proposed, which takes features of current frame and a neighboring frame as input to filter out effective hierarchical features f e t+i from the neighboring frame as follows:</p><formula xml:id="formula_3">f c t+i = CAT (f t , f t+i ),<label>(4)</label></formula><formula xml:id="formula_4">f e t+i = L(GAP (f c t+i )) ? P (f c t+i ),<label>(5)</label></formula><p>where i ? {?2, ?1, 1, 2}; GAP (?) refers to global averaging pooling <ref type="bibr" target="#b22">[23]</ref>; L(?) refers to a series of linear transformation with activation function as ReLU <ref type="bibr" target="#b27">[28]</ref> and Sigmoid function for channel weight generation; P (?) refers to a series of 1 ? 1 convolutional operations for feature fusion. Finally, GSA module will fuse the f t with all effective hierarchical features from neighboring frames to get the output F t as follows:</p><formula xml:id="formula_5">F t = Conv (CAT (f e t?2 , f e t?1 , f e t+1 , f e t+2 , f t )).<label>(6)</label></formula><p>The output F t of GSA module will be upsampled by deconvolutional layers <ref type="bibr" target="#b6">[7]</ref> in re-constructor module for generating latent image for the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset for Video Deblurring</head><p>In this section, we first briefly introduce the mainstream synthetic video deblurring datasets and the corresponding simulation method. Then we present the details of our beam-splitter acquisition system and the corresponding BSD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthesized Dataset</head><p>At present, there are still very limited methods for building a video deblurring dataset. The mainstream way is to average a series of consecutive short-exposure images in order to mimic the phenomenon of blur caused by relatively long exposure time <ref type="bibr" target="#b16">[17]</ref>. The process of that can be described as follows:</p><formula xml:id="formula_6">I b = CRF 1 N N n=1 S n ,<label>(7)</label></formula><p>where N , S n denote the number of sampled high-FPS frames and the signal of n th sharp frame; CRF denotes the camera response function. This kind of method requires a high-speed camera to capture high-fps video and then synthesizes pairs of sharp and blurry videos based on the high-FPS videos. Before averaging, video frame interpolation algorithms <ref type="bibr" target="#b28">[29]</ref> are usually used in advance to supplement the information of missing exposure time in the high-fps video, which helps to avoid unnatural spikes or steps in the blur trajectory caused by inadequate frame rate <ref type="bibr" target="#b43">[44]</ref>. Mainstream public datasets for video deblurring, such as GOPRO <ref type="bibr" target="#b25">[26]</ref> and REDS <ref type="bibr" target="#b24">[25]</ref>, are all born by the above method. There are 22 training video sequences and 11 evaluation video sequences in GOPRO with 2103 training samples and 1111 evaluation samples respectively. As for REDS, there are 240 training sequences and 30 evaluation sequences with 100 frames for each sequence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Beam-Splitter Deblurring Dataset (BSD)</head><p>It is questionable whether aforementioned synthetic way truly reflects the blur in real scenarios. Here, we provide a new solution for building video deblurring dataset by using a beam splitter acquisition system with two synchronized cameras, as shown in <ref type="figure">Fig. 5(a)</ref>. In our solution, by controlling the length of exposure time and strength of exposure intensity during video shooting as shown in <ref type="figure">Fig. 5(b)</ref>, the system could obtain a pair of sharp and blurry videos in one shot. We adopt center-aligned synchronization scheme to properly delay the pulse of C2, so that the sharp exposure time lies exactly in the middle of the blurry exposure time. Compared to start-aligned or end-aligned synchronization scheme, center-aligned scheme can avoid large displacement between blurry/sharp image pairs. To further realize photometric alignment, we insert a 12.5% neutral density filter in the front of C1 to reduce the irradiance intensity as 1/8 of camera C2. Correspondingly, the exposure time of camera C1 is always kept as 8 times of camera C2.</p><p>The configurations of the proposed BSD dataset is illustrated in <ref type="table" target="#tab_0">Table 1</ref>. We collected blurry/sharp video sequences for three different blur intensity settings (sharp exposure time -blurry exposure time), 1ms-8ms, 2ms-16ms and 3ms-24ms, respectively. The acquisition frequency is 15fps. For each setting, the training and validation sets have 60 and 20 video sequences with 100 frames in each, respectively, and the test set has 20 video sequences with 150 frames in each. There are 11000 blurry/sharp image pairs in total for each setting. The resolution of all videos is uniformly 640 ? 480. Blurry/sharp image pairs of different motion patterns are illustrated in <ref type="figure">Fig. 6</ref>. The 1 st row shows blurry images, and the 2 nd row shows the corresponding sharp images. Specifically, <ref type="figure">Fig. 6(a)</ref> represents the case where only the camera is moving; <ref type="figure">Fig. 6(b)</ref> represents the case where only the object is moving while the camera is static. Thus, the background is sharp but the car is blurry; <ref type="figure">Fig. 6(c)</ref> represents the case where the camera and the object are moving in the opposite directions; <ref type="figure">Fig. 6(d)</ref> represents the case where the camera and the object are moving in the same direction. As a result, the car appears sharp while the background is blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>For comparison on synthetic datasets, we conduct the experiments on GOPRO and REDS, respectively. As for GOPRO, we choose the same version as <ref type="bibr" target="#b26">[27]</ref>. Due to the huge size of REDS dataset and limited computational resources, we train our model and other SoTA models only on first-half training sequences of REDS (120 sequences) for comparison. We train the model for 500 epochs by ADAM optimizer <ref type="bibr" target="#b17">[18]</ref> (? 1 = 0.9, ? 2 = 0.999) with initial learning rate as 10 ?4 (decay rate as 0.5, decay step as 200 epochs). We use RGB patches of 256 ? 256 size in subsequence of 10 frames as input to train the models. Also, we implement horizontal and vertical flipping for each subsequence as data augmentation. Mini-batch size is set to 4. The loss function is defined as MSE loss L M SE as follows:</p><formula xml:id="formula_7">L M SE = 1 T CHW T t=1 O t ? O GT t 2 2 ,<label>(8)</label></formula><p>where T , C, H, W denote the number of frames and the number of channel, height, width for each frame; O GT t refers to the ground truth of the t th frame. Whereas, in the experiments on the proposed realworld dataset BSD, we adopt cosine annealing schedule to adjust learning rate with default learning rate as 3 ? 10 ?4 . Mini-batch size is set to 8. Length of subsequence is 8. We use charbonnier loss function L char ( = 1 ? 10 ?3 ) as follows: </p><formula xml:id="formula_8">L char = 1 T CHW T t=1 O t ? O GT t 2 + 2 .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Synthetic Datasets</head><p>For fair comparison, we mainly compare our method with lightweight deblurring methods without using pretrained optical flow estimator. We note that the SoTA models, such as CDVD-TSP and PVDNet, use a pre-trained optical flow estimator (PWC-Net <ref type="bibr" target="#b37">[38]</ref> for CDVD-TSP and LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> for PVDNet) as part of the model, which means additional information is introduced into the model. Yet we also present the comparison results with CDVD-TSP <ref type="bibr" target="#b30">[31]</ref> and PVDNet <ref type="bibr" target="#b35">[36]</ref> for your reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparison on GOPRO</head><p>First, we compare our method with the SoTA video deblurring methods on GOPRO dataset. We implement 7 variants of our model with different computational cost by modifying the number of channels (C # ) of the model and keeping the number of RDB blocks (B # ) as 9. The larger C # is, the higher computational cost it needs. We report the deblurring performance and the corresponding computational cost for processing one frame in the video of all compared models in terms of PSNR <ref type="bibr" target="#b9">[10]</ref>, SSIM and GMACs, respectively, in <ref type="table" target="#tab_1">Table 2</ref>. From the perspective of quantitative analysis, it is clear that our model can achieve higher PSNR and SSIM value with less computational cost, which means our model has higher network efficiency. To further validate the deblurring performance of proposed model, we also show the deblurred image generated by each model, as illustrated in <ref type="figure" target="#fig_3">Fig. 7</ref>. We can see that the proposed model can restore sharper image with more details, such as the textures of tiles on the path and the characters on the poster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison on REDS</head><p>We also do the comparison on REDS, which has more diverse scenes from different places. From <ref type="table" target="#tab_1">Table 2</ref>, we can see our model B 9 C 90 achieves best results as 32.63 PSNR with only around 200 GMACs computational cost for one 720P frame. Even our small model B 9 C 60 with cost less than 100 GMACs can achieve same level performance as c2h3 of IFI-RNN, the computational cost of which is as twice as the former. In terms of qualitative results illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>, the proposed model can significantly reduce ambiguous parts for the deblurred frame, and the restored details such as the texture of the wall, characters, and human body are closer to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Network Efficiency Analysis</head><p>We collect the computational cost for one frame as well as the performance (PSNR) of the SoTA lightweight image and video deblurring models on GOPRO dataset, as illustrated in <ref type="figure">Fig. 1(b)</ref>. The proposed model includes 7 red nodes that represent different variants of our ES-TRNN from B 9 C 60 to B 9 C90 in <ref type="table" target="#tab_1">Table 2</ref>. Also, the three blue nodes represent different variants of IFI-RNN as c2h1, c2h2 and c2h3. Because the computational cost of different models varies drastically, we take log 10 (GMACs) as abscissa unit to better display the results. An ideal model with high network efficiency will locate at upper-left corner of the coordinate. The proposed models are closer to the upper-left corner than the existing image or video deblurring models, which reflects the high network efficiency of our model. We further present computational cost and inference time comparison in <ref type="table">Table 3</ref>. Because the parallelism of RNNs is in general worse than CNNs, RNNbased models do not have a considerable advantage in inference time, although they have a greater advantage in GMACs. However, our model still demonstrates efficiency improvements. Take the small variant ESTRNN (B9C60) as example, it is superior to other models in terms of performance, inference speed and computational cost. We also note that the parallelism of RNNs can be improved according to related literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Ablation Study</head><p>We conduct an ablation study to demonstrate the effectiveness of the high-level feature fusion strategy, RDB cell, as well as GSA module, as shown in <ref type="table">Table 4</ref>. When ablating the modules, we keep the computational cost almost unchanged by adjusting the number of channels (C # ) for fair comparison. Specifically, without using fusion strategy means that the model directly reconstructs the result according to high-level features only from current frame; without RDB cell, the model will use residual block <ref type="bibr" target="#b8">[9]</ref> instead, in the same way as <ref type="bibr" target="#b25">[26]</ref> does; without GSA module, high-level features will be directly concatenated in channel dimension. The results clearly demonstrate that each module or design can improve the deblurring efficiency, because each mod-   ule can improve the overall performance of model when the computational cost keeps unchanged. Besides, we show visual results to provide some intuitions about GSA module for high-level feature fusion in <ref type="figure" target="#fig_5">Fig. 9</ref>. In these two cases, the results of model without using GSA <ref type="table">Table 6</ref> Effectiveness of number of neighboring frames used by GSA module. F # and P # refers to the number of future and past frames used by the model. The base model is B 9 C 80 . The experiments are conducted on GOPRO. to fuse features from neighboring frames (only features from current frame as input for the re-constructor) are inferior to the results of the complete model. The gain should come from the time instances, such as t+1 in the first case and t+2 in the second case, where the "blurry" input image has relatively sharp but misaligned appearance.</p><formula xml:id="formula_9">F 0 P 1 F 0 P 2 F 0 P 3 F 1 P 1 F 2 P 2 F 3 P 3<label>PSNR</label></formula><p>We further explore the effectiveness of the number of RDB blocks and the number of past and future frames used by the model as <ref type="table" target="#tab_3">Table 5</ref> and <ref type="table">Table 6</ref>, respectively. First, from the perspective of the number of RDB blocks, this is intuitive that more blocks which means more computational cost will achieve better per-  formance. If we compare the variant B 15 C 80 with variant B 9 C 90 in <ref type="table" target="#tab_1">Table 2</ref> which has almost same computational cost, we can find that it is better to increase the number of RDB blocks rather than the channels, when the number of channels is relatively sufficient. As for the number of neighboring frames, <ref type="table">Table 6</ref> shows that, considering the increased computational cost, the benefit of using more neighboring frames as F 3 P 3 is relatively small. Besides, the results of F 0 P 1 , F 0 P 2 and F 0 P 3 show that the proposed model can still achieve comparative good results even without high-level features from the future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Real-world Dataset BSD</head><p>We also conduct comparison experiments on the proposed BSD dataset with different blur intensity settings. The visual results for 1ms-8ms, 2ms-16ms, and 3ms-24ms are illustrated in <ref type="figure">Fig. 10(a)</ref>, <ref type="figure">Fig. 10(b)</ref> and <ref type="figure">Fig. 10(c)</ref>, respectively. The proposed method (B15C80) achieves more visually appealing results in all settings of the real-world deblurring dataset. The quantitative results are shown as <ref type="table" target="#tab_5">Table 7</ref>. It indicates that the dataset setting with longer exposure time, i.e., higher blur intensity, is more difficult to restore. Our method achieves the best PSNR and SSIM scores of 33.36dB and 0.937 for 1ms-8ms setting, while the best scores of 31.39dB and 0.296 for 3ms-24ms setting. Both qualitative and quantitative results verify the effectiveness of our method on real-world video deblurring task. Also, comparison results with huge models with pretrained optical flow estimator are presented in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dataset Cross-validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Qualitative cross-validation with synthetic datasets</head><p>To verify the advantages of using real-world data for training, we further conduct cross-validation between BSD and GOPRO. The predicted results for GOPRO by using our model (B15C80) trained on BSD (2ms-16ms) are illustrated in <ref type="figure">Fig. 11(a)</ref>; while the predicted results for BSD (2ms-16ms) by using our model trained on GOPRO are illustrated in <ref type="figure">Fig. 11(b)</ref>. The comparison in <ref type="figure">Fig. 11</ref> demonstrates that the model trained on real-world dataset has much better generalization ability than the model trained on synthetic dataset. The model trained on BSD has decent deblurring performance on GOPRO. In contrast, the model trained on GOPRO cannot deblur well on BSD but introduce severe artifacts. The above observations also apply to the other two BSD settings, 1ms-8ms and 3ms-24ms. We also show the results of cross-validation between BSD and REDS <ref type="bibr" target="#b24">[25]</ref> in <ref type="figure" target="#fig_1">Fig. 12</ref>, as well as BSD and DVD <ref type="bibr" target="#b36">[37]</ref> in <ref type="figure">Fig. 13</ref>. The results demonstrate that REDS dataset with higher quality images also suffers from the same issue. While the model trained with DVD performed better than the model trained with the GOPRO dataset or the REDS dataset when tested on real-world data. However, even if DVDs perform better than other syn- thetic datasets, the trained model still produces undesired artifacts when processing real-world images with certain strongly blurred regions. This validates that real datasets are still the preferred choice for training deblurring models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Quantitative cross-validation with synthetic datasets</head><p>We also implement the quantitatively comparison for dataset cross-validation, as illustrated in <ref type="table">Table.</ref> 8. We</p><p>show the bottom-score for each dataset in the first row. This bottom-score is calculated by using the original blur input without any processing and the corresponding sharp ground-truth. If the scores calculated from the processed images of the model are lower than the bottom-score, it can be considered that the model has a negative impact on the test data set in general. We can find that the model trained using the BSD can consistently obtain at least positive gains on the synthetic datasets. However, the model trained using the (a) Cross-validation from BSD to GOPRO (b) Cross-validation from GOPRO to BSD <ref type="figure">Fig. 11</ref> Cross-validation between BSD and GOPRO.</p><p>(a) Cross-validation from BSD to REDS (b) Cross-validation from REDS to BSD <ref type="figure" target="#fig_1">Fig. 12</ref> Cross-validation between BSD and REDS.</p><p>GOPRO and REDS will result in a lower performance than the bottom-score of the BSD dataset. The model trained on DVD can also achieve positive gain on BSD, which is consistent with the observation of qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Possible factors affecting the generalization ability of synthetic data</head><p>The key difference between synthetic data pairs and real-world data pairs is that synthetic data is averaging with discrete images to simulate the formation of blur. We believe that the continuity of the discrete im-ages used for synthesis and the noise distribution of the synthetic blur are two important factors that affect the generalization ability of the synthetic dataset.</p><p>First, we assume one of the reasons for causing the unsatisfactory results of the model trained on synthetic dataset is the insufficient frame rate of the original captured high-fps video (240 fps for GOPRO and 120 fps for REDS). Although video frame interpolation algorithms can be used to increase the frame rate, it is questionable whether the interpolated frames match the distribution of natural images.  To figure out how insufficient frame rate affects the quality of the synthetic deblurring dataset, we use a camera with a much higher frame rate (2000 fps) to record videos for making a new synthetic dataset. In this case, the readout time is negligible and the exposure time is very close to 0.5 ms, thus video interpolation is not needed to supplement the missing information. We build a high-fps synthetic dataset with the same setting as BSD (2ms-16ms, 15fps) using the captured 2000 fps videos. Then, we conduct another crossvalidation experiment between BSD (2ms-16ms) and the synthetic high-fps dataset to investigate whether a high enough frame rate could solve the problem of poor migration of the synthetic dataset. The results are illustrated in <ref type="figure">Fig. 14.</ref> Basically, the experimental results are consistent with the cross-validation between BSD and GOPRO, i.e., the model trained on BSD can work well on the synthetic high-fps dataset (see <ref type="figure">Fig. 14(a)</ref>), but not vice-versa (see <ref type="figure">Fig. 14(b)</ref>). Therefore, simply satisfying sufficient continuity cannot improve the generalization ability of the synthetic data.</p><p>From the perspective of noise distribution of blurred images, the present synthesis method cannot simulate the noise generated in real blurred images at the RAW acquisition and ISP stages. The process of averaging successive sharp images suppresses some of the noise. Thus, a likely reason for the one-sided relationship is that the model trained on BSD has seen more complex noise models so it can handle synthetic data with noise suppressed, which is the simpler case, while the reverse is difficult.</p><p>In addition, we trained a series of models on DVD by adding Gaussian noise to the RGB space of blurred images with different standard deviations ?. The visual results in <ref type="figure" target="#fig_8">Fig. 15</ref> indicate that adding the appropriate noise can slightly improve artifact, but it is still far from eradicating it. Starting from RAW space to synthesize blur and considering the appropriate noise model should be a promising direction.</p><p>The above experiments demonstrate the significance of the real-world dataset and indicate that it is not trivial to produce a high-quality deblurring datasets by synthetic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Test on Third-part Real-world Videos</head><p>Our real-world dataset BSD is obtained under some specific exposure settings. To verify the generalization ability of BSD, we further verify our model trained by using BSD on the commonly used real-world videos from DVD <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>, as illustrated in <ref type="figure">Fig. 16</ref>. We can see that our model trained on the synthetic datasets, GO-PRO <ref type="bibr" target="#b25">[26]</ref> and REDS <ref type="bibr" target="#b24">[25]</ref> inevitably introduce undesired artifacts to the final results, such as the cover of the book, the wheel of the car, and the body of the man. Our model trained on BSD (2ms16ms) works well in most of these cases. It is worth noting that in the bicycle example, even though the model trained on BSD cannot recover the very challenging blurred bicycle wheel, the model does not force to add some artifacts. We believe that this is advantageous in terms of visual perception and can be further addressed by expanding the dataset.</p><p>We also test our model by shooting blurred videos without any specific constraints using iPhone 13. The predicted results from our model trained on synthetic dataset GOPRO and on our real-world dataset BSD are illustrated in <ref type="figure" target="#fig_3">Fig. 17</ref>. It is clear that the model trained on real-world data successfully restored the latent images and generated clearer details such as the handle of the bag. While the model trained on synthetic dataset suffered from unsharp details and undesired artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a novel RNN-based method for more computationally efficient video deblurring. Residual dense blocks were adopted to the RNN cell to generate hierarchical features from current frame for better restoration. Moreover, to make full use of the spatio-temporal correlation, our model utilized the global spatio-temporal fusion module for fusing the effective components of hierarchical features from past and future frames. Furthermore, we have developed a beam-splitter acquisition system and contributed the first real-world dataset for image/video deblurring tasks. The experimental results show that our model is more computationally efficient for video deblurring, which can achieve better performance with less computational cost. Cross-validation experiments between real-world and synthetic datasets demonstrate the high generality of the proposed BSD dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Framework of the proposed efficient spatio-temporal recurrent neural network. I t refers to the t th input blurry frame; h t and f t refer to the extracted hidden state and hierarchical features of RDB-based RNN cell (see Sec. 3.2) from t th frame; F t refers to the fused features generated by GSA module (see Sec. 3.3) for t th frame; O t refers to the t th deblurred frame by the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 AFig. 6</head><label>56</label><figDesc>beam splitter acquisition system for building video deblurring dataset. (a) is the profile of our beam splitter acquisition system. C1 and C2 refer to two same cameras with different configurations for generating blurry and sharp videos, respectively; (b) shows the center-aligned exposure scheme of C1 and C2 to generate blurry/sharp video pairs. Samples of blurry/sharp image pairs in BSD. (a) represents the case where there is only camera ego-motion. (b) represents the case where only the object is moving. (c) represents the case where the object and the camera are moving in the opposite directions. (d) represents the case where the object and the camera are moving in the same direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7</head><label>7</label><figDesc>Visual comparisons on GOPRO<ref type="bibr" target="#b25">[26]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Visual comparisons on REDS<ref type="bibr" target="#b24">[25]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9</head><label>9</label><figDesc>Visual ablation study of the GSA module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )Fig. 10</head><label>a10</label><figDesc>Visual results on BSD (1ms-8ms) (b) Visual results on BSD (2ms-16ms) (c) Visual results on BSD (3ms-24ms) Visual comparisons on different settings of the proposed BSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Cross-validation from BSD to DVD (b) Cross-validation from DVD to BSDFig. 13Cross-validation between BSD and DVD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 15</head><label>15</label><figDesc>BSD test results of models (ESTRNN B15C80) trained by using DVD with additional Gaussian noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 16 Fig. 17</head><label>1617</label><figDesc>Qualitative results on real-world videos from<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref> Testing on real-world blurry videos from iPhone 13. Ours (Synthetic) denotes the results of ESTRNN trained on synthetic dataset GOPRO. Ours (real) denotes the results of ESTRNN trained on real-world dataset BSD (2ms-16ms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Configuration of the proposed BSD dataset</figDesc><table><row><cell></cell><cell cols="2">train validation</cell><cell>test</cell></row><row><cell>sequences</cell><cell>60</cell><cell>20</cell><cell>20</cell></row><row><cell>frames/seq.</cell><cell>100</cell><cell>100</cell><cell>150</cell></row><row><cell>frames</cell><cell>6000</cell><cell>2000</cell><cell>3000</cell></row><row><cell>resolution</cell><cell></cell><cell>640 ? 480</cell><cell></cell></row><row><cell>frequency</cell><cell></cell><cell>15 fps</cell><cell></cell></row><row><cell>settings</cell><cell cols="3">1ms-8ms, 2ms-16ms, 3ms-24ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Quantitative results on both GOPRO and REDS datasets. Cost refers to the computational cost of the model for deblurring one frame of HD (720P) video in terms of GMACs. The meaning of cost is same for other tables and figures in this paper. For our model, B # and C # denote the number of RDB blocks in RDB cell and the number of channels for each RDB block, respectively.</figDesc><table><row><cell>Model</cell><cell cols="2">GOPRO PSNR SSIM</cell><cell cols="2">REDS PSNR SSIM</cell><cell>Cost</cell></row><row><cell>STRCNN [16]</cell><cell>28.74</cell><cell>0.8465</cell><cell>30.23</cell><cell>0.8708</cell><cell>276.20</cell></row><row><cell>DBN [37]</cell><cell>29.91</cell><cell>0.8823</cell><cell>31.55</cell><cell>0.8960</cell><cell>784.75</cell></row><row><cell>IFI-RNN (c2h1) [27]</cell><cell>29.79</cell><cell>0.8817</cell><cell>31.29</cell><cell>0.8913</cell><cell>116.29</cell></row><row><cell>IFI-RNN (c2h2) [27]</cell><cell>29.92</cell><cell>0.8838</cell><cell>31.35</cell><cell>0.8929</cell><cell>167.09</cell></row><row><cell>IFI-RNN (c2h3) [27]</cell><cell>29.97</cell><cell>0.8859</cell><cell>31.36</cell><cell>0.8942</cell><cell>217.89</cell></row><row><cell>STFAN [56]</cell><cell>30.51</cell><cell>0.9054</cell><cell>32.03</cell><cell>0.9024</cell><cell>566.61</cell></row><row><cell>CDVD-TSP [31]</cell><cell>30.94</cell><cell>0.9153</cell><cell>32.57</cell><cell cols="2">0.9161 5211.28</cell></row><row><cell>PVDNet [36]</cell><cell>32.13</cell><cell>0.9322</cell><cell>33.92</cell><cell cols="2">0.9322 1754.90</cell></row><row><cell>ESTRNN (B9C60)</cell><cell>30.12</cell><cell>0.8837</cell><cell>31.64</cell><cell>0.8930</cell><cell>92.57</cell></row><row><cell>ESTRNN (B9C65)</cell><cell>30.30</cell><cell>0.8892</cell><cell>31.63</cell><cell>0.8965</cell><cell>108.20</cell></row><row><cell>ESTRNN (B9C70)</cell><cell>30.45</cell><cell>0.8909</cell><cell>31.94</cell><cell>0.8968</cell><cell>125.55</cell></row><row><cell>ESTRNN (B9C75)</cell><cell>30.58</cell><cell>0.8923</cell><cell>32.06</cell><cell>0.9022</cell><cell>143.71</cell></row><row><cell>ESTRNN (B9C80)</cell><cell>30.79</cell><cell>0.9016</cell><cell>32.33</cell><cell>0.9060</cell><cell>163.61</cell></row><row><cell>ESTRNN (B9C85)</cell><cell>31.01</cell><cell>0.9013</cell><cell>32.34</cell><cell>0.9074</cell><cell>184.25</cell></row><row><cell>ESTRNN (B9C90)</cell><cell>31.07</cell><cell>0.9023</cell><cell>32.63</cell><cell>0.9110</cell><cell>206.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Table 4</head><label>34</label><figDesc>Computational cost and inference time comparison. We show the performance of each model in terms of GMACs, Million parameters, inference time (ms), frame rate (fps), and the corresponding PSNR score. The size of test image is 1280?720, and the test hardware is GeForce RTX 2080 Ti. The average value is reported. Ablation study of ESTRNN. Fusion refers to the fusion strategy that utilizes the high level features from neighboring frames. The experiments are conducted on GOPRO.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">STRCNN [16] DBN [37] IFI-RNN [27] ESTRNN (B9C60) ESTRNN (B15C80)</cell></row><row><cell cols="2">GMACs</cell><cell></cell><cell>276.20</cell><cell cols="2">784.75</cell><cell>217.89</cell><cell>92.57</cell><cell>203.74</cell></row><row><cell cols="2">M Parameters</cell><cell></cell><cell>0.93</cell><cell></cell><cell>15.31</cell><cell>1.64</cell><cell>0.99</cell><cell>2.47</cell></row><row><cell cols="3">Inference Time (ms)</cell><cell>42.03</cell><cell></cell><cell>61.9</cell><cell>67.0</cell><cell>33.7</cell><cell>62.2</cell></row><row><cell cols="2">Frame rate (fps)</cell><cell></cell><cell>23.79</cell><cell></cell><cell>16.16</cell><cell>14.93</cell><cell>29.66</cell><cell>16.07</cell></row><row><cell cols="2">PSNR</cell><cell></cell><cell>28.74</cell><cell></cell><cell>29.91</cell><cell>29.97</cell><cell>30.12</cell><cell>31.27</cell></row><row><cell>Model</cell><cell cols="4">Fusion RDB Cell GSA PSNR</cell><cell>Cost</cell></row><row><cell>B 9 C 110</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>30.29</cell><cell>163.48</cell></row><row><cell>B 9 C 100</cell><cell></cell><cell>?</cell><cell>?</cell><cell>30.46</cell><cell>165.59</cell></row><row><cell>B 9 C 100</cell><cell>?</cell><cell></cell><cell>?</cell><cell>30.51</cell><cell>168.56</cell></row><row><cell>B 9 C 90</cell><cell></cell><cell></cell><cell>?</cell><cell>30.55</cell><cell>161.28</cell></row><row><cell>B 9 C 85</cell><cell>?</cell><cell></cell><cell></cell><cell>30.69</cell><cell>162.69</cell></row><row><cell>B 9 C 80</cell><cell></cell><cell></cell><cell></cell><cell>30.79</cell><cell>163.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Effectiveness of number of RDB blocks. The experiments are conducted on GOPRO.B 3 C 80 B 6 C 80 B 9 C 80 B 12 C 80 B 15 C 80</figDesc><table><row><cell>PSNR</cell><cell>29.74</cell><cell>30.31</cell><cell>30.79</cell><cell>31.03</cell><cell>31.27</cell></row><row><cell>Cost</cell><cell>123.03</cell><cell>143.32</cell><cell>163.31</cell><cell>183.90</cell><cell>204.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Quantitative results on BSD dataset. The configuration of ESTRNN is B15C80. The configuration of IFI-RNN is C2H3.</figDesc><table><row><cell>Model</cell><cell cols="2">1ms-8ms PSNR SSIM</cell><cell cols="2">2ms-16ms PSNR SSIM</cell><cell cols="2">3ms-24ms PSNR SSIM</cell></row><row><cell>STRCNN [16]</cell><cell>32.20</cell><cell>0.924</cell><cell>30.33</cell><cell>0.902</cell><cell>29.42</cell><cell>0.893</cell></row><row><cell>DBN [37]</cell><cell>33.22</cell><cell>0.935</cell><cell>31.75</cell><cell>0.922</cell><cell>31.21</cell><cell>0.922</cell></row><row><cell>SRN [41]</cell><cell>31.84</cell><cell>0.917</cell><cell>29.95</cell><cell>0.891</cell><cell>28.92</cell><cell>0.882</cell></row><row><cell>IFI-RNN [27]</cell><cell>33.00</cell><cell>0.933</cell><cell>31.53</cell><cell>0.919</cell><cell>30.89</cell><cell>0.917</cell></row><row><cell>STFAN [56]</cell><cell>32.78</cell><cell>0.922</cell><cell>32.19</cell><cell>0.919</cell><cell>29.47</cell><cell>0.872</cell></row><row><cell>CDVD-TSP [31]</cell><cell>33.54</cell><cell>0.942</cell><cell>32.16</cell><cell>0.926</cell><cell>31.58</cell><cell>0.926</cell></row><row><cell>PVDNet [36]</cell><cell>33.34</cell><cell>0.937</cell><cell>32.22</cell><cell>0.926</cell><cell>31.35</cell><cell>0.923</cell></row><row><cell>ESTRNN</cell><cell>33.36</cell><cell>0.937</cell><cell>31.95</cell><cell>0.925</cell><cell>31.39</cell><cell>0.926</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Dataset cross-validation in terms of PSNR/SSIM. We choose the 2ms16ms setting for the BSD dataset. The setting of ESTRNN is (B15C80). Cross-validation between BSD and high-fps synthetic dataset.</figDesc><table><row><cell></cell><cell>BSD</cell><cell>GOPRO</cell><cell>REDS</cell><cell>DVD</cell></row><row><cell>Blur</cell><cell>26.64/0.818</cell><cell>25.47/0.785</cell><cell>26.28/0.769</cell><cell>27.23/0.813</cell></row><row><cell>BSD</cell><cell>31.95/0.925</cell><cell>26.46/0.817</cell><cell>27.00/0.801</cell><cell>27.88/0.844</cell></row><row><cell>GOPRO</cell><cell>19.48/0.598</cell><cell>31.27/0.903</cell><cell>28.21/0.829</cell><cell>28.90/0.869</cell></row><row><cell>REDS</cell><cell>24.14/0.773</cell><cell>28.43/0.869</cell><cell>32.82/0.915</cell><cell>28.06/0.848</cell></row><row><cell>DVD</cell><cell>28.67/0.875</cell><cell>26.57/0.820</cell><cell>26.64/0.787</cell><cell>30.68/0.897</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t will be fed into a series of RDB modules. The collective outputs of stacked RDB modules are represented asf R t = {f R1 t , ? ? ? , f R N t },where N refers to the number of RDB modules. RDB cell could obtain the global hierarchical features f t by fusing the concatenation of local hierarchical features f R t with 1 ? 1 convolutional layer as follows:f t = Conv (CAT (f R t )),(2)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by JSPS KAKENHI Grant Numbers JP20H05951 and JP20H05953.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A variational framework for simultaneous motion estimation and restoration of motion-blurred video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berkels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards real-world video deblurring by exploring blur formation process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.13184</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning adaptive warping for real-world rolling shutter correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="17785" to="17793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video deblurring for hand-held cameras using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blur-kernel estimation from spectral irregularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="622" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6334" to="6342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4038" to="4047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic video deblurring using a locally adaptive blur model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2761348</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2374" to="2387" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous localization, mapping and deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1203" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense 3d reconstruction from severely blurred images using a single moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Simple recurrent units for highly parallelizable recurrence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="783" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8102" to="8111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blur-invariant deep learning for blind-deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4752" to="4760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3043" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video deblurring via semantic segmentation and pixel-wise non-linear kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blurry video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5114" to="5123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent video deblurring with blur-invariant motion estimation and pixel volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1279" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Good image priors for non-blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="231" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalerecurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13374</idno>
		<title level="m">Efficient video deblurring guided by motion magnitude</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Blurred target tracking by blur-driven tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1100" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling blurred video with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02291</idno>
		<title level="m">Sliced recurrent neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3762" to="3770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient spatiotemporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Animation from blur: Multi-modal blur decomposition with motion guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10123</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards rolling shutter correction and deblurring in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9219" to="9228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
