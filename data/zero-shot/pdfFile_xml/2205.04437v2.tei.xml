<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Activating More Pixels in Image Super-Resolution Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Zhou</surname></persName>
							<email>jtzhou@um.edu.mo</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>chao.dong@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Activating More Pixels in Image Super-Resolution Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines channel attention and self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally propose a same-task pre-training strategy to bring further improvement. Extensive experiments show the effectiveness of the proposed modules, and the overall method significantly outperforms the state-of-the-art methods by more than 1dB. Codes and models will be available at https://github.com/chxy95/HAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Networks for Image SR</head><p>Since SRCNN [9] first introduces deep convolution neural networks (CNNs) to the image SR task and obtains superior performance over conventional SR meth-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SR) is a classic problem in computer vision and image processing. It aims to reconstruct a high-resolution image from a given low-resolution input. Since deep learning has been successfully applied to the SR task <ref type="bibr" target="#b8">[9]</ref>, numerous methods based on the convolutional neural network (CNN) have been proposed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">19,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b45">65,</ref><ref type="bibr" target="#b47">67,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">41]</ref> and almost dominate this field in the past few years. Recently, due to the success in natural language processing, Transformer <ref type="bibr" target="#b30">[50]</ref> has attracted the attention of the computer vision community. After making rapid progress on high-level vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b31">51]</ref>, Transformerbased methods are also developed for low-level vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">54,</ref><ref type="bibr" target="#b42">62]</ref>, as well as for SR <ref type="bibr">[26,</ref><ref type="bibr">29]</ref>. Especially, a newly designed network, SwinIR <ref type="bibr">[29]</ref>, obtains a breakthrough improvement in this task.</p><p>Despite the success, "why Transformer is better than CNN" remains a mystery. An intuitive explanation is that this kind of network can benefit from the self-attention mechanism and utilize long-range information. However, we employ the attribution analysis method LAM <ref type="bibr" target="#b13">[14]</ref> to examine the involved range of utilized information for reconstruction in SwinIR. Interestingly, we find that arXiv:2205.04437v2 [eess.IV] 16 May 2022 SwinIR does NOT exploit more input pixels than CNN-based methods (e.g., RCAN <ref type="bibr" target="#b45">[65]</ref>) in super-resolution, as shown in <ref type="figure">Fig. 2(a)</ref>. Besides, although SwinIR obtains higher quantitative performance, it produces inferior results to RCAN in some cases, due to the limited range of utilized information. These phenomena illustrate that Transformer has a stronger ability to model local information, but the range of its utilized information needs to be expanded.</p><p>To address the above-mentioned problem and further develop the potential of Transformer for SR, we propose a Hybrid Attention Transformer, namely HAT. Our HAT combines channel attention and self-attention schemes, in order to take advantage of the former's capability in using global information and the powerful representative ability of the latter. Besides, to better aggregate the cross-window information, we also introduce an overlapping cross-attention module. Motivated by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">26]</ref>, we additionally explore the effect of pre-training on the SR task and provide a same-task pre-training strategy. Experimental results show that this strategy can perform better than multi-related-task pre-training <ref type="bibr">[26]</ref>. Equipped with the above designs and improvements, our approach can surpass the stateof-the-art methods by a huge margin(0.3dB?1.2dB), as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Overall, our contributions include three aspects: 1) We introduce channel attention to Transformer to utilize more input information. 2) We propose an overlapping cross-attention module to better aggregate the cross-window information. 3) We provide a same-task pre-training strategy to further activate the potential of the proposed network. ods, numerous deep networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">45,</ref><ref type="bibr">19,</ref><ref type="bibr">23,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b47">67,</ref><ref type="bibr" target="#b45">65,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">41,</ref><ref type="bibr" target="#b20">40,</ref><ref type="bibr">29,</ref><ref type="bibr">26]</ref> have been proposed for SR to further improve the reconstruction quality. For instance, many methods apply more elaborate convolution module designs, such as residual block <ref type="bibr">[24,</ref><ref type="bibr">30]</ref> and dense block <ref type="bibr" target="#b33">[53,</ref><ref type="bibr" target="#b47">67]</ref>, to enhance the model representation ability. Several works explore more different frameworks like recursive neural network <ref type="bibr">[20,</ref><ref type="bibr" target="#b26">46]</ref> and graph neural network <ref type="bibr" target="#b49">[69]</ref>. To improve perceptual quality, <ref type="bibr">[24,</ref><ref type="bibr" target="#b33">53,</ref><ref type="bibr" target="#b44">64,</ref><ref type="bibr" target="#b32">52]</ref> introduce adversarial learning to generate more realistic results. By using attention mechanism, <ref type="bibr" target="#b45">[65,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">66,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b21">41,</ref><ref type="bibr" target="#b20">40]</ref> achieve further improvement in terms of reconstruction fidelity. Recently, a series of Transformer-based networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">29,</ref><ref type="bibr">26]</ref> are proposed and constantly refresh the state-of-the-art of SR task, showing the powerful representation ability of Transformer.</p><p>To better understand the working mechanisms of SR networks, several works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b39">59,</ref><ref type="bibr">22]</ref> are proposed to analyze and interpret the SR networks. LAM <ref type="bibr" target="#b13">[14]</ref> adopts the integral gradient method to explore which input pixels contribute most to the final performance. DDR [35] reveals the deep semantic representations in SR networks based on deep feature dimensionality reduction and visualization. FAIG <ref type="bibr" target="#b39">[59]</ref> is proposed to find discriminative filters for specific degradations in blind SR. <ref type="bibr">[22]</ref> introduces channel saliency map to demonstrate that Dropout can help prevent co-adapting for real SR networks. In this work, we exploit LAM <ref type="bibr" target="#b13">[14]</ref> to analyse and understand the behavior of SR networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Recently, Transformer <ref type="bibr" target="#b30">[50]</ref> has attracted the attention of computer vision community due to its success in the field of natural language processing. A series of Transformer-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b36">56,</ref><ref type="bibr" target="#b31">51,</ref><ref type="bibr">36,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">57,</ref><ref type="bibr" target="#b40">60,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b22">42]</ref> have been developed for high-level vision tasks, including image classification [36, <ref type="bibr" target="#b12">13,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b24">44,</ref><ref type="bibr" target="#b29">49]</ref>, object detection [34, <ref type="bibr" target="#b28">48,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>, segmentation <ref type="bibr" target="#b35">[55,</ref><ref type="bibr" target="#b31">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>, etc. Although vision Transformer has shown its superiority on modeling long-range dependency <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">43]</ref>, there are still many works demonstrating that the convolution can help Transformer achieve better visual representation <ref type="bibr" target="#b36">[56,</ref><ref type="bibr" target="#b38">58,</ref><ref type="bibr" target="#b41">61,</ref><ref type="bibr" target="#b40">60,</ref><ref type="bibr">25]</ref>. Due to the impressive performance, Transformer has also been introduced for low-level vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">54,</ref><ref type="bibr" target="#b17">37,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b42">62,</ref><ref type="bibr">28,</ref><ref type="bibr">26</ref>]. Specifically, <ref type="bibr" target="#b4">[5]</ref> develops a ViT-style network and introduces multi-task pre-training for image processing. SwinIR <ref type="bibr">[29]</ref> proposes an image restoration Transformer based on <ref type="bibr">[36]</ref>. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">28]</ref> introduce Transformer-based networks to video restoration. [26] adopts self-attention mechanism and multirelated-task pre-training strategy to further refresh the state-of-the-art of SR. However, existing works still cannot fully exploit the potential of Transformer, while our method can activate more input information for better reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Swin than CNN-based methods. To reveal its working mechanisms, we resort to a diagnostic tool -LAM <ref type="bibr" target="#b13">[14]</ref>, which is an attribution method designed for SR. With LAM, we could tell which input pixels contribute most to the selected region. As shown in <ref type="figure">Fig. 2(a)</ref>, the red marked points are informative pixels that contribute to the reconstruction. Intuitively, the more information is utilized, the better performance can be obtained. This is true for CNN-based methods, as comparing RCAN <ref type="bibr" target="#b45">[65]</ref> and EDSR <ref type="bibr">[30]</ref>. However, for the Transformer-based method -SwinIR, its LAM does not show a larger range than RCAN. This is in contradiction with our common sense, but could also provide us with additional insights. First, it implies that SwinIR has a much stronger mapping capability than CNN, and thus could use less information to achieve better performance. Second, SwinIR still has improvement space if it could exploit more input pixels. As depicted in <ref type="figure">Fig. 2(a)</ref>, the reconstructed pattern that is marked in blue box by SwinIR is inferior to RCAN. The channel attention scheme helps RCAN see more pixels, which may also be beneficial for Transformer. Besides, we can observe obvious blocking artifacts in the intermediate feature maps of SwinIR, as presented in <ref type="figure">Fig. 2(b)</ref>. These artifacts are caused by the window partition mechanism, and this phenomenon suggests that the shifted window mechanism is inefficient to build the cross-window connection. Some works for high-level vision tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b37">57,</ref><ref type="bibr" target="#b22">42]</ref> also point out that enhancing the connection among windows can improve the window-based self-attention methods. Based on the above two points, we investigate channel attention in the Transformer-based model and propose an overlapping cross-attention module to better aggregate cross-window information for the window-based SR Transformer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>The Overall Structure. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the overall network consists of three parts, including shallow feature extraction, deep feature extraction and image reconstruction. Specifically, for a given low-resolution (LR) input I LR ? R H?W ?Cin , we first use one convolutional layer H SF (?) to extract the shallow feature F 0 ? R H?W ?C as</p><formula xml:id="formula_0">F 0 = H SF (I LR ),<label>(1)</label></formula><p>where C in and C denote the channel number of the input and the intermediate feature, respectively. The shallow feature extraction can simply map the input from low-dimensional space to high-dimensional space, while achieving the highdimensional embedding for each pixel token. Moreover, the early convolutional layer can help learn better visual representation [25] and lead to stable optimization <ref type="bibr" target="#b38">[58]</ref>. We then perform deep feature extraction H DF (?) to further obtain the deep feature F DF ? R H?W ?C as</p><formula xml:id="formula_1">F DF = H DF (F 0 ),<label>(2)</label></formula><p>where H DF (?) consists of N residual hybrid attention groups (RHAG) and one 3 ? 3 convolutional layer H Conv (?), which can progressively process the intermediate features as</p><formula xml:id="formula_2">F i = H RHAGi (F i?1 ), i = 1, 2, ..., N, F DF = H Conv (F N ),<label>(3)</label></formula><p>where H RHAGi (?) represents the i-th RHAG. Following [29], we also introduce a convolutional layer at the tail of this part to better aggregate information of deep features. After that, we add a global residual connection to fuse shallow features and deep features, and then reconstruct the high-resolution result via a reconstruction module as</p><formula xml:id="formula_3">I SR = H Rec (F 0 + F D F ),<label>(4)</label></formula><p>where H Rec (?) denotes the reconstruction module. Specifically, we adopt the pixel-shuffle method <ref type="bibr" target="#b25">[45]</ref> to up-sample the fused feature. We simply use L 1 loss to optimize the parameters.</p><p>Residual Hybrid Attention Group (RHAG). As depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>, each RHAG contains M hybrid attention blocks (HAB), an overlapping cross-attention block (OCAB) and a 3?3 convolutional layer. To be specific, for the i-th RHAG, it can be formulated as</p><formula xml:id="formula_4">F i?1,0 = F i?1 , F i?1,j = H HABi,j (F i?1,j?1 ), j = 1, 2, ..., M,<label>(5)</label></formula><formula xml:id="formula_5">F i = H Convi (H OCABi (F i?1,M )) + F i?1 ,</formula><p>where F i?1,0 indicates the input feature of i-th RHAG and F i?1,j represents the j-th output feature of j-th HAB in i-th RHAG. After the mapping of a series of HABs, we insert an OCAB to enlarge the receptive field for the windowbased self-attention and better aggregate cross-window information. At the end of RHAG, we reserve the convolutional layer following <ref type="bibr">[29]</ref>. A residual connection is also added to stabilize the training process <ref type="bibr" target="#b45">[65]</ref>.</p><p>Hybrid Attention Block (HAB). As shown in <ref type="figure">Fig. 2</ref>(a), more pixels are activated when channel attention is adopted, since global information is involved to calculate the channel attention weights. Besides, many works illustrate that convolution can help Transformer get better visual representation or achieve easier optimization <ref type="bibr" target="#b36">[56,</ref><ref type="bibr" target="#b38">58,</ref><ref type="bibr" target="#b40">60,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b48">68]</ref>. Therefore, we incorporate a channel attentionbased convolution block into the standard Transformer block to further enhance the representation ability of the network. As demonstrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, a channel attention block (CAB) is inserted into the standard Swin Transformer block after the first LayerNorm (LN) layer in parallel with the window-based multi-head self-attention (W-MSA) module. Note that shifted window-based self-attention (SW-MSA) is adopted at intervals in consecutive HABs similar to <ref type="bibr">[36,</ref><ref type="bibr">29]</ref>. To avoid the possible conflict of CAB and MSA on optimization and visual representation, a small constant ? is multiplied to the output of CAB. For a given input feature X, the whole process of HAB is computed as</p><formula xml:id="formula_6">X N = LN(X), X M = (S)W-MSA(X N ) + ?CAB(X N ) + X,<label>(6)</label></formula><formula xml:id="formula_7">Y = MLP(LN(X M )) + X M ,</formula><p>where X N and X M denote the intermediate features. Y represents the output of HAB. Especially, we treat each pixel as a token for embedding (i.e., set patch size as 1 for patch embedding following <ref type="bibr">[29]</ref>). MLP denotes a multi-layer perceptron.</p><p>(a) The structure of OCAB.</p><p>(b) The overlapping window partition for OCA. <ref type="figure">Fig. 4</ref>: The structure of OCAB is similar to the standard Swin Transformer block. The difference is that self-attention in OCAB is calculated based on the overlapping window partition, which generates key/value from a larger crosswindow feature than query.</p><p>For specific calculation of the self-attention module, given an input feature of size H ?W ?C, it is first partitioned into HW M 2 local windows of size M ?M , then self-attention is calculated inside each window. For a local window feature X W ? R M 2 ?C , the query, key and value matrices are computed by linear mappings as Q, K and V . Then the window-based self-attention is formulated as</p><formula xml:id="formula_8">Attention(Q, K, V ) = SoftMax(QK T / ? d + B)V,<label>(7)</label></formula><p>where d represents the dimension of query/key. B denotes the relative position encoding and is calculated as <ref type="bibr" target="#b30">[50]</ref>. Besides, to build the connections between neighboring non-overlapping windows, we also utilize the shifted window partitioning approach [36] and set the shift size to half of the window size. A CAB consists of two standard convolution layers with a GELU activation function <ref type="bibr" target="#b14">[15]</ref> between them and a channel attention (CA) module, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Since the Transformer-based structure often requires a large number of channels for token embedding, directly using convolutions with constant width incurs a large computation cost. Thus, we compress the channel number by a constant ? between the two convolutional layers. For an input feature with C channels, the channel number of the output feature after the first convolutional layer is squeezed to C ? , then the feature is expanded to C channels through the second layer. Next, a standard CA module <ref type="bibr" target="#b45">[65]</ref> is exploited to adaptively rescale channel-wise features. The whole process is formulated as</p><formula xml:id="formula_9">X out = CA(Conv 2 (GELU(Conv 1 (X in )))),<label>(8)</label></formula><p>where X in , X out , Conv 1 , Conv 2 indicate the input feature, the output feature, the first convolutional layer and the second convolutional layer, respectively.</p><p>Overlapping Cross-Attention Block (OCAB). We introduce OCAB to directly establish cross-window connections and enhance the representative ability for the window self-attention. Our OCAB consists of an overlapping crossattention (OCA) layer and an MLP layer similar to the standard Swin Transformer block <ref type="bibr">[36]</ref>. But for OCA, as depicted in <ref type="figure">Fig. 4</ref>, we use different window sizes to partition the projected features. Concretely, for the X Q ,</p><formula xml:id="formula_10">X K , X V ? R H?W ?C of the input feature X, X Q is partitioned into HW M 2 non-overlapping windows of size M ?M , while X K , X V are unfolded to HW M 2 overlapping windows of size M o ? M o . It is calculated as M o = (1 + 2?) ? M,<label>(9)</label></formula><p>where ? is a constant to control the overlapping size. To better understand this operation, the standard window partition can be considered as a sliding partition with the kernel size and the stride both equal to the window size M . In contrast, the overlapping window partition can be viewed as a sliding partition with the kernel size equal to M o , while the stride is equal to M . Zero-padding with size ?M is used to ensure the size consistency of overlapping windows. The attention matrix is calculated as Equ. 7, and the relative position bias B ? R M ?Mo is also adopted. Unlike WSA whose query, key and value are calculated from the same window feature, OCA computes key/value from a larger field where more useful information can be utilized for the query. Note that although Multi-resolution Overlapped Attention (MOA) module in <ref type="bibr" target="#b22">[42]</ref> performs similar overlapping window partition, our OCA is different from MOA, since MOA calculates global attention using window features as tokens while OCA computes cross-attention inside each window feature using pixel token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training on ImageNet</head><p>Recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">26]</ref> demonstrate that pre-training plays an important role in low-level tasks. IPT <ref type="bibr" target="#b4">[5]</ref> emphasizes the use of various low-level tasks, such as denoising, deraining, super-resolution and etc., while EDT [26] utilizes different degradation levels of a specific task to do pre-training. These works aim to explore the effect of multi-task pre-training for a target task. In contrast, we directly perform pre-training on a larger-scale dataset (i.e., ImageNet <ref type="bibr" target="#b7">[8]</ref>) based on the same task. For example, when we want to train a model for ?4 SR, we first train a ?4 SR model on ImageNet, then fine-tune it on the specific dataset, such as DF2K. As presented in Sec. 4.5, the proposed strategy, namely same-task pretraining, is simpler while bringing more performance improvements. It is worth mentioning that sufficient training iterations for pre-training and an appropriate small learning rate for fine-tuning are very important for the effectiveness of the pre-training strategy. We believe that it is because Transformer requires more data and iterations to learn general knowledge for the task, but needs a small learning rate for fine-tuning to avoid overfitting to the specific dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We use DF2K dataset (DIV2K [31]+Flicker2K <ref type="bibr" target="#b27">[47]</ref>) as the original training dataset, following the latest publications <ref type="bibr">[29,</ref><ref type="bibr">32]</ref>. When utilizing pre-training, we adopt ImageNet <ref type="bibr" target="#b7">[8]</ref> following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">26]</ref>. For the structure of HAT, the RHAG number and HAB number are both set to 6. The channel number of the whole network is set to 180. The attention head number and window size are set to 6 and 16 for both (S)W-MSA and OCA. For the specific hyper-parameters of the proposed modules, we set the weighting factor of CAB output (?), the squeeze factor between two convolution layers in CAB (?), and the overlapping ratio of OCA (?) as 0.01, 3 and 0.5, respectively. For the larger variant HAT-L, we directly double the depth of HAT by increasing the number of RHAG from 6 to 12. To evaluate the quantitative performance, PSNR and SSIM (calculated on the Y channel) are reported. More training details can refer to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effects of different window sizes</head><p>As discussed in Sec. 3.1, activating more input pixels for SR tends to achieve better performance. Enlarging window size for the calculation of self-attention is an intuitive way. In <ref type="bibr">[26]</ref>, the authors investigate the effects of different window sizes. However, they conduct experiments based on the shifted cross local attention and only explore the window size up to 12?12. We further explore how the window size of self-attention influences the representation ability. To eliminate the influence of our newly-introduced blocks, we conduct the following experiments directly on SwinIR. As shown in Tab. 1, the model with a large window size of 16 obtains better performance, especially on the Urban100 <ref type="bibr" target="#b16">[17]</ref>. We also provide the qualitative comparison as depicted in <ref type="figure" target="#fig_2">Fig. 5</ref>. For the marked patch in the red box, the model with window size of 16 utilizes much more input pixels than the model with window size of 8. The quantitative performance of the reconstructed results also demonstrates the effectiveness of large window size. Based on this conclusion, we directly use window size 16 as our default setting.  improvement of 0.16dB. We also provide qualitative comparison to further illustrate the influence of OCAB and CAB, as depicted in <ref type="figure" target="#fig_3">Fig. 6</ref>. We can observe that the model with OCAB has a larger scope of the utilized pixels and generate better-reconstructed results. When CAB is adopted, the used pixels even expand to the full image. Moreover, the result of our method with OCAB and CAB obtains the highest DI <ref type="bibr" target="#b13">[14]</ref>, which means our method utilizes the most input pixels.</p><p>Although it obtains a little lower performance than w/OCAB, our method gets the highest SSIM and reconstructs the clearest textures. Effects of the overlapping size. As presented in Sec. 4, we set a constant ? to control the overlapping size of OCAB. To explore the effects of different overlapping sizes for the method, we set a group of ? from 0 to 0.75 to examine the performance change, as shown in Tab. 3. Note that ? = 0 means a standard Transformer block. It can be found that the model with ? = 0.5 performs best. In contrast, when ? is set to 0.25 or 0.75, the model has no obvious performance gain or even has a performance drop. It illustrates that inappropriate overlapping size cannot benefit the interaction of neighboring windows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of different designs of CAB.</head><p>We conduct experiments to explore the effects of different designs of CAB. First, we investigate the influence of convolution design and channel attention. As shown in Tab.4, using depth-wise convolution causes a severe performance drop, which means depth-wise convolution weakens the representative ability of CAB. Besides, we can observe that channel attention brings obvious performance improvement. It demonstrates the effectiveness of channel attention. We also conduct experiments to explore the effects of the weighting factor ? of CAB. As presented in Sec. 3.2, ? is used to control the weight of CAB features for feature fusion. A larger ? means a larger weight of features extracted by CAB and ? = 0 represents CAB is not used. As shown in Tab. 5, the model with ? of 0.01 obtains the best performance. It indicates that CAB and self-attention may have potential issue in optimization, while a small weighting factor can suppress this issue for the better combination.   Urban100 dataset, as it contains more structured and self-repeated patterns. This is beneficial for our method to utilize more useful information. All these results have demonstrated the effectiveness of our method.</p><p>Visual comparison. We also provide the visual comparison as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. For the images "img 011", "img 044" and "img 073" in Urban100 dataset, our method can successfully recover the clear lattice content. In contrast, the other approaches all suffer from severe blurry effects. We can observe similar behaviors on "HealingPlanet" in Manga109 dataset. When recovering the characters in the image, HAT obtains clearer textures than the other methods. Combining the quantitative comparison results, we demonstrate the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of the same-task pre-training</head><p>As shown in Tab. 6, models using the same-task pre-training strategy significantly outperform models without pre-training. EDT [26] also explores the effects of different pre-training strategies for the SR task. It demonstrates that pre-training on ImageNet based on multi-related-tasks (i.e., pre-train the SR model on ?2, ?3, ?4 SR) is the most effective strategy compared to single-task pre-training (e.g., pre-training on ?2 SR setup for ?4 SR task) and multiunrelated-task pre-training similar to <ref type="bibr" target="#b4">[5]</ref>. To demonstrate the effectiveness and superiority of our strategy, we also apply the same strategy as EDT in our network. For a fair comparison, both strategies adopt the full ImageNet dataset with 1.28 million images. We provide the quantitative results of the pre-trained models as well as the fine-tuned models on ?4 SR, as shown in Tab. 7. As one can see that the same-task pre-training performs better, not only in the pre-training stage but also in the fine-tuning process. Compared to pre-training on the specific task, the multi-task pre-training seems to weaken the performance. From this perspective, we tend to believe that the reason "why pre-training works" is attributed to the diversity of data instead of the correlation between tasks. <ref type="table">Table 7</ref>: Quantitative results of HAT using two kinds of pre-training strategies on ?4 SR under the same training setting. The full ImageNet dataset is adopted to perform pre-training and DF2K dataset is used for fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel Hybrid Attention Transformer, HAT, for image super-resolution. Our model combines channel attention and self-attention to activate more pixels for reconstructing high-resolution results. Besides, we propose an overlapping cross-attention module that calculates attention between features with different window sizes to better aggregate the cross-window information. Moreover, we introduce a same-task pre-training strategy to further activate the potential of the proposed model. Extensive experiments show the effectiveness of the proposed modules, and our HAT significantly outperforms the state-of-the-art methods. semantics" in super-resolution networks. arXiv preprint arXiv:2108.00406 (2021) 36. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training Details</head><p>We use DF2K (DIV2K [31]+Flicker2K <ref type="bibr" target="#b27">[47]</ref>) with 3360 images as the training dataset when training from scratch. The low-resolution images are generated from the ground truth images by the "bicubic" down-sampling in MATLAB. We set the input patch size to 64 ? 64 and use random rotation and horizontally flipping for data augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Analysis of Model Complexity</head><p>We conduct experiments to analyze the computational complexity of our method from three aspects: large window size, overlapping cross-attention block (OCAB) and channel attention block (CAB) in the hybrid attention block. We evaluate the performance based on the results of ?4 SR on the Urban100 <ref type="bibr" target="#b16">[17]</ref> and the number of Multiply-Add operations is counted at the input size of 64 ? 64. Note that basic models without using pre-training are used for experiments here. First, we use the standard Swin Transformer block <ref type="bibr">[29]</ref> as the backbone to explore the influence on different window sizes. As shown in Tab. 8, enlarging window size can bring significant performance improvement (+0.36dB) with a little increase in parameters and ?%19 increase in Multiply-Add operations.</p><p>We use window size 16 as the baseline to investigate the computational complexity of the proposed OCAB and CAB. As illustrated in Tab. 9, our OCAB obtains a performance gain with a limited increase of parameters and Multi-Adds. It demonstrates that the proposed OCAB is effective and efficient. Besides, Adding CAB to the baseline model also achieves better performance.</p><p>Since CAB seems to be computationally expensive, we further explore the influence on different sizes of CAB by modulating the squeeze factor ? in CAB (mentioned in Sec. 3.2 in the main paper). As shown in Tab. 10, adding a small CAB whose ? equals 6 can bring performance improvement. When we continuously reduce ?, the performance increases but with larger model sizes. To balance the performance and computational cost, we set ? to 3 as the default setting.</p><p>Furthermore, we compare HAT and SwinIR [29] with the similar numbers of parameters and Multi-Adds in two different settings. For SwinIR-1 and SwinIR-2, we increase the width and depth of the original SwinIR to achieve similar computational complexity as HAT. As shown in Tab.11, HAT obtains the best performance with the lowest computational cost.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More LAM Results</head><p>We provide more qualitative and quantitative comparisons of the LAM results between SwinIR and our method. The red points in LAM results represent the used pixels for reconstructing the patch marked with a red box in the HR image, and DI is computed to reflect the range of involved pixels. The more pixels are utilized to recover the specific input patch, the wider the distribution of red points is in LAM and the higher Diffusion Index (DI) <ref type="bibr" target="#b13">[14]</ref> is. As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, the LAM attribution of HAT expands to the almost full image, while that of SwinIR only gathers in a limited range. For the quantitative metric, HAT also obtains a much higher DI value than SwinIR. All these results demonstrate that our method activates more pixels to reconstruct the low-resolution input image. As a result, SR results generated by our method have higher PSNR/SSIM and better visual quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Comparison of Intermediate Features</head><p>We also provide the visual comparison of intermediate features. Since features generated by different models cannot be exactly aligned, to make an alignment, we select the features in the same layers as that in SwinIR by minimizing the L1 distance. As depicted in <ref type="figure" target="#fig_7">Fig. 9</ref>, severe blocking artifacts can be observed in the features generated by SwinIR, while our HAT obtains cleaner textures without blocking artifacts. Note that "Layer N " means the features are extracted after the N th layer (i.e., RSTB in SwinIR and RHAG in HAT, respectively).</p><p>The results demonstrate that our method can better aggregate cross-window information and alleviate the blocking artifacts in the intermediate features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Performance comparison of the proposed HAT with the state-of-the-art methods SwinIR [29] and EDT [26]. HAT-L represents a larger variant of HAT. Our approach can surpass the state-of-the-art methods by 0.3dB?1.2dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The overall architecture of HAT and the structure of RHAG and HAB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparison on models with different window sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative comparisons for the proposed OCAB and CAB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Visual comparison for ?4 SR. The patches for comparison are marked with red boxes in the original images. PSNR/SSIM is calculated based on the patches to better reflect the performance difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>18. Huang, Z., Ben, Y., Luo, G., Cheng, P., Yu, G., Fu, B.: Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650 (2021) 19. Kim, J., Lee, J.K.,Lee, K.M.: Accurate image super-resolution using very deep convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1646-1654 (2016) 20. Kim, J., Lee, J.K., Lee, K.M.: Deeply-recursive convolutional network for image super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1637-1645 (2016) 21. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 22. Kong, X., Liu, X., Gu, J., Qiao, Y., Dong, C.: Reflash dropout in image superresolution. arXiv preprint arXiv:2112.12089 (2021) 23. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks for fast and accurate super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 624-632 (2017) 24. Ledig, C., Theis, L., Husz?r, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic single image superresolution using a generative adversarial network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4681-4690 (2017) 25. Li, K., Wang, Y., Zhang, J., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450 (2022) 26. Li, W., Lu, X., Lu, J., Zhang, X., Jia, J.: On efficient transformer and image pre-training for low-level vision. arXiv preprint arXiv:2112.10175 (2021) 27. Li, Y., Zhang, K., Cao, J., Timofte, R., Van Gool, L.: Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707 (2021) 28. Liang, J., Cao, J., Fan, Y., Zhang, K., Ranjan, R., Li, Y., Timofte, R., Van Gool, L.: Vrt: A video restoration transformer. arXiv preprint arXiv:2201.12288 (2022) 29. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R.: Swinir: Image restoration using swin transformer. In: ICCVW, 2021 30. Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single image super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 136-144 (2017) 31. Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single image super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 136-144 (2017) 32. Lin, Z., Garg, P., Banerjee, A., Magid, S.A., Sun, D., Zhang, Y., Van Gool, L., Wei, D., Pfister, H.: Revisiting rcan: Improved training for image super-resolution. arXiv preprint arXiv:2201.11279 (2022) 33. Liu, D., Wen, B., Fan, Y., Loy, C.C., Huang, T.S.: Non-local recurrent network for image restoration. Advances in neural information processing systems 31 (2018) 34. Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., Pietik?inen, M.: Deep learning for generic object detection: A survey. International journal of computer vision 128(2), 261-318 (2020) 35. Liu, Y., Liu, A., Gu, J., Zhang, Z., Wu, W., Qiao, Y., Dong, C.: Discovering"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>More LAM comparisons between SwinIR and HAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Visual comparisons of intermediate features between SwinIR and our method. "Layer N " represents the intermediate features after the N th layer (i.e., RSTB in SwinIR and RHAG in our HAT, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Transformer [36] has already demonstrated excellent performance in image super-resolution [29]. Then we are eager to know what makes it work better</figDesc><table><row><cell>(a) LAM [14] results for different networks.</cell><cell>(b) Blocking artifacts.</cell></row></table><note>Fig. 2: (a) The LAM attribution reflects the importance of each pixel in the input LR image when reconstructing the patch marked with a red box. DI [14] values are provided below the LAM results. (DI reflects the range of involved pixels. A higher DI represents a wider range of utilized pixels.) The results indicate that SwinIR utilize less information compared to RCAN. (b) The blocking artifacts in the intermediate features of SwinIR. The top is a feature map after the 1st RSTB in SwinIR [29] and the bottom is after the 3rd RSTB.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of models with different window sizes for ?4 SR. We conduct experiments to demonstrate the effectiveness of the proposed CAB and OCAB. The quantitative performance tested on the Urban100 dataset for ?4 SR is shown in Tab. 2. Compared with the baseline, both the proposed OCAB and CAB bring a performance gain of 0.1dB. Benefiting from the two modules, the model obtains a further performance</figDesc><table><row><cell>Window size</cell><cell>Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 BSD100 Urban100 Manga109</cell></row><row><cell>(8,8)</cell><cell>32.88 0.9033 29.09 0.7946 27.92 0.7489 27.45 0.8254 32.03 0.9260</cell></row><row><cell>(16,16)</cell><cell>32.97 0.9049 29.12 0.7958 27.95 0.7504 27.81 0.8336 32.15 0.9274</cell></row><row><cell cols="2">4.3 Ablation Study</cell></row><row><cell cols="2">Effectiveness of OCAB and CAB.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the proposed OCAB and CAB.</figDesc><table><row><cell>Baseline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the overlapping ratio of OCAB.</figDesc><table><row><cell>?</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell></row><row><cell>PSNR/SSIM</cell><cell>27.85/0.8341</cell><cell>27.81/0.8338</cell><cell>27.91/0.8352</cell><cell>27.86/0.8347</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effects of different structures of CAB. "DWCAB" means depth-wise convolution is adopted in CAB. "CA" represents channel attention.</figDesc><table><row><cell>Structure</cell><cell>DWCAB</cell><cell>w/o CA</cell><cell>w/ CA</cell></row><row><cell>PSNR/SSIM</cell><cell>27.86/0.8329</cell><cell>27.92/0.8362</cell><cell>27.97/0.8367</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :4.4 Comparison with State-of-the-Art Methods Quantitative results</head><label>5</label><figDesc>Effects of the weighting factor ?. We can see that our approach outperforms the other methods significantly on all benchmark datasets. Especially, HAT surpasses SwinIR by 0.48dB?0.64dB on the Urban100 dataset and 0.34dB?0.45dB on the Manga109 dataset. When compared with the approaches using pre-training, HAT also has large performance gains of more than 0.5dB against EDT on the Urban100 dataset for all three scales. Besides, HAT with pre-training outperforms SwinIR by a huge margin of up to 1dB on the Urban100 dataset for ?2 SR. Moreover, the large model HAT-L can even bring further improvement. It is noteworthy that the performance gaps are much larger on the</figDesc><table><row><cell>?</cell><cell>0</cell><cell>1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>PSNR/SSIM</cell><cell>27.81/0.8336</cell><cell>27.86/0.8347</cell><cell>27.90/0.8358</cell><cell>27.97/0.8367</cell></row></table><note>. Tab. 6 shows the quantitative comparison of our ap- proach and the state-of-the-art methods: EDSR [30], RCAN [65], SAN [7], IGNN [69], HAN [41], NLSN [40], RCAN-it [32], as well as approaches using Ima- geNet pre-training, i.e., IPT [5] and EDT [26].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Quantitative comparison with state-of-the-art methods on benchmark datasets. The top three results are marked in red, blue and green. " ?" indicates that methods adopt pre-training strategy on ImageNet.</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>Training Dataset</cell><cell cols="9">Set5 [1] PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 [63] BSD100 [38] Urban100 [17] Manga109 [39]</cell></row><row><cell>EDSR [30]</cell><cell>?2</cell><cell cols="10">DIV2K 38.11 0.9602 33.92 0.9195 32.32 0.9013 32.93 0.9351 39.10 0.9773</cell></row><row><cell>RCAN [65]</cell><cell>?2</cell><cell cols="10">DIV2K 38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786</cell></row><row><cell>SAN [7]</cell><cell>?2</cell><cell cols="10">DIV2K 38.31 0.9620 34.07 0.9213 32.42 0.9028 33.10 0.9370 39.32 0.9792</cell></row><row><cell>IGNN [69]</cell><cell>?2</cell><cell cols="10">DIV2K 38.24 0.9613 34.07 0.9217 32.41 0.9025 33.23 0.9383 39.35 0.9786</cell></row><row><cell>HAN [41]</cell><cell>?2</cell><cell cols="10">DIV2K 38.27 0.9614 34.16 0.9217 32.41 0.9027 33.35 0.9385 39.46 0.9785</cell></row><row><cell>NLSN [40]</cell><cell>?2</cell><cell cols="10">DIV2K 38.34 0.9618 34.08 0.9231 32.43 0.9027 33.42 0.9394 39.59 0.9789</cell></row><row><cell>RCAN-it [32]</cell><cell>?2</cell><cell cols="10">DF2K 38.37 0.9620 34.49 0.9250 32.48 0.9034 33.62 0.9410 39.88 0.9799</cell></row><row><cell>SwinIR [29]</cell><cell>?2</cell><cell cols="10">DF2K 38.42 0.9623 34.46 0.9250 32.53 0.9041 33.81 0.9427 39.92 0.9797</cell></row><row><cell>EDT [26]</cell><cell>?2</cell><cell cols="10">DF2K 38.45 0.9624 34.57 0.9258 32.52 0.9041 33.80 0.9425 39.93 0.9800</cell></row><row><cell>HAT (ours)</cell><cell>?2</cell><cell cols="10">DF2K 38.63 0.9630 34.86 0.9274 32.62 0.9053 34.45 0.9466 40.26 0.9809</cell></row><row><cell>IPT  ? [5]</cell><cell cols="3">?2 ImageNet 38.37</cell><cell>-</cell><cell>34.43</cell><cell>-</cell><cell>32.48</cell><cell>-</cell><cell>33.76</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ? [26]</cell><cell>?2</cell><cell cols="10">DF2K 38.63 0.9632 34.80 0.9273 32.62 0.9052 34.27 0.9456 40.37 0.9811</cell></row><row><cell>HAT  ? (ours)</cell><cell>?2</cell><cell cols="10">DF2K 38.73 0.9637 35.13 0.9282 32.69 0.9060 34.81 0.9489 40.71 0.9819</cell></row><row><cell cols="2">HAT-L  ? (ours) ?2</cell><cell cols="10">DF2K 38.91 0.9646 35.29 0.9293 32.74 0.9066 35.09 0.9505 41.01 0.9831</cell></row><row><cell>EDSR [30]</cell><cell>?3</cell><cell cols="10">DIV2K 34.65 0.9280 30.52 0.8462 29.25 0.8093 28.80 0.8653 34.17 0.9476</cell></row><row><cell>RCAN [65]</cell><cell>?3</cell><cell cols="10">DIV2K 34.74 0.9299 30.65 0.8482 29.32 0.8111 29.09 0.8702 34.44 0.9499</cell></row><row><cell>SAN [7]</cell><cell>?3</cell><cell cols="10">DIV2K 34.75 0.9300 30.59 0.8476 29.33 0.8112 28.93 0.8671 34.30 0.9494</cell></row><row><cell>IGNN [69]</cell><cell>?3</cell><cell cols="10">DIV2K 34.72 0.9298 30.66 0.8484 29.31 0.8105 29.03 0.8696 34.39 0.9496</cell></row><row><cell>HAN [41]</cell><cell>?3</cell><cell cols="10">DIV2K 34.75 0.9299 30.67 0.8483 29.32 0.8110 29.10 0.8705 34.48 0.9500</cell></row><row><cell>NLSN [40]</cell><cell>?3</cell><cell cols="10">DIV2K 34.85 0.9306 30.70 0.8485 29.34 0.8117 29.25 0.8726 34.57 0.9508</cell></row><row><cell>RCAN-it [32]</cell><cell>?3</cell><cell cols="10">DF2K 34.86 0.9308 30.76 0.8505 29.39 0.8125 29.38 0.8755 34.92 0.9520</cell></row><row><cell>SwinIR [29]</cell><cell>?3</cell><cell cols="10">DF2K 34.97 0.9318 30.93 0.8534 29.46 0.8145 29.75 0.8826 35.12 0.9537</cell></row><row><cell>EDT [26]</cell><cell>?3</cell><cell cols="10">DF2K 34.97 0.9316 30.89 0.8527 29.44 0.8142 29.72 0.8814 35.13 0.9534</cell></row><row><cell>HAT (ours)</cell><cell>?3</cell><cell cols="10">DF2K 35.06 0.9329 31.08 0.8555 29.54 0.8167 30.23 0.8896 35.53 0.9552</cell></row><row><cell>IPT  ? [5]</cell><cell cols="3">?3 ImageNet 34.81</cell><cell>-</cell><cell>30.85</cell><cell>-</cell><cell>29.38</cell><cell>-</cell><cell>29.49</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ? [26]</cell><cell>?3</cell><cell cols="10">DF2K 35.13 0.9328 31.09 0.8553 29.53 0.8165 30.07 0.8863 35.47 0.9550</cell></row><row><cell>HAT  ? (ours)</cell><cell>?3</cell><cell cols="10">DF2K 35.16 0.9335 31.33 0.8576 29.59 0.8177 30.70 0.8949 35.84 0.9567</cell></row><row><cell cols="2">HAT-L  ? (ours) ?3</cell><cell cols="10">DF2K 35.28 0.9345 31.47 0.8584 29.63 0.8191 30.92 0.8981 36.02 0.9576</cell></row><row><cell>EDSR [30]</cell><cell>?4</cell><cell cols="10">DIV2K 32.46 0.8968 28.80 0.7876 27.71 0.7420 26.64 0.8033 31.02 0.9148</cell></row><row><cell>RCAN [65]</cell><cell>?4</cell><cell cols="10">DIV2K 32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173</cell></row><row><cell>SAN [7]</cell><cell>?4</cell><cell cols="10">DIV2K 32.64 0.9003 28.92 0.7888 27.78 0.7436 26.79 0.8068 31.18 0.9169</cell></row><row><cell>IGNN [69]</cell><cell>?4</cell><cell cols="10">DIV2K 32.57 0.8998 28.85 0.7891 27.77 0.7434 26.84 0.8090 31.28 0.9182</cell></row><row><cell>HAN [41]</cell><cell>?4</cell><cell cols="10">DIV2K 32.64 0.9002 28.90 0.7890 27.80 0.7442 26.85 0.8094 31.42 0.9177</cell></row><row><cell>NLSN [40]</cell><cell>?4</cell><cell cols="10">DIV2K 32.59 0.9000 28.87 0.7891 27.78 0.7444 26.96 0.8109 31.27 0.9184</cell></row><row><cell>RRDB [53]</cell><cell>?4</cell><cell cols="10">DF2K 32.73 0.9011 28.99 0.7917 27.85 0.7455 27.03 0.8153 31.66 0.9196</cell></row><row><cell>RCAN-it [32]</cell><cell>?4</cell><cell cols="10">DF2K 32.69 0.9007 28.99 0.7922 27.87 0.7459 27.16 0.8168 31.78 0.9217</cell></row><row><cell>SwinIR [29]</cell><cell>?4</cell><cell cols="10">DF2K 32.92 0.9044 29.09 0.7950 27.92 0.7489 27.45 0.8254 32.03 0.9260</cell></row><row><cell>EDT [26]</cell><cell>?4</cell><cell cols="10">DF2K 32.82 0.9031 29.09 0.7939 27.91 0.7483 27.46 0.8246 32.05 0.9254</cell></row><row><cell>HAT (ours)</cell><cell>?4</cell><cell cols="10">DF2K 33.04 0.9056 29.23 0.7973 28.00 0.7517 27.97 0.8368 32.48 0.9292</cell></row><row><cell>IPT  ? [5]</cell><cell cols="3">?4 ImageNet 32.64</cell><cell>-</cell><cell>29.01</cell><cell>-</cell><cell>27.82</cell><cell>-</cell><cell>27.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ? [26]</cell><cell>?4</cell><cell cols="10">DF2K 33.06 0.9055 29.23 0.7971 27.99 0.7510 27.75 0.8317 32.39 0.9283</cell></row><row><cell>HAT  ? (ours)</cell><cell>?4</cell><cell cols="10">DF2K 33.18 0.9073 29.38 0.8001 28.05 0.7534 28.37 0.8447 32.87 0.9319</cell></row><row><cell cols="2">HAT-L  ? (ours) ?4</cell><cell cols="10">DF2K 33.30 0.9083 29.47 0.8015 28.09 0.7551 28.60 0.8498 33.09 0.9335</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The mini-batch size is set to 32 and total training iterations are set to 500K. The learning rate is initialized as 2e-4 and reduced by half at [250K,400K,450K,475K]. For ?4 SR, we initialize the model with pre-trained ?2 SR weights and halve the iterations for each learning rate decay as well as total iterations. We adopt Adam [21] optimizer with ? 1 = 0.9 and ? 2 = 0.99 to train the model. For the same-task pre-training, the full ImageNet dataset [8] with 1.28 million images is first exploited to pre-train the model for 800K iterations. The initial learning rate is also set to 2e-4 but reduced by half at [300K,500K,650K,700K,750k]. Then, we also adopt DF2K dataset to fine-tune the pre-trained model. For fine-tuning, we set the initial learning rate to 1e-5 and halve it at [125K,200K,230K,240K] for total of 250K training iterations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of computational complexity on different window sizes.</figDesc><table><row><cell>window size</cell><cell>#Params. (M)</cell><cell>#Multi-Adds. (G)</cell><cell>PSNR (dB)</cell></row><row><cell>(8, 8)</cell><cell>11.9</cell><cell>53.6</cell><cell>27.45</cell></row><row><cell>(16, 16)</cell><cell>12.1</cell><cell>63.8</cell><cell>27.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of computational complexity on OCAB and CAB.</figDesc><table><row><cell>Method</cell><cell>#Params. (M)</cell><cell>#Multi-Adds. (G)</cell><cell>PSNR (dB)</cell></row><row><cell>Baseline</cell><cell>12.1</cell><cell>63.8</cell><cell>27.81</cell></row><row><cell>Baseline w/ OCAB</cell><cell>13.7</cell><cell>74.7</cell><cell>27.91</cell></row><row><cell>Baseline w/ CAB</cell><cell>19.2</cell><cell>92.8</cell><cell>27.91</cell></row><row><cell>Ours</cell><cell>20.8</cell><cell>103.7</cell><cell>27.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of computational complexity on different sizes of CAB.</figDesc><table><row><cell>? in CAB</cell><cell>#Params. (M)</cell><cell>#Multi-Adds. (G)</cell><cell>PSNR (dB)</cell></row><row><cell>1</cell><cell>33.2</cell><cell>150.1</cell><cell>27.97</cell></row><row><cell>2</cell><cell>22.7</cell><cell>107.1</cell><cell>27.92</cell></row><row><cell>3 (default)</cell><cell>19.2</cell><cell>92.8</cell><cell>27.91</cell></row><row><cell>6</cell><cell>15.7</cell><cell>78.5</cell><cell>27.88</cell></row><row><cell>w/o CAB</cell><cell>12.1</cell><cell>63.8</cell><cell>27.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of computational complexity between SwinIR and HAT.</figDesc><table><row><cell>Method</cell><cell>#Params. (M)</cell><cell>#Multi-Adds. (G)</cell><cell>PSNR (dB)</cell></row><row><cell>SwinIR-1</cell><cell>24.0</cell><cell>104.4</cell><cell>27.53</cell></row><row><cell>SwinIR-2</cell><cell>23.1</cell><cell>102.4</cell><cell>27.58</cell></row><row><cell>HAT</cell><cell>20.8</cell><cell>103.7</cell><cell>27.97</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<title level="m">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<title level="m">Swinunet: Unet-like pure transformer for medical image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06847</idno>
		<title level="m">Video super-resolution transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<publisher>ECCV</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpreting super-resolution networks with local attribution maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9199" to="9208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Glance and focus networks for dynamic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03014</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient transformer for single image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11084</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image super-resolution with non-local sparse attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Aggregating global features into local vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12903</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Studying stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-esrgan: Training real-world blind super-resolution with pure synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1905" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV) workshops</title>
		<meeting>the European conference on computer vision (ECCV) workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<title level="m">Visual transformers: Token-based image representation and processing for computer vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pale transformer: A general vision transformer backbone with pale-shaped attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14000</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Finding discriminative filters for specific degradations in blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hrformer: High-resolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09408</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09881</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ranksrgan: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10082</idno>
		<title level="m">Residual non-local attention networks for image restoration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13002</idno>
		<title level="m">A battle of network structures: An empirical study of cnn, transformer, and mlp</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-scale internal graph neural network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3499" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
