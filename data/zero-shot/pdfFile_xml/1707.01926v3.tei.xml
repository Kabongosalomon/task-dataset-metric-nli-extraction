<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12% -15% over state-of-the-art baselines. Published as a conference paper at ICLR 2018 the traffic data. Most recently, deep learning models for traffic forecasting have been developed in Lv et al. (2015); Yu et al. (2017b), but without considering the spatial structure. Wu &amp; Tan (2016) and Ma et al. (2017) model the spatial correlation with Convolutional Neural Networks (CNN), but the spatial structure is in the Euclidean space (e.g., 2D images). Bruna et al. (2014), Defferrard et al. (2016) studied graph convolution, but only for undirected graphs.</p><p>In this work, we represent the pair-wise spatial correlations between traffic sensors using a directed graph whose nodes are sensors and edge weights denote proximity between the sensor pairs measured by the road network distance. We model the dynamics of the traffic flow as a diffusion process and propose the diffusion convolution operation to capture the spatial dependency. We further propose Diffusion Convolutional Recurrent Neural Network (DCRNN) that integrates diffusion convolution, the sequence to sequence architecture and the scheduled sampling technique. When evaluated on realworld traffic datasets, DCRNN consistently outperforms state-of-the-art traffic forecasting baselines by a large margin. In summary:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Spatiotemporal forecasting is a crucial task for a learning system that operates in a dynamic environment. It has a wide range of applications from autonomous vehicles operations, to energy and smart grid optimization, to logistics and supply chain management. In this paper, we study one important task: traffic forecasting on road networks, the core component of the intelligent transportation systems. The goal of traffic forecasting is to predict the future traffic speeds of a sensor network given historic traffic speeds and the underlying road networks. <ref type="figure">Figure 1</ref>: Spatial correlation is dominated by road network structure. (1) Traffic speed in road 1 are similar to road 2 as they locate in the same highway.</p><p>(2) Road 1 and road 3 locate in the opposite directions of the highway. Though close to each other in the Euclidean space, their road network distance is large, and their traffic speeds differ significantly.</p><p>This task is challenging mainly due to the complex spatiotemporal dependencies and inherent difficulty in the long term forecasting. On the one hand, traffic time series demonstrate strong temporal dynamics. Recurring incidents such as rush hours or accidents can cause nonstationarity, making it difficult to forecast longterm. On the other hand, sensors on the road network contain complex yet unique spatial correlations. <ref type="figure">Figure 1</ref> illustrates an example. Road 1 and road 2 are correlated, while road 1 and road 3 are not. Although road 1 and road 3 are close in the Euclidean space, they demonstrate very different behaviors. Moreover, the future traffic speed is influenced more by the downstream traffic than the upstream one. This means that the spatial structure in traffic is non-Euclidean and directional.</p><p>Traffic forecasting has been studied for decades, falling into two main categories: knowledgedriven approach and data-driven approach. In transportation and operational research, knowledgedriven methods usually apply queuing theory and simulate user behaviors in traffic <ref type="bibr" target="#b6">(Cascetta, 2013)</ref>. In time series community, data-driven methods such as Auto-Regressive Integrated Moving Average (ARIMA) model and Kalman filtering remain popular <ref type="bibr" target="#b22">(Liu et al., 2011;</ref><ref type="bibr" target="#b21">Lippi et al., 2013)</ref>. However, simple time series models usually rely on the stationarity assumption, which is often violated by ? We study the traffic forecasting problem and model the spatial dependency of traffic as a diffusion process on a directed graph. We propose diffusion convolution, which has an intuitive interpretation and can be computed efficiently. ? We propose Diffusion Convolutional Recurrent Neural Network (DCRNN), a holistic approach that captures both spatial and temporal dependencies among time series using diffusion convolution and the sequence to sequence learning framework together with scheduled sampling. DCRNN is not limited to transportation and is readily applicable to other spatiotemporal forecasting tasks. ? We conducted extensive experiments on two large-scale real-world datasets, and the proposed approach obtains significant improvement over state-of-the-art baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>We formalize the learning problem of spatiotemporal traffic forecasting and describe how to model the dependency structures using diffusion convolutional recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TRAFFIC FORECASTING PROBLEM</head><p>The goal of traffic forecasting is to predict the future traffic speed given previously observed traffic flow from N correlated sensors on the road network. We can represent the sensor network as a weighted directed graph G = (V, E, W ), where V is a set of nodes |V| = N , E is a set of edges and W ? R N ?N is a weighted adjacency matrix representing the nodes proximity (e.g., a function of their road network distance). Denote the traffic flow observed on G as a graph signal X ? R N ?P , where P is the number of features of each node (e.g., velocity, volume). Let X (t) represent the graph signal observed at time t, the traffic forecasting problem aims to learn a function h(?) that maps T historical graph signals to future T graph signals, given a graph G:</p><formula xml:id="formula_0">[X (t?T +1) , ? ? ? , X (t) ; G] h(?) ??? [X (t+1) , ? ? ? , X (t+T ) ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPATIAL DEPENDENCY MODELING</head><p>We model the spatial dependency by relating traffic flow to a diffusion process, which explicitly captures the stochastic nature of traffic dynamics. This diffusion process is characterized by a random walk on G with restart probability ? ? [0, 1], and a state transition matrix D ?1 O W . Here D O = diag(W 1) is the out-degree diagonal matrix, and 1 ? R N denotes the all one vector. After many time steps, such Markov process converges to a stationary distribution P ? R N ?N whose ith row P i,: ? R N represents the likelihood of diffusion from node v i ? V, hence the proximity w.r.t. the node v i . The following Lemma provides a closed form solution for the stationary distribution. Lemma 2.1. <ref type="bibr" target="#b29">(Teng et al., 2016)</ref> The stationary distribution of the diffusion process can be represented as a weighted combination of infinite random walks on the graph, and be calculated in closed form:</p><formula xml:id="formula_1">P = ? k=0 ?(1 ? ?) k D ?1 O W k<label>(1)</label></formula><p>where k is the diffusion step. In practice, we use a finite K-step truncation of the diffusion process and assign a trainable weight to each step. We also include the reversed direction diffusion process, such that the bidirectional diffusion offers the model more flexibility to capture the influence from both the upstream and the downstream traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Convolution</head><p>The resulted diffusion convolution operation over a graph signal X ? R N ?P and a filter f ? is defined as:</p><formula xml:id="formula_2">X :,p G f ? = K?1 k=0 ? k,1 D ?1 O W k + ? k,2 D ?1 I W k X :,p for p ? {1, ? ? ? , P }<label>(2)</label></formula><p>where ? ? R K?2 are the parameters for the filter and D ?1 O W , D ?1 I W represent the transition matrices of the diffusion process and the reverse one, respectively. In general, computing the convolution can be expensive. However, if G is sparse, Equation 2 can be calculated efficiently using O(K) recursive sparse-dense matrix multiplication with total time complexity O(K|E|) O(N 2 ). See Appendix B for more detail.</p><p>Diffusion Convolutional Layer With the convolution operation defined in Equation 2, we can build a diffusion convolutional layer that maps P -dimensional features to Q-dimensional outputs. Denote the parameter tensor as ? ? R Q?P ?K?2 = [?] q,p , where ? q,p,:,: ? R K?2 parameterizes the convolutional filter for the pth input and the qth output. The diffusion convolutional layer is thus:</p><formula xml:id="formula_3">H :,q = a P p=1 X :,p G f ?q,p,:,: for q ? {1, ? ? ? , Q}<label>(3)</label></formula><p>where X ? R N ?P is the input, H ? R N ?Q is the output, {f ?q,p,,: } are the filters and a is the activation function (e.g., ReLU, Sigmoid). Diffusion convolutional layer learns the representations for graph structured data and we can train it using stochastic gradient based method.</p><p>Relation with Spectral Graph Convolution Diffusion convolution is defined on both directed and undirected graphs. When applied to undirected graphs, we show that many existing graph structured convolutional operations including the popular spectral graph convolution, i.e., ChebNet , can be considered as a special case of diffusion convolution (up to a similarity transformation). Let D denote the degree matrix, and L = D ? 1 2 (D ? W )D ? 1 2 be the normalized graph Laplacian, the following Proposition demonstrates the connection. Proposition 2.2. The spectral graph convolution defined as</p><formula xml:id="formula_4">X :,p G f ? = ? F (?) ? X :,p</formula><p>with eigenvalue decomposition L = ??? and F (?) = K?1 0 ? k ? k , is equivalent to graph diffusion convolution up to a similarity transformation, when the graph G is undirected.</p><p>Proof. See Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TEMPORAL DYNAMICS MODELING</head><p>We leverage the recurrent neural networks (RNNs) to model the temporal dependency. In particular, we use Gated Recurrent Units (GRU) <ref type="bibr" target="#b9">(Chung et al., 2014)</ref>, which is a simple yet powerful variant of RNNs. We replace the matrix multiplications in GRU with the diffusion convolution, which leads to our proposed Diffusion Convolutional Gated Recurrent Unit (DCGRU).</p><formula xml:id="formula_5">r (t) = ?(? r G [X (t) , H (t?1) ] + b r ) u (t) = ?(? u G [X (t) , H (t?1) ] + b u ) C (t) = tanh(? C G X (t) , (r (t) H (t?1) ) + b c ) H (t) = u (t) H (t?1) + (1 ? u (t) ) C (t)</formula><p>where X (t) , H (t) denote the input and output of at time t, r (t) , u (t) are reset gate and update gate at time t, respectively. G denotes the diffusion convolution defined in Equation 2 and ? r , ? u , ? C are parameters for the corresponding filters. Similar to GRU, DCGRU can be used to build recurrent neural network layers and be trained using backpropagation through time.</p><p>In multiple step ahead forecasting, we employ the Sequence to Sequence architecture <ref type="bibr" target="#b28">(Sutskever et al., 2014)</ref>. Both the encoder and the decoder are recurrent neural networks with DCGRU. During training, we feed the historical time series into the encoder and use its final states to initialize the ? ... ...  decoder. The decoder generates predictions given previous ground truth observations. At testing time, ground truth observations are replaced by predictions generated by the model itself. The discrepancy between the input distributions of training and testing can cause degraded performance. To mitigate this issue, we integrate scheduled sampling <ref type="bibr" target="#b2">(Bengio et al., 2015)</ref> into the model, where we feed the model with either the ground truth observation with probability i or the prediction by the model with probability 1 ? i at the ith iteration. During the training process, i gradually decreases to 0 to allow the model to learn the testing distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion</head><p>With both spatial and temporal modeling, we build a Diffusion Convolutional Recurrent Neural Network (DCRNN). The model architecture of DCRNN is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The entire network is trained by maximizing the likelihood of generating the target future time series using backpropagation through time. DCRNN is able to capture spatiotemporal dependencies among time series and can be applied to various spatiotemporal forecasting problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Traffic forecasting is a classic problem in transportation and operational research which are primarily based on queuing theory and simulations <ref type="bibr" target="#b12">(Drew, 1968)</ref>. Data-driven approaches for traffic forecasting have received considerable attention, and more details can be found in a recent survey paper (Vlahogianni et al., 2014) and the references therein. However, existing machine learning models either impose strong stationary assumptions on the data (e.g., auto-regressive model) or fail to account for highly non-linear temporal dependency (e.g.,  <ref type="formula" target="#formula_1">2017)</ref>, the authors study time series forecasting using deep Recurrent Neural Networks (RNN). Convolutional Neural Networks (CNN) have also been applied to traffic forecasting. <ref type="bibr" target="#b36">Zhang et al. (2016;</ref> convert the road network to a regular 2-D grid and apply traditional CNN to predict crowd flow. <ref type="bibr" target="#b8">Cheng et al. (2017)</ref> propose DeepTransport which models the spatial dependency by explicitly collecting upstream and downstream neighborhood roads for each individual road and then conduct convolution on these neighborhoods respectively.</p><p>Recently, CNN has been generalized to arbitrary graphs based on the spectral graph theory. Graph convolutional neural networks (GCN) are first introduced in <ref type="bibr" target="#b4">Bruna et al. (2014)</ref>, which bridges the spectral graph theory and deep neural networks.  propose ChebNet which improves GCN with fast localized convolutions filters. Kipf &amp; Welling (2017) simplify ChebNet and achieve state-of-the-art performance in semi-supervised classification tasks. <ref type="bibr" target="#b26">Seo et al. (2016)</ref> combine ChebNet with Recurrent Neural Networks (RNN) for structured sequence modeling. <ref type="bibr" target="#b33">Yu et al. (2017a)</ref> model the sensor network as a undirected graph and applied ChebNet and convolutional sequence model <ref type="bibr" target="#b14">(Gehring et al., 2017)</ref> to do forecasting. One limitation of the mentioned spectral based convolutions is that they generally require the graph to be undirected to calculate meaningful  <ref type="formula" target="#formula_1">(2016)</ref> propose diffusion-convolutional neural network (DCNN) which defines convolution as a diffusion process across each node in a graph-structured input. <ref type="bibr" target="#b17">Hechtlinger et al. (2017)</ref> propose GraphCNN to generalize convolution to graph by convolving every node with its p nearest neighbors. However, both these methods do not consider the temporal dynamics and mainly deal with static graph settings.</p><p>Our approach is different from all those methods due to both the problem settings and the formulation of the convolution on the graph. We model the sensor network as a weighted directed graph which is more realistic than grid or undirected graph. Besides, the proposed convolution is defined using bidirectional graph random walk and is further integrated with the sequence to sequence learning framework as well as the scheduled sampling to model the long-term temporal dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct experiments on two real-world large-scale datasets: (1) METR-LA This traffic dataset contains traffic information collected from loop detectors in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment.</p><p>(2) PEMS-BAY This traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment. The sensor distributions of both datasets are visualized in <ref type="figure" target="#fig_6">Figure 8</ref> in the Appendix.</p><p>In both of those datasets, we aggregate traffic speed readings into 5 minutes windows, and apply Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining 10% for validation. To construct the sensor graph, we compute the pairwise road network distances between sensors and build the adjacency matrix using thresholded Gaussian kernel <ref type="bibr" target="#b27">(Shuman et al., 2013)</ref>.</p><formula xml:id="formula_6">W ij = exp ? dist(vi,vj ) 2 ? 2 if dist(v i , v j ) ? ?, otherwise 0,</formula><p>where W ij represents the edge weight between sensor v i and sensor v j , dist(v i , v j ) denotes the road network distance from sensor v i to sensor v j . ? is the standard deviation of distances and ? is the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETTINGS</head><p>Baselines We compare DCRNN 1 with widely used time series regression models, including (1) HA: Historical Average, which models the traffic flow as a seasonal process, and uses weighted ,WHUDWLRQ All neural network based approaches are implemented using Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>, and trained using the Adam optimizer with learning rate annealing. The best hyperparameters are chosen using the Tree-structured Parzen Estimator (TPE) (Bergstra et al., 2011) on the validation dataset.</p><formula xml:id="formula_7">9DOLGDWLRQ0$( '&amp;5111R&amp;RQY '&amp;5118QL&amp;RQY '&amp;511</formula><p>Detailed parameter settings for DCRNN as well as baselines are available in Appendix E. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead forecasting on both datasets. These methods are evaluated based on three commonly used metrics in traffic forecasting, including (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error (MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded in calculating these metrics. Detailed formulations of these metrics are provided in Appendix E.2. We observe the following phenomenon in both of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRAFFIC FORECASTING PERFORMANCE COMPARISON</head><p>(1) RNN-based methods, including FC-LSTM and DCRNN, generally outperform other baselines which emphasizes the importance of modeling the temporal dependency.</p><p>(2) DCRNN achieves the best performance regarding all the metrics for all forecasting horizons, which suggests the effectiveness of spatiotemporal dependency modeling. <ref type="formula" target="#formula_3">(3)</ref> Deep neural network based methods including FNN, FC-LSTM and DCRNN, tend to have better performance than linear baselines for long-term forecasting, e.g., 1 hour ahead. This is because the temporal dependency becomes increasingly non-linear with the growth of the horizon. Besides, as the historical average method does not depend on short-term data, its performance is invariant to the small increases in the forecasting horizon.</p><p>Note that, traffic forecasting on the METR-LA (Los Angeles, which is known for its complicated traffic conditions) dataset is more challenging than that in the PEMS-BAY (Bay Area) dataset. Thus we use METR-LA as the default dataset for following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EFFECT OF SPATIAL DEPENDENCY MODELING</head><p>To further investigate the effect of spatial dependency modeling, we compare DCRNN with the following variants: (1) DCRNN-NoConv, which ignores spatial dependency by replacing the transition matrices in the diffusion convolution (Equation 2) with identity matrices. This essentially means the forecasting of a sensor can be only be inferred from its own historical readings;</p><p>(2) DCRNN-UniConv,   which only uses the forward random walk transition matrix for diffusion convolution; <ref type="figure">Figure 3</ref> shows the learning curves of these three models with roughly the same number of parameters. Without diffusion convolution, DCRNN-NoConv has much higher validation error. Moreover, DCRNN achieves the lowest validation error which shows the effectiveness of using bidirectional random walk. The intuition is that the bidirectional random walk gives the model the ability and flexibility to capture the influence from both the upstream and the downstream traffic.</p><p>To investigate the effect of graph construction, we construct a undirected graph by setting W ij = W ji = max(W ij , W ji ), where W is the new symmetric weight matrix. Then we develop a variant of DCRNN denotes GCRNN, which uses the sequence to sequence learning with ChebNet graph convolution (Equation 5) with roughly the same amount of parameters. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison between DCRNN and GCRNN in the METR-LA dataset. DCRNN consistently outperforms GCRNN. The intuition is that directed graph better captures the asymmetric correlation between traffic sensors. <ref type="figure">Figure 4</ref> shows the effects of different parameters. K roughly corresponds to the size of filters' reception fields while the number of units corresponds to the number of filters. Larger K enables the model to capture broader spatial dependency at the cost of increasing learning complexity. We observe that with the increase of K, the error on the validation dataset first quickly decrease, and then slightly increase. Similar behavior is observed for varying the number of units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EFFECT OF TEMPORAL DEPENDENCY MODELING</head><p>To evaluate the effect of temporal modeling including the sequence to sequence framework as well as the scheduled sampling mechanism, we further design three variants of DCRNN: (1) DCNN: in which we concatenate the historical observations as a fixed length vector and feed it into stacked diffusion convolutional layers to predict the future time series. We train a single model for one step ahead prediction, and feed the previous prediction into the model as input to perform multiple steps ahead prediction. (2) DCRNN-SEQ: which uses the encoder-decoder sequence to sequence learning framework to perform multiple steps ahead forecasting. (3) DCRNN: similar to DCRNN-SEQ except for adding scheduled sampling.  <ref type="figure">Figure 7</ref>: Visualization of learned localized filters centered at different nodes with K = 3 on the METR-LA dataset. The star denotes the center, and the colors represent the weights. We observe that weights are localized around the center, and diffuse alongside the road network. <ref type="figure" target="#fig_3">Figure 5</ref> shows the comparison of those four methods with regards to MAE for different forecasting horizons. We observe that: (1) DCRNN-SEQ outperforms DCNN by a large margin which conforms the importance of modeling temporal dependency. (2) DCRNN achieves the best result, and its superiority becomes more evident with the increase of the forecasting horizon. This is mainly because the model is trained to deal with its mistakes during multiple steps ahead prediction and thus suffers less from the problem of error propagation. We also train a model that always been fed its output as input for multiple steps ahead prediction. However, its performance is much worse than all the three variants which emphasizes the importance of scheduled sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">MODEL INTERPRETATION</head><p>To better understand the model, we visualize forecasting results as well as learned filters. <ref type="figure" target="#fig_4">Figure 6</ref> shows the visualization of 1 hour ahead forecasting. We have the following observations: (1) DCRNN generates smooth prediction of the mean when small oscillation exists in the traffic speeds ( <ref type="figure" target="#fig_4">Figure 6(a)</ref>). This reflects the robustness of the model. (2) DCRNN is more likely to accurately predict abrupt changes in the traffic speed than baseline methods (e.g., FC-LSTM). As shown in <ref type="figure" target="#fig_4">Figure 6(b)</ref>, DCRNN predicts the start and the end of the peak hours. This is because DCRNN captures the spatial dependency, and is able to utilize the speed changes in neighborhood sensors for more accurate forecasting. <ref type="figure">Figure 7</ref> visualizes examples of learned filters centered at different nodes. The star denotes the center, and colors denote the weights. We can observe that (1) weights are well localized around the center, and (2) the weights diffuse based on road network distance. More visualizations are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we formulated the traffic prediction on road network as a spatiotemporal forecasting problem, and proposed the diffusion convolutional recurrent neural network that captures the spatiotemporal dependencies. Specifically, we use bidirectional graph random walk to model spatial dependency and recurrent neural network to capture the temporal dynamics. We further integrated the encoder-decoder architecture and the scheduled sampling technique to improve the performance for long-term forecasting. When evaluated on two large-scale real-world traffic datasets, our approach obtained significantly better prediction than baselines. For future work, we will investigate the following two aspects (1) applying the proposed model to other spatial-temporal forecasting tasks;</p><p>(2) modeling the spatiotemporal dependency when the underlying graph structure is evolving, e.g., the K nearest neighbor graph for moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research has been funded in part by NSF grants CNS-1461963, IIS-1254206, IIS-1539608, Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and the USC METRANS Transportation Center. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any of the sponsors such as NSF. Also, the authors would like to thank Shang-Hua Teng, Dehua Cheng and Siyang Li for helpful discussions and comments. APPENDIX A NOTATION <ref type="table" target="#tab_3">Table 3</ref>: Notation Name G a graph V, v i nodes of a graph, |V| = N and the i-th node. E edges of a graph W , W ij , weight matrix of a graph and its entries D, D I , D O undirected degree matrix, In-degree/out-degree matrix L normalized graph Laplacian ?, ? eigen-vector matrix and eigen-value matrix of L X,X ? R N ?P a graph signal, and the predicted graph signal.</p><formula xml:id="formula_8">X (t) ? R N ?P a graph signal at time t. H ? R N ?Q</formula><p>output of the diffusion convolutional layer. f ? , ? convolutional filter and its parameters. f ? , ? convolutional layer and its parameters.  <ref type="bibr" target="#b7">(Cheng et al., 2015)</ref> to make it sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C RELATION WITH SPECTRAL GRAPH CONVOLUTION</head><p>Proof. The spectral graph convolution utilizes the concept of normalized graph Laplacian L = D ? 1 2 (D ? W )D ? 1 2 = ??? . ChebNet parametrizes f ? to be a K order polynomial of ?, and calculates it using stable Chebyshev polynomial basis.</p><formula xml:id="formula_9">X :,p G f ? = ? K?1 k=0 ? k ? k ? X :,p = K?1 k=0 ? k L k X :,p = K?1 k=0? k T k (L)X :,p<label>(5)</label></formula><p>where T 0 (x) = 1, T 1 (x) = x, T k (x) = xT k?1 (x) ? T k?2 (x) are the basis of the Cheyshev polynomial. Let ? max denote the largest eigenvalue of L, andL = 2 ?max L ? I represents a rescaling of the graph Laplacian that maps the eigenvalues from [0, ? max ] to [?1, 1] since Chebyshev polynomial forms an orthogonal basis in <ref type="bibr">[?1, 1]</ref>. Equation 5 can be considered as a polynomial ofL and we will show that the output of ChebNet Convolution is similar to the output of diffusion convolution up to constant scaling factor. Assume ? max = 2 and D I = D O = D for undirected graph.</p><formula xml:id="formula_10">L = D ? 1 2 (D ? W )D ? 1 2 ? I = ?D ? 1 2 W D ? 1 2 ? ?D ?1 W<label>(6)</label></formula><p>L is similar to the negative random walk transition matrix, thus the output of Equation 5 is also similar to the output of Equation 2 up to constant scaling factor. GPs are hard to scale to the large dataset and are generally not suitable for relatively long-term traffic prediction like 1 hour (i.e.,12 steps ahead), as the variance can be accumulated and becomes extremely large. <ref type="bibr" target="#b5">Cai et al. (2016)</ref> propose to use spatiotemporal nearest neighbor for traffic forecasting (ST-KNN). Though ST-KNN considers both the spatial and the temporal dependencies, it has the following drawbacks. As shown in <ref type="bibr" target="#b13">Fusco et al. (2016)</ref>, ST-KNN performs independent forecasting for each individual road. The prediction of a road is a weighted combination of its own historical traffic speeds. This makes it hard for ST-KNN to fully utilize information from neighbors. Besides, ST-KNN is a non-parametric approach and each road is modeled and calculated separately <ref type="bibr" target="#b5">(Cai et al., 2016)</ref>, which makes it hard to generalize to unseen situations and to scale to large datasets. Finally, in ST-KNN, all the similarities are calculated using hand-designed metrics with few learnable parameters, and this may limit its representational power. <ref type="bibr" target="#b8">Cheng et al. (2017)</ref> propose DeepTransport which models the spatial dependency by explicitly collecting certain number of upstream and downstream roads for each individual road and then conduct convolution on these roads respectively. Comparing with <ref type="bibr" target="#b8">Cheng et al. (2017)</ref>, DCRNN models the spatial dependency in a more systematic way, i.e., generalizing convolution to the traffic sensor graph based on the diffusion nature of traffic. Besides, we derive DCRNN from the property of random walk and show that the popular spectral convolution ChebNet is a special case of our method.</p><p>The proposed approach is also related to graph embedding techniques, e.g., Deepwalk <ref type="bibr" target="#b25">(Perozzi et al., 2014)</ref>, node2vec <ref type="bibr" target="#b15">(Grover &amp; Leskovec, 2016</ref>) which learn a low dimension representation for each node in the graph. DCRNN also learns a representation for each node. The learned representations capture both the spatial and the temporal dependency and at the same time are optimized with regarding to the objective, e.g., future traffic speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILED EXPERIMENTAL SETTINGS</head><p>HA Historical Average, which models the traffic flow as a seasonal process, and uses weighted average of previous seasons as the prediction. The period used is 1 week, and the prediction is based on aggregated data from previous weeks. For example, the prediction for this Wednesday is the averaged traffic speeds from last four Wednesdays. As the historical average method does not depend on short-term data, its performance is invariant to the small increases in the forecasting horizon ARIMA kal : Auto-Regressive Integrated Moving Average model with Kalman filter. The orders are (3, 0, 1), and the model is implemented using the statsmodel python package.</p><p>VAR Vector Auto-regressive model <ref type="bibr" target="#b16">(Hamilton, 1994)</ref>. The number of lags is set to 3, and the model is implemented using the statsmodel python package.</p><p>SVR Linear Support Vector Regression, the penalty term C = 0.1, the number of historical observation is 5.</p><p>The following deep neural network based approaches are also included.</p><p>FNN Feed forward neural network with two hidden layers, each layer contains 256 units. The initial learning rate is 1e ?3 , and reduces to 1 10 every 20 epochs starting at the 50th epochs. In addition, for all hidden layers, dropout with ratio 0.5 and L2 weight decay 1e ?2 is used. The model is trained with batch size 64 and MAE as the loss function. Early stop is performed by monitoring the validation error.</p><p>FC-LSTM The Encoder-decoder framework using LSTM with peephole <ref type="bibr" target="#b28">(Sutskever et al., 2014)</ref>. Both the encoder and the decoder contain two recurrent layers. In each recurrent layer, there are 256 LSTM units, L1 weight decay is 2e ?5 , L2 weight decay 5e ?4 . The model is trained with batch size 64 and loss function MAE. The initial learning rate is 1e-4 and reduces to 1 10 every 10 epochs starting from the 20th epochs. Early stop is performed by monitoring the validation error.</p><p>DCRNN : Diffusion Convolutional Recurrent Neural Network. Both encoder and decoder contain two recurrent layers. In each recurrent layer, there are 64 units, the initial learning rate is 1e ?2 , and reduces to 1 10 every 10 epochs starting at the 20th epoch and early stopping on the validation dataset is used. Besides, the maximum steps of random walks, i.e., K, is set to 3. For scheduled sampling, the thresholded inverse sigmoid function is used as the probability decay:</p><formula xml:id="formula_11">i = ? ? + exp (i/? )</formula><p>where i is the number of iterations while ? are parameters to control the speed of convergence. ? is set to 3,000 in the experiments. The implementation is available in https://github.com/ liyaguang/DCRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 DATASET</head><p>We conduct experiments on two real-world large-scale datasets:</p><p>? METR-LA This traffic dataset contains traffic information collected from loop detectors in the highway of Los Angeles County <ref type="bibr" target="#b18">(Jagadish et al., 2014)</ref>. We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment. The total number of observed traffic data points is 6,519,002.</p><p>? PEMS-BAY This traffic dataset is collected by California Transportation Agencies (Cal-Trans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment. The total number of observed traffic data points is 16,937,179.</p><p>The sensor distributions of both datasets are visualized in <ref type="figure" target="#fig_6">Figure 8</ref>.</p><p>In both of those datasets, we aggregate traffic speed readings into 5 minutes windows, and apply Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining 10% for validation. To construct the sensor graph, we compute the pairwise road network distances between sensors and build the adjacency matrix using thresholded Gaussian kernel <ref type="bibr" target="#b27">(Shuman et al., 2013)</ref>.</p><formula xml:id="formula_12">W ij = exp ? dist(v i , v j ) 2 ? 2 if dist(v i , v j ) ? ?, otherwise 0</formula><p>where W ij represents the edge weight between sensor v i and sensor v j , dist(v i , v j ) denotes the road network distance from sensor v i to sensor v j . ? is the standard deviation of distances and ? is the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 METRICS</head><p>Suppose x = x 1 , ? ? ? , x n represents the ground truth,x =x 1 , ? ? ? ,x n represents the predicted values, and ? denotes the indices of observed samples, the metrics are defined as follows. : Sensor correlations between the center sensor and its neighborhoods for different forecasting horizons. The correlations are estimated using regularized VAR. We observe that the correlations are localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation quickly decay with the increase of distance which is consistent with the diffusion process on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>System architecture for the Diffusion Convolutional Recurrent Neural Network designed for spatiotemporal traffic forecasting. The historical time series are fed into an encoder whose final states are used to initialize the decoder. The decoder makes predictions based on either previous ground truth or the model output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>latent space model Yu et al. (2016); Deng et al. (2016)). Deep learning models deliver new promise for time series forecasting problem. For example, in Yu et al. (2017b); Laptev et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Learning curve for DCRNN and DCRNN without diffusion convolution. Removing diffusion convolution results in much higher validation error. Moreover, DCRNN with bidirectional random walk achieves the lowest val-Effects of K and the number of units in each layer of DCRNN. K corresponds to the reception field width of the filter, and the number of units corresponds to the number of filters. average of previous seasons as the prediction; (2) ARIMA kal : Auto-Regressive Integrated Moving Average model with Kalman filter which is widely used in time series prediction; (3) VAR: Vector Auto-Regression (Hamilton, 1994). (4) SVR: Support Vector Regression which uses linear support vector machine for the regression task; The following deep neural network based approaches are also included: (5) Feed forward Neural network (FNN): Feed forward neural network with two hidden layers and L2 regularization. (6) Recurrent Neural Network with fully connected LSTM hidden units (FC-LSTM) (Sutskever et al., 2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison for different DCRNN variants. DCRNN, with the sequence to sequence framework and scheduled sampling, achieves the lowest MAE on the validation dataset. The advantage becomes more clear with the increase of the forecasting horizon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Traffic time series forecasting visualization. DCRNN generates smooth prediction and is usually better at predict the start and end of peak hours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Sensor distribution of the METR-LA and PEMS-BAY dataset. D MORE RELATED WORK AND DISCUSSION Xie et al. (2010) introduce a Gaussian processes (GPs) based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 9: Sensor correlations between the center sensor and its neighborhoods for different forecasting horizons. The correlations are estimated using regularized VAR. We observe that the correlations are localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation quickly decay with the increase of distance which is consistent with the diffusion process on the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>... ... ... ... ... ...</figDesc><table><row><cell>Convolutional</cell><cell cols="2">Diffusion Convolutional</cell><cell cols="2">Diffusion Convolutional</cell><cell>Diffusion Convolutional</cell></row><row><cell>Recurrent Layer</cell><cell>Recurrent Layer</cell><cell></cell><cell>Recurrent Layer</cell><cell>Recurrent Layer</cell></row><row><cell>Input Graph</cell><cell></cell><cell></cell><cell></cell><cell>Predictions</cell></row><row><cell>Signals</cell><cell></cell><cell></cell><cell></cell></row><row><cell>...</cell><cell>... ... ...</cell><cell cols="2">&lt;GO&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Delay =1</cell></row><row><cell></cell><cell cols="3">Copy States</cell><cell>Decoder</cell></row></table><note>Encoder</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of different approaches for traffic speed forecasting. DCRNN achieves the best performance with all three metrics for all forecasting horizons, and the advantage becomes more evident with the increase of the forecasting horizon.</figDesc><table><row><cell></cell><cell>T</cell><cell>Metric</cell><cell>HA</cell><cell>ARIMA Kal</cell><cell>VAR</cell><cell>SVR</cell><cell cols="3">FNN FC-LSTM DCRNN</cell></row><row><cell></cell><cell></cell><cell>MAE</cell><cell>4.16</cell><cell>3.99</cell><cell>4.42</cell><cell>3.99</cell><cell>3.99</cell><cell>3.44</cell><cell>2.77</cell></row><row><cell></cell><cell>15 min</cell><cell>RMSE</cell><cell>7.80</cell><cell>8.21</cell><cell>7.89</cell><cell>8.45</cell><cell>7.94</cell><cell>6.30</cell><cell>5.38</cell></row><row><cell>METR-LA</cell><cell>30 min</cell><cell cols="2">MAPE 13.0% MAE 4.16 RMSE 7.80 MAPE 13.0% MAE 4.16</cell><cell>9.6% 5.15 10.45 12.7% 6.90</cell><cell cols="3">10.2% 9.3% 5.41 5.05 9.13 10.87 12.7% 12.1% 12.9% 9.9% 4.23 8.17 6.52 6.72 4.49</cell><cell>9.6% 3.77 7.23 10.9% 4.37</cell><cell>7.3% 3.15 6.45 8.8% 3.60</cell></row><row><cell></cell><cell>1 hour</cell><cell>RMSE</cell><cell>7.80</cell><cell>13.23</cell><cell>10.11</cell><cell>13.76</cell><cell>8.69</cell><cell>8.69</cell><cell>7.59</cell></row><row><cell></cell><cell></cell><cell cols="2">MAPE 13.0%</cell><cell>17.4%</cell><cell cols="3">15.8% 16.7% 14.0%</cell><cell>13.2%</cell><cell>10.5%</cell></row><row><cell></cell><cell></cell><cell>MAE</cell><cell>2.88</cell><cell>1.62</cell><cell>1.74</cell><cell>1.85</cell><cell>2.20</cell><cell>2.05</cell><cell>1.38</cell></row><row><cell></cell><cell>15 min</cell><cell>RMSE</cell><cell>5.59</cell><cell>3.30</cell><cell>3.16</cell><cell>3.59</cell><cell>4.42</cell><cell>4.19</cell><cell>2.95</cell></row><row><cell>PEMS-BAY</cell><cell>30 min</cell><cell cols="2">MAPE 6.8% MAE 2.88 RMSE 5.59 MAPE 6.8% MAE 2.88</cell><cell>3.5% 2.33 4.76 5.4% 3.38</cell><cell>3.6% 2.32 4.25 5.0% 2.93</cell><cell cols="2">3.8% 5.19% 2.48 2.30 5.18 4.63 5.5% 5.43% 3.28 2.46</cell><cell>4.8% 2.20 4.55 5.2% 2.37</cell><cell>2.9% 1.74 3.97 3.9% 2.07</cell></row><row><cell></cell><cell>1 hour</cell><cell>RMSE</cell><cell>5.59</cell><cell>6.50</cell><cell>5.44</cell><cell>7.08</cell><cell>4.98</cell><cell>4.96</cell><cell>4.74</cell></row><row><cell></cell><cell></cell><cell cols="2">MAPE 6.8%</cell><cell>8.3%</cell><cell>6.5%</cell><cell cols="2">8.0% 5.89%</cell><cell>5.7%</cell><cell>4.9%</cell></row></table><note>spectral decomposition. Going from spectral domain to vertex domain, Atwood &amp; Towsley</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison for DCRNN and GCRNN on the METRA-LA dataset.</figDesc><table><row><cell></cell><cell></cell><cell>15 min</cell><cell></cell><cell></cell><cell>30 min</cell><cell></cell><cell></cell><cell>1 hour</cell></row><row><cell cols="10">MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE</cell></row><row><cell cols="2">DCRNN 2.77</cell><cell>5.38</cell><cell>7.3%</cell><cell>3.15</cell><cell>6.45</cell><cell>8.8%</cell><cell>3.60</cell><cell>7.60</cell><cell>10.5%</cell></row><row><cell cols="2">GCRNN 2.80</cell><cell>5.51</cell><cell>7.5%</cell><cell>3.24</cell><cell>6.74</cell><cell>9.0%</cell><cell>3.81</cell><cell>8.16</cell><cell>10.9%</cell></row><row><cell>'&amp;11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>'&amp;5116(4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>'&amp;511</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0$(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0LQ</cell><cell cols="2">0LQ +RUL]RQ</cell><cell>+RXU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Consequently, the time complexities of both Equation 2 and Equation 4 are O(K|E|). For dense graph, we may use spectral sparsification</figDesc><table><row><cell cols="2">summarizes the main notations used in the paper.</cell></row><row><cell cols="2">B EFFICIENT CALCULATION OF EQUATION 2</cell></row><row><cell cols="3">Equation 2 can be decomposed into two parts with the same time complexity, i.e., one part with D ?1 O W and the other part with D ?1 I W . Thus we will only show the time complexity of the first</cell></row><row><cell>part.</cell><cell></cell></row><row><cell>Let T k (x) = D ?1 O W</cell><cell>k x, The first part of Equation 2 can be rewritten as</cell></row><row><cell></cell><cell>K?1</cell></row><row><cell></cell><cell>? k T k (X :,p )</cell><cell>(4)</cell></row><row><cell></cell><cell>k=0</cell></row><row><cell cols="3">As T k+1 (x) = D ?1 O W T k (x) and D ?1 O W is sparse, it is easy to see that Equation 4 can be calculated using O(K) recursive sparse-dense matrix multiplication each with time complexity</cell></row><row><cell>O(|E|).</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://github.com/liyaguang/DCRNN.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A spatiotemporal correlative k-nearest neighbor model for short-term traffic multistep forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinlong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangquan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="21" to="34" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transportation systems engineering: theory and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ennio</forename><surname>Cascetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient sampling for gaussian graphical models via spectral sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="364" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeptransport: Learning spatial-temporal dependency for traffic condition forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09585</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent space model for road networks to predict time-varying traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingxiong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Traffic flow theory and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Short-term speed predictions exploiting big data on large urban road networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Fusco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Colombaroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Isaenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="183" to="201" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamilton</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Princeton university press Princeton</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A generalization of convolutional neural networks to graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Hechtlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purvasha</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jining</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08165</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Big data and its technical challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Labrinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jignesh</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Time-series extreme event forecasting with neural networks at Uber</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawek</forename><surname>Smyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning Time Series Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Short-term traffic flow forecasting: An experimental comparison of time-series analysis and supervised learning. ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="871" to="882" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering spatio-temporal causal interactions in traffic data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1010" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Traffic flow prediction with big data: A deep learning approach. ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwen</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="865" to="873" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">818</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable algorithms for data and network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Short-term traffic forecasting: Where we are and where were going</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eleni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlahogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Karlaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gaussian processes for short-term traffic volume forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiguang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record: Journal of the Transportation Research Board</title>
		<imprint>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning: A generic approach for extreme condition traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dnn-based prediction model for spatio-temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
