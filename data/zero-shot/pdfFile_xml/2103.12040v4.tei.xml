<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaneAF: Robust Multi-Lane Detection with Affinity Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Abualsaud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Situ</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Rangesh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
						</author>
						<title level="a" type="main">LaneAF: Robust Multi-Lane Detection with Affinity Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object Detection</term>
					<term>Segmentation and Categoriza- tion</term>
					<term>Deep Learning for Visual Perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study presents an approach to lane detection involving the prediction of binary segmentation masks and perpixel affinity fields. These affinity fields, along with the binary masks, can then be used to cluster lane pixels horizontally and vertically into corresponding lane instances in a post-processing step. This clustering is achieved through a simple row-byrow decoding process with little overhead; such an approach allows LaneAF to detect a variable number of lanes without assuming a fixed or maximum number of lanes. Moreover, this form of clustering is more interpretable in comparison to previous visual clustering approaches, and can be analyzed to identify and correct sources of error. Qualitative and quantitative results obtained on popular lane detection datasets demonstrate the model's ability to detect and cluster lanes effectively and robustly. Our proposed approach sets a new state-of-the-art on the challenging CULane dataset and the recently introduced Unsupervised LLAMAS dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L ANE detection is the process of automatically perceiving the shape and position of marked lanes and is a crucial component of autonomous driving systems, directly influencing the guidance and steering of vehicles while also aiding the interaction between numerous agents on the road. As the number of drivers on the roads has increased, autonomous driving systems have received considerable attention in the automotive and tech industries as well as in academia <ref type="bibr" target="#b0">[1]</ref>. According to the Insurance Institute for Highway Safety (IIHS), in the US alone, car accidents claimed 36,560 lives in 2018, underscoring the importance of any technology that can help prevent crashes.</p><p>Since roads commonly have different types of lane lines (solid white, broken white, solid yellow, etc.), each of which have specific implications with regards to how vehicles may interact with them, automated lane detection systems can also help alert drivers when there are changes in lane topology on the road. Furthermore, there are several factors that make lane detection a challenging task. Firstly, there is a wide variety of road infrastructure in use around the world. Additionally, <ref type="figure">Fig. 1</ref>: In our approach, we propose to train a model that outputs binary segmentation masks and affinity fields, which can then be decoded together to produce multiple lane instances. This is opposed to the standard approach to (anchor-free) lane detection that treats each lane as a separate class and trains a model to perform multi-class segmentation. the lane detection system must be able to identify instances where lanes are ending, merging, and splitting. Finally, the lane detection system must possess the ability to discern worn or unclear lane markings. Precise detection of lanes also enable more robust trajectory prediction of surrounding vehicles; as discussed in <ref type="bibr" target="#b1">[2]</ref>, this is critical for successful path planning in autonomous driving. Therefore, while lane detection is a significant and complex task, it is a key factor in developing any autonomous vehicle system. While binary classification is used for the detection of lanes in our approach, a limitation of this type of classification is that it produces a single-channel output, which does not allow for the identification of separate lane entities. To dissociate different lane instances, we propose a novel clustering scheme based on affinity fields (see <ref type="figure">Figure 1</ref>). Affinity fields were originally introduced in <ref type="bibr" target="#b2">[3]</ref> for the purpose of multi-person 2D pose estimation, and are comprised of unit vectors that encode location and orientation. This technique was also used for the detection of hands inside a vehicle, as demonstrated in <ref type="bibr" target="#b3">[4]</ref>. In this paper, we have defined two types of affinity fields, the horizontal affinity field (HAF) and vertical affinity field (VAF). It is these affinity fields that enable unique lane instances to be identified and segmented. Since these affinity fields are present wherever there are foreground lane pixels, they are not bound to a pre-determined number of lanes. The model is therefore agnostic to the number of lanes present on the road.</p><p>The main contributions of this paper are as follows:</p><p>1) We show that using an off-the-shelf convolutional neural network (CNN) backbone <ref type="bibr" target="#b4">[5]</ref> that intrinsically aggregates and refines multi-scale features can result in superior performance when compared to other bespoke architectures and losses previously proposed for lane detection. 2) We propose affinity fields that are suitable for clustering and associating pixels belonging to amorphous entities like lanes. 3) We detail the procedure and losses to train models that predict binary segmentation masks and affinity fields for the purpose of lane instance segmentation. 4) We introduce efficient methods for generating and decoding such affinity fields into an unknown number of clustered lane instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH</head><p>Lane detection has traditionally been tackled by featurebased approaches <ref type="bibr" target="#b5">[6]</ref> which then evolved to model-based approaches to detect lane boundaries. However, these are not practical in real world scenarios since they require ideal road scenes to work effectively. Currently, data-driven approaches are commonly used to detect both lane boundaries as well as lane regions. While several shortcomings of the traditional lane detection methods (i.e. lane segmentation via hand-crafted features) have been resolved with more robust methods in recent years, there is still room for improvement. In more recent times, deep learning and large-scale datasets have provided solutions to many of these issues. However, lane detection in unconstrained environments and complex scenarios remain a challenge.</p><p>Lane detection nowadays is typically modelled as a semantic segmentation problem to extract features using deep learning methods. New approaches tackle lane detection as a multiclass segmentation problem, where each lane forms a separate class. Some of these approaches include: <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, the authors combine a recurrent neural network (RNN) with a CNN for lane prediction and detection. The use of an embedding loss was introduced in [10] which uses generative adversarial networks (GANs) to better preserve the structure of lanes and to mitigate the problem of complex post-processing for the output of semantic segmentation; 96% accuracy on the TuSimple dataset was obtained. In <ref type="bibr" target="#b11">[12]</ref>, a sequential prediction network has been used to avoid heuristicbased clustering post-processing. Another network architecture was presented in <ref type="bibr" target="#b12">[13]</ref> with two elements: a deep network which generates weighted pixel coordinates in addition to a differentiable weighted least-squares fitting module. In <ref type="bibr" target="#b13">[14]</ref>, the authors introduced Self Attention Distillation (SAD) loss to avoid models that propagate data sequentially and to decrease inference time. However, the fully connected layer that the SAD model employs is computationally expensive and cannot adapt to any number of lanes.</p><p>Other lane detection approaches choose to first perform binary segmentation of all lanes, followed by a clustering stage to separate each individual lane instance as in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, and <ref type="bibr" target="#b17">[17]</ref>. Instance segmentation is usually approached with the use of complex pipelines; however, many powerful approaches and research were put to come up with better performance techniques including the approach presented in <ref type="bibr" target="#b18">[18]</ref>, where they used an end-to-end convolutional neural network to tackle the problem that was inspired by the classical watershed transform. Another method toward instance segmentation was based on using a fully convolutional network to predict semantic labels along with depth and an instancebased encoding. This was implemented by using each pixel's direction toward its corresponding instance center; with the help of low-level computer vision techniques, impressive scene understanding by predicting pixel-wise depth, semantics, and instance-level direction cues was achieved <ref type="bibr" target="#b19">[19]</ref>. Lane detection is posed as an instance segmentation problem in <ref type="bibr" target="#b14">[15]</ref> so that each lane can be detected in an end-to-end manner, adapting to changing numbers of lanes on the road. In <ref type="bibr" target="#b16">[16]</ref>, a combination of instance segmentation and classification was used as an end-to-end deep learning real-time method to avoid reliance on two-step detection networks. Although recent methods of lane detection show high accuracy when applied to the popular published datasets, some drawbacks of these current methods are that they are not robust when encountering occlusion and that they require a fixed number of lanes in a scene; thus, they cannot work for a random number of lanes present on the road. Acknowledging this problem in <ref type="bibr" target="#b17">[17]</ref>, the authors use a key points estimation approach to allow for lane detection of an arbitrary numbers of lanes regardless of orientation.</p><p>More recently, some approaches have modelled lane detection as an anchor-based object detection problem such as <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, and <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b23">[23]</ref>, a spatio-temporal deep learning method was proposed to mitigate the errors that can occur when experiencing harsh weather or other complex problems in the road, jeopardizing the accuracy of detecting a lane in the scene. Meanwhile, in <ref type="bibr" target="#b20">[20]</ref>, lane markers were tracked temporally. Additionally, <ref type="bibr" target="#b22">[22]</ref> presents an anchorbased single-stage deep lane detection model using anchors for feature pooling. In <ref type="bibr" target="#b21">[21]</ref>, the authors developed 3D-LaneNet, a network that predicts the 3D layout of lanes using a single image. A combination of LiDAR and camera sensors were used in <ref type="bibr" target="#b24">[24]</ref> for their network to obtain accurate lane detection in 3D space directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our proposed methodology involves a feed-forward CNN that is trained to predict binary lane segmentation masks and per-pixel affinity fields. More specifically, the model is trained to predict two affinity fields, which we call the horizontal affinity field (HAF) and vertical affinity field (VAF), respectively. Affinity fields can be thought of as vector fields that map any 2D location on the image plane to a unit vector in 2D. A unit vector in the VAF encodes the direction in which the next set of lane pixels above it is located. On the other hand, a unit vector in the HAF points toward the center of the lane in the current row, thereby allowing us to cluster lanes of arbitrary widths. These two affinity fields, in conjunction with the predicted binary segmentation, can then be used to cluster foreground pixels into lanes as a post-processing step. In the Algorithm 1 Creating affinity fields from ground truth data Inputs:</p><p>SEG(H ? W ): ground truth segmentation lmax: maximum number of lanes</p><formula xml:id="formula_0">HAF, V AF ? zeros(H, W, 2) initialize affinity fields for l ? 1 to L do go through each lane prev cols ? nonzero(SEG[H, :] == l) initialize /* row-by-row, from bottom to top */ for y ? H ? 1 to 1 do cols ? f ind(SEG[row, :] == l) find lane pixels /* horizontal affinity field */ for x in cols do HAF [y, x] ? #? Hgt(x, y) Eq. 1 end for /* vertical affinity field */ for x in prev cols do V AF [y + 1, x] ? #? V gt(x, y + 1)</formula><p>Eq. 2 end for prev cols ? cols end for end for return HAF, V AF next few subsections, we discuss each individual block in our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Backbone</head><p>Recent lane detection approaches have made use of a variety of backbone architectures, but most popular among them are usually the ResNet family of architectures <ref type="bibr" target="#b26">[25]</ref>, ENet <ref type="bibr" target="#b6">[7]</ref>, and ERFNet <ref type="bibr" target="#b27">[26]</ref>. Although these architectures have proven benefits across a variety of tasks, we believe that more recent developments in the field can be leveraged for lane detection. To this end, we make use of the DLA-34 backbone presented in <ref type="bibr" target="#b4">[5]</ref>.</p><p>The DLA family of models make use of deep layer aggregation, which unifies semantic and spatial fusion for better localization and semantic interpretation. In particular, this architecture extends densely connected networks <ref type="bibr" target="#b28">[27]</ref> and feature pyramid networks with hierarchical and iterative skip connections that deepen the representation and refine resolution. They employ two forms of aggregation: iterative deep aggregation (IDA), focusing on fusing resolutions and scales, and hierarchical deep aggregation (HDA), focusing on merging features from all modules and channels. These architectures also incorporate deformable convolution operations <ref type="bibr" target="#b29">[28]</ref> that can adapt the spatial sampling grid for convolutions based on their inputs. We believe these are desirable properties for the tasks of lane detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Affinity Fields</head><p>In addition to binary lane segmentation masks, our model is trained to predict horizontal and vertical affinity fields (HAFs and VAFs respectively). For any given image, the HAF and VAF can be thought of as vector fields #? H(?, ?) and #? V (?, ?), that assign a unit vector to each (x, y) location in the image. As we alluded to earlier, the HAF enables us to cluster lane pixels horizontally and the VAF vertically. With the predicted affinity fields and binary mask, clustering lane pixels is achieved through a simple row-by-row decoding process from bottom to top. The rest of this subsection provides details on how to create such affinity fields using the ground truth and how to use the predicted affinity fields to decode individual lanes. Creating HAFs and VAFs: Affinity fields are created using ground truth segmentation masks on the fly as detailed in Algorithm 1. This proceeds row-by-row, from bottom to top.</p><p>For any row y in the image, the HAF vectors are computed for each lane point (x l i , y) using the ground truth vector field mapping #? H gt (?, ?) as follows:</p><formula xml:id="formula_1">#? H gt (x l i , y) = x l y ? x l i |x l y ? x l i | , y ? y |y ? y| = x l y ? x l i |x l y ? x l i | , 0 ,<label>(1)</label></formula><p>where x l y is the mean x-coordinate of all points belonging to lane l in row y. This process is illustrated in <ref type="figure" target="#fig_0">Figure 2a</ref>, where pixels in green and blue represent points belonging to lanes l and l + 1 respectively.</p><p>Similarly, the VAF vectors are computed for each lane point (x l i , y) in row y using the ground truth vector field mapping #? V gt (?, ?) as follows:</p><formula xml:id="formula_2">#? V gt (x l i , y) = x l y?1 ? x l i |x l y?1 ? x l i | , y ? 1 ? y |y ? 1 ? y| = x l y?1 ? x l i |x l y?1 ? x l i | , ?1 ,<label>(2)</label></formula><p>where x l y?1 is the mean x-coordinate of all points belonging to lane l in row y ? 1. This process is illustrated in <ref type="figure" target="#fig_0">Figure 2c</ref>, where pixels in green represent points belonging to lanes l. Note that unlike the HAF, unit vectors in the VAF point to the mean location of the lane in the previous row. Decoding HAFs and VAFs: After a model is trained to predict the HAFs and VAFs detailed above, a decoding procedure is carried out to cluster foreground pixels into lanes during testing. This procedure is presented in Algorithm 2, and similarly operates row-by-row, from bottom to top.</p><p>Assuming #? H pred (?, ?) is the vector field corresponding to the predicted HAF, foreground pixels in a row y ? 1 are first assigned to clusters based on the following rule:</p><formula xml:id="formula_3">c * haf (x f g i , y ? 1) = ? ? ? ? ? C k+1 if #? H pred (x f g i?1 , y ? 1) 0 ? 0 ? #? H pred (x f g i , y ? 1) 0 &gt; 0, C k otherwise,<label>(3)</label></formula><p>where c * haf (x f g i , y ?1) denotes the optimal cluster assignment for a foreground pixel (x f g i , y ? 1); C k and C k+1 denote two different clusters indexed by k and k + 1 respectively. This assignment is illustrated in <ref type="figure" target="#fig_0">Figure 2b</ref>, where pixels in red are assigned the same cluster.</p><p>Next, these horizontal clusters are assigned to existing lanes indexed by l using the vector field #? V pred (?, ?) corresponding to the VAF as follows:</p><formula xml:id="formula_4">c * vaf (l) = arg min C k d C k (l),<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">d * (l) = min C k d C k (l).<label>(5)</label></formula><p>Here, d C k (l) denotes the error of associating cluster C k to an existing lane l:</p><formula xml:id="formula_6">d C k (l) = 1 N l y N l y ?1 i=0 (x C k , y ? 1) ? (x l i , y) ? #? V pred (x l i , y) ? ||(x C k , y ? 1) ? (x l i , y) || ,<label>(6)</label></formula><p>where N l y are the number of pixels belonging to lane l in row y. We illustrate this process in <ref type="figure" target="#fig_0">Figure 2d</ref>, where the cluster in red is assigned to the existing lane in green. By repeating the above steps row-by-row starting from the bottom and working to the top, we are able to assign every foreground pixel to their respective lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Losses</head><p>To train the proposed model, we use a separate loss at each prediction head. For our binary segmentation branch, we used weighted binary cross-entropy loss, a standard loss for imbalanced binary segmentation tasks. The raw logits produced by the model are first passed through a sigmoid activation for normalization. The loss is then calculated as:</p><formula xml:id="formula_7">L BCE = ? 1 N i w?t i ?log(o i )+(1?t i )?log(1?o i ) ,<label>(7)</label></formula><p>where t i is the target value for the pixel i and o i is the sigmoid output. Since this is an unbalanced segmentation task, a weight w was used to increase penalization for foreground pixels.</p><p>To further account for the imbalanced dataset, an additional intersection over union loss was used for the segmentation branch:</p><formula xml:id="formula_8">L IoU = 1 N i 1 ? t i ? o i t i + o i ? t i ? o i .<label>(8)</label></formula><p>For the affinity field branches of the model, a simple L1 regression loss was applied only to the foreground locations of both the vertical and horizontal affinity fields:</p><formula xml:id="formula_9">L AF = 1 N f g i |t haf i ? o haf i | + |t vaf i ? o vaf i | . (9)</formula><p>The total loss applied to the model is a simple summation of the individual losses:</p><formula xml:id="formula_10">L total = L BCE + L IoU + L AF .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Our backbone architecture (DLA-34) is a fully convolutional network that does not retain the original resolution, but rather downsizes the outputs by a factor of 4; thus, we rescaled the input images to one-half their original resolution during run-time and reshaped the ground truth affinity fields and segmentation masks to one-eighth the original resolution (accounting for the model's downsizing factor). This has the added benefit of making our decoding process faster since we now process only an eighth of the original rows. The decoding time typically depends on the number of lanes, the quality of the outputs produced by the model, and the output size. On average, it takes about 15-20ms on a modern CPU without any code optimizations. However, since this an entirely CPUbased operation, it should not affect the overall latency of the approach. We also make use of random rotations, crops, scales and horizontal flips during training.</p><p>We use the Adam optimizer as our solver with a learning rate of 0.0001, weight decay of 0.001, and train for a total of 40 epochs. We also employ a scheduler that reduces the learning rate by a factor of 5 every 10 epochs. The weight w for the loss in Eq. 7 was set to 9.6 because there are approximately 9.6 times as many background pixels than there are foreground (lane) pixels in most public datasets. To avoid overfitting, early stopping was implemented by retaining the model parameters that best performed on the validation set. Using a single GTX Titan X Maxwell GPU, training our model on the CULane dataset until convergence (about 25-30 epochs) takes 2-3 days. Significant speedup can be obtained by using more modern GPUs and by employing multiple GPUs when available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>To train and benchmark our proposed approach, we make use of the popular TuSimple, CULane <ref type="bibr" target="#b10">[11]</ref>, and LLAMAS <ref type="bibr" target="#b30">[29]</ref> datasets. TuSimple features good and fair weather conditions in various daytime lighting and traffic conditions, employing highways with up to five lanes. Meanwhile, CULane contains significantly more data and also divides test images into nine categories that contain more complex scenarios, including images with challenging lighting conditions. Finally, the LLAMAS dataset is a newer dataset with a sizeable amount of images all obtained using highway recordings and generated from an automated labeling pipeline. A summary of all datasets is compiled in <ref type="table" target="#tab_1">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metrics</head><p>We use the same evaluation metrics used in past literature to make a representative comparison between our approach and prior work. This consists of the official metric of the TuSimple  <ref type="bibr" target="#b10">11)</ref> where N pred is the number of lane points that have been correctly predicted and N gt is the number of ground-truth lane points. Additionally, we report the F 1 measure, which is based on the intersection over union (IoU) and is the only metric for CULane. This is calculated as in <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_11">Accuracy = N pred N gt<label>(</label></formula><formula xml:id="formula_12">F 1 = 2 ? precision ? recall precision + recall<label>(12)</label></formula><p>where precision is defined as T P T P +F P , recall is defined as T P T P +F N , T P is the number of lane points that have been correctly predicted, F P is the number of false positives, and F N is the number of false negatives. This same F 1 measure is also used for the lane approximations benchmark of the LLAMAS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Experiments</head><p>In this subsection, we conduct a series of ablation experiments to validate our design choices. All ablation studies were conducted on the TuSimple validation set and can be seen in <ref type="table" target="#tab_1">Table II</ref>. The first row contains the results of the standard LaneAF model, which we denote as the baseline model B. First, we train variants without the IoU loss (B w/o IoU) and the weighted binary cross-entropy loss (B w/o wBCE). Removing these losses decreased accuracy quite drastically while increasing the false positive and false negative rate. In fact, without the weighted binary cross-entropy loss, the F1 score in particular dropped significantly. The same is observed for the baseline model without random transformations during training (B w/o RT), as depicted in the fourth row.</p><p>With regards to the down-sampling factor of the outputs, it is clear that the baseline model's factor of 4 achieved the best results; decreasing it to 2 (B (DS-2)) increased runtime and worsened accuracy and F1 slightly, while increasing it to 8 (B (DS-8)) had the most damaging effect on accuracy out of all modifications. We also trained a variant with 128 channels in the output head (B (HC-128)) compared to the original 256, and while this change had the smallest impact with respect to accuracy, it is evident that the baseline's 256 channels yields superior results. Finally, to validate the benefits of our clustering approach over standard multi-class segmentation, we trained a DLA-34 model to directly perform multi-class segmentation of all lanes (DLA-34 multi-class). This model obtained the worst F1 and accuracy scores out of  all variants. This result clearly illustrates the effectiveness of binary segmentation followed by a separate affinity field-based clustering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results</head><p>Performance results from LaneAF on the TuSimple benchmark are shown in <ref type="table" target="#tab_1">Table III</ref>. It can be seen that our false positive rate sets a new standard (0.0280) among the current state-of-the-art. This demonstrates that our model does not incorrectly detect a lane pixel as often as other networks and that LaneAF's multi-branch approach leads to confident lane pixel predictions. While we obtain superior accuracy to other backbone architectures such as ResNet-18 and -34 <ref type="bibr" target="#b26">[25]</ref>, our approach falls slightly short of current state-of-the-art models such as PINet <ref type="bibr" target="#b17">[17]</ref>, ENet-SAD <ref type="bibr" target="#b13">[14]</ref>, and SCNN <ref type="bibr" target="#b10">[11]</ref>. However, our false negative rate is only marginally higher, signifying that the incorrectly classified lane pixels are most likely at the very ends of the lanes. Additionally, six training runs were conducted on this dataset with different random seeds, producing a standard deviation of 0.12 on the accuracy metric. From the consistency of these results, we can see that our proposed method is robust.</p><p>Table IV displays the state-of-the-art results of our model on the CULane benchmark. With this significantly larger and more complex dataset, we can see that LaneAF's performance improves greatly with respect to other models and demonstrates our network's ability to generalize. LaneAF (with DLA-34) outperforms the current state-of-the-art with an F1 score of For the CULane dataset, we additionally trained LaneAF models with ENet <ref type="bibr" target="#b6">[7]</ref> and ERFNet <ref type="bibr" target="#b27">[26]</ref> backbones, where we forego the final few upsampling/transposed convolution layers to ensure a downsampling factor of 4 (same as the DLA-34 variant). This allows us to make direct comparisons between our approach and other approaches using the same backbone architecture. For instance, LaneAF with the ENet backbone outperforms ENet-SAD <ref type="bibr" target="#b13">[14]</ref> by over 3% with respect to the F1 score. When using ERFNet as the backbone network, LaneAF's F1 score eclipses other ERFNet-based models such as ERFNet-E2E <ref type="bibr" target="#b35">[34]</ref> and ERFNet-Intra-KD [31] by 1.63% and 3.23%, respectively. These comparisons confirm that the performance gains of LaneAF are achieved through a combination of the DLA-34 backbone and our proposed affinity fields based clustering.</p><p>Furthermore, LaneAF again achieves state-of-the-art performance on the LLAMAS dataset with an F1 score of 96.07%. This surpasses LaneATT's <ref type="bibr" target="#b22">[22]</ref> best model by over 2%, as shown in <ref type="table" target="#tab_5">Table V</ref>. This gap in performance is due to LaneAF's high Recall score, which indicates that the model is more adept at retrieving true lane pixels.</p><p>Qualitative examples of the predicted affinity fields from our approach are depicted in <ref type="figure" target="#fig_1">Figures 3a and 3b</ref>. The clustered outputs shown here were created using the affinity field decoder, outlined in Algorithm 2. In <ref type="figure" target="#fig_1">Figure 3a</ref>, the HAF vectors point towards the center of their respective lane lines for each   Another key point to note is the accuracy of the model at lane points that are farther away from the camera. Since the ground truth segmentation masks for each lane are of approximately the same thickness in the image plane from top to bottom, the model is trained to predict thick foreground masks for lane points even if they are farther away. This results in little to no degradation for lane points that are far away. However, at the horizon, some clusters will be occasionally assigned to non-optimal lanes due to the close proximity of the lane lines.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we show additional qualitative results from the TuSimple dataset (row 1), the CULane dataset (rows 2-4), and the LLAMAS dataset (row 4). The TuSimple examples demonstrate LaneAF's high performance on curved highways and on lanes that are merging and splitting due to entrances and exits, highlighting our model's flexibility to the number of lanes present on a given road. Also notable in the middle image of the first row is the false detection of a lane line due to an airplane contrail. The displayed results from CULane include challenging scenarios that illustrate LaneAF's robustness on curved roads and in very poor lighting conditions. This diverse set of examples exhibit characteristics of the Dazzle, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>In this paper, we proposed a novel approach to lane detection and instance segmentation through the use of binary segmentation masks and per-pixel affinity fields. The horizontal and vertical affinity fields, along with the predicted binary masks were demonstrated to successfully cluster lane pixels into unique lane instances in a post-processing step. This is accomplished using a simple row-by-row decoding process with little overhead, and enables LaneAF to detect a variable number of lanes of arbitrary width without assuming a fixed or maximum number of lanes. This form of clustering is also more interpretable in comparison to previous visual approaches since it can be analyzed to easily identify and correct sources of error. The ablation study conducted also validated the effectiveness of the approach over standard multi-class segmentation. Our proposed method achieves the lowest reported false positive rate (0.0280) on the TuSimple benchmark; on the larger and more comprehensive CULane dataset, LaneAF sets a new state-of-the-art result with a total F1 score of 77.41%, surpassing much deeper and more complex models. LaneAF also achieves a state-of-the-art F1 score on the LLAMAS benchmark by a significant margin (+2%), underscoring its robust performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) HAF creation during training (b) HAF decoding during testing (c) VAF creation during training (d) VAF decoding during testing Illustrations of HAF and VAF creation and decoding processes during training and testing respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Predicted horizontal affinity field (HAF) (b) Predicted vertical affinity field (VAF) Example outputs produced by LaneAF with color coded affinity fields; each color represents a unique lane instance based on affinity field decoding. Of note is the successful discrimination of lane instances even as the lanes converge. 77.41%, surpassing models of similar size and even LaneATT<ref type="bibr" target="#b22">[22]</ref> with its largest backbone, ResNet-122. Moreover, LaneAF sets a new benchmark in a majority of categories, including difficult ones such as Dazzle, Shadow, No line, Curve, and Night, exhibiting our model's high adaptability to curving roads and challenging lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. Likewise, in Figure 3b, the VAF vectors point along the lane towards the mean location of the next row's lane pixels. This is visualized in the yellow box of Figure 3b, where for each unique lane instance, the unit vector points towards the next row's mean lane pixel location. For both Figures 3a and 3b, the blue boxes clearly display how the HAF and VAF are implemented for a single detected lane instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>LaneAF qualitative results on TuSimple (row 1), CULane (rows 2-4), and LLAMAS (row 5). Shadow, Curve, and Night categories of the CULane dataset. Finally, the LLAMAS samples show excellent performance on additional highway scenes similar to the TuSimple examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2</head><label>2</label><figDesc>Decoding predicted affinity fields into lanes Inputs: BW (H ? W ): binary segmentation mask HAF (H ? W ? 2): horizontal affinity field V AF (H ? W ? 2): vertical affinity field ? : clustering threshold SEG ? zeros(H, W ) initialize segmentation output lane end points ? [] keeps track of the latest points added to each lane L ? 0 initialize number of lanes to 0 /* row-by-row, from bottom to top */ for y ? H to 1 do cols ? f ind(BW [row, :] &gt; 0)</figDesc><table><row><cell></cell><cell cols="2">find foreground pixels</cell></row><row><cell>/* cluster horizontally */</cell><cell></cell></row><row><cell>clusters ? []</cell><cell></cell></row><row><cell>for x in cols do</cell><cell></cell></row><row><cell cols="2">clusters.update(c  *  haf (x, y))</cell><cell>Eq. 3</cell></row><row><cell>end for</cell><cell></cell></row><row><cell cols="2">/* assign clusters to existing lanes */</cell></row><row><cell>for l ? 1 to L do</cell><cell></cell></row><row><cell>if d  *  (l) &lt;= ? then</cell><cell cols="2">error less than threshold (Eq. 5)</cell></row><row><cell cols="2">lane end points[l] ? c  *  vaf (l)</cell><cell>Eq. 4,Eq. 6</cell></row><row><cell cols="3">update latest points added to lane</cell></row><row><cell>for x in c  *  vaf (l) do</cell><cell></cell></row><row><cell>SEG[y, x] ? l</cell><cell cols="2">assign cluster to lane l</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell cols="2">/* spawn new lanes with unassigned clusters */</cell></row><row><cell>for cluster in clusters do</cell><cell></cell></row><row><cell cols="2">if cluster is not assigned then</cell></row><row><cell>L ? L + 1</cell><cell></cell></row><row><cell cols="2">lane end points[L] ? cluster</cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>return SEG</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Attributes of popular lane detection datasets</figDesc><table><row><cell>Dataset</cell><cell>TuSimple</cell><cell>CULane</cell><cell>LLAMAS</cell></row><row><cell># Frames</cell><cell>6,408</cell><cell>133,325</cell><cell>100,042</cell></row><row><cell>Train</cell><cell>3,268</cell><cell>88,880</cell><cell>58,269</cell></row><row><cell>Validation</cell><cell>358</cell><cell>9,675</cell><cell>20,844</cell></row><row><cell>Test</cell><cell>2,782</cell><cell>34,680</cell><cell>20,929</cell></row><row><cell>Resolution</cell><cell>1280 ? 720</cell><cell>1640 ? 590</cell><cell>1280 ? 717</cell></row><row><cell>Road Type</cell><cell>highway</cell><cell>urban, rural, highway</cell><cell>highway</cell></row><row><cell cols="4">dataset (accuracy), the false positive (FP) rate, and the false</cell></row><row><cell cols="4">negative (FN) rate. The TuSimple accuracy is calculated as:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>LaneAF ablation experiments on the TuSimple validation set</figDesc><table><row><cell>Model type</cell><cell cols="2">F1 (%) Acc (%)</cell><cell>FP</cell><cell>FN</cell></row><row><cell>B 1</cell><cell>95.31</cell><cell>94.62</cell><cell>0.0435</cell><cell>0.0500</cell></row><row><cell>B w/o IoU 2</cell><cell>95.17</cell><cell>94.31</cell><cell cols="2">0.0456 0.0507</cell></row><row><cell>B w/o wBCE 3</cell><cell>93.75</cell><cell>94.19</cell><cell>0.0598</cell><cell>0.0649</cell></row><row><cell>B w/o RT 4</cell><cell>93.56</cell><cell>94.29</cell><cell cols="2">0.0614 0.0670</cell></row><row><cell>B (DS-2) 5</cell><cell>94.76</cell><cell>94.22</cell><cell>0.0484</cell><cell>0.0559</cell></row><row><cell>B (DS-8)</cell><cell>93.94</cell><cell>92.73</cell><cell cols="2">0.0549 0.0656</cell></row><row><cell>B (HC-128) 6</cell><cell>94.80</cell><cell>94.56</cell><cell>0.0503</cell><cell>0.0535</cell></row><row><cell cols="2">DLA-34 multi-class 7 88.86</cell><cell>92.59</cell><cell cols="2">0.1115 0.1114</cell></row><row><cell cols="5">1 B: baseline model with down-sampling factor 4 and 256</cell></row><row><cell cols="5">channels in output head 2 IoU: IoU loss 3 wBCE: weighted</cell></row><row><cell cols="5">BCE loss 4 RT: random transformations during training</cell></row><row><cell cols="5">5 DS-x: down-sampling factor for outputs 6 HC-x: number of</cell></row><row><cell cols="5">channels in output head 7 DLA-34 multi-class: DLA-34 model</cell></row><row><cell cols="4">trained for lane detection via multi-class segmentation</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>LaneAF results on the TuSimple benchmark</figDesc><table><row><cell>Method</cell><cell cols="2">F1 (%) Acc (%)</cell><cell>FP</cell><cell>FN</cell><cell>MACs (G)</cell></row><row><cell>ResNet-18 [25]</cell><cell>87.87</cell><cell>92.69</cell><cell>0.0948</cell><cell>0.0822</cell><cell>-</cell></row><row><cell>PolyLaneNet [30]</cell><cell>90.62</cell><cell>93.36</cell><cell cols="2">0.0942 0.0933</cell><cell>1.7</cell></row><row><cell>Cascaded-CNN [16]</cell><cell>90.82</cell><cell>95.24</cell><cell>0.1197</cell><cell>0.0620</cell><cell>-</cell></row><row><cell>LaneNet [15]</cell><cell>94.80</cell><cell>96.38</cell><cell cols="2">0.0780 0.0244</cell><cell>-</cell></row><row><cell>ENet-SAD [14]</cell><cell>95.92</cell><cell>96.64</cell><cell>0.0602</cell><cell>0.0205</cell><cell>-</cell></row><row><cell>SCNN [11]</cell><cell>96.53</cell><cell>96.53</cell><cell cols="2">0.0617 0.0180</cell><cell>-</cell></row><row><cell>ResNet-34 [25]</cell><cell>96.77</cell><cell>92.84</cell><cell>0.0918</cell><cell>0.0796</cell><cell>-</cell></row><row><cell>PINet [17]</cell><cell>97.20</cell><cell>96.75</cell><cell cols="2">0.0310 0.0250</cell><cell>-</cell></row><row><cell>LaneATT(ResNet-18) [22]</cell><cell>96.71</cell><cell>95.57</cell><cell>0.0356</cell><cell>0.0301</cell><cell>9.3</cell></row><row><cell>LaneATT (ResNet-34) [22]</cell><cell>96.77</cell><cell>95.63</cell><cell cols="2">0.0353 0.0292</cell><cell>18.0</cell></row><row><cell>LaneATT (ResNet-122) [22]</cell><cell>96.06</cell><cell>96.10</cell><cell>0.0564</cell><cell>0.0217</cell><cell>70.5</cell></row><row><cell>LaneAF (DLA-34)</cell><cell>96.49</cell><cell>95.62</cell><cell>0.0280</cell><cell>0.0418</cell><cell>22.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>LaneAF state-of-the-art results on the CULane benchmark</figDesc><table><row><cell>Method</cell><cell>Total</cell><cell>Normal</cell><cell cols="9">Crowded Dazzle Shadow No line Arrow Curve Cross Night MACs (G)</cell></row><row><cell>ResNet-18 [25]</cell><cell>68.40</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell>40.20</cell><cell>81.00</cell><cell>57.90</cell><cell>1743</cell><cell>62.10</cell><cell>-</cell></row><row><cell>ENet-SAD [14]</cell><cell>70.80</cell><cell>90.10</cell><cell>68.80</cell><cell>60.20</cell><cell>65.90</cell><cell>41.60</cell><cell>84.00</cell><cell>65.70</cell><cell>1998</cell><cell>66.00</cell><cell>-</cell></row><row><cell>SCNN [11]</cell><cell>71.60</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell>43.40</cell><cell>84.10</cell><cell>64.40</cell><cell>1990</cell><cell>66.10</cell><cell>-</cell></row><row><cell>ResNet-34 [25]</cell><cell>72.30</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell>44.40</cell><cell>85.70</cell><cell>69.50</cell><cell>2037</cell><cell>66.70</cell><cell>-</cell></row><row><cell>ERFNet-Intra-KD [31]</cell><cell>72.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CurveLanes-NAS-M [32]</cell><cell>73.50</cell><cell>90.20</cell><cell>70.50</cell><cell>65.90</cell><cell>69.30</cell><cell>48.80</cell><cell>85.70</cell><cell>67.50</cell><cell>2359</cell><cell>68.20</cell><cell>33.7</cell></row><row><cell>SIM-CycleGAN [33]</cell><cell>73.90</cell><cell>91.80</cell><cell>71.80</cell><cell>66.40</cell><cell>76.20</cell><cell>46.10</cell><cell>87.80</cell><cell>67.10</cell><cell>2346</cell><cell>69.40</cell><cell>-</cell></row><row><cell>ERFNet-E2E [34]</cell><cell>74.00</cell><cell>91.00</cell><cell>73.10</cell><cell>64.50</cell><cell>74.10</cell><cell>46.60</cell><cell>85.80</cell><cell>71.90</cell><cell>2022</cell><cell>67.90</cell><cell>-</cell></row><row><cell>PINet [17]</cell><cell>74.40</cell><cell>90.30</cell><cell>72.30</cell><cell>66.30</cell><cell>68.40</cell><cell>49.80</cell><cell>83.70</cell><cell>65.60</cell><cell>1427</cell><cell>67.70</cell><cell>-</cell></row><row><cell>LaneATT (ResNet-18) [22]</cell><cell>75.09</cell><cell>91.11</cell><cell>72.96</cell><cell>65.72</cell><cell>70.91</cell><cell>48.35</cell><cell>85.49</cell><cell>63.37</cell><cell>1170</cell><cell>68.95</cell><cell>9.3</cell></row><row><cell>RESA-50 [35]</cell><cell>75.30</cell><cell>92.10</cell><cell>73.10</cell><cell>69.20</cell><cell>72.80</cell><cell>47.70</cell><cell>88.30</cell><cell>70.30</cell><cell>1503</cell><cell>69.9</cell><cell>-</cell></row><row><cell>LaneATT (ResNet-34) [22]</cell><cell>76.68</cell><cell>92.14</cell><cell>75.03</cell><cell>66.47</cell><cell>78.15</cell><cell>49.39</cell><cell>88.38</cell><cell>67.72</cell><cell>1330</cell><cell>70.72</cell><cell>18.0</cell></row><row><cell cols="2">LaneATT (ResNet-122) [22] 77.02</cell><cell>91.74</cell><cell>76.16</cell><cell>69.47</cell><cell>76.31</cell><cell>50.46</cell><cell>86.29</cell><cell>64.05</cell><cell>1264</cell><cell>70.81</cell><cell>70.5</cell></row><row><cell>LaneAF (ENet)</cell><cell>74.24</cell><cell>90.12</cell><cell>72.19</cell><cell>68.70</cell><cell>76.34</cell><cell>49.13</cell><cell>85.13</cell><cell>64.40</cell><cell>1934</cell><cell>68.67</cell><cell>2.2</cell></row><row><cell>LaneAF (ERFNet)</cell><cell>75.63</cell><cell>91.10</cell><cell>73.32</cell><cell>69.71</cell><cell>75.81</cell><cell>50.62</cell><cell>86.86</cell><cell>65.02</cell><cell>1844</cell><cell>70.90</cell><cell>22.2</cell></row><row><cell>LaneAF (DLA-34)</cell><cell>77.41</cell><cell>91.80</cell><cell>75.61</cell><cell>71.78</cell><cell>79.12</cell><cell>51.38</cell><cell>86.88</cell><cell>72.70</cell><cell>1360</cell><cell>73.03</cell><cell>23.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>LaneAF results on the LLAMAS benchmark</figDesc><table><row><cell>Method</cell><cell>F1 (%)</cell><cell cols="2">Prec (%) Recall (%)</cell></row><row><cell>PolyLaneNet [30]</cell><cell>88.40</cell><cell>88.87</cell><cell>87.93</cell></row><row><cell>LaneATT(ResNet-18) [22]</cell><cell>93.46</cell><cell>96.92</cell><cell>90.24</cell></row><row><cell>LaneATT (ResNet-34) [22]</cell><cell>93.74</cell><cell>96.79</cell><cell>90.88</cell></row><row><cell>LaneATT (ResNet-122) [22]</cell><cell>93.54</cell><cell>96.82</cell><cell>90.47</cell></row><row><cell>LaneAF (DLA-34)</cell><cell>96.07</cell><cell>96.91</cell><cell>95.26</cell></row></table><note>row of the output image. Lane clusters are still successfully separated despite being closely located for numerous rows, demonstrated in yellow box of Figure 3a</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENTS</head><p>We are grateful to the reviewers for their valuable comments. We also thank our sponsors and colleagues at LISA, UC San Diego for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Medasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="18" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How would surround vehicles move? a unified framework for maneuver classification and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Looking at hands in autonomous vehicles: A convnet approach using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On enhancing lane estimation using contextual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1870" to="1881" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1606</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drive analysis using vehicle dynamics and vision-based lane semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust lane detection from continuous driving scenes using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Elgan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end lane detection through differentiable leastsquares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="905" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lane detection and classification using cascaded cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garc?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Systems Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5221" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust lane detection using multiple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sikchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charkravarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1470" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d-lanenet: end-to-end 3d multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attentionguided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial-temproal based lane detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP International conference on artificial Intelligence applications and innovations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3102" to="3109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane markers using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6150" to="6156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inter-region affinity distillation for road marking segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">495</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lane detection in low-light conditions using an efficient data enhancement: Light conditions style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1394" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Resa: Recurrent feature-shift aggregator for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3547" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
