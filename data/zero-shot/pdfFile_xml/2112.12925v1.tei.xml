<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
							<email>zeng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit Semantic Scene Completion (SSC), a useful task to predict the semantic and occupancy representation of 3D scenes, in this paper. A number of methods for this task are always based on voxelized scene representations for keeping local scene structure. However, due to the existence of visible empty voxels, these methods always suffer from heavy computation redundancy when the network goes deeper, and thus limit the completion quality. To address this dilemma, we propose our novel point-voxel aggregation network for this task. Firstly, we transfer the voxelized scenes to point clouds by removing these visible empty voxels and adopt a deep point stream to capture semantic information from the scene efficiently. Meanwhile, a light-weight voxel stream containing only two 3D convolution layers preserves local structures of the voxelized scenes. Furthermore, we design an anisotropic voxel aggregation operator to fuse the structure details from the voxel stream into the point stream, and a semantic-aware propagation module to enhance the up-sampling process in the point stream by semantic labels. We demonstrate that our model surpasses state-of-the-arts on two benchmarks by a large margin, with only depth images as the input.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With a partial 2D observation, humans are capable of understanding the 3D space and inferring the objects behind the occlusion. Similarly, the capability to capture the structure and semantic information of 3D scenes is beneficial for many real-world applications, including robotics, virtual reality, and interior design. To achieve this goal, we need to perform scene completion and scene labeling tasks, which are proved to be closely correlated by <ref type="bibr" target="#b33">(Song et al. 2017)</ref>. Semantic Scene Completion (SSC) is therefore put forward to predict 3D geometry and semantics simultaneously from a partial observation, which is an emerging topic in recent years.</p><p>Previously, most methods <ref type="bibr" target="#b33">(Song et al. 2017;</ref><ref type="bibr" target="#b8">Guo and Tong 2018;</ref><ref type="bibr" target="#b6">Garbade et al. 2019;</ref><ref type="bibr" target="#b22">Liu et al. 2018</ref>) solve this challenging problem with voxelized partial observations. To handle  <ref type="figure">Figure 1</ref>: Visualization of Semantic Scene Completion task. Our method generates more reasonable results compared to 3D-Sketch <ref type="bibr" target="#b1">(Chen et al. 2020a</ref>) while significantly lowering the computational costs. these voxelized scenes, 3D convolution networks are always adopted by these methods to learn the occupancy and semantic information of each voxel. Although voxel representations preserve abundant structure details of the partial 2D observation, not all voxels are of equal importance in this volume. In particular, there exist lots of visible empty voxels (e.g., atmosphere in the visible region) in the voxelized SSC data by nature. These voxel-based methods have to perform unnecessary calculations on them in forward propagation, but ignore them in backward propagation since the labels are already known. Therefore, they always suffer from heavy computation redundancy, especially for trying to keep the high scene resolution in a deep 3D convolution network. To solve this problem, an efficient sparse data structure should be used, such as the point cloud or voxel octree <ref type="bibr" target="#b34">Takikawa et al. 2021</ref>). An early attempt <ref type="bibr" target="#b50">(Zhong and Zeng 2020)</ref> removes these visible empty voxels and adopts a point cloud based network to extract features from this nongrid data for SSC task, but it is intrinsically weak in local structure modeling since the point cloud representation is sparse.</p><p>Therefore, it is crucial to consider the complementarity between voxel-based and point-based scene representation into the SSC framework. Unlike most other 3D computer vision tasks, such as 3D detection or segmentation, the data for SSC tasks is usually voxelized since the goal is to predict the semantic and occupancy of voxels in this 3D scene. The point clouds are extracted from these voxels. In fact, the voxel representation is denser than the point representation in our setting, for that only a part of point clouds are sampled as input during training. Thus, we design our Point-Voxel Aggregation Network (PVA-Net), where two 3D convolution layers keep the details from the voxelized scenes and a deep point cloud based network captures semantic information efficiently.</p><p>We adopt the point stream as the main stream of our network for its low memory requirement. Meanwhile, a lightweight voxel stream is used to extract structure details, which acts as a complement to the point stream. To efficiently fuse the point stream and the voxel stream, we propose a novel Anisotropic Voxel Aggregation (AVA) module to aggregate information in voxels for each center point. Given the position of a center point, we apply three ellipsoidal receptive fields to extract feature patterns from the voxels in different directions and concatenate them with the center point's features. Moreover, we design a semantic-guided decoder that consists of several Semantic-aware Propagation (SP) modules, which encourages feature propagation between points belonging to the same semantic class.</p><p>We summarize our contributions as follows:</p><p>? To avoid the redundant computation in visible empty voxels in the SSC task, we convert the valid volume data to points and introduce the Point-Voxel Aggregation Network, which combines the low memory requirement of point-based methods and the local structure modelling ability of voxel-based methods. ? We propose the Anisotropic Voxel Aggregation module to efficiently fuse the structure information from a light-weight voxel stream into the point stream, and the Semantic-aware Propagation module to encourage feature propagation between points of the same semantic class. ? Our method outperforms state-of-the-arts by a large margin on two public benchmarks, with only depth images as the input.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning for 3D Scene Analysis</head><p>Learning semantic information of given scene with 3D information has drawn increasing attention in recent years. Rather than directly using 3D data, previous methods always focus on RGBD images <ref type="bibr">(Xing et al. 2019a,b;</ref><ref type="bibr" target="#b44">Xing, Wang, and Zeng 2020;</ref><ref type="bibr" target="#b2">Chen et al. 2020b</ref>) to understand the semantic information of the given scene. Different from 2D images, 3D data have various data representations such as voxels and point clouds and can facilitate various different applications <ref type="bibr" target="#b29">Rong, Shiratori, and Joo 2021;</ref>. Lots of methods have been proposed to handle different representations. 3D CNNs are the straightforward extension of 2D CNNs to 3D voxels. Early researches <ref type="bibr" target="#b0">(Chang et al. 2015;</ref><ref type="bibr" target="#b41">Wu et al. 2015;</ref><ref type="bibr" target="#b24">Maturana and Scherer 2015;</ref><ref type="bibr" target="#b51">Zhou and Tuzel 2018)</ref> rely on 3D convolutions to process 3D voxel data in regular grids. Point-based methods learn 3D point cloud representations directly by defining permutation-invariant point convolutions in irregular space. PointNet <ref type="bibr" target="#b26">(Qi et al. 2017a)</ref> first uses a shared MLP on every point individually followed by global max-pooling to extract global features. Pointnet++ <ref type="bibr" target="#b27">(Qi et al. 2017b)</ref> introduces hierarchical architectures to learn local features and increases modal capacity. Later works <ref type="bibr" target="#b28">(Rethage et al. 2018;</ref><ref type="bibr" target="#b12">Landrieu and Simonovsky 2018;</ref><ref type="bibr" target="#b39">Wu et al. 2018;</ref><ref type="bibr" target="#b48">Zhao et al. 2019;</ref><ref type="bibr" target="#b25">Milioto et al. 2019;</ref><ref type="bibr" target="#b11">Komarichev, Zhong, and Hua 2019;</ref><ref type="bibr" target="#b13">Lang et al. 2019;</ref><ref type="bibr" target="#b9">Hu et al. 2020;</ref><ref type="bibr" target="#b45">Xu et al. 2021</ref>) focus on more effective and general point operations, such as explicit point convolution kernels where the weights can be directly learned without intermediate MLP representations <ref type="bibr" target="#b10">(Hua, Tran, and Yeung 2018;</ref><ref type="bibr" target="#b35">Thomas et al. 2019;</ref><ref type="bibr" target="#b20">Lin et al. 2020</ref>) and graph convolutions <ref type="bibr" target="#b40">(Wu, Qi, and Li 2019;</ref><ref type="bibr" target="#b14">Li et al. 2019a;</ref><ref type="bibr" target="#b37">Wang et al. 2019a</ref>).</p><p>Multi-modality fusion is also a long-term problem in 3D deep learning. Recently, a few works begin to leverage the advantages of point cloud and voxel representation together in deep neural networks. PV-CNN ) is proposed to represent 3D data in sparse points to save memory cost and perform convolutions in voxels to obtain the contiguous memory access pattern. PV-RCNN <ref type="bibr" target="#b31">(Shi et al. 2020</ref>) defines a Voxel Set Abstraction (VSA) module to summarize voxel features into key points to further explore this problem. However, this work focuses on object detection and relies on a heavy voxel stream to regress object proposals. To improve the learning efficiency and capability of the framework, we differ from the design choices in PV-RCNN as follows: 1) We only use a light-weight voxel stream to assist the main point stream; 2) Our AVA module is more general than the VSA module, which can be seen as a special case of our AVA module using spherical receptive fields; 3) We focus on scene completion and propose the SP module to aggregate features at different stages with semantic guidance during up-sampling, which is not considered by PV-RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Scene Completion</head><p>Semantic Scene Completion (SSC) aims to predict a complete voxel representation of a 3D scene and each voxel's semantic label, usually from a single-view depth map observation. SSCNet <ref type="bibr" target="#b33">(Song et al. 2017)</ref> first combines semantic segmentation and scene completion in an end-to-end way, showing that the two tasks are highly coupled and can be learned together to improve the performance. ESSCNet <ref type="bibr" target="#b46">(Zhang et al. 2018)</ref> introduces Spatial Group Convolution (SGC) which divides the voxels into different groups to save computational cost. Later works <ref type="bibr" target="#b8">(Guo and Tong 2018;</ref><ref type="bibr" target="#b38">Wang et al. 2019b;</ref><ref type="bibr" target="#b47">Zhang et al. 2019;</ref><ref type="bibr" target="#b3">Chen, Xing, and Zeng 2020)</ref> improve the performance with better architecture, such as 2D-3D combination, cascaded context pyramid and so on. <ref type="bibr" target="#b7">Guedes et. al. (Guedes, Campos, and Hilton 2018)</ref> first investigates the potential of the RGB images to improve SSCNet. After that, many methods <ref type="bibr" target="#b6">(Garbade et al. 2019;</ref><ref type="bibr" target="#b22">Liu et al. 2018;</ref><ref type="bibr" target="#b4">Dourado et al. 2019;</ref><ref type="bibr" target="#b17">Li et al. 2019b</ref><ref type="bibr" target="#b18">Li et al. , 2020b</ref><ref type="bibr" target="#b1">Chen et al. 2020a</ref>) take RGB images ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Down-sampling</head><p>Up-sampling <ref type="figure">Figure 2</ref>: The overall architecture of the proposed method. We generate point clouds from input TSDF volumes and use an encoder-decoder architecture to predict the semantic labels, with an Anisotropic Voxel Aggregation module to aggregate the local structure information from the voxels.</p><p>as an additional input with the depth map and explore the complementarity between the two modalities. Most recent work <ref type="bibr" target="#b15">(Li, Ding, and Huang 2021)</ref> further explores the interaction between 2D segmentation and 3D SSC, but they rely on heavy networks to perform the feature extraction. These methods all utilize 3D CNNs as the backbone, causing unnecessary computational cost in the visible empty voxels. Whereas AIC-Net ) also proposes an anisotropic convolution to model the voxel-wisely dimensional anisotropy, our AVA module differs in both formulation and functionality, focusing on synchronizing features from voxels to point representation with a more flexible ellipsoidal receptive field.</p><p>Different from those voxel-based methods above, SPC-Net (Zhong and Zeng 2020) first introduces a point-based network to address the SSC problem, by training a point network on the observed points and then obtaining the features of the occluded points through bilinear interpolation. Due to the interpolation process that depends on distance metrics, the occluded points far from the visible points are hard to predict in their method. Furthermore, point-based method alone is not enough to retain detailed local structure information during the down-sampling progress. Therefore, we draw ideas from the Point-Voxel methods and propose a two-stream network, where an efficient point stream extracts semantic features and a light-weight voxel stream provides dense local structure information through the AVA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the Proposed Method</head><p>The overall architecture of the proposed method is illustrated in <ref type="figure">Figure 2</ref>. Our model consists of a point stream and a voxel stream. To reduce the computation redundancy, we convert the TSDF volume to a point cloud by removing the visible empty voxels, which serves as the input of the point stream.</p><p>The point stream adopts a PointNet++ <ref type="bibr" target="#b27">(Qi et al. 2017b</ref>)like encoder-decoder architecture. The encoder extracts the semantic features in a hierarchical way and the decoder encourages feature propagation in points of the same class. Meanwhile, a light-weight voxel stream that only contains two 3D dense convolutional layers is applied to the TSDF volume. An AVA module is proposed to aggregate the local voxel features to the point features. Finally, the predicted point labels are converted back to the voxel representation to calculate the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Point Clouds Generation</head><p>Voxel-based methods always encode the depth map into a 3D TSDF volume <ref type="bibr" target="#b33">(Song et al. 2017)</ref>, and carry out later procedures in this voxel space. However, we argue that not all voxels in this volume are of equal importance for the SSC task. In fact, there are three kinds of voxels inside this volume: 1) the observed surface voxels which are directly projected from the given depth image, 2) the occluded voxels behind the observed surface which we need to complete and recognize, and 3) the visible empty voxels (such as the atmosphere) between camera and the observed surface. The last kind of voxel is useless for our task since we already know it's empty. Thus, these voxels are removed during our point clouds generation process. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we only keep the observed surface and the occluded regions in our point clouds.</p><p>Each input point p i has a 5-dim feature vector</p><formula xml:id="formula_0">f i = (x i , y i , z i , t i , h i ). Suppose p i is generated from voxel v i in the 60 ? 36 ? 60 3D volume, then x i , y i , z i are normalized x-y-z indexes of v i in the volume. t i is the TSDF value of v i ,</formula><p>and h i is the normalized height value of v i . Please note that x i , y i , z i are normalized according to the mass center of the points in the scene, while h i is normalized by 36, the maximum height of the voxelized scene. We think the normalized height serves as a prior that describes the positions of objects in the room. This could help to distinguish some categories with significantly different height values, such as the floor and the ceiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anisotropic Voxel Aggregation</head><p>Due to the sparsity of the point clouds, it is hard for the point stream to model the detailed structure information which is important for the scene completion and recognition. Since we have the denser volume data as well, we design a voxel stream to extract the structure features and propose the Anisotropic Voxel Aggregation (AVA) module to fuse the point-voxel features.</p><p>We first extract the local features of the TSDF volume through two simple convolution layers. This requires little computational cost and takes about only 15.0% of the overall memory cost, but enables each voxel in the volume to have a suitable receptive field to encode local geometry information. As shown in the top part of <ref type="figure">Figure 2</ref>, for each center point p i = (x i , y i , z i ) in the point cloud, we define three ellipsoidal receptive fields with x, y, z axis as the major axis respectively. Taking the x-axis as an example, the receptive field N x (i) of p i in the volume could be defined as:</p><formula xml:id="formula_1">N x (i) = v j (x j ? x i ) 2 (kr) 2 + (y j ? y i ) 2 r 2 + (z j ? z i ) 2 r 2 &lt; 1<label>(1)</label></formula><p>where v j is the j-th voxel and (x j , y j , z j ) is its position, r is the radius for minor axes, and k &gt; 1 is a scale factor for the major axis. Unless mentioned specifically, we use 3 as the default value for k. The receptive field along the y-axis and z-axis could be defined in a similar way. From the perspective of pattern recognition, the anisotropic receptive field ensures us to activate feature patterns in three directions, which is more flexible and effective than the isotropic spherical receptive field. Then we could aggregate the structure features of voxels around p i with:</p><formula xml:id="formula_2">f fuse i = d?{x,y,z} max j?N d (i) {? d (f point i , f voxel j )}<label>(2)</label></formula><p>where f fuse max denotes max-pooling operation that keeps the maximum activation in the neighborhood, and N d (i) represents the set of neighbor voxels of p i inside the ellipsoidal receptive field with the d-axis as the major axis. This AVA module enables the sparse center points to aggregate local structure information from nearby dense voxels. Therefore, the information from voxels could positively affect the completion and recognition of point clouds through back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantic-aware Propagation</head><p>During the encoding process, Set Abstract (SA) <ref type="bibr" target="#b27">(Qi et al. 2017b</ref>) layers will down-sample the input points. Suppose we have N SA layers in the network, named SA (1) , ..., SA (N ) . We denote</p><formula xml:id="formula_3">X (0) = {(p (0) i , f (0) i )} as the raw input point set, where f (0) i is the feature of the point p (0)</formula><p>i . Then the corresponding output point sets of N SA layers are X (1) , ..., X (N ) , respectively. Please note that if a point belongs to X (i) (i ? 1), then it must belong to X (i?1) as well, because each SA layer only down-samples points from the former layer.</p><p>To obtain the features of all the raw input points, we propose the Semantic-aware Propagation (SP) module, which is a hierarchical propagation strategy as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. For a target point p</p><formula xml:id="formula_4">(l) i in X (l) , we find its k-neighbors (p (l+1) 1 , ..., p (l+1) k</formula><p>) in X (l+1) according to the xyz coordinates. To interpolate the feature of p (l) , the general feature propagation can be represented by:</p><formula xml:id="formula_5">f (l) i = j?N (l+1) k (i) w (l) i,j f (l+1) j j?N (l+1) k (i) w (l) i,j<label>(3)</label></formula><p>where N i . An intuitive idea is to measure the similarity between the point p (l) i and p (l+1) j for j ? N (l+1) k (i) and use the similarity as the weight factor. However, since p (l) i and p (l+1) j belong to different levels and thus are embedded to different feature spaces, it is not suitable to directly compare their feature vectors. We notice that the point p (l+1) j also exists in X (l) since the points in X (l+1) is a subset of X (l) . Then we could measure the similarity between p (l) i and p (l) j in a learnable manner: w</p><formula xml:id="formula_6">(l) i,j = ?(?(f (l) i ||f (l) j ))<label>(4)</label></formula><p>where ? is the sigmoid function, ? is a MLP and || means channel-wise concatenation.</p><p>In this way, we could interpolate point features with semantic information, which is helpful in SSC task. We explicitly supervise the learned weights during training, by setting the ground truth of w (l) i,j to 1 if the two points belong to the same semantic class, and 0 if they belong to different classes. We think this could encourage the network to only propagate semantically similar features, which weakens the effect of neighbor points from different classes during interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Loss</head><p>The training loss involves two terms: SSC loss L SSC and SP loss L SP . The SSC loss is a weighted voxel-wise crossentropy loss:</p><formula xml:id="formula_7">L SSC = 1 N valid i,j,k m i,j,k L CE (p i,j,k , y i,j,k )<label>(5)</label></formula><p>where m i,j,k is set to 1 if the voxel at index (i, j, k) is not visible empty (i.e., can be converted to a point) or 0 otherwise. y i,j,k is the ground truth label, p i,j,k is the prediction of the voxel mapped back from the corresponding point, N valid = i,j,k m i,j,k is the number of valid voxels in this volume, and L CE is the cross-entropy loss.</p><p>The SP loss is designed to supervise the pairwise similarity introduced in Equation 4. The SP loss could be formulated as:</p><formula xml:id="formula_8">L SP = 1 N pairs l 0?i?|X (l) | j?N (l+1) k (i) L CE (w (l) i,j , G (l) i,j ) (6)</formula><p>where N pairs represents the number of point pairs involved, |X (l) | means the number of points in the l-th level, N j belong to the same category, it is 1, otherwise it is 0.</p><p>We optimize the entire network by the balanced combination of the two terms:</p><formula xml:id="formula_9">L = L SSC + ?L SP<label>(7)</label></formula><p>4 Experiments</p><p>In this section, we evaluate the proposed method and compare it with state-of-the-art methods on two public datasets, NYU <ref type="bibr" target="#b32">(Silberman et al. 2012</ref>) and NYUCAD <ref type="bibr" target="#b5">(Firman et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets. The NYU dataset <ref type="bibr" target="#b32">(Silberman et al. 2012</ref>) consists of 1, 449 realistic indoor RGB-D scenes captured via a Kinect sensor <ref type="bibr" target="#b33">(Song et al. 2017</ref>). Since the real-world completed scenes are hard to be captured, human annotations provided by <ref type="bibr" target="#b7">(Guo, Zou, and Hoiem 2015)</ref> are widely used as the ground truth completion. However, as discussed in <ref type="bibr" target="#b33">(Song et al. 2017)</ref>, there exists many misalignments between the depth images and the corresponding 3D labels in the NYU dataset, which makes it hard to evaluate accurately. To solve this problem, the high-quality synthetic NYUCAD dataset is proposed by <ref type="bibr" target="#b5">(Firman et al. 2016)</ref>, where the depth maps are projected from the ground truth annotations and thus avoid the misalignments. Following previous works <ref type="bibr" target="#b33">(Song et al. 2017;</ref><ref type="bibr" target="#b1">Chen et al. 2020a;</ref><ref type="bibr" target="#b6">Garbade et al. 2019)</ref>, we choose NYU and NYUCAD to evaluate our method.</p><p>Evaluation Metrics. We follow <ref type="bibr" target="#b33">(Song et al. 2017)</ref> to use precision, recall and voxel-level intersection over union (IoU) as the evaluation metrics. Two tasks are considered, namely, semantic scene completion and scene completion. For the task of semantic scene completion, we evaluate on both the observed surface and occluded regions and report the mIoU of each semantic class. For the task of scene completion, we treat all non-empty voxels as class '1' and all empty voxels as class '0', and then evaluate the binary IoU on the occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use the PyTorch framework with two Nvidia Titan Xp GPUs to conduct our experiments. Mini-batch SGD with momentum of 0.9 is adopted to train our network. The initial learning rate is 0.05, batch size is 8 and the weight decay is 0.0005. We employ a Poly learning rate decay policy where the initial learning rate is multiplied by (1 ? now_iter max_iter ) 0.9 . We train our network for 1000 epochs on the NYUCAD dataset and the NYU dataset. The radius of the ellipsoidal receptive field is set to 0.09 for the major axis and 0.03 for the minor axes. We sample at most 8 voxels inside each ellipsoidal receptive field. The balancing factor ? in Equation 7 is set to 0.5. Since the output of the SSC task is usually at the resolution of 60 ? 36 ? 60, we adopt the same input resolution of voxels following SATNet <ref type="bibr" target="#b22">(Liu et al. 2018</ref>) and 3D-Sketch <ref type="bibr" target="#b1">(Chen et al. 2020a)</ref>. Different from (Zhong and Zeng 2020), we feed both the observed and occluded points into our network during training. Since the number of points for each scene is not fixed, we randomly sample a fixed number of observed points (2048) and occluded points (8192) for each scene to enable batched training. At inference time, we use all the generated points as input.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Methods</head><p>We compare the proposed method with state-of-the-art methods. <ref type="table" target="#tab_0">Table 1</ref> lists the results on the NYUCAD dataset. Our method outperforms all the existing voxel-based or pointbased methods, gaining an increase of 1.7% SSC mIoU and 2.1% SC IoU compared to the previous best method <ref type="bibr" target="#b1">(Chen et al. 2020a</ref>). While some methods use a higher input resolution, we already achieve good enough results with the 60 ? 36 ? 60 input resolution. The advantage of our method is not from a higher input resolution than others, but the novel and efficient point-voxel framework that modelling the local details and global context in a computationally-friendly manner. We also conduct experiments on the NYU dataset to validate the performance of our method on realistic data. As listed in <ref type="table" target="#tab_1">Table 2</ref>, our method consistently outperforms previous best method <ref type="bibr" target="#b15">(Li, Ding, and Huang 2021)</ref> in both SC IoU and SSC mIoU metrics. Notably, our method only requires one-pass forward, while IMENet <ref type="bibr" target="#b15">(Li, Ding, and Huang 2021)</ref> performs multiple iterations between a 2D and a 3D network and introduces a very large computational cost. We provide some visualizations on the NYUCAD dataset in <ref type="figure" target="#fig_6">Figure 5</ref>. With only the depth images as input, our method achieves good inter-class distinction and intra-class consistency. We think the superiority of the proposed method comes from the two-stream framework, where the point stream extracts the high-level semantics and the voxel stream extracts the detailed local structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficiency Analysis</head><p>Since the computational cost of our method depends on the number of visible empty points in each scene, we report  the average number of these statistics on the NYUCAD test set in <ref type="table">Table 3</ref>. The proposed method avoids the redundancy caused by the visible empty voxels and achieves higher performance with lower computational cost. Even though our FLOPs are only 5.4% of SSCNet and 3.0% of 3D-Sketch, we still achieve a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, we conduct ablation studies to verify the effectiveness of each aforementioned component. All the results are tested on the NYUCAD test set.  <ref type="table">Table 3</ref>: Efficiency analysis on the NYUCAD dataset.</p><p>Proposed modules. Firstly, we do ablation studies on different modules in our method in <ref type="table" target="#tab_5">Table 4</ref>. The AVA module aggregates the local structure information from the voxel stream to the point stream, and the SP module encourages feature propagation of points belonging to the same category. Adopting either of these two modules improves the performance, and the combination of them achieves the maximum benefits. Note that our point-based baseline already outperforms some of the voxel-based methods. Thanks to the removal of redundant parts in the input, the point stream could have a deeper architecture and higher feature dimensions, leading to stronger representation power.   Voxel Aggregation Strategies. As illustrated before, we propose the AVA module that aggregates the voxel structure features in an anisotropic manner. Here, we try to compare AVA with other voxel aggregation strategies to verify its effectiveness. We provide two other strategies: 'Nearest Aggregation' and 'Spherical Aggregation'. 'Nearest Aggregation' means we only concatenate the point features with the features of the nearest voxel from the point. 'Spherical Aggregation' means we adopt a spherical receptive field with the radius r to aggregate the voxel features inside the receptive field. r is 0.09 here so that it is the same with the radius of the major axis in AVA. Results are listed in <ref type="table" target="#tab_6">Table 5</ref>. As shown in the table, both 'Nearest Aggregation' and 'Spherical Aggregation' could boost the performance, because they more or less introduce some detailed geometric information to the point stream. However, the proposed AVA module achieves the best performance with the anisotropic aggregation design, because it could capture more feature patterns in different directions.</p><p>Position of the AVA Module. We try to embed AVA modules to different positions in the network, as listed in <ref type="table" target="#tab_8">Table 6</ref>. We find that if we embed the AVA module to a higher level layer in the network, the additional gain brought by voxel features decreases. This makes sense for that the AVA module exploits local features in the voxel representation and if we embed it into the first SA layer, the structure information provided by the AVA module will be further encoded as the network goes deeper. Also, if we embed AVA modules to all the SA layers in the network, the performance is just similar to the proposed method. Hence, we only embed it to the first SA layer for lower computational cost.</p><p>Feature Propagation Strategies. We then conduct experiments on the proposed SP module. The SP module encourages feature propagation in points belonging to the same category through the w i,j defined in Equation 3 and Equation 4. We compare SP with other feature propagation strategies in <ref type="table">Table 7</ref>. Our SP achieves the best performance, since it considers pairwise semantic relations and avoids the errors caused by feature propagation from the wrong categories.   <ref type="table">Table 7</ref>: Ablation study on different Feature Propagation Strategies. 'Inverse Euclidean' means the inverse Euclidean distance between two points used in Pointnet++ <ref type="bibr" target="#b27">(Qi et al. 2017b)</ref>. 'Cosine Similarity' means the cosine similarity between features of the two points. The proposed SP module achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce the Point-Voxel Aggregation Network for Semantic Scene Completion, which combines the advantages of the computationally efficient point representation and the rich-detailed voxel representation. Besides, an AVA module is proposed to aggregate the structure information from voxels and a SP module is proposed to encourage semantic-aware feature propagation in points. Experimental results demonstrate the effectiveness and efficiency of our method with state-of-the-art performance on two public benchmarks, with only the depth images as input. In the future, we believe the representation of the scene is crucial for this topic, such as the modality of inputs (e.g., RGB and depth map), as well as the point cloud with different sampling rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Points generation from the voxel volumes. Only the observed surface (yellow) and occluded regions (dark gray) are kept, while the visible empty voxels (light gray) are discarded. For example, the average number of the kept points is 16313 for the 60 ? 36 ? 60 input voxels from the NYUCAD dataset, which means about 87% of the input voxels are redundant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>iFigure 4 :</head><label>4</label><figDesc>is the fused point-voxel feature, f point i is the feature of p i , f voxel j is the feature of j-th voxel in the volume, ? d is a MLP layer for non-linear feature extraction in the d-axis, Different Feature Propagation strategies. (a) Feature Propagation (FP) in PointNet++ (Qi et al. 2017b). (b) The proposed Semantic-aware Propagation (SP). Different colors mean different semantic classes. Larger points are from a deeper layer, while smaller points are in the current layer. Dashed lines mean smaller weight. The boundary color of the center point means the interpolated features are dominated by which semantic class. In (a), the center point is dominated by the wrong class due to the unsuitable FP strategy, while the proposed method (b) avoids this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is the set of k nearest neighbors of p (l) i in X (l+1) , and w (l) i,j is the weight factor for f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations on the NYUCAD dataset. From left to right: (a) RGB-D images, (b) results of (Song et al. 2017), (c) results of (Chen et al. 2020a), (d) our results, and (e) ground truth. Best viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. mIoU Zheng et al. (Zheng et al. Results on the NYUCAD dataset.Resolution(a, b)means the input resolution is (a ? 0.6a ? a) and the output resolution is (b ? 0.6b ? b).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">semantic scene completion</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">Input Resolution prec. recall 2013) D (240, 60) 60.1 46.7 34.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Firman et al. (Firman et al. 2016)</cell><cell>D</cell><cell>(240, 60)</cell><cell>66.5</cell><cell cols="2">69.7 50.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SSCNet (Song et al. 2017)</cell><cell>D</cell><cell>(240, 60)</cell><cell>75.4</cell><cell cols="6">96.3 73.2 32.5 92.6 40.2 8.9</cell><cell cols="4">33.9 57.0 59.5 28.3</cell><cell>8.1</cell><cell cols="2">44.8 25.1</cell><cell>40.0</cell></row><row><cell cols="2">CCPNet (Zhang et al. 2019)</cell><cell>D</cell><cell cols="2">(240, 240) 91.3</cell><cell cols="13">92.6 82.4 56.2 94.6 58.7 35.1 44.8 68.6 65.3 37.6 35.5 53.1 35.2</cell><cell>53.2</cell></row><row><cell cols="2">SPCNet (Zhong and Zeng 2020)</cell><cell>D</cell><cell>(60, 60)</cell><cell>81.4</cell><cell cols="10">70.9 61.0 58.1 91.6 53.7 13.0 52.1 68.9 57.7 31.9</cell><cell>6.4</cell><cell cols="2">50.5 28.1</cell><cell>46.6</cell></row><row><cell cols="2">TS3D (Garbade et al. 2019)</cell><cell>RGB+D</cell><cell>(240, 60)</cell><cell>-</cell><cell>-</cell><cell cols="12">76.1 25.9 93.8 48.9 33.4 31.2 66.1 56.4 31.6 38.5 51.4 30.8</cell><cell>46.2</cell></row><row><cell>DDRNet (Li et al. 2019b)</cell><cell></cell><cell>RGB+D</cell><cell>(240, 60)</cell><cell>88.7</cell><cell cols="10">88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8</cell><cell>9.2</cell><cell cols="2">44.1 27.8</cell><cell>42.8</cell></row><row><cell>AIC-Net (Li et al. 2020a)</cell><cell></cell><cell>RGB+D</cell><cell>(240, 60)</cell><cell>88.2</cell><cell cols="10">90.3 80.5 53.0 91.2 57.2 20.2 44.6 58.4 56.2 36.2</cell><cell>9.7</cell><cell cols="2">47.1 30.4</cell><cell>45.8</cell></row><row><cell cols="2">3D-Sketch (Chen et al. 2020a)</cell><cell>RGB+D</cell><cell>(60, 60)</cell><cell>90.6</cell><cell cols="13">92.2 84.2 59.7 94.3 64.3 32.6 51.7 72.0 68.7 45.9 19.0 60.5 38.5</cell><cell>55.2</cell></row><row><cell cols="3">IMENet (Li, Ding, and Huang 2021) RGB+D</cell><cell>(60, 60)</cell><cell>84.8</cell><cell cols="2">92.3 79.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.5</cell></row><row><cell>Ours</cell><cell></cell><cell>D</cell><cell>(60, 60)</cell><cell>95.1</cell><cell cols="13">90.3 86.3 71.5 94.1 66.6 23.7 60.0 78.5 72.2 45.3 16.7 60.1 36.9</cell><cell>56.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">semantic scene completion</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell>Input</cell><cell cols="15">Resolution prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. mIoU</cell></row><row><cell cols="2">SSCNet (Song et al. 2017)</cell><cell>D</cell><cell>(240,60)</cell><cell>57.0</cell><cell cols="6">94.5 55.1 15.1 94.7 24.4 0.0</cell><cell cols="4">12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell cols="2">27.1 10.1</cell><cell>24.7</cell></row><row><cell cols="2">ESSCNet (Zhang et al. 2018)</cell><cell>D</cell><cell>(240,60)</cell><cell>71.9</cell><cell cols="6">71.9 56.2 17.5 75.4 25.8 6.7</cell><cell cols="4">15.3 53.8 42.4 11.2</cell><cell>0</cell><cell cols="2">33.4 11.8</cell><cell>26.7</cell></row><row><cell cols="2">VVNet (Guo and Tong 2018)</cell><cell>D</cell><cell>(120,60)</cell><cell>69.8</cell><cell cols="13">83.1 61.1 19.3 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3</cell><cell>32.9</cell></row><row><cell cols="2">ForkNet (Wang et al. 2019b)</cell><cell>D</cell><cell>(80,80)</cell><cell>-</cell><cell>-</cell><cell cols="12">63.4 36.2 93.8 29.2 18.9 17.7 61.6 52.9 23.3 19.5 45.4 20.0</cell><cell>37.1</cell></row><row><cell cols="2">CCPNet (Zhang et al. 2019)</cell><cell>D</cell><cell>(240,240)</cell><cell>74.2</cell><cell cols="13">90.8 63.5 23.5 96.3 35.7 20.2 25.8 61.4 56.1 18.1 28.1 37.8 20.1</cell><cell>38.5</cell></row><row><cell cols="2">SPCNet (Zhong and Zeng 2020)</cell><cell>D</cell><cell>(240,60)</cell><cell>72.1</cell><cell cols="6">42.2 36.3 33.8 64.4 38.3 7.5</cell><cell cols="4">30.7 53.4 42.6 19.7</cell><cell>5.5</cell><cell cols="2">34.2 13.9</cell><cell>31.3</cell></row><row><cell cols="2">TS3D (Garbade et al. 2019)</cell><cell>RGB+D</cell><cell>(240,60)</cell><cell>-</cell><cell>-</cell><cell cols="2">60.0 9.7</cell><cell cols="10">93.4 25.5 21.0 17.4 55.9 49.2 17.0 27.5 39.4 19.3</cell><cell>34.1</cell></row><row><cell>SATNet (Liu et al. 2018)</cell><cell></cell><cell>RGB+D</cell><cell>(60,60)</cell><cell>67.3</cell><cell cols="13">85.8 60.6 17.3 92.1 28.0 16.6 19.3 57.5 53.8 17.2 18.5 38.4 18.9</cell><cell>34.4</cell></row><row><cell>DDRNet (Li et al. 2019b)</cell><cell></cell><cell>RGB+D</cell><cell>(60,60)</cell><cell>71.5</cell><cell cols="6">80.8 61.0 21.1 92.2 33.5 6.8</cell><cell cols="7">14.8 48.3 42.3 13.2 13.9 35.3 13.2</cell><cell>30.4</cell></row><row><cell>AIC-Net (Li et al. 2020a)</cell><cell></cell><cell>RGB+D</cell><cell>(60,60)</cell><cell>62.4</cell><cell cols="13">91.8 59.2 23.2 90.8 32.3 14.8 18.2 51.1 44.8 15.2 22.4 38.3 15.7</cell><cell>33.3</cell></row><row><cell cols="2">3D-Sketch (Chen et al. 2020a)</cell><cell>RGB+D</cell><cell>(60,60)</cell><cell>85.0</cell><cell cols="13">81.6 71.3 43.1 93.6 40.5 24.3 30.0 57.1 49.3 29.2 14.3 42.5 28.6</cell><cell>41.1</cell></row><row><cell cols="3">IMENet (Li, Ding, and Huang 2021) RGB+D</cell><cell>(60,60)</cell><cell>90.0</cell><cell cols="13">78.4 72.1 43.6 93.6 42.9 31.3 36.6 57.6 48.4 32.1 16.0 47.8 36.7</cell><cell>44.2</cell></row><row><cell>Ours</cell><cell></cell><cell>D</cell><cell>(60,60)</cell><cell>91.1</cell><cell cols="13">79.7 74.0 51.4 94.0 49.9 15.9 41.9 68.3 58.8 35.4 12.9 48.5 29.1</cell><cell>46.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the NYU dataset. Resolution(a, b) means the input resolution is (a ? 0.6a ? a) and the output resolution is(b ? 0.6b ? b).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different modules.</figDesc><table><row><cell>The baseline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on different Voxel Aggregation Strategies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the position of AVA module. SA-i indicates AVA module is embedded into the i-th SA layer.</figDesc><table><row><cell>Methods</cell><cell cols="2">SC IoU(%) SSC mIoU(%)</cell></row><row><cell>Inverse Euclidean</cell><cell>85.3</cell><cell>54.9</cell></row><row><cell>Cosine Similarity</cell><cell>83.9</cell><cell>54.4</cell></row><row><cell>Semantic-aware Propagation</cell><cell>86.3</cell><cell>56.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Key Research and Development Program of China (2017YFB1002601), National Natural Science Foundation of China (61632003, 61375022, 61403005), Beijing Advanced Innovation Center for Intelligent Robots and Systems (2018IRS11), and PEK-SenseTime Joint Laboratory of Machine Vision.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository. ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D Sketch-aware Semantic Scene Completion via Semisupervised Structure Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4193" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-Time Semantic Scene Completion Via Feature Aggregation And Conditioned Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2830" to="2834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EdgeNet: Semantic Scene Completion from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<idno>abs/1908.02893</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two Stream 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion Combining Colour and Depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1504.02437</idno>
	</analytic>
	<monogr>
		<title level="m">Predicting Complete 3D Models of Indoor Scenes. ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View-Volume Network for Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno>abs/1806.05361</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11105" to="11114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A-CNN: Annularly Convolutional Neural Networks on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7413" to="7422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-Scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection From Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12689" to="12697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs Go As Deep As CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9266" to="9275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15413</idno>
		<title level="m">IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anisotropic Convolutional Networks for 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7685" to="7694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attentionbased Multi-modal Fusion Network for Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution On X-Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FPConv: Learning Local Flattening for Point Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11571</idno>
		<title level="m">Neural sparse voxel fields</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">See and Think: Disentangling Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RangeNet ++: Fast and Accurate LiDAR Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully-Convolutional Point Networks for Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<idno>abs/1808.06840</idno>
	</analytic>
	<monogr>
		<title level="m">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1749" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00763</idno>
		<title level="m">Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10526" to="10535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural geometric level of detail: Real-time rendering with implicit 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Litalien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11358" to="11367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene-aware Generative Network for Human Motion Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12206" to="12215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Attention Convolution for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10288" to="10297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8608" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-Seg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">PointConv: Deep Convolutional Networks on 3D Point Clouds. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9613" to="9622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">2.5 D convolution for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1410" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coupling two-stream RGB-D semantic segmentation network by idempotent mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1850" to="1854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Malleable 2.5 d convolution: Learning receptive fields along the depth-axis for rgb-d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="555" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7801" to="7810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5560" to="5568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic Point Completion Network for 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
