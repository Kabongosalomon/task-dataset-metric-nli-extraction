<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>B?cigneul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
						</author>
						<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning graph representations via lowdimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Producing high quality feature representations of data such as text or images is a central point of interest in artificial intelligence. A large line of research focuses on embedding discrete data such as graphs <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b11">Goyal &amp; Ferrara, 2017)</ref> or linguistic instances <ref type="bibr" target="#b21">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b26">Pennington et al., 2014;</ref><ref type="bibr" target="#b17">Kiros et al., 2015)</ref> into continuous spaces that exhibit certain desirable geometric properties. This class of models has reached state-of-theart results for various tasks and applications, such as link prediction in knowledge bases <ref type="bibr" target="#b24">(Nickel et al., 2011;</ref><ref type="bibr" target="#b3">Bordes et al., 2013)</ref> or in social networks <ref type="bibr" target="#b15">(Hoff et al., 2002)</ref>, text disambiguation <ref type="bibr" target="#b10">(Ganea &amp; Hofmann, 2017)</ref>, word hypernymy <ref type="bibr" target="#b32">(Shwartz et al., 2016)</ref>, textual entailment <ref type="bibr" target="#b28">(Rockt?schel et al., 2015)</ref> or taxonomy induction <ref type="bibr" target="#b9">(Fu et al., 2014)</ref>.</p><p>Popular methods typically embed symbolic objects in low dimensional Euclidean vector spaces using a strategy that aims to capture semantic information such as functional similarity. Symmetric distance functions are usually minimized between representations of correlated items during the learning process. Popular examples are word embedding algorithms trained on corpora co-occurrence statistics which have shown to strongly relate semantically close words and their topics <ref type="bibr" target="#b21">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b26">Pennington et al., 2014)</ref>.</p><p>However, in many fields (e.g. Recommender Systems, Genomics <ref type="bibr" target="#b0">(Billera et al., 2001)</ref>, Social Networks), one has to deal with data whose latent anatomy is best defined by non-Euclidean spaces such as Riemannian manifolds <ref type="bibr" target="#b5">(Bronstein et al., 2017)</ref>. Here, the Euclidean symmetric models suffer from not properly reflecting complex data patterns such as the latent hierarchical structure inherent in taxonomic data. To address this issue, the emerging trend of geometric deep learning 1 is concerned with non-Euclidean manifold representation learning.</p><p>In this work, we are interested in geometrical modeling of hierarchical structures, directed acyclic graphs (DAGs) and entailment relations via low dimensional embeddings. Starting from the same motivation, the order embeddings method <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref> explicitly models the partial order induced by entailment relations between embedded objects. Formally, a vector x ? R n represents a more general concept than any other embedding from the Euclidean entailment region O x := {y | y i ? x i , ?1 ? i ? n}. A first concern is that the capacity of order embeddings grows only linearly with the embedding space dimension. Moreover, the regions O x suffer from heavy intersections, implying that their disjoint volumes rapidly become bounded 2 . As a consequence, representing wide (with high branching factor) and deep hierarchical structures in a bounded region of the Euclidean space would cause many points to end up undesirably close to each other. This also implies that Euclidean distances would no longer be capable of reflecting the original tree metric.</p><p>Fortunately, the hyperbolic space does not suffer from the aforementioned capacity problem because the volume of any ball grows exponentially with its radius, instead of polynomially as in the Euclidean space. This exponential growth property enables hyperbolic spaces to embed any weighted tree while almost preserving their metric 3 <ref type="bibr" target="#b12">(Gromov, 1987;</ref><ref type="bibr" target="#b4">Bowditch, 2006;</ref><ref type="bibr" target="#b29">Sarkar, 2011)</ref>. The tree-likeness of hyperbolic spaces has been extensively studied <ref type="bibr" target="#b14">(Hamann, 2017)</ref>. Moreover, hyperbolic spaces are used to visualize large hierarchies <ref type="bibr" target="#b20">(Lamping et al., 1995)</ref>, to efficiently forward information in complex networks <ref type="bibr" target="#b18">(Krioukov et al., 2009;</ref><ref type="bibr" target="#b7">Cvetkovski &amp; Crovella, 2009)</ref> or to embed heterogeneous, scale-free graphs <ref type="bibr">(Shavitt &amp; Tankel, 2008;</ref><ref type="bibr" target="#b19">Krioukov et al., 2010;</ref><ref type="bibr" target="#b1">Bl?sius et al., 2016)</ref>.</p><p>From a machine learning perspective, recently, hyperbolic spaces have been observed to provide powerful representations of entailment relations <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>. The latent hierarchical structure surprisingly emerges as a simple reflection of the space's negative curvature. However, the approach of <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> suffers from a few drawbacks: first, their loss function causes most points to collapse on the border of the Poincar? ball, as exemplified in <ref type="figure">Figure 3</ref>. Second, the hyperbolic distance alone (being symmetric) is not capable of encoding asymmetric relations needed for entailment detection, thus a heuristic score is chosen to account for concept generality or specificity encoded in the embedding norm.</p><p>We here inspire ourselves from hyperbolic embeddings <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> and order embeddings <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref>. Our contributions are as follows:</p><p>? We address the aforementioned issues of <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> and <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref>. We propose to replace the entailment regions O x of order-embeddings by a more efficient and generic class of objects, namely geodesically convex entailment cones. These cones are defined on a large class of Riemannian manifolds and induce a partial ordering relation in the embedding space.</p><p>? The optimal entailment cones satisfying four natural properties surprisingly exhibit canonical closed-form expressions in both Euclidean and hyperbolic geometry that we rigorously derive.</p><p>? An efficient algorithm for learning hierarchical embeddings of directed acyclic graphs is presented. This learning process is driven by our entailment cones.</p><p>? Experimentally, we learn high quality embeddings and improve over experimental results in <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> and <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref> on hypernymy link prediction for word embeddings, both in terms of capacity of the model and generalization performance.</p><p>3 See end of Section 2.2 for a rigorous formulation.</p><p>We also compute an analytic closed-form expression for the exponential map in the n-dimensional Poincar? ball, allowing us to perform full Riemannian optimization <ref type="bibr" target="#b2">(Bonnabel, 2013)</ref> in the Poincar? ball, as opposed to the approximate optimization method used by <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mathematical preliminaries</head><p>We now briefly visit some key concepts needed in our work.</p><p>Notations. We always use ? to denote the Euclidean norm of a point (in both hyperbolic or Euclidean spaces). We also use ?, ? to denote the Euclidean scalar product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Differential geometry</head><p>For a rigorous reasoning about hyperbolic spaces, one needs to use concepts in differential geometry, some of which we highlight here. For an in-depth introduction, we refer the reader to <ref type="bibr" target="#b33">(Spivak, 1979)</ref> and <ref type="bibr" target="#b16">(Hopper &amp; Andrews, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manifold.</head><p>A manifold M of dimension n is a set that can be locally approximated by the Euclidean space R n . For instance, the sphere S 2 and the torus T 2 embedded in R 3 are 2-dimensional manifolds, also called surfaces, as they can locally be approximated by R 2 . The notion of manifold is a generalization of the notion of surface. </p><formula xml:id="formula_0">(?) = 1 0 g ?(t) (? (t), ? (t))dt.<label>(1)</label></formula><p>Riemannian manifold. A smooth manifold equipped with a Riemannian metric is called a Riemannian manifold. Subsequently, due to their metric properties, we will only consider such manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geodesics.</head><p>A geodesic (straight line) between two points x, y ? M is a smooth curve of minimal length joining x to y in M. Geodesics define shortest paths on the manifold. They are a generalization of lines in the Euclidean space.</p><p>Exponential map. The exponential map exp x : T x M ? M around x, when well-defined, maps a small perturbation of x by a vector v ? T x M to a point exp</p><formula xml:id="formula_1">x (v) ? M, such that t ? [0, 1] ? exp x (tv) is a geodesic join- ing x to exp x (v). In Euclidean space, we simply have exp x (v) = x + v.</formula><p>The exponential map is important, for instance, when performing gradient-descent over parameters lying in a manifold <ref type="bibr" target="#b2">(Bonnabel, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conformality.</head><p>A metricg on M is said to be conformal to g if it defines the same angles, i.e. for all x ? M and</p><formula xml:id="formula_2">u, v ? T x M \ {0}, g x (u, v) g x (u, u) g x (v, v) = g x (u, v) g x (u, u) g x (v, v) .<label>(2)</label></formula><p>This is equivalent to the existence of a smooth function ? : M ? (0, ?) such thatg x = ? 2 x g x , which is called the conformal factor ofg (w.r.t. g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hyperbolic geometry</head><p>The hyperbolic space of dimension n ? 2 is a fundamental object in Riemannian geometry. It is (up to isometry) uniquely characterized as a complete, simply connected Riemannian manifold with constant negative curvature <ref type="bibr" target="#b6">(Cannon et al., 1997)</ref>. The other two model spaces of constant sectional curvature are the flat Euclidean space R n (zero curvature) and the hyper-sphere S n (positive curvature).</p><p>The hyperbolic space has five models which are often insightful to work in. They are isometric to each other and conformal to the Euclidean space <ref type="bibr" target="#b6">(Cannon et al., 1997;</ref><ref type="bibr">Parkkonen, 2013) 4</ref> . We prefer to work in the Poincar? ball model D n for the same reasons as <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> and, additionally, because we can derive a closed form expression of geodesics and exponential map.</p><p>Poincar? metric tensor. The Poincar? ball model (D n , g D ) is defined by the manifold D n = {x ? R n : x &lt; 1} equipped with the following Riemannian metric</p><formula xml:id="formula_3">g D x = ? 2 x g E , where ? x := 2 1 ? x 2 ,<label>(3)</label></formula><p>and g E is the Euclidean metric tensor with components I n of the standard space R n with the usual Cartesian coordinates.</p><p>As the above model is a Riemannian manifold, its metric tensor is fundamental in order to uniquely define most of its geometric properties like distances, inner products (in 4 https://en.wikipedia.org/wiki/ Hyperbolic_space tangent spaces), straight lines (geodesics), curve lengths or volume elements. In the Poincar? ball model, the Euclidean metric is changed by a simple scalar field, hence the model is conformal (i.e. angle preserving), yet distorts distances.</p><p>Induced distance and norm. It is known <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> that the induced distance between 2 points x, y ? D n is given by</p><formula xml:id="formula_4">d D (x, y) = cosh ?1 1 + 2 x ? y 2 (1 ? x 2 ) ? (1 ? y 2 ) .<label>(4)</label></formula><p>The Poincare norm is then defined as:</p><formula xml:id="formula_5">x D := d D (0, x) = 2 tanh ?1 ( x )<label>(5)</label></formula><p>Geodesics and exponential map. We derive parametric expressions of unit-speed geodesics and exponential map in the Poincar? ball. Geodesics in D n are all intersections of the Euclidean unit ball D n with (degenerated) Euclidean circles orthogonal to the unit sphere ?D n (equations are derived below). We know from the Hopf-Rinow theorem that the hyperbolic space is complete as a metric space. This guarantees that D n is geodesically complete. Thus, the exponential map is defined for each point x ? D n and any v ? R n (= T x D n ). To derive its closed form expression, we first prove the following.</p><formula xml:id="formula_6">Theorem 1. (Unit-speed geodesics) Let x ? D n and v ? T x D n (= R n ) such that g D x (v, v) = 1. The unit- speed geodesic ? x,v : R + ? D n with ? x,v (0) = x an? ? x,v (0) = v is given by ? x,v (t) = ? x cosh(t) + ? 2 x x, v sinh(t) x + ? x sinh(t)v 1 + (? x ? 1) cosh(t) + ? 2 x x, v sinh(t)<label>(6)</label></formula><p>Proof. See appendix B.</p><formula xml:id="formula_7">Corollary 1.1. (Exponential map) The exponential map at a point x ? D n , namely exp x : T x D n ? D n , is given by exp x (v) = ? x cosh(? x v ) + x, v v sinh(? x v ) 1 + (? x ? 1) cosh(? x v ) + ? x x, v v sinh(? x v ) x+ 1 v sinh(? x v ) 1 + (? x ? 1) cosh(? x v ) + ? x x, v v sinh(? x v ) v<label>(7)</label></formula><p>Proof. See appendix C.</p><p>We also derive the following fact (useful for future proofs).</p><p>Corollary 1.2. Given any arbitrary geodesic in D n , all its points are coplanar with the origin O.</p><p>Proof. See appendix D.</p><p>Angles in hyperbolic space. It is natural to extend the Euclidean notion of an angle to any geodesically complete Riemannian manifold. For any points A, B, C on such a manifold, the angle ?ABC is the angle between the initial tangent vectors of the geodesics connecting B with A, and B with C, respectively. In the Poincar? ball, the angle between two tangent vectors u, v ? T x D n is given by</p><formula xml:id="formula_8">cos(?(u, v)) = g D x (u, v) g D x (u, u) g D x (v, v) = u, v u v<label>(8)</label></formula><p>The second equality happens since g D is conformal to g E .</p><p>Hyperbolic trigonometry. The notion of angles and geodesics allow definition of the notion of a triangle in the Poincar? ball. Then, the classic theorems in Euclidean geometry have hyperbolic formulations <ref type="bibr" target="#b25">(Parkkonen, 2013)</ref>.</p><p>In the next section, we will use the following theorems.</p><p>Let A, B, C ? D n . Denote by ?B := ?ABC and by c = d D (B, A) the length of the hyperbolic segment BA (and others). Then, the hyperbolic laws of cosines and sines hold respectively</p><formula xml:id="formula_9">cos(?B) = cosh(a) cosh(c) ? cosh(b) sinh(a) sinh(c) (9) sin(?A) sinh(a) = sin(?B) sinh(b) = sin(?C) sinh(c)<label>(10)</label></formula><p>Embedding trees in hyperbolic vs Euclidean space. Finally, we briefly explain why hyperbolic spaces are better suited than Euclidean spaces for embedding trees. However, note that our method is applicable to any DAG. <ref type="bibr" target="#b12">(Gromov, 1987)</ref> introduces a notion of ?-hyperbolicity in order to characterize how 'hyperbolic' a metric space is. For instance, the Euclidean space R n for n ? 2 is not ?hyperbolic for any ? ? 0, while the Poincar? ball D n is log(1 + ? 2)-hyperbolic. This is formalized in the following theorem 5 (section 6.2 of <ref type="bibr" target="#b12">(Gromov, 1987)</ref>, proposition 6.7 of <ref type="bibr" target="#b4">(Bowditch, 2006)</ref>):</p><p>Theorem: For any ? &gt; 0, any ?-hyperbolic metric space (X, d X ) and any set of points x 1 , ..., x n ? X, there exists a finite weighted tree (T, d T ) and an embedding f : T ? X such that for all i, j,</p><formula xml:id="formula_10">|d T (f ?1 (x i ), f ?1 (x j )) ? d X (x i , x j )| = O(? log(n)).<label>(11)</label></formula><p>5 https://en.wikipedia.org/wiki/ Hyperbolic_metric_space Conversely, any tree can be embedded with arbitrary low distortion into the Poincar? disk (with only 2 dimensions), whereas this is not true for Euclidean spaces even when an unbounded number of dimensions is allowed <ref type="bibr" target="#b29">(Sarkar, 2011;</ref><ref type="bibr" target="#b8">De Sa et al., 2018)</ref>.</p><p>The difficulty in embedding trees having a branching factor at least 2 in a quasi-isometric manner comes from the fact that they have an exponentially increasing number of nodes with depth. The exponential volume growth of hyperbolic metric spaces confers them enough capacity to embed trees quasi-isometrically, unlike the Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Entailment Cones in the Poincar? Ball</head><p>In this section, we define "entailment" cones that will be used to embed hierarchical structures in the Poincar? ball. They generalize and improve over the idea of order embeddings <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref>. Convex cones in a complete Riemannian manifold. We are interested in generalizing the notion of a convex cone to any geodesically complete Riemannian manifold M (such as hyperbolic models). In a vector space, a convex cone S (at the origin) is a set that is closed under non-negative linear combinations</p><formula xml:id="formula_11">v 1 , v 2 ? S =? ?v 1 + ?v 2 ? S (??, ? ? 0) . (12)</formula><p>The key idea for generalizing this concept is to make use of the exponential map at a point x ? M.</p><formula xml:id="formula_12">exp x : T x M ? M, T x M = tangent space at x (13)</formula><p>We can now take any cone in the tangent space S ? T x M at a fixed point x and map it into a set S x ? M, which we call the S-cone at x, via</p><formula xml:id="formula_13">S x := exp x (S) , S ? T x M .<label>(14)</label></formula><p>Note that, in the above definition, we desire that the exponential map be injective. We already know that it is a local diffeomorphism. Thus, we restrict the tangent space in Eq. 14 to the ball B n (O, r), where r is the injectivity radius of M at x. Note that for hyperbolic space models the injectivity radius of the tangent space at any point is infinite, thus no restriction is needed.</p><p>Angular cones in the Poincar? ball. We are interested in special types of cones in D n that can extend in all space directions. We want to avoid heavy cone intersections and to have capacity that scales exponentially with the space dimension. To achieve this, we want the definition of cones to exhibit the following four intuitive properties detailed below. Subsequently, solely based on these necessary conditions, we formally prove that the optimal cones in the Poincar? ball have a closed form expression.</p><p>1) Axial symmetry. For any x ? D n \ {0}, we require circular symmetry with respect to a central axis of the cone S x . We define this axis to be the spoke through x from x:</p><formula xml:id="formula_14">A x := {x ? D n : x = ?x, 1 x &gt; ? ? 1}<label>(15)</label></formula><p>Then, we fix any tangent vector with the same direction as x, e.g.x = exp ?1</p><p>x 1+</p><p>x 2 x x ? T x D n . One can verify using Corollary 1.1 thatx generates the axis-oriented geodesic as:</p><formula xml:id="formula_15">A x = exp x ({y ? R n : y = ?x, ? &gt; 0}) .<label>(16)</label></formula><p>We next define the angle ?(v,x) for any tangent vector v ? T x D n as in Eq. 8. Then, the axial symmetry property is satisfied if we define the angular cone at x to have a non-negative aperture 2?(x) ? 0 as follows:</p><formula xml:id="formula_16">S ?(x) x := {v ? T x D n : ?(v,x) ? ?(x)} (17) S ?(x) x := exp x (S ?(x) x )</formula><p>.</p><p>We further define the conic border (face):</p><formula xml:id="formula_17">?S ? := {v : ?(v,x) = ?(x)}, ?S ? x := exp x (?S ? x ).<label>(18)</label></formula><p>2) Rotation invariance. We want the definition of cones S ?(x) x to be independent of the angular coordinate of the apex x, i.e. to only depend on the (Euclidean) norm of x:</p><formula xml:id="formula_18">?(x) = ?(x ) (?x, x ? D n \ {0}, s.t. x = x ).<label>(19)</label></formula><p>This implies that there exists? : (0, 1)</p><formula xml:id="formula_19">? [0, ?) s. t. for all x ? D n \ {0} we have ?(x) =?( x ).</formula><p>3) Continuous cone aperture functions. We require the aperture ? of our cones to be a continuous function. Using Eq. 19, it is equivalent to the continuity of?. This requirement seems reasonable and will be helpful in order to prove uniqueness of the optimal entailment cones. When optimization-based training is employed, it is also necessary that this function be differentiable. Surprisingly, we will show below that the optimal functions? are actually smooth, even when only requiring continuity.</p><p>4) Transitivity of nested angular cones. We want cones to determine a partial order in the embedding space. The difficult property is transitivity. We are interested in defining a cone width function ?(x) such that the resulting angular cones satisfy the transitivity property of partial order relations, i.e. they form a nested structure as follows</p><formula xml:id="formula_20">?x, x ? D n \ {0} : x ? S ?(x) x =? S ?(x ) x ? S ?(x) x .<label>(20)</label></formula><p>Closed form expression of the optimal ?. We now analyze the implications of the above necessary properties. Surprisingly, the optimal form of the function ? admits an interesting closed-form expression. We will see below that mathematically ? cannot be defined on the entire open ball D n . Towards these goals, we first prove the following. Lemma 2. If transitivity holds, then</p><formula xml:id="formula_21">?x ? Dom(?) : ?(x) ? ? 2 .<label>(21)</label></formula><p>Proof. See appendix E.</p><p>Note that so far we removed the origin 0 of D n from our definitions. However, the above surprising lemma implies that we cannot define a useful cone at the origin. To see this, we first note that the origin should "entail" the entire space D n , i.e. S 0 = D n . Second, similar with property 3, we desire the cone at 0 be a continuous deformation of the cones of any sequence of points (x n ) n?0 in D n \ {0} that converges to 0. Formally, lim n?? S xn = S 0 when lim n?? x n = 0. However, this is impossible because Lemma 2 implies that the cone at each point x n can only cover at most half of D n . We further prove the following: Theorem 3. If transitivity holds, then the function</p><formula xml:id="formula_22">h : (0, 1) ? Dom(?) ? R + , h(r) := r 1 ? r 2 sin(?(r)),<label>(22)</label></formula><p>is non-increasing.</p><p>Proof. See appendix F.</p><p>The above theorem implies that a non-zero? cannot be defined on the entire (0, 1) because lim r?0 h(r) = 0, for </p><formula xml:id="formula_23">?r ? [ , 1) : sin(?(r)) r 1 ? r 2 ? sin(?( )) 1 ? 2 .<label>(23)</label></formula><p>Since we are interested in cones with an aperture as large as possible (to maximize model capacity), it is natural to set all terms h(r) equal to K := h( ), i.e. to make h constant:</p><formula xml:id="formula_24">?r ? [ , 1) : sin(?(r)) r 1 ? r 2 = K,<label>(24)</label></formula><p>which gives both a restriction on (in terms of K):</p><formula xml:id="formula_25">K ? 1 ? 2 ?? ? 2K 1 + ? 1 + 4K 2 , 1 ,<label>(25)</label></formula><p>as well as a closed form expression for ?</p><formula xml:id="formula_26">? : D n \ B n (O, ) ? (0, ?/2) x ? arcsin(K(1 ? x 2 )/ x ),<label>(26)</label></formula><p>which is also a sufficient condition for transitivity to hold:</p><p>Theorem 4. If ? is defined as in Eqs.25-26, then transitivity holds.</p><p>The above theorem has a proof similar to that of Thm. 3.</p><p>So far, we have obtained a closed form expression for hyperbolic entailment cones. However, we still need to understand how they can be used during embedding learning. For this goal, we derive an equivalent (and more practical) definition of the cone S ?(x) x :</p><p>Theorem 5. For any x, y ? D n \ B n (O, ), we denote the angle between the half-lines (xy and (0x as</p><formula xml:id="formula_27">?(x, y) := ? ? ?Oxy,<label>(27)</label></formula><p>Then, this angle equals arccos x, y (1 + x 2 ) ? x 2 (1 + y 2 )</p><p>x ? x ? y 1 + x 2 y 2 ? 2 x, y ,</p><p>Moreover, we have the following equivalent expression of the Poincar? entailment cones satisfying Eq. 26:</p><formula xml:id="formula_29">S ?(x) x = y ? D n ?(x, y) ? arcsin K 1 ? x 2 x .<label>(29)</label></formula><p>Proof. See appendix G.</p><p>Examples of 2-dimensional Poincar? cones corresponding to apex points located at different radii from the origin are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. This figure also shows that transitivity is satisfied for some points on the border of the hypercones.</p><p>Euclidean entailment cones. One can easily adapt the above proofs to derive entailment cones in the Euclidean space (R n , g E ). The only adaptations are: i) replace the hyperbolic cosine law by usual Euclidean cosine law, ii) geodesics are straight lines, and iii) the exponential map is given by exp x (v) = x + v. Thus, one similarly obtains that h(r) = r sin(?(r)) is non-decreasing, the optimal values of ? are obtained for constant h being equal to K ? ? and</p><formula xml:id="formula_30">S ?(x) x = {y ? R n | ?(x, y) ? ?(x)},<label>(30)</label></formula><p>where ?(x, y) now becomes</p><formula xml:id="formula_31">?(x, y) = arccos y 2 ? x 2 ? x ? y 2 2 x ? x ? y ,<label>(31)</label></formula><p>for all x, y ? R n \ B(O, ?). From a learning perspective, there is no need to be concerned about the Riemannian optimization described in Section 4.2, as the usual Euclidean gradient-step is used in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning with entailment cones</head><p>We now describe how embedding learning is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Max-margin training on angles</head><p>We learn hierarchical word embeddings from a dataset X of entailment relations (u, v) ? X , also called hypernym links, defining that u entails v, or, equivalently, that v is a subconcept of u 6 .</p><p>We choose to model the embedding entailment relation <ref type="bibr">(u, v)</ref> as v belonging to the entailment cone S ?(u) u . <ref type="bibr">6</ref> We prefer this notation over the one in <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> Figure 3. Two dimensional embeddings of two datasets: a toy uniform tree of depth 7 and branching factor 3, with root removed (left); the mammal subtree of WordNet with 4230 relations, 1165 nodes and top 2 nodes removed (right). <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017</ref>) (each left side) has most of the nodes and edges collapsed on the space border, while our hyperbolic cones (each right side) nicely reveal the data structure.</p><p>Our model is trained with a max-margin loss function similar to the one in <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref>: </p><formula xml:id="formula_32">L = (u,v)?P E(u, v) + (u ,v )?N max(0, ? ? E(u , v )),<label>(32)</label></formula><p>where ?(u, v) is defined in Eqs. 28 and 31. Note that <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref> use max(0, v ? u) 2 . This loss function encourages positive samples to satisfy E(u, v) = 0 and negative ones to satisfy E(u, v) ? ?. The same loss is used both in the hyperbolic and Euclidean cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Full Riemannian optimization</head><p>As the parameters of the model live in the hyperbolic space, the back-propagated gradient is a Riemannian gradient. Indeed, if u is in the Poincar? ball, and if we compute the usual (Euclidean) gradient ? u L of our loss, then</p><formula xml:id="formula_34">u ? u ? ?? u L<label>(34)</label></formula><p>makes no sense as an operation in the Poincar? ball, since the substraction operation is not defined in this manifold. Instead, one should compute the Riemannian gradient ? R u L indicating a direction in the tangent space T u D n , and should move u along the corresponding geodesic in D n <ref type="bibr" target="#b2">(Bonnabel, 2013)</ref>:</p><formula xml:id="formula_35">u ? exp u (??? R u L),<label>(35)</label></formula><p>where the Riemannian gradient is obtained by rescaling the Euclidean gradient by the inverse of the metric tensor. As our metric is conformal, i.e. g D = ? 2 g E where g E = I is the Euclidean metric (see Eq 3), this leads to a simple formulation</p><formula xml:id="formula_36">? R u L = (1/? u ) 2 ? u L.<label>(36)</label></formula><p>Previous work <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> optimizing word embeddings in the Poincar? ball used the retraction map R x (v) := x + v as a first order approximation of exp x (v).</p><p>Note that since we derived a closed-form expression of the exponential map in the Poincar? ball (Corollary 1.1), we are able to perform full Riemannian optimization in this model of the hyperbolic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the representational and generalization power of hyperbolic entailment cones and of other baselines using data that exhibits a latent hierarchical structure. We follow previous work <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017;</ref><ref type="bibr" target="#b34">Vendrov et al., 2015)</ref> and use the full transitive closure of the WordNet noun hierarchy <ref type="bibr" target="#b22">(Miller et al., 1990</ref>). Our binary classification task is link prediction for unseen edges in this directed acyclic graph.</p><p>Dataset splitting. Train and evaluation settings. We remove the tree root since it carries little information and only has trivial edges to predict. Note that this implies that we co-embed the resulting subgraphs together to prevent overlapping embeddings (see smaller examples in <ref type="figure">Figure 3</ref>). The remaining WordNet dataset contains 82,114 nodes and 661,127 edges in the full transitive closure. We split it into train -validation -test sets as follows. We first compute the transitive reduction 7 of this directed acyclic graph, i.e. "basic" edges that form the minimal edge set for which the original transitive closure can be fully recovered. These edges are hard to predict, so we will always include them in the training set. The remaining "non-basic" edges (578,477)  <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>, Order Emb is proposed by <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref>.</p><p>are split into validation (5%), test (5%) and train (fraction of the rest).</p><p>We augment both the validation and the test parts with sets of negative pairs as follows: for each true (positive) edge (u, v), we randomly sample five (u , v) and five (u, v ) negative corrupted pairs that are not edges in the full transitive closure. These are then added to the respective negative set. Thus, ten times as many negative pairs as positive pairs are used. They are used to compute standard classification metrics associated with these datasets: precision, recall, F1. For the training set, negative pairs are dynamically generated as explained below.</p><p>We make the task harder in order to understand the generalization ability of various models when differing amounts of transitive closure edges are available during training. We generate four training sets that include 0%, 10%, 25%, or 50% of the non-basic edges, selected randomly. We then train separate models using each of these four sets after being augmented with the basic edges.</p><p>Baselines. We compare against the strong hierarchical embedding methods of Order embeddings <ref type="bibr" target="#b34">(Vendrov et al., 2015)</ref> and Poincar? embeddings <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>. Additionally, we also use Simple Euclidean embeddings, i.e. the Euclidean version of the method presented in <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017</ref>) (one of their baselines). We note that Poincar? and Simple Euclidean embeddings were trained using a symmetric distance function, and thus cannot be directly used to evaluate asymmetric entailment relations. Thus, for these baselines we use the heuristic scoring function proposed in <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>:</p><formula xml:id="formula_37">score(u, v) = (1 + ?( u ? v ))d(u, v)<label>(37)</label></formula><p>and tune the parameter ? on the validation set. For all the other methods (our proposed cones and order embeddings), we use the energy penalty E(u, v), e.g. Eq. 33 for hyperbolic cones. This scoring function is then used at test time for binary classification as follows: if it is lower than a threshold, we predict an edge; otherwise, we predict a nonedge. The optimal threshold is chosen to achieve maximum F1 on the validation set by passing over the sorted array of scores of positive and negative validation pairs.</p><p>Training details. For all methods except Order embeddings, we observe that initialization is very important. Being able to properly disentangle embeddings from different subparts of the graph in the initial learning stage is essential in order to train qualitative models. We conjecture that initialization is hard because these models are trained to minimize highly non-convex loss functions. In practice, we obtain our best results when initializing the embeddings corresponding to the hyperbolic cones using the Poincar? embeddings pretrained for 100 epochs. The embeddings for the Euclidean cones are initialized using Simple Euclidean embeddings pre-trained also for 100 epochs. For the Simple Euclidean embeddings and Poincar? embeddings, we find the burn-in strategy of <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref> to be essential for a good initial disentanglement. We also observe that the Poincar? embeddings are heavily collapsed to the unit ball border (as also pictured in <ref type="figure">Fig. 3</ref>) and so we rescale them by a factor of 0.7 before starting the training of the hyperbolic cones.</p><p>Each model is trained for 200 epochs after the initialization stage, except for order embeddings which were trained for 500 epochs. During training, 10 negative edges are generated per positive edge by randomly corrupting one of its end points. We use batch size of 10 for all models. For both cone models we use a margin of ? = 0.01.</p><p>All Euclidean models and baselines are trained using stochastic gradient descent. For the hyperbolic models, we do not find significant empirical improvements when using full Riemannian optimization instead of approximating it with a retraction map as done in <ref type="bibr" target="#b23">(Nickel &amp; Kiela, 2017)</ref>. We thus use the retraction approximation since it is faster. For the cone models, we always project outside of the ball centered on the origin during learning as constrained by Eq. 26 and its Euclidean version. For both we use = 0.1. A learning rate of 1e-4 is used for both Euclidean and hyperbolic cone models.</p><p>Results and discussion. <ref type="table" target="#tab_0">Table 1</ref> shows the obtained results. For a fair comparison, we use models with the same number of dimensions. We focus on the low dimensional setting (5 and 10 dimensions) which is more informative.</p><p>It can be seen that our hyperbolic cones are better than all the baselines in all settings, except in the 0% setting for which order embeddings are better. However, once a small percentage of the transitive closure edges becomes available during training, we observe significant improvements of our method, sometimes by more than 8% F1 score. Moreover, hyperbolic cones have the largest growth when transitive closure edges are added at train time. We further note that, while mathematically not justified 8 , if embeddings of our proposed Euclidean cones model are initialized with the Poincar? embeddings instead of the Simple Euclidean ones, then they perform on par with the hyperbolic cones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Learning meaningful graph embeddings is relevant for many important applications. Hyperbolic geometry has proven to be powerful for embedding hierarchical structures. We here take one step forward and propose a novel model based on geodesically convex entailment cones and show its theoretical and practical benefits. We empirically discover that strong embedding methods can vary a lot with the percentage of the taxonomy observable during training and demonstrate that our proposed method benefits the most from increasing size of the training data. As future work, it would be interesting to understand if the proposed entailment cones can be used to embed more complex data such as sentences or images.</p><p>Our code is publicly available 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Geodesics in the Hyperboloid Model</head><p>The hyperboloid model is (H n , ?, ? 1 ), where H n := {x ? R n,1 : x, x 1 = ?1, x 0 &gt; 0}. The hyperboloid model can be viewed from the extrinsically as embedded in the pseudo-Riemannian manifold Minkowski space (R n,1 , ?, ? 1 ) and inducing its metric. The Minkowski metric tensor g R n,1 of signature (n, 1) has the components</p><formula xml:id="formula_38">g R n,1 = ? ? ? ? ?1 0 . . . 0 0 1 . . . 0 0 0 . . . 0 0 0 . . . 1 ? ? ? ?</formula><p>The associated inner-product is x, y 1 := ?x 0 y 0 + n i=1 x i y i . Note that the hyperboloid model is a Riemannian manifold because the quadratic form associated with g H is positive definite.</p><p>In the extrinsic view, the tangent space at H n can be described as T x H n = {v ? R n,1 : v, x 1 = 0}. See <ref type="bibr" target="#b27">Robbin &amp; Salamon (2011);</ref><ref type="bibr" target="#b25">Parkkonen (2013)</ref>.</p><p>Geodesics of H n are given by the following theorem (Eq (6.4.10) in <ref type="bibr" target="#b27">Robbin &amp; Salamon (2011)</ref>):</p><formula xml:id="formula_39">Theorem 6. Let x ? H n and v ? T x H n such that v, v = 1. The unique unit-speed geodesic ? x,v : [0, 1] ? H n with ? x,v (0) = x and? x,v (0) = v is ? x,v (t) = x cosh(t) + v sinh(t).<label>(38)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 1</head><p>Proof. From theorem 6, appendix A, we know the expression of the unit-speed geodesics of the hyperboloid model H n . We can use the Egregium theorem to project the geodesics of H n to the geodesics of D n . We can do that because we know an isometry ? : D n ? H n between the two spaces:</p><formula xml:id="formula_40">?(x) := (? x ? 1, ? x x), ? ?1 (x 0 , x ) = x 1 + x 0 (39) Formally, let x ? D n , v ? T x D n with g D (v, v) = 1.</formula><p>Also, let ? : [0, 1] ? D n be the unique unit-speed geodesic in D n with ?(0) = x and?(0) = v. Then, by Egregium theorem, ? := ? ? ? is also a unit-speed geodesic in H n . From theorem 6, we have that ?(t) = x cosh(t) + v sinh(t), for some x ? H n , v ? T x H n . One derives their expression:</p><formula xml:id="formula_41">x = ? ? ?(0) = (? x ? 1, ? x x)<label>(40)</label></formula><p>v =?(0) = ??(y 0 , y) ?y</p><formula xml:id="formula_42">?(0)? (0) = ? 2 x x, v ? 2 x x, v x + ? x v</formula><p>Inverting once again, ?(t) = ? ?1 ??(t), one gets the closedform expression for ? stated in the theorem.</p><p>One can sanity check that indeed the formula from theorem 1 satisfies the conditions:</p><formula xml:id="formula_43">? d D (?(0), ?(t)) = t, ?t ? [0, 1] ? ?(0) = x ??(0) = v ? lim t?? ?(t) := ?(?) ? ?D n C. Proof of Corollary 1.1 Proof. Denote u = 1 ? g D x (v,v) v.</formula><p>Using the notations from</p><formula xml:id="formula_44">Thm. 1, one has exp x (v) = ? x,u ( g D x (v, v)</formula><p>). Using Eq. 3 and 6, one derives the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Corollary 1.2</head><p>Proof. For any geodesic ? x,v (t), consider the plane spanned by the vectors x and v. Then, from Thm. 1, this plane contains all the points of ? x,v (t), i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Lemma 2</head><p>Proof. Assume the contrary and let x ? D n \ {0} s.t. ?( x ) &gt; ? 2 . We will show that transitivity implies that</p><formula xml:id="formula_45">?x ? ?S ?(x) x : ?( x ) ? ? 2<label>(42)</label></formula><p>If the above is true, by moving x on any arbitrary (continuous) curve on the cone border ?S ?(x) x that ends in x, one will get a contradiction due to the continuity of ?( ? ).</p><p>We now prove the remaining fact, namely Eq. 42. Let any arbitrary x ? ?S ?(x) x . Also, let y ? ?S ?(x) x be any arbitrary point on the geodesic half-line connecting x with x starting from x (i.e. excluding the segment from x to x ). Moreover, let z be any arbitrary point on the spoke through x radiating from x , namely z ? A x (notation from Eq. 15). Then, based on the properties of hyperbolic angles discussed before (based on Eq. 8), the angles ?yx z and ?zx x are well-defined. From Cor. 1.2 we know that the points O, x, x , y, z are coplanar. We denote this plane by P. Furthermore, the metric of the Poincar? ball is conformal with the Euclidean metric. Given these two facts, we derive that ?yx z + ?zx x = ?(yx x) = ? (43) thus min(?yx z, ?zx x) ? ? 2 (44)</p><p>It only remains to prove that</p><formula xml:id="formula_46">?yx z ? ?(x ) &amp; ?zx x ? ?(x )<label>(45)</label></formula><p>Indeed, assume w.l.o.g. that ?yx z &lt; ?(x ). Since ?yx z &lt; ?(x ), there exists a point t in the plane P such that ?Oxt &lt; ?Oxy &amp; ?(x ) ? ?tx z &gt; ?yx z <ref type="formula" target="#formula_4">(46)</ref> Then, clearly, t ? S ?(x ) x , and also t / ? S ?(x) x , which contradicts the transitivity property (Eq. 20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Proof of Theorem 3</head><p>Proof. We first need to prove the following fact:</p><p>Lemma 7. Transitivity implies that for all x ? D n \ {0}, ?x ? ?S ?(x) x : sin(?( x )) sinh( x D ) ? sin(?( x )) sinh( x D ).</p><p>(47)</p><p>Proof. We will use the exact same figure and notations of points y, z as in the proof of lemma 2. In addition, we assume w.l.o.g that</p><formula xml:id="formula_47">?yx z ? ? 2<label>(48)</label></formula><p>Further, let b ? ?D n be the intersection of the spoke through x with the border of D n . Following the same argument as in the proof of lemma 2, one proves Eq. 45 which gives:</p><formula xml:id="formula_48">?yx z ? ?(x )<label>(49)</label></formula><p>In addition, the angle at x between the geodesics xy and Oz can be written in two ways:</p><formula xml:id="formula_49">?Ox x = ?yx z (50) Since x ? ?S ?(x) x , one proves ?Oxx = ? ? ?x xb = ? ? ?(x)<label>(51)</label></formula><p>We apply hyperbolic law of sines (Eq. 10) in the hyperbolic triangle Oxx :</p><formula xml:id="formula_50">sin(?Oxx ) sinh(d D (O, x )) = sin(?Ox x) sinh(d D (O, x))<label>(52)</label></formula><p>Putting together Eqs. 48,49,50,51,52, and using the fact that sin(?) is an increasing function on [0, ? 2 ], we derive the conclusion of this helper lemma.</p><p>We now return to the proof of our theorem. Consider any arbitrary r, r ? (0, 1) ? Dom(?) with r &lt; r . Then, we claim that is enough to prove that ?x ? D n , x ? ?S ?(x) x s.t. x = r, x = r (53) Indeed, if the above is true, then one can use the fact 5, i.e. sinh( x D ) = sinh ln 1 + r 1 ? r = 2r 1 ? r 2 (54) and apply lemma 7 to derive h(r ) ? h(r)</p><p>which is enough for proving the non-increasing property of function h.</p><p>We are only left to prove the fact 53. Let any arbitrary x ? D n s.t. x = r. Also, consider any arbitrary geodesic ? x,v :</p><formula xml:id="formula_52">R + ? ?S ?(x) x</formula><p>that takes values on the cone border, i.e. ?(v, x) = ?(x). We know that ? x,v (0) = x = r (56) and that this geodesic "ends" on the ball's border ?D n , i.e. lim t?? ? x,v (t) = 1</p><p>Thus, because the function ? x,v (?) is continuous, we obtain that for any r ? (r, 1) there exists an t ? R + s.t. ? x,v (t ) = r . By setting x := ? x,v (t ) ? ?S ?(x) x we obtain the desired result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Tangent space. For x ? M, the tangent space T x M of M at x is defined as the n-dimensional vector-space approximating M around x at a first order. It can be defined as the set of vectors v that can be obtained asv := c (0), where c : (??, ?) ? M is a smooth path in M such that c(0) = x. Riemannian metric. A Riemannian metric g on M is a collection (g x ) x of inner-products g x : T x M ? T x M ? R on each tangent space T x M,depending smoothly on x. Although it defines the geometry of M locally, it induces a global distance function d : M ? M ? R + by setting d(x, y) to be the infimum of all lengths of smooth curves joining x to y in M, where the length of a curve ? : [0, 1] ? M is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Convex cones in a complete Riemannian manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Poincar? angular cones satisfying Eq. 26 for K = 0.1. Left: examples of cones for points with Euclidean norm varying from 0.1 to 0.9. Right: transitivity for various points on the border of their parent cones. any function?. As a consequence, we are forced to restrict Dom(?) to some [ , 1), i.e. to leave the open ball B n (O, ) outside of the domain of ?. Then, theorem 3 implies that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for some margin ? &gt; 0, where P and N define samples of positive and negative edges respectively. The energy E(u, v) measures the penalty of a wrongly classified pair (u, v), which in our case measures how far is point v from belonging to S ?(u) u expressed as the smallest angle of a rotation of center u bringing v into S ?(u) u : E(u, v) := max(0, ?(u, v) ? ?(u)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>{? x,v (t) : t ? R} ? {ax + bv : a, b ? R}(41)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>8% 71.3% 73.8% 72.8% 29.4% 75.4% 78.4% 78.1% POINCAR? EMB 29.4% 70.2% 78.2% 83.6% 28.9% 71.4% 82.0% 85.3% ORDER EMB 34.4% 70.2% 75.9% 81.7% 43.0% 69.7% 79.4% 84.1% Test F1 results for various models. Simple Euclidean Emb and Poincar? Emb are the Euclidean and hyperbolic methods proposed by</figDesc><table><row><cell></cell><cell cols="4">EMBEDDING DIMENSION = 5</cell><cell cols="4">EMBEDDING DIMENSION = 10</cell></row><row><cell></cell><cell cols="8">PERCENTAGE OF TRANSITIVE CLOSURE (NON-BASIC) EDGES IN TRAINING</cell></row><row><cell></cell><cell>0%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>0%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell></row><row><cell cols="5">SIMPLE EUCLIDEAN EMB 26.OUR EUCLIDEAN CONES 28.5% 69.7% 75.0% 77.4%</cell><cell cols="4">31.3% 81.5% 84.5% 81.6%</cell></row><row><cell>OUR HYPERBOLIC CONES</cell><cell cols="4">29.2% 80.1% 86.0% 92.8%</cell><cell cols="4">32.2% 85.9% 91.0% 94.4%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://geometricdeeplearning.com/ 2 For example, in n dimensions, no n + 1 distinct regions Ox can simultaneously have unbounded disjoint sub-volumes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://en.wikipedia.org/wiki/ Transitive_reduction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Indeed, mathematically, hyperbolic embeddings cannot be considered as Euclidean points. 9 https://github.com/dalab/hyperbolic_ cones.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Maximilian Nickel, Colin Evans, Chris Waterson, Marius Pasca, Xiang Li and Vered Shwartz for helpful discussions about related work and evaluation settings. This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement number 167176. Gary B?cigneul is also funded by the Max Planck ETH Center for Learning Systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. For any y ? S ?(x) x , the axial symmetry property implies that ? ? ?Oxy ? ?(x). Applying the hyperbolic cosine law in the triangle Oxy and writing the above angle inequality in terms of the cosines of the two angles, one gets</p><p>Eq. 28 is then derived from the above by an algebraic reformulation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometry of the space of phylogenetic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Billera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vogtmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="733" to="767" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient embedding of scale-free graphs in the hyperbolic plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bl?sius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krohmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LIPIcs-Leibniz International Proceedings in Informatics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A course on geometric group theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Bowditch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Parry</surname></persName>
		</author>
		<title level="m">Hyperbolic geometry. Flavors of geometry</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic embedding and routing for dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cvetkovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crovella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM 2009</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03329</idno>
		<title level="m">Representation tradeoffs for hyperbolic embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04920</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02801</idno>
		<title level="m">Graph embedding techniques, applications, and performance: A survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the tree-likeness of hyperbolic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamann</surname></persName>
		</author>
		<idno>doi: 10.1017</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="page">117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ricci flow in riemannian geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Greedy forwarding in scale-free networks embedded in hyperbolic metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bogu??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="17" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bogun?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A focus+ context technique based on hyperbolic geometry for visualizing large hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lamping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to wordnet: An on-line lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Poincar? embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6341" to="6350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parkkonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introduction to differential geometry. ETH, Lecture Notes, preliminary version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Robbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Salamon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low distortion delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hyperbolic embedding of internet graph for distance estimation and overlay construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tankel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A comprehensive introduction to differential geometry. volume four</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361</idno>
		<title level="m">Orderembeddings of images and language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
