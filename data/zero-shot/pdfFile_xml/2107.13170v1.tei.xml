<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate Grid Keypoint Learning for Efficient Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
						</author>
						<title level="a" type="main">Accurate Grid Keypoint Learning for Efficient Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the longterm horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-ofthe-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/ xjgaocs/Grid-Keypoint-Learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Unsupervised video prediction aims to synthesize future frames based on observations in previous frames without requiring any annotation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Its look-ahead capability enables essential board applications in robotic navigation, video surveillance, and autonomous vehicles <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Through timely anticipation of the future, it aids intelligent decision making and also emergency-response system <ref type="bibr" target="#b4">[5]</ref>. Significantly, precisely predicting videos for a more extended period while upholding computing efficiency can further widen the applicability of deployments on mobile robots and domestic service robots. However, generating future frames with plausible motion dynamics is very challenging due to the difficulty of processing the high-dimensional video data <ref type="bibr" target="#b5">[6]</ref>. Thus, predictions by existing approaches tend to miss critical visual details and suffer from motion blurry and image distortion <ref type="bibr" target="#b6">[7]</ref>. These issues are even amplified with increases in prediction steps.  To capture various tendencies in the future, stochastic video prediction approaches were developed by defining a prior distribution over a set of latent variables, allowing different ways of sampling the distribution. Most of them focused on the direct pixel-wise synthesis of predicted future frames, known as the image-based prediction model. For this stream of image-based prediction models, recent studies pointed out that increasing the model scale could improve performances <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, a vast prediction model, on the other hand, would consume extremely large memory and energy, being impractical for real-world deployments.</p><p>One promising direction to enhance efficiency is by reducing the prediction space from dense pixel-wise images to some high-level representations, such as keypoint coordinates <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, where future frames are synthesized by analogy making with the predicted high-level representations and a reference frame. This representation creates an explicit high-level structure and simplifies the dynamics to be learned, thereby substantially decreasing the model complexity. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, keypoint-based methods (denoted in yellow) require much fewer parameters and computing resources than image-based approaches (denoted in blue). However, there is still a performance gap between the stateof-the-art keypoint-based model, i.e., Struct-VRNN <ref type="bibr" target="#b11">[12]</ref>, and image-based methods.</p><p>The inferior results of existing keypoint-based video prediction models are due to two significant problems. First, keypoints are detected and represented in a continuous coordinate space, where spatial relationships and constraints transferred from video frames could hardly be preserved without sophisticated regularization. Hence, the keypoints exhibit limited representative capacity, and artifacts are produced in synthesized frames when transforming information back to image space. Second, they propagate keypoints in temporal dimensions by regressing continuous coordinates, thus further destroy the keypoint structures due to inaccurate predictions. Notably, for long-term predictions, the adverse effect becomes more severe given that the compounding of errors accumulates over time.</p><p>To address these critical issues mentioned above, we propose a novel grid keypoint representation learning framework for long-term video prediction with various possibilities by enhancing the keypoint representation capacity and coordinate propagation reliability. Our main contributions are: 1). To regularize the detected keypoints, we develop a new gridding operation to compress the keypoint coordinates from infinite and continuous space to finite and discrete grid space, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. To our best knowledge, this is the first method that conducts grid keypoint learning for video prediction. 2). We propose a novel condensation loss to encourage the model to concentrate on the most informative region. Combining with the gridding operation, it vastly promotes the representative capability of keypoints, thus concentrated and meaningful keypoints are inferred.</p><p>3). To facilitate keypoint coordinate propagation, we devise a 2D binary map to represent the spatial relationships of keypoints and predict future keypoint by choosing its location in the finite grid space, transferring the prediction task from previous regression to classification. Thus, the compounding of coordinate errors are substantially reduced to enable future frame generation with high-fidelity. 4). Extensive results demonstrate that our method maintains keypoint structures in long-term horizons and achieves superior performances and efficiency over the state-of-the-art stochastic video prediction models. We also illustrate the great potential of our method on robotic-assisted surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Existing video prediction methods can be divided into two categories: deterministic and stochastic prediction. In this study, we focus on the latter one that could learn to represent a diverse future. Kalchbrenner et al. <ref type="bibr" target="#b12">[13]</ref> presented an autoregressive model that directly maximizes the log-likelihood of the data at a slow speed. Kumar et al. <ref type="bibr" target="#b1">[2]</ref> proposed a flowbased method to allow direct optimization of the data likelihood, which might fail to capture complex motion. GANbased models were also applied to model inexplicit data distribution. Tulyakov et al. <ref type="bibr" target="#b13">[14]</ref> used GANs for unconditional video generation, however, using adversarial losses generally encounters training difficulties such as mode collapse. Other vital foundations of probabilistic models are VAE and variational recurrent neural network (VRNN) <ref type="bibr" target="#b14">[15]</ref>. Babaeizadeh et al. <ref type="bibr" target="#b15">[16]</ref> applied VAE on video prediction by encoding the entire video sequence to estimate a posterior distribution. A stochastic video generation model using learned prior (SVG-LP) instead of the standard Gaussian prior was proposed in <ref type="bibr" target="#b16">[17]</ref>. Lee et al. <ref type="bibr" target="#b17">[18]</ref> combined GAN with VAE to produce sharp and realistic future frames. Remarkable performance boosts were achieved by increasing the expressive capacity of the latent variables <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, however, the resulted models were too big to be trained with general computers. The above methods generally rely on image-autoregressive processes for updating temporal recurrence and suffer from gradually noisy outputs as time step increases. Franceschi et al. <ref type="bibr" target="#b18">[19]</ref> proposed a computationally appealing method by separating the temporal dynamics from frame synthesis inexplicitly. Disentangling hidden dynamics and appearance representation explicitly, keypoint-based video prediction methods were suggested <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which first represent images with keypoints in an unsupervised manner and then synthesize future frames given predicted keypoints.</p><p>Unsupervised keypoint learning was first proposed in images <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, where a representational bottleneck forces a neural network to encode structural information into several keypoints with continuous coordinates. To predict the dynamics of detected keypoint sequences for generating future videos, coordinates are regressed using a basic VRNN architecture <ref type="bibr" target="#b11">[12]</ref> or a stochastic sequence-to-sequence model conditioning on class labels <ref type="bibr" target="#b19">[20]</ref>. Villegas et al. <ref type="bibr" target="#b5">[6]</ref> also predicted keypoint coordinates with a sequence-to-sequence model based on LSTM yet in a deterministic way, which gained good outcomes thanks to the manually annotated keypoints. These approaches employ recurrent architectures to regress the coordinates represented by 1D vectors, producing unsatisfying results due to inaccurate predictions of keypoint coordinates. Since the keypoints generated in an unsupervised manner could not maintain the point correspondence, such as confusion about left and right legs of humans, these keypoints are more inclined to suffer from propagation noise, thereby leading to weird results. How to more accurately predict future keypoints without human annotations is of great importance to produce more realistic videos.</p><p>III. METHOD <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates an overview of our proposed grid keypoint learning framework. Given observed video frames V 1:t , we first detect corresponding keypoints in the proposed grid space, followed by our grid keypoint prediction network for accurate coordinate propagation. By elegantly designing binary maps, our method substantially decreases the accumulated errors of keypoint coordinates, thus generates future framesV t+1:T with high-fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Keypoint Detection in Grid Space</head><p>Given a video frame V t ? R C?H?W , we aim to represent it with K keypoints without supervision, which try to restore original image details as many as possible helped by a decoder network. Previous keypoint detection methods employed a bottleneck to reconstruct the frame V t based a reference frame V 1 by analogy making using corresponding keypoints <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Instead of detecting keypoints with continuous coordinates, we propose to identify appropriate keypoints in a novel grid plane because image pixels are stored in standard grid forms, and there is no need to produce keypoints with higher resolution than images. Moreover, our grid constraint serves as regularization by sparing minimum distances among keypoints to prevent overfitting, which promotes the generalization ability of our keypoints to represent unseen pictures. However, searching grid keypoints discretely is intractable due to exponential complexity.</p><p>To meet this challenge, we devise a novel grid keypoint learning, which updates keypoint locations in a grid space, denoted as I HW with a resolution of H ? W . With V t as input, our keypoint detector D tries to output K grid keypoint coordinates ] by computing the spatial expectations of the heatmaps. The K keypoints with continuous coordinates are then pushed to their nearest grid points respectively to generate grid keypoints X 1:K 1 is also concatenated for inpainting the background regions. The final results are input to a decoder network G dec to reconstruct V t by generating V t to finish the forward pass. As for the backward pass to update network parameters, D and G : {G enc , G dec }, are jointly training to optimize an 2 reconstruction loss:</p><formula xml:id="formula_0">L rec = T t=1 V t ? V t 2 2 .</formula><p>(2)</p><p>Note that F t could also be used as a reference frame, and slightly better results could be obtained. To this end, the keypoint detector D and the image synthesizer G constitute an autoencoder architecture to encode frame V t into keypoint-based representations. The gradients from L rec encourage D to adjust its parameters to generate optimal keypoint patterns in the grid space. We demonstrate in Section IV-C.1 that our grid keypoints exhibit a more robust capability to restore original images than keypoints with continuous coordinates by helping preserve a lot more details of the original frame V t . Condensation Loss for Robust Grid Keypoint. To interpret a single keypoint, the interesting areas in each heatmap should be concentrated, and the activation values of the irrelevant regions are relatively low. As each heatmap H k t is activated by a sigmoid function, the optimal structure of each heatmap shall contain a single entry as value 1 and the rest as 0, showing the maximum contrast. By contrast, a heatmap with the same values generates the most ambiguous detection (the worst condition), where max(H k t ) is equal to mean(H k t ). To make the keypoints sparse and robust to noisy images, we introduce a new regularization term called condensation loss. It is devised by broadening the gap between max(H k t ) and mean(H k t ) for all produced heatmaps to enhance centralized distributions of heatmaps:</p><formula xml:id="formula_1">L con = ? t k (max(H k t ) ? mean(H k t )).<label>(3)</label></formula><p>In practice, we find that only optimizing the worst heatmap among the K channels for all time steps also creates an excellent performance. In Section IV-C.1, we show that L con contributes to a better reconstruction performance by facilitating concentrated keypoint configurations. Thus, our grid keypoint detection network is trained by jointly optimizing D and G using a combined loss:</p><formula xml:id="formula_2">L det = L rec + ?L con ,<label>(4)</label></formula><p>where ? is a constant to balance the two terms. Note that the well-trained G dec is directly reused in the generation of future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grid Keypoint Prediction via Binary Map</head><p>With our detected grid keypoints, we develop a keypoint prediction method to alleviate side effects from compounding of keypoint coordinate errors for realistic future frame generation. Previous methods predict future keypoints by regressing the coordinates in 1D vector form <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which can hardly maintain spatial structures of keypoints. Although 2D Gaussian maps could express spatial information of keypoints, they still suffer from the accumulation of errors severely due to regressing the continuous coordinates. We propose to select keypoint locations in the finite grid space, which eschews accumulated errors due to continuous coordinate regression.</p><p>We first devise a novel binary map for precise representations of keypoints in the grid space. Concretely, given a detected keypoint with coordinate as [x k t , y k t ], we scale it to find its corresponding entry in an H ? W grid map and make the entry be 1 while the rest 0, forming our binary map B k t ? {0, 1} H?W to represent the k-th keypoint of X t . As a kind of sparse representation, our binary map shares a similar spirit with AlphaGo <ref type="bibr" target="#b20">[21]</ref> that represents a stone on the board of Go. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we indicate each keypoint location of X t by a single channel of B t , which further inputs to our keypoint prediction network.</p><p>To reduce the coordinate error in prediction, we propose to choose keypoint locations indicating their discrete coordinates in the finite grid space rather than regress continuous coordinates. Therefore, we formulate the keypoint coordinate prediction as a classification task. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, our keypoint prediction network takes an input as the binary maps B 1:K t and outputs probability maps P 1:K t+1 to specify the presence of all keypoints over possible positions for the next time step. An argmax operation is used to determine the predicted keypoint coordinatesX 1:K t+1 and binary map? B 1:K t+1 by selecting entries with the maximum probabilities in P 1:K t+1 for each keypoint. The binary mapsB 1:K t+1 are also taken as the input to our prediction model when B 1:K t+1 are not available during testing. Compared to coordinate regression methods, our suggested style can purify a large amount of noise existing in raw outputs of the network by forming standard inputs as binary maps. To train our model, we leverage the cross-entropy loss to measure the prediction error between the predicted P t+1 and ground truth binary maps B t+1 , which is derived from our well-trained grid keypoint detection network. We define the loss as</p><formula xml:id="formula_3">L kp = ? T ?1 t=1 B t+1 log P t+1 .<label>(5)</label></formula><p>With the above formulated coordinate prediction scheme, we extend it to consider the dynamics of keypoints and account for stochasticity in the future. We establish our stochastic keypoint prediction network based on VRNN architecture <ref type="bibr" target="#b14">[15]</ref>. The core insight is referring to a latent belief z to predict possible keypoint locations, where the latent belief z ? R H/4?W/4 is a single-channel response map <ref type="bibr" target="#b15">[16]</ref> to model the stochasticity in keypoint sequences. It is conditioned on the information of all previous frames recorded by hidden states of an RNN. To model the spatiotemporal relations of keypoints on binary maps, we employ a convolutional LSTM (convLSTM) <ref type="bibr" target="#b21">[22]</ref> denoted as ? to generate hidden states h t ? R 64?H/4?W/4 . In the prediction of keypoint at time step t + 1, the prior latent belief z t+1 observes the information from B 1 to B t modeled by h t :</p><formula xml:id="formula_4">p(z t+1 |B 1:t , z 1:t ) = ? prior (h t ).<label>(6)</label></formula><p>The posterior belief of z t+1 is obtained given additional information of time step t + 1:</p><formula xml:id="formula_5">q(z t+1 |B 1:t+1 , z 1:t ) = ? post (B t+1 , h t ),<label>(7)</label></formula><p>where ? prior and ? post are our prior and posterior networks, respectively, to output the expectation and standard deviation of Gaussian distributions. With the latent belief z t+1 , a keypoint decoder ? dec predicts the keypoints of the next time step by p(B t+1 |z 1:t+1 , B 1:t ) = ? dec (z t+1 , h t ).</p><p>Finally, the hidden states are updated by incorporating newly available information to close the recurrent loop:</p><formula xml:id="formula_7">h t+1 = ? (? enc (B t+1 ), z t+1 , h t ) ,<label>(9)</label></formula><p>where ? enc is an encoder for size reduction. During training, the recurrence in ? is updated using B 1:T and the posterior belief output by ? post . When B t+1:T is no more available during the inference stage, the predicted binary mapsB t+1:T are applied with the prior belief from ? prior that is fitted to ? post during training. Our VRNN architecture is optimized by maximizing the evidence lower bound (ELBO) using the re-parametrization trick <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_8">T ?1 t=1 E[log p(B t+1 |z 1:t+1 , B 1:t ) ? ?KL(q(z t+1 )||p(z t+1 ))],<label>(10)</label></formula><p>where ? is used to keep a balance between the reconstruction and prior fitting errors.</p><p>In our keypoint prediction network, we replace the reconstruction term in ELBO by our keypoint prediction loss L kp , and the overall training loss is given by where</p><formula xml:id="formula_9">L pred = L kp + ?L KL ,<label>(11)</label></formula><formula xml:id="formula_10">L KL = T ?1 t=1 KL(q(z t+1 )||p(z t+1 ))</formula><p>is the KLdivergence between prior and posterior probabilities. Finally, the predictedB t+1 with stochasticity is input to G dec for diverse future frame generation. Our smart design yields substantially less noise in keypoint coordinate propagation and the synthesized future frames enjoy high fidelity, which is verified in Section IV-C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>Datasets. We extensively validate our method on two datasets commonly used for the evaluation of stochastic video predictions. The KTH dataset <ref type="bibr" target="#b23">[24]</ref> contains real-world videos of 25 people performing six types of actions, and we use persons 1-16 for training and 17-25 for testing. We use the same setting as <ref type="bibr" target="#b16">[17]</ref> to predict the subsequent 10 frames based on 10 observed frames. The prediction range extends to 40 frames in testing. The Human3.6M dataset <ref type="bibr" target="#b24">[25]</ref> also contains video sequences of human actors performing different actions. We split the training and testing set and follow the experimental settings in <ref type="bibr" target="#b11">[12]</ref>. During the training, models are conditioned on 8 observed frames and predict 8 frames. When testing, models predict 42 frames. Metrics. For quantitative evaluation, we employ three commonly-used frame-wise metrics and average over time: Structural Similarity (SSIM) <ref type="bibr" target="#b25">[26]</ref>, Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b26">[27]</ref>. Unlike SSIM and PSNR, LPIPS is a perceptual metric in the feature level of convolutional neural networks, which is more relevant to human judgment. For SSIM and PSNR, higher values indicate better results, while lower results are preferred for LPIPS. We also adopt Fr?chet Video Distance (FVD) <ref type="bibr" target="#b27">[28]</ref> to evaluate the results in video-level. Implementation Details. In all datasets, the keypoint grid resolution is set to 64 ? 64, and the size of the hidden state map is 16 ? 16. We train our models using the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with an initial learning rate of 1e-3 and an exponential decay rate of 0.25. We empirically set the keypoint number as K = 12 (see Section IV-C.3 for ablation study). We set ? and ? to 0.01 and 0.1, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with Existing Methods</head><p>We compared our model with several state-of-the-art image-based stochastic video prediction approaches using image-autoregressive recurrent networks, including two variants of SV2P <ref type="bibr" target="#b15">[16]</ref>, SVG-LP <ref type="bibr" target="#b16">[17]</ref>, SAVP, and its VAEonly variant <ref type="bibr" target="#b17">[18]</ref>. Additionally, we compare with the latest keypoint-based video prediction method Struct-VRNN <ref type="bibr" target="#b11">[12]</ref>. For methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we obtain the results by directly running the available pre-trained models that authors released online. For Struct-VRNN <ref type="bibr" target="#b11">[12]</ref>, we reimplement the method based on their released code under the same experimental settings. Our evaluation process also strictly follows the previous methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, where we first perform 100 random samples for each test sequence and choose the best scores with respect to the ground truth for each metric. Average values over the entire test set are reported as the final results. Notably, we make the test sequences of all models precisely the same for a fair comparison.</p><p>1) Results on the KTH Dataset: As shown in <ref type="table" target="#tab_0">Table I</ref> and <ref type="figure" target="#fig_4">Fig. 3</ref>, our method significantly outperforms previous imagebased stochastic video prediction methods on all frame-wise metrics. With large parameters, these methods give good results in short-term horizons, however, their performances deteriorate very quickly as time goes on because synthesis in the dense pixel-wise space tends to accumulate more errors. Owing to our grid framework to diminish accumulated errors, our method achieves superior prediction quality and less deviation than the compared methods, especially in the long-term future. Additionally, our method attains performance boosts over the state-of-the-art keypoint-based   <ref type="figure">Fig. 4</ref>. Qualitative results on the KTH dataset. We show the best sample with the highest SSIM of different methods (best). We also present a random sample to demonstrate the diversity of our prediction model (random). method Struct-VRNN by a large margin. Notably, our model also enjoys the least network parameter, which implies its promising prospect in large-scale applications. We illustrate the qualitative results in <ref type="figure">Fig. 4</ref>. It is observed that image-based methods (SV2P, SAVP-VAE, and SVG-LP) tend to lose the person as time goes on, although SAVP-VAE gains an almost equal FVD score as ours. The keypoint-based method Struct-VRNN also hardly preserves the person's shape in the long term and predicts gradually distorted frames due to the damage of keypoint spatial structures. Our model well preserves completeness and fidelity during a complete action period and can generate diverse and reasonable future frames (see the attached video for more examples).</p><p>2) Results on the Human3.6M Dataset: The results are reported in <ref type="table" target="#tab_0">Table II</ref> and note that we did not list the results of SV2P and SAVP given their pre-trained models on this dataset are not available. We observe that SVG-LP gives inferior results to keypoint-based methods due to the diffi- culty of modeling long-term movements in pixel-level space. Our method attains the best performance on all four metrics with the least model parameters. The qualitative results are presented in <ref type="figure" target="#fig_6">Fig. 5</ref>. We see that SVG-LP gives inconsistent predictions and even loses the person at the time step of 40. Struct-VRNN fails to preserve the dress information of the person and generates frames with artifacts in the background. As shown in both best and random samples, our model achieves consistently reasonable and various predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness of Key Components</head><p>We progressively evaluate the effectiveness of our critical components in frame reconstruction (V ) and future frame prediction (V) by answering the following questions: i) does our grid keypoint detection style improve the representation ability of keypoints? ii) does our grid keypoint prediction method boost the propagation accuracy of keypoint coordinates, thereby promoting video prediction performances? 1) Different Keypoint Detection Methods: We first investigate the effectiveness of crucial components in keypoint detection by illustrating the performance of frame reconstruction. We design the following ablation settings: i) baseline: only employing reconstruction loss L rec to detect keypoints in continuous space; ii) baseline + L con : adding condensation loss L con to detect keypoints in continuous space; iii) baseline + gridding: only using L rec and detecting keypoints in finite grid space; iv) our full model: adding L con and detecting keypoints in grid space. We also include the detection part of Struct-VRNN <ref type="bibr" target="#b11">[12]</ref> for comparison, which   <ref type="table" target="#tab_0">Table III</ref> and <ref type="figure" target="#fig_7">Fig. 6</ref>. We see that compared with baseline, either submodules contributes to improvements in keypoint representation for better frame restoration, boosting SSIM from 0.759 to 0.805 and 0.855, respectively. As shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, L con encourages the model to concentrate the keypoints on the foreground region and bypasses the keypoint diffusion on the trivial background (see the third and fifth rows). Our gridding regularization enhances the representation capability to reconstruct the more complete frames (see the second to fourth rows where the head or arms of the person tend to miss). Equipped with both key components, our full model achieves the best keypoint representation, peaking SSIM at 0.862 with the reconstructed frames closest to ground truths.</p><p>2) Different Keypoint Propagation Styles: We then investigate how our gridding regularization helps retain keypoint structures during coordinate propagation for future frame prediction. To purely validate the effectiveness for propagation, we design the following configurations that are all based on the best-detected keypoints from our full detection model, and we only vary the keypoint representation styles in propagation: i) 1D vector: directly using a 1D vector to represent keypoint coordinates and an LSTM to model dynamics; ii) Gaussian map: transforming keypoint coordinates to Gaussian maps (the form used in keypoint detection) and using a convLSTM to model dynamics; iii)   Binary map: changing to our proposed binary maps and using a convLSTM. We also compare with Struct-VRNN, with both its detection and prediction parts unchanged. Video prediction performances of keypoint-based methods are upper-bounded by their reconstruction quality. We first show the deterioration rate of predicted frames relative to the corresponding upper-bounds of different settings on the KTH dataset. We see that from <ref type="table" target="#tab_0">Table IV</ref>, our binary map with convLSTM achieves the least performance degradation. Though employing convLSTM, representing keypoint as Gaussian map shall reversely affect the propagation given much uncertain and ambiguous information involved in this style. This observation demonstrates that our gridding regularization is indispensable to yield the efficacy of convLSTM to preserve the spatial structure.</p><p>We further provide a more intuitive and comprehensive analysis by calculating the coordinate errors at different prediction time steps. The error is measured by grid distances averaged over each keypoint between predicted keypoint coordinates and their ground truth positions, i.e., keypoints produced by our detection model. The results are reported in <ref type="table" target="#tab_5">Table V</ref>. We see that the prediction error in all three settings  grows slower than Struct-VRNN, demonstrating that our method provides a more substantial representation base in the detected keypoints for further propagation. Our proposed binary map further beats other settings in all time steps with apparent gaps. We also illustrate the qualitative results in <ref type="figure" target="#fig_8">Fig. 7</ref>. We observe that our method can sufficiently hold the complete information in the long-range prediction.</p><p>3) Different Numbers of Keypoints: We also analyze the impact of different keypoint numbers for frame reconstruction and prediction on the KTH dataset. The results using 6, 12, and 18 keypoints are listed in <ref type="table" target="#tab_0">Table VI</ref>. We see that slight performance improvements are gained when increasing the keypoints from 6 to 12. However, results decrease especially for SSIM after the keypoint number further increasing to 18. The reason might be that deficient keypoints could not represent the key video dynamics, and excessive keypoints lead to overfitting on trivial details. Experiments on other datasets also verify this observation. Therefore, we choose 12 keypoints to implement our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Robot-assisted Surgical Videos</head><p>Our keypoint-based method also enjoys significant advantages to deploy in robots, given its lightweight model scale. We evaluate our method on the JIGSAWS <ref type="bibr" target="#b29">[30]</ref>, a complicated real-world robot-assisted surgery dataset. It contains surgical robotic motions on the dual-arm da Vinci robot system <ref type="bibr" target="#b30">[31]</ref>. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, our method generates promising qualitative results with diverse and reasonable movements of robotic arms, demonstrating the great potential of our approach for robotic applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a new grid keypoint learning framework for stochastic video prediction. We detect discrete keypoints in a grid space, which are further regularized by our condensation loss to encourage explainable high-level configurations. Owing to our proposed binary maps, accurate keypoint coordinate prediction in a long-term horizon is realized to improve the transformed future frames. We validate our approach on several popular datasets and show the superior results of our method with high parameter-efficiency in terms of both quantitative and qualitative evaluations. For future works, we plan to investigate the potential of our framework to deal with skeleton data and explore its promising applications for humans or robots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Equal contribution. This work was supported by Hong Kong Research Grants Council with Project No. CUHK 14201620. X. Gao, Y. Jin, Q. Dou, C.-W. Fu, and P.-A. Heng are with the Department of Computer Science and Engineering, The Chinese University of Hong Kong, China. Q. Dou is also with the CUHK T Stone Robotics Institute. P.-A. Heng is also with Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China. Corresponding author at: qdou@cse.cuhk.edu.hk (Qi Dou).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>By transforming frames into our smartly designed grid keypoint space, accurate keypoint configurations can be predicted using our framework, thereby enabling the best video prediction performance efficiently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our grid keypoint learning framework. Our pipeline contains three stages: grid keypoints of observed frames are first detected in the canonical grid space; future keypoint coordinates are propagated by choosing the grid locations with maximum probabilities; future frames are generated by translating the predicted keypoints via analogy making.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>X 1:K t . As shown in Fig. 2, D first produces K heatmaps H 1:K t activated by a sigmoid function, which are transformed into intermediate keypoint coordinatesX 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Quantitative evaluation with respect to each time step for all models on the KTH dataset. The models are conditioned on the first 10 frames and predict the following 40 frames. The vertical dotted line indicates the time step the models were trained to predict up to. Mean SSIM, PSNR, and LPIPS over all test videos are plotted with 95% confidence interval shaded. Higher SSIM, PSNR and lower LPIPS indicate better performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Visual results on the Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Reconstruction results (V ) of each method to indicate their representative capabilities on the KTH dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Future frame generation results (V) from different keypoint prediction methods on the KTH dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results on the JIGSAWS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I AVERAGE</head><label>I</label><figDesc>COMPARISON RESULTS OF DIFFERENT METHODS ON THE KTH DATASET. THE BEST RESULTS ARE MARKED IN BOLD.</figDesc><table><row><cell>Method</cell><cell cols="2">SSIM? PSNR?</cell><cell cols="3">LPIPS? FVD? #param</cell></row><row><cell>SV2P time-invariant [16]</cell><cell>0.772</cell><cell>25.70</cell><cell>0.260</cell><cell>253.5</cell><cell>8.3M</cell></row><row><cell>SV2P time-variant [16]</cell><cell>0.782</cell><cell>25.87</cell><cell>0.232</cell><cell>209.5</cell><cell>8.3M</cell></row><row><cell>SVG-LP [17]</cell><cell>0.800</cell><cell>23.91</cell><cell>0.129</cell><cell>157.9</cell><cell>22.8M</cell></row><row><cell>SAVP [18]</cell><cell>0.699</cell><cell>23.79</cell><cell>0.126</cell><cell>183.7</cell><cell>17.6M</cell></row><row><cell>SAVP-VAE [18]</cell><cell>0.806</cell><cell>26.00</cell><cell>0.116</cell><cell>145.7</cell><cell>7.3M</cell></row><row><cell>Struct-VRNN [12]</cell><cell>0.766</cell><cell>24.29</cell><cell>0.124</cell><cell>395.0</cell><cell>2.3M</cell></row><row><cell>Grid keypoint (ours)</cell><cell>0.837</cell><cell>27.11</cell><cell>0.092</cell><cell>144.2</cell><cell>2.0M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISONS ON THE HUMAN3.6M DATASET. THE BEST RESULTS UNDER EACH METRIC ARE MARKED IN BOLD.</figDesc><table><row><cell>Method</cell><cell>SSIM?</cell><cell cols="3">PSNR? LPIPS? FVD?</cell><cell>#param</cell></row><row><cell>SVG-LP [17]</cell><cell>0.893</cell><cell>24.67</cell><cell>0.084</cell><cell>179.5</cell><cell>22.8M</cell></row><row><cell>Struct-VRNN [12]</cell><cell>0.901</cell><cell>24.98</cell><cell>0.056</cell><cell>193.8</cell><cell>2.3M</cell></row><row><cell>Grid keypoint (ours)</cell><cell>0.915</cell><cell>26.06</cell><cell>0.055</cell><cell>166.1</cell><cell>2.0M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">FRAME RECONSTRUCTION RESULTS OF DIFFERENT DETECTION</cell></row><row><cell cols="3">METHODS ON THE KTH DATASET.</cell><cell></cell></row><row><cell>Method</cell><cell>SSIM?</cell><cell cols="2">PSNR? LPIPS?</cell></row><row><cell>Struct-VRNN [12]</cell><cell>0.821</cell><cell>27.86</cell><cell>0.089</cell></row><row><cell>Baseline</cell><cell>0.759</cell><cell>24.93</cell><cell>0.179</cell></row><row><cell>Baseline + Lcon</cell><cell>0.805</cell><cell>25.23</cell><cell>0.114</cell></row><row><cell>Baseline + gridding</cell><cell>0.855</cell><cell>29.31</cell><cell>0.095</cell></row><row><cell>Baseline + Lcon + gridding (ours)</cell><cell>0.862</cell><cell>29.68</cell><cell>0.076</cell></row><row><cell cols="4">employs a (x, y, ?)-triplet to denote coordinate and scale.</cell></row><row><cell cols="4">The results on the KTH dataset are shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV IMAGE</head><label>IV</label><figDesc>DEGENERATION RATE OF DIFFERENT KEYPOINT PROPAGATION METHODS ON THE KTH DATASET.</figDesc><table><row><cell>Method</cell><cell>SSIM</cell><cell>PSNR</cell><cell>LPIPS</cell></row><row><cell>Struct-VRNN [12]</cell><cell>6.7%</cell><cell>12.8%</cell><cell>39.8%</cell></row><row><cell>1D vector</cell><cell>5.2%</cell><cell>12.1%</cell><cell>46.4%</cell></row><row><cell>Gaussian map</cell><cell>5.7%</cell><cell>12.9%</cell><cell>50.1%</cell></row><row><cell>Binary map (ours)</cell><cell>2.9%</cell><cell>8.7%</cell><cell>21.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V AVERAGE</head><label>V</label><figDesc>COORDINATE PREDICTION ERROR IN THE GRID SPACE WITH 95% CONFIDENCE INTERVAL ON THE KTH DATASET.</figDesc><table><row><cell>Method</cell><cell>t = 20</cell><cell>t = 30</cell><cell>t = 40</cell><cell>t = 50</cell></row><row><cell cols="5">Struct-VRNN [12] 4.75?0.13 5.39?0.22 6.07?0.29 8.24?0.52</cell></row><row><cell>1D vector</cell><cell cols="4">2.87?0.14 3.36?0.21 3.94?0.34 5.49?0.60</cell></row><row><cell>Gaussian map</cell><cell cols="4">3.01?0.15 3.89?0.29 4.57?0.45 5.99?0.78</cell></row><row><cell>Binary map (ours)</cell><cell cols="4">2.43?0.18 3.07?0.33 3.49?0.38 4.60?0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI ABLATIVE</head><label>VI</label><figDesc>TESTING RESULTS FOR DIFFERENT NUMBER OF KEYPOINTS.</figDesc><table><row><cell cols="2">Keypoint number</cell><cell cols="4">Reconstruction (V ) SSIM? PSNR? LPIPS? SSIM?</cell><cell cols="2">Prediction (V) PSNR? LPIPS?</cell></row><row><cell></cell><cell>6</cell><cell>0.848</cell><cell>28.76</cell><cell>0.105</cell><cell>0.833</cell><cell>27.06</cell><cell>0.110</cell></row><row><cell></cell><cell>12</cell><cell>0.862</cell><cell>29.68</cell><cell>0.076</cell><cell>0.837</cell><cell>27.11</cell><cell>0.092</cell></row><row><cell></cell><cell>18</cell><cell>0.854</cell><cell>29.42</cell><cell>0.094</cell><cell>0.819</cell><cell>26.42</cell><cell>0.113</cell></row><row><cell></cell><cell cols="2">Observed frames</cell><cell></cell><cell cols="3">Predicted frames</cell></row><row><cell></cell><cell>t=5</cell><cell>t=10</cell><cell>t=15</cell><cell>t=20</cell><cell></cell><cell>t=25</cell><cell>t=30</cell></row><row><cell>GT</cell><cell>13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (best)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (Random)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = [x 1:K t , y 1:K t ]. A trivial way for this operation is using round operation, but gradients cannot be backpropagated through the network for parameter update. Instead, to enable training of D, we realize this pushing operation by elegantly adding the k-th intermediate keypointX k t with a constant difference:?X k t = arg min X || X ?X k t || 2 2 ?X k t ,(1)where X is the coordinate of a grid point in I HW . Then, X 1:K t are represented with Gaussian-shaped blobs at their grid locations to form Gaussian maps G 1:K t . To bring the semantic information for reconstruction, we concatenate G 1:K t with the appearance feature maps of the reference frame F 1 output from an encoder network G enc . Gaussian map of the reference frame G 1:K</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ContextVP: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Videoflow: A conditional flow-based model for stochastic video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VarNet: Exploring variations for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic gesture recognition in robot-assisted surgery with reinforcement learning and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring spatial-temporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved conditional VRNNs for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object structure and dynamics from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Franceschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised keypoint learning for guiding classconditional video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">JHU-ISI gesture and skill assessment working set (JIGSAWS): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Technical review of the da Vinci surgical telemanipulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Med Robot</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
