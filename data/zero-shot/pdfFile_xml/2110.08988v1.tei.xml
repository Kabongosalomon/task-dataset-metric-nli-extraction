<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEANet: Feature-Enhanced Attention Network for RGB-Thermal Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyue</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><forename type="middle">Lun</forename><surname>Lam</surname></persName>
						</author>
						<title level="a" type="main">FEANet: Feature-Enhanced Attention Network for RGB-Thermal Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The RGB-Thermal (RGB-T) information for semantic segmentation has been extensively explored in recent years. However, most existing RGB-T semantic segmentation usually compromises spatial resolution to achieve real-time inference speed, which leads to poor performance. To better extract detail spatial information, we propose a two-stage Feature-Enhanced Attention Network (FEANet) for the RGB-T semantic segmentation task. Specifically, we introduce a Feature-Enhanced Attention Module (FEAM) to excavate and enhance multi-level features from both the channel and spatial views. Benefited from the proposed FEAM module, our FEANet can preserve the spatial information and shift more attention to high-resolution features from the fused RGB-T images. Extensive experiments on the urban scene dataset demonstrate that our FEANet outperforms other state-of-the-art (SOTA) RGB-T methods in terms of objective metrics and subjective visual comparison (+2.6% in global mAcc and +0.8% in global mIoU). For the 480 ? 640 RGB-T test images, our FEANet can run with a real-time speed on an NVIDIA GeForce RTX 2080 Ti card.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As a fundamental but challenging task in computer vision, semantic segmentation can be broadly applied to the fields of path planning, autonomous driving, and video surveillance <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. Most of the existing deep learning-based semantic segmentation networks <ref type="bibr" target="#b3">[4]</ref> [5] mainly deal with RGB images. However, RGB images could provide less information for the model training and produce inaccurate prediction results on the scenarios of similar texture, complex background with dim light, or total darkness. With the popularity of thermal imaging cameras, some researchers found that thermal information is robust and effective for reducing the ambiguity in challenging lighting conditions <ref type="bibr" target="#b5">[6]</ref>, such as in urban street scenes. Therefore, the thermal images created by the thermal <ref type="figure">Fig. 1</ref>. Qualitative comparison with two latest networks in daytime and nighttime urban street scenes. The color cones (the objects marked in the red frame) are too small to detect and segment. We can see that the color cone boundaries segmented by the RTFNet are not so sharp or fail to segment them correctly (e.g., (d), (e), (j), (k)), whereas our FEANet gives a more ideal segmentation result (e.g., (f), (l)).</p><p>imaging cameras can be added as important supplements to improve the performance of RGB-T semantic segmentation.</p><p>Recently, RGB-T semantic segmentation has received increasing research attention. Various RGB-T models <ref type="bibr" target="#b6">[7]</ref> [8] <ref type="bibr" target="#b8">[9]</ref> have been proposed to improve the segmentation performance by combining RGB and thermal information. However, the performance of existing RGB-T models tends to drastically decrease when faced with certain complex scenarios (e.g., cluttered backgrounds, varying illuminations). Therefore, the existing RGB-T methods still need to solve the following challenges in order to make further progress.</p><p>The first challenge is to effectively extract multi-level features from RGB-T fused data. Generally, high-level features contain rich semantic information which can be used for object location, while low-level features provide plentiful micro details that are useful for reducing glitch noise and refining segmentation boundaries. Therefore, current RGB-T semantic segmentation methods (e.g., MFNet <ref type="bibr" target="#b5">[6]</ref>, RTFNet <ref type="bibr" target="#b6">[7]</ref>) use either a direct feature extracting strategy or a progressive multi-data fusing process to leverage multi-level features. However, due to the direct multi-level features ex-tracting and merging strategy without considering differences between levels, these processes suffer from the incomplete extraction problem of noisy low-level features. As shown in <ref type="figure">Fig. 1</ref>, the segmented object boundaries in the RTFNet prediction maps are not sharp (e.g., (d), (e), (j), (k)).</p><p>The second challenge is to excavate informative features from the thermal modality. Thermal images are of low quality, which leads to unpredictable noise during the data fusion process. Previous RGB-T models usually treat the extra thermal images as a fourth-channel input without the modification of three-channel RGB encoder stream or fuse RGB and thermal features by simple summation and multiplication. These methods treat thermal and RGB information from the same perspective and ignore the fact that RGB images contain color and texture, whereas thermal maps contain the spatial relations among objects. Due to this modality difference, the above-mentioned simple combination methods <ref type="bibr" target="#b6">[7]</ref> [9] are not effective. As shown in <ref type="figure">Fig. 1</ref> (d), (e), (j), (k), the RTFNet fails to detect and segment the small target objects (e.g., color cones).</p><p>To address the above issues, we propose a two-stage FEANet for better RGB-T semantic segmentation performance. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our FEANet contains a twostages process for feature extraction and fusion. In stage 1, we introduce a FEAM module, which exploits the interchannel and spatial relations for the detail information. The proposed FEAM exploits multi-level features in a progressive refinement way to suppress distractors in the encoder stream. This strategy is based on the observation that low-level features provide discriminative semantic information with micro details, which may contribute significantly to eliminating the background distractors. In stage 2, to improve the compatibility of RGB and thermal features, the corresponding RGB and thermal feature maps are aggregated through elementwise summation into the RGB encoder stream. Our main contributions are summarized as follows:</p><p>? We design a two-stage FEANet to deal with the object boundaries and the small target object for RGB-T semantic segmentation in urban scenes. ? We introduce a FEAM module to enhance multi-level features and fuse RGB and thermal information in a complementary way. The remainder of this letter is structured as follows. In section II, related works have been reviewed. In section III, we describe our network in detail. In section IV, experimental results and discussions are presented. Conclusions and future work are drawn in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation</head><p>Over the past few years, semantic segmentation is a great challenge for detecting and locating target objects in computer vision. The Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> had been applied to improve the accuracy in the image classification and semantic segmentation tasks since 2012. In 2015, Fully Convolutional Networks (FCN) <ref type="bibr" target="#b10">[11]</ref> had been proposed for the semantic segmentation, which had an endto-end network architecture and outperformed the traditional methods that rely on the hand-crafted features extraction mechanism.</p><p>Similar to the FCN, the SegNet <ref type="bibr" target="#b11">[12]</ref> adopted the Encoder-Decoder network architecture as the backbone for semantic segmentation tasks. The SegNet used a pre-trained VGG16 architecture as its encoder, and then applied the output of the encoder as the input of the up-sampling decoder. Seg-Net achieved state-of-the-art accuracy while getting the low inference speed. In subsequent years, the Encoder-Decoder network structure is widely used in semantic segmentation methods. UNet <ref type="bibr" target="#b12">[13]</ref> and DeepLabv3 <ref type="bibr" target="#b13">[14]</ref> have the large Encoder-Decoder architecture, specially, the decoder can restore the high-resolution feature maps from the low layers through the short-cut connections.</p><p>Real-time semantic segmentation methods aim to generate high-quality segmentation results in real-time. ENet <ref type="bibr" target="#b14">[15]</ref> also followed the Encoder-Decoder architecture to achieve real-time semantic segmentation, but it was optimized for fast inference and high accuracy. Due to the efficiency of ENet, it can effectively process images (480?640 RGB) in the requiring high-speed inference situations. However, ENet failed to perform as well as SegNet on spectral images datasets, such as SUN RGB-D <ref type="bibr" target="#b15">[16]</ref>.</p><p>To improve the accuracy and speed of semantic segmentation, the BiSeNet <ref type="bibr" target="#b16">[17]</ref> had the spatial path that preserves the spatial information and the semantic path that obtains the sufficient receptive field. Based on these two paths, a new Feature Fusion Module was developed to combine the features efficiently. However, the BiSeNet just captures the information from the lower layer to sharpen the boundaries with a slow inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-T Semantic Segmentation</head><p>Some methods adopting CNNs were designed for the RGB-D dataset, which contains images that were acquired by multispectral cameras. In these works, we found that some ideas were useful for designing our method. Hazirbas et al. <ref type="bibr" target="#b17">[18]</ref> proposed a new CNN network named FuseNet, which contained an Encoder-Decoder structure that simultaneously extracts features from RGB and depth images. In <ref type="bibr" target="#b18">[19]</ref>, RGB and spectral images feature maps were not only processed separately in the encoder stream but also in the decoder stream.</p><p>The existing urban scenario image segmentation datasets are based on visible spectral images (RGB images), such as Cityscapes <ref type="bibr" target="#b19">[20]</ref> and Daimler Urban dataset <ref type="bibr" target="#b20">[21]</ref>. Naturally, semantic segmentation methods based on these datasets can only be used to process RGB images. Furthermore, most of these methods focused only on improving the segmentation accuracy while neglecting the inference speed. For RGB-T semantic segmentation of urban scenes, MFNet, RTFNet, and FuseSeg-161 were proposed to fuse RGB and thermal data in a novel Encoder-Decoder structure. In this structure, two identical encoders were employed to extract features from RGB and thermal data, respectively, and one decoder was designed to gradually restore the resolution. In addition to the above methods, recently, other RGB-T fusion methods <ref type="bibr" target="#b21">[22]</ref> [23] utilized the combination of omnidirectional (O-D) infrared sensors and O-D visual RGB sensor for semantic segmentation in autonomous robotic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we first introduce the overall architecture of our FEANet, which contains two extracting encoder streams and an output decoder stream. As the Encoder-Decoder structure has been confirmed as an effective architecture in many semantic segmentation networks, our FEANet also adopts this structure. Then, to fully excavate informative cues from both the RGB and thermal feature maps, we present the FEAM to enhance the multi-level features for superior segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our FEANet contains two main steps: feature extracting and resolution restoring. Two encoder streams and one decoder stream are designed for the feature extraction and recovery, respectively. In the feature extraction progress, two encoders extract the multi-level features from three-channel RGB and one-channel thermal images, respectively. With increasing encoder stream depth, the high-level features (e.g., L3, L4 in <ref type="figure" target="#fig_0">Fig. 2</ref>) will be more useful for capturing global context, while they lose the object details. When we up-sample the high-level feature maps, the output prediction will be blurred and object boundaries will become unclear. Instead, the proposed FEAM can distinguish object regions which are too small to detect. In the resolution restoration progress, the decoder gets dense output predictions. At the end of FEANet, the final softmax layer is adapted to get the prediction output map for the RGB-T semantic segmentation results.</p><p>The proposed FEANet explores the two-stages crossmodal fusion methods. In the first stage, we first extract both the RGB and thermal feature maps by the ResNet block and then refine the detail features through the FEAM module. In the second stage, the corresponding RGB and thermal feature maps are aggregated through elementwise summation into the RGB encoder stream. At the end of two encoders, the final refined feature maps are transmitted to the decoder. With the two-stage feature extracting strategy, the loss of rich semantic information through the intensive feature extracting could be recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder-Feature Extracting</head><p>In the proposed FEANet, both the RGB and thermal features are extracted in two encoder streams. Specifically, both the RGB and thermal encoder streams employ five convolutional blocks from ResNet <ref type="bibr" target="#b24">[24]</ref> as the standard backbone and attach an additional FEAM after every single convolutional block, respectively. Usually, the existing ResNet is designed for the three-channel RGB images extracting, which is not suitable for the single-channel images, then we modify the number of the first convolutional layer to be one to extend it to the thermal image.</p><p>To effectively extract features from both RGB and thermal images is the focus of this paper. When in the nighttime, some colorful objects in RGB maps are invisible but can be clearly seen in the thermal maps. Considering the modality difference, RGB and thermal features need to be enhanced. Inspired by <ref type="bibr" target="#b25">[25]</ref>, we design a FEAM module using an attention component to learn features from the fused data and then refine the prediction. In <ref type="figure" target="#fig_0">Fig. 2</ref>, the FEAM is added after each convolutional layer in two encoder streams, which can enhance the compatibility of the features. This extraction process improves the representation of image features and preserves the multi-level information. To better understand the working mechanism of the FEAM module, the channelwise feature maps from FEAM are visualized at different levels.</p><p>As illustrated in <ref type="figure">Fig. 3</ref>, the FEAM contains a sequential channel attention operation and a spatial attention operation. Channel attention operation shifts attention to the feature that extracted from the convolutional layer and then explores foreground cues. Complementarily, spatial attention operation focuses on the global area to explore the informative cues, looking for possible small target objects within it. To the best of our knowledge, we are the first to introduce the attention mechanism to excavate informative cues from both RGB and thermal multi-level features. Our experiments in Tab. II and <ref type="figure" target="#fig_2">Fig. 5</ref> demonstrate the effectiveness of our approach in improving RGB-T semantic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decoder-Resolution Restoring</head><p>After computing multi-level features from two encoder streams, which are the final map of the RGB and thermal features. The decoder is mainly designed to efficiently leverage the multi-level information to carry out the detail pixels refinement. Our decoder architecture is refined from the RTFNet decoder and then restores the feature map to the original images. Different from RTFNet, we delete two sequential 1 ? 1 convolutions of the original block, which avoids the complicated up-sample process in the decoder. As illustrated in <ref type="figure">Fig. 4</ref>, the decoder consists of two blocks <ref type="figure">(Transposed blocks A and B)</ref>. Specifically, Transposed block B contains an additional branch to enlarge the receptive field and a residual connection to preserve the information.</p><p>Detailed configurations for the neural network layers in the Transposed blocks are displayed in Tab. I. In block A, there is a batch normalization (BN) layer <ref type="bibr" target="#b26">[26]</ref> and a ReLu activation layer <ref type="bibr" target="#b27">[27]</ref> followed by the convolutional layer as the feature resolution. The short cut from the input and the output of the final BN layer is element-wisely added up. In block B, it consists of Conv 1 and two TransConv layers. Each residual-based transposed block contains a 3?3 convolution and a residual-based transposed convolution. Through Conv 1, the resolution of the map is the same as the original one, however, the number of feature channels is decreased by a factor of 2. And the TransConv 1 keeps the number of channels unchanged and increases the resolution by a factor of 2. Different from the TransConv 1, the TransConv 2 needs to increase the resolution and decrease the number of feature channels. Finally, the decoder will get more details to generate the final predicted map in a progressive upsampling way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The RGB-T Dataset</head><p>We use the public data set released by the MFNet <ref type="bibr" target="#b5">[6]</ref>. It was recorded in urban street scenes, which contains eight hand-labeled object classes and one unlabelled background class. This dataset contains 1569 pairs of RGB and thermal images, in which 820 taken at daytime and 749 taken at nighttime. We follow the dataset splitting scheme proposed in <ref type="bibr" target="#b5">[6]</ref>. The training set consists of 784 pairs of images. The validation set consists of 392 pairs of images. The other images are used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>We use the Stochastic Gradient Descent (SGD) optimization solver for training. The momentum and weight decay are set to 0.9 and 0.0005, respectively. The initial learning rate is set to 0.03. We adapt the CosineAnnealingWarmRestarts <ref type="bibr" target="#b28">[28]</ref> to gradually decrease the learning rate. The loss function uses the DiceLoss <ref type="bibr" target="#b29">[29]</ref> and the SoftCrossEntropy <ref type="bibr" target="#b30">[30]</ref> for training. We give the DiceLoss and the SoftCrossEntropy a weight of 0.5 and add them to get the loss function:</p><formula xml:id="formula_0">DiceLoss = 1 ? 2 N i p i g i N i p 2 i + N i g 2 i<label>(1)</label></formula><p>where the sums run over the N voxels, of the predicted binary segmentation volume p i ? P and the ground truth binary volume g i ? G. This formulation of DiceLoss can be differentiated yielding the gradient.</p><formula xml:id="formula_1">SoftCrossEntropyloss = ? 1 n n i=1 c j=1? ij log(y d ij )<label>(2)</label></formula><p>where the n is the number of the batch. In this work n = 5, y ij is is binary indicator if class label c is the correct classification for pixel i, and y d ij is the corresponding predicted probability be normalized to a probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The Accuracy (Acc) and the Intersection-over-Union (IoU) are used as the evaluation indicators of our model. The Acc defines the overall accuracy as the probability of correspondence between a positive decision and true condition. The IoU calculates the intersection of the true label and the predicted result separately for each class. Both mAcc and mIoU are the average values across all the classes for the Acc and the IoU.</p><formula xml:id="formula_2">mAcc = 1 k + 1 k i=0 p ii k j=0 p ij (3) mIoU = 1 k + 1 k i=0 p ii k j=0 p ij + k j=0 p ji ? p ii (4)</formula><p>where the k is the number of the hand-labeled object classes, in this work, k = 8. p ii is the number of the pixels of class i that are correctly classified as class i, p ij is the number of pixels of class i that are wrongly classified as class j, p ji is the number of pixels of class j that are wrongly classified as class i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results And Analysis</head><p>The complete quantitative evaluation results of the networks are listed in Tab. II. Our FEANet achieves some remarkable advantages over the comparison methods in terms of both mAcc and mIoU indicators. Compared to the SOTA RGB-T semantic segmentation methods, our proposed FEANet has a great improvement on small target object detection and segmentation, especially in the Guardrail class. Compared to the SOTA FuseSeg-161, We find the Guardrail class has the +7.1 % Acc and +0.2 % IoU results in improvement. At the same time, the Color Cone class has the +5.4 % Acc and +8.4 % IoU results in improvement. And the other objects also have good segmentation performance. This indicates that our FEANet can make more efficient detection and segmentation on small target objects. To further demonstrate the effectiveness of our FEANet, we visualize the prediction maps of our FEANet and other top 2 methods in <ref type="figure" target="#fig_2">Fig. 5</ref>. Experiments show that Our FEANet effectively utilizes the RGB and thermal information for sharp object boundaries, while the others are disturbed by the background.</p><p>Although our FEANet can get prior results in the tiny object classes, there are some limitations in our network. According to Tab. II, FuseSeg-161 gets the best results in the Person and Bike classes. This proves the effectiveness of the DenseNet, which can keep the feature map resolution unchanged in the encoder stream. And for the RTFNet, our results are better than those of the RTFNet in most object classes, however, the RTFNet152 gets the best indicators in the Car Stop class. This proves network can get the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>In order to verify that our FEAM module is effective at each feature level, we removed the FEAM module from the RGB stream and thermal stream, respectively, then we can see the performance without using the attention module FEAM. Therefore, we call no FEAM in a thermal stream as NFTS. Similarly, no FEAM in RGB stream as NFRS and no FEAM in RGB and thermal stream is named as NFRTS, respectively. FRTS means that the FEAM is both in RGB and thermal stream. Tab. IV shows the quantitative comparison test results. By comparing the results of NFRTS, NFRS, NFTS, and FRTS, we find that FRTS usually provides better performance than NFRS, NFRTS and NFTS in the RGB-T semantic segmentation task. The performance in FRTS can also prove that FEAM can enhance the fusion effect of RGB image information and thermal image information. In this experiment, the FEAM in every layer facilitates a universal improvement in detection performance. In addition, we find that FEAM applied in the thermal stream contributes more to the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We proposed a novel two-stage FEANet to excavate informative thermal cues from both RGB and thermal images for the semantic segmentation of urban scenes. Specifically, we introduce a FEAM to excavate and enhance informative features from both the channel and spatial views. The experimental results demonstrate that FEANet performs better on small target object segmentation and produces sharp object boundaries. The proposed FEANet runs at real-time speed on a single GPU, making it a potential solution for autonomous driving applications. In the future, we would like to fuse more different modalities of information (e. g., depth, audio) into a network for segmentation improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the proposed FEANet. From left to right are Thermal Stream, RGB Stream, and Output Stream. The encoder in Thermal Stream and RGB Stream contains two extracting stages. In stage 1, Thermal Stream and RGB Stream use ResNet [24] as the feature extractor layer. The output part of each layer is weighted through the FEAM. In stage 2, the output map of Thermal Stream is fused into the RGB Stream. The decoder in Output Stream is composed of Transposed blocks A and B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Architecture of the Feature-Enhanced Attention Module (FEAM) Architecture of the Transposed block. Conv, TransConv and BN refer to the convolutional layers, transposed convolutional layers and the batch normalization layers, respectively. The detailed channel numbers for Conv and TransConv layers are listed in Tab. I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative demonstrations for the fusion networks in daytime or nighttime. We can see that our FEANet can provide acceptable results in various lighting conditions. The comparative results demonstrate our superiority.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFIGURATION</head><label>I</label><figDesc>FOR THE CONVOLUTION (CONV) AND TRANSPOSED CONVOLUTION (TRANSCONV) LAYERS IN THE INDIVIDUAL MODULE OF THE DECODER.</figDesc><table><row><cell></cell><cell>Name</cell><cell cols="3">Kernel Size Stride Padding</cell></row><row><cell>Block A</cell><cell>Conv 1</cell><cell>3x3</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>Conv 2</cell><cell>3x3</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>Conv 1</cell><cell>3x3</cell><cell>1</cell><cell>1</cell></row><row><cell>Block B</cell><cell>TransConv 1</cell><cell>2x2</cell><cell>2</cell><cell>0</cell></row><row><cell></cell><cell>TransConv 2</cell><cell>2x2</cell><cell>2</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>RESULT ON THE TEST SET (%). 3CAND 4C REPRESENT THAT THE NETWORKS ARE TESTED WITH THE THREE-CHANNEL RGB DATA AND FOUR-CHANNEL RGB-THERMAL DATA, RESPECTIVELY. NOTE THAT MACC AND MIOU ARE CALCULATED WITH THE UNLABELED CLASSES, BUT THE RESULTS FOR THE UNLABELED CLASSES ARE NOT DISPLAYED. THE BOLD FONT HIGHLIGHTS THE BEST RESULT IN EACH COLUMN.</figDesc><table><row><cell>Methods</cell><cell>Car</cell><cell>Person</cell><cell>Bike</cell><cell>Curve</cell><cell cols="2">Car Stop</cell><cell cols="2">Guardrail</cell><cell>Color Cone</cell><cell>Bump</cell><cell>mAcc mIoU</cell></row><row><cell></cell><cell cols="5">Acc IoU Acc IoU Acc IoU Acc IoU Acc</cell><cell cols="5">IoU Acc IoU Acc IoU Acc IoU</cell></row><row><cell>FRRN(4c)</cell><cell cols="6">81.9 74.7 66.2 60.8 62.8 50.3 41.2 35.0 12.5 11.5</cell><cell>0.0</cell><cell cols="3">0.0 37.2 34.0 35.2 34.6 48.5</cell><cell>44.2</cell></row><row><cell>FRRN(3c)</cell><cell cols="6">80.0 71.2 53.0 46.1 65.1 53.0 34.0 27.1 21.6 19.1</cell><cell>0.0</cell><cell cols="3">0.0 34.7 32.5 36.2 30.5 47.1</cell><cell>41.8</cell></row><row><cell>BiSeNet(4c)</cell><cell cols="10">89.7 84.1 72.0 63.2 74.1 60.1 45.1 36.7 34.2 25.3 18.2 5.0 47.4 42.2 39.8 35.9 57.7</cell><cell>50.0</cell></row><row><cell>BiSeNet(3c)</cell><cell cols="6">90.0 84.5 65.0 54.3 75.0 61.4 32.1 25.7 32.3 26.2</cell><cell>3.2</cell><cell cols="3">0.9 49.6 43.3 48.1 40.5 54.9</cell><cell>48.2</cell></row><row><cell>DFN(4c)</cell><cell cols="10">90.0 84.4 73.2 65.0 75.5 60.9 54.0 40.4 38.9 25.7 10.2 2.7 48.3 42.5 55.8 47.4 60.5</cell><cell>52.0</cell></row><row><cell>DFN(3c)</cell><cell cols="6">90.7 81.4 67.7 52.8 71.5 57.5 49.2 34.9 35.1 23.8</cell><cell>4.1</cell><cell cols="3">1.4 44.2 31.0 54.6 47.5 57.3</cell><cell>47.5</cell></row><row><cell>SegHRNet(4c)</cell><cell cols="10">92.8 87.6 79.3 71.0 78.3 63.4 59.8 42.5 25.7 19.1 18.8 0.0 56.5 49.8 63.5 44.5 63.7</cell><cell>53.2</cell></row><row><cell>SegHRNet(3c)</cell><cell cols="6">92.2 86.6 73.1 59.8 74.9 61.3 47.0 33.2 23.8 28.7</cell><cell>7.3</cell><cell cols="3">0.0 54.6 47.2 61.5 46.2 60.9</cell><cell>51.3</cell></row><row><cell>MFNet</cell><cell cols="5">77.2 65.9 67.0 58.9 53.9 42.9 36.2 29.9 19.1</cell><cell>9.9</cell><cell>0.1</cell><cell cols="3">8.5 30.3 25.2 30.0 27.7 45.1</cell><cell>39.7</cell></row><row><cell>FuseNet</cell><cell cols="6">81.0 75.6 75.2 66.3 64.5 51.9 51.0 37.8 28.7 15.0</cell><cell>0.0</cell><cell cols="3">0.0 31.1 21.4 51.9 45.0 52.4</cell><cell>45.6</cell></row><row><cell cols="5">DepthAwareCNN 85.2 77.0 61.7 53.4 76.0 56.5 40.2 30.9</cell><cell>9.9</cell><cell cols="5">29.3 22.8 6.4 32.9 30.1 36.5 32.3 55.1</cell><cell>46.1</cell></row><row><cell>RTFNet-50</cell><cell cols="10">91.3 86.3 78.2 67.8 71.5 58.2 69.8 43.7 32.1 24.3 13.4 3.6 40.4 26.0 73.5 57.2 62.2</cell><cell>51.7</cell></row><row><cell>RTFNet-152</cell><cell cols="6">93.0 87.4 79.3 70.3 76.8 62.7 60.7 45.3 38.5 29.8</cell><cell>0.0</cell><cell cols="3">0.0 45.5 29.1 74.7 55.7 63.1</cell><cell>53.2</cell></row><row><cell>FuseSeg-161</cell><cell cols="10">93.1 87.9 81.4 71.7 78.5 64.6 68.4 44.8 29.1 22.7 63.7 6.4 55.8 46.9 66.4 47.9 70.6</cell><cell>54.5</cell></row><row><cell>FEANet(Ours)</cell><cell cols="10">93.3 87.8 82.7 71.1 76.7 61.1 65.5 46.5 26.6 22.1 70.8 6.6 66.6 55.3 77.3 48.9 73.2</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III INFERENCE</head><label>III</label><figDesc>SPEED OF SOTA NETWORKS. MS AND FPS REPRESENT THE TIME OF MILLISECONDS AND THE SPEED OF FRAMES PER SECOND,</figDesc><table><row><cell></cell><cell>RESPECTIVELY.</cell><cell></cell></row><row><cell>Methods</cell><cell>RTX 2080 Ti</cell><cell></cell></row><row><cell></cell><cell>ms</cell><cell>FPS</cell></row><row><cell>RTFNet-50</cell><cell>11.25</cell><cell>88.87</cell></row><row><cell>RTFNet-152</cell><cell>30.47</cell><cell>32.81</cell></row><row><cell>FuseSeg-161</cell><cell>33.32</cell><cell>30.01</cell></row><row><cell>FEANet(ours)</cell><cell>28.52</cell><cell>35.06</cell></row><row><cell cols="3">dense feature in a deeper layer to improve the segmentation</cell></row><row><cell cols="3">performance. As indicated in Tab. III, our FEANet achieves</cell></row><row><cell cols="3">real-time inference speed (approximately 35 images/s) on a</cell></row><row><cell cols="2">single NVIDIA Geforce RTX 2080 Ti GPU.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV THE</head><label>IV</label><figDesc>COMPARISON OF MACC (%) AND MIOU (%) ON THE TEST-SETS FOR THE NFRTS, NFRS, NFTS AND FRTS(OURS) VARIANTS. THE BOLD FONT HIGHLIGHTS THE BETTER RESULTS IN EACH SCENARIO.</figDesc><table><row><cell>Variants</cell><cell>Test-set</cell><cell></cell></row><row><cell></cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>NFRTS</cell><cell>63.9</cell><cell>50.0</cell></row><row><cell>NFRS</cell><cell>69.5</cell><cell>54.5</cell></row><row><cell>NFTS</cell><cell>65.3</cell><cell>50.6</cell></row><row><cell>FRTS(ours)</cell><cell>73.2</cell><cell>55.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiband image segmentation and object recognition for understanding road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ninomiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1423" to="1433" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Traffic scene segmentation based on rgb-d image and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1664" to="1669" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Importance-aware semantic segmentation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rtfnet: Rgb-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pst900: Rgb-thermal calibration, dataset and segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9441" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fuseseg: semantic segmentation of urban scenes based on rgb and thermal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Asian conference on computer vision (ACCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient multi-cue scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharw?chter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepfusenet of omnidirectional far-infrared and visual stream for vegetation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Motai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Active perception for outdoor localisation with an omnidirectional camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jayasuriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4567" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
