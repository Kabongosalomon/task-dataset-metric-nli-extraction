<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Atashgahi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Kou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Leeds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">JD Explore Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
							<email>m.pechenizkiy@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Jyv?skyl?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Decebal</forename><forename type="middle">Constantin</forename><surname>Mocanu</surname></persName>
							<email>d.c.mocanu@utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zerocost neuroregeneration (GraNet), that advances state of the art. Perhaps most impressively, its sparse-to-sparse version for the first time boosts the sparse-tosparse training performance over various dense-to-sparse methods with ResNet-50 on ImageNet without extending the training time. We release all codes in https://github.com/Shiweiliuiiiiiii/GraNet.</p><p>Recently emerged methods for pruning at initialization significantly reduce the training cost by identifying a trainable sub-network before the main training process. While promising, the existing methods fail to match the performance achieved by the magnitude pruning after training <ref type="bibr" target="#b10">[11]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network pruning is the most common technique to reduce the parameter count, storage requirements, and computational costs of modern neural network architectures. Recently, posttraining pruning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b74">75]</ref> and before-training pruning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> have been two fast-rising fields, boosted by lottery tickets hypothesis (LTH) <ref type="bibr" target="#b9">[10]</ref> and singleshot network pruning (SNIP) <ref type="bibr" target="#b30">[31]</ref>. The process of post-training pruning typically involves fully pre-training a dense network as well as many cycles of retraining (either fine-tuning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39]</ref> or rewinding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref>). As the training costs of the state-of-the-art models, e.g., GPT-3 <ref type="bibr" target="#b3">[4]</ref> and FixEfficientNet-L2 <ref type="bibr" target="#b63">[64]</ref> have exploded, this process can lead to a large amount of overhead cost.  <ref type="figure">Figure 1</ref>: Schematic view of GraNet. Left: Gradual pruning starts with a sparse subnetwork and gradually prune the subnetwork to the target sparsity during training. Right: We perform zero-cost neuroregeneration after each gradual pruning step. Light blue blocks/lines refer to the "damaged" connections and orange blocks/lines refer to the regenerated new connections.</p><p>Compared with the above-mentioned two classes of pruning, during-training pruning is a class of methods that reap the acceleration benefits of sparsity early on the training and meanwhile achieve promising performance by consulting the information obtained during training. There are some works <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref> attempting to gradually prune the network to the desired sparsity during training, while they mainly focus on the performance improvement. Up to now, the understanding of duringtraining pruning has been less explored due to its more complicated dynamical process, and the performance gap still exists between pruning during training and full dense training.</p><p>To better understand the effect of pruning during the optimization process (not at inference), we study the ability of the pruned models to recover the original performance after a short continued training with the current learning rate, which we call pruning plasticity (see Section 3.1 for a more formal definition). Inspired by the neuroregeneration mechanism in the nervous system where new neurons and connections are synthesized to recover the damage in the nervous system <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b72">73]</ref>, we examine if allowing the pruned network to regenerate new connections can improve pruning plasticity, and hence contribute to pruning during training. We consequently propose a parameter-efficient method to regenerate new connections during the gradual pruning process. Different from the existing works for pruning understanding which mainly focus on dense-to-sparse training <ref type="bibr" target="#b41">[42]</ref> (training a dense model and prune it to the target sparsity), we also consider sparse-to-sparse training (training a sparse model yet adaptively re-creating the sparsity pattern) which recently has received an upsurge of interest in machine learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In short, we have the following main findings during the course of the study:</p><p>#1. Both pruning rate and learning rate matter for pruning plasticity. When pruned with low pruning rates (e.g., 0.2), both dense-to-sparse training and sparse-to-sparse training can easily recover from pruning. On the contrary, if too many parameters are removed at one time, almost all models suffer from accuracy drops. This finding makes a connection to the success of the iterative magnitude pruning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b64">65]</ref>, where usually a pruning process with a small pruning rate (e.g., 0.2) needs to be iteratively repeated for good performance.</p><p>Pruning plasticity also gradually decreases as the learning rate drops. When pruning happens during the training phase with large learning rates, models can easily recover from pruning (up to a certain level). However, pruning plasticity drops significantly after the second learning rate decay, leading to a situation where the pruned networks can not recover with continued training. This finding helps to explain several observations (1) for gradual magnitude pruning (GMP), it is always optimal to end pruning before the second learning rate drop <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref>; <ref type="bibr" target="#b1">(2)</ref> dynamic sparse training (DST) benefits from a monotonically decreasing pruning rate with cosine or linear update schedule <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>; <ref type="bibr" target="#b2">(3)</ref> rewinding techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref> outperform fine-tuning as rewinding retrains subnetworks with the original learning rate schedule whereas fine-tuning often retrains with the smallest learning rate.</p><p>#2. Neuroregeneration improves pruning plasticity. Neuroregeneration <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b72">73]</ref> refers to the regrowth or repair of nervous tissues, cells, or cell products. Conceptually, it involves synthesizing new neurons, glia, axons, myelin, or synapses, providing extra resources in the long term to replace those damaged by the injury, and achieving a lasting functional recovery. Such mechanism is closely related to the brain plasticity <ref type="bibr" target="#b50">[51]</ref>, and we borrow this concept to developing a computational regime.</p><p>We show that, while regenerating the same number of connections as pruned, the pruning plasticity is observed to improve remarkably, indicating a more neuroplastic model being developed. However, it increases memory and computational overheads and seems to contradict the benefits of pruningduring-training. This however raises the question: can we achieve efficient neuroregeneration during training with no extra costs? We provide an affirmative answer to this question.</p><p>#3. Pruning plasticity with neuroregeneration can be leveraged to substantially boost sparse training performance. The above-mentioned findings of pruning plasticity can generalize to the final performance level under a full continued training to the end. Imitating the neuroregeneration behavior <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b72">73]</ref>, we propose a new sparse training method -gradual pruning with zero-cost neuroregeneration (GraNet), which is capable of performing regeneration without increasing the parameter count.</p><p>In experiments, GraNet establishes the new state-of-the-art performance bar for dense-to-sparse training and sparse-to-sparse training, respectively. Particularly, the latter for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods by a large margin without extending the training time, with ResNet-50 on ImageNet. Besides the consistent performance improvement, we find the subnetworks that GraNet learns are more accurate than the ones learned by the existing gradual pruning method, providing explanations for the success of GraNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Post-Training Pruning. Methods that yield a sparse neural network from a pre-trained network by pruning the unimportant weights or neurons, to the best of our knowledge, were proposed in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b49">[50]</ref>. After that, various pruning methods have emerged to provide increasingly efficient methods to identify sparse neural networks for inference. The pruning criterion includes weight magnitude <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref>, gradient <ref type="bibr" target="#b60">[61]</ref> Hessian <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b58">59]</ref>, Taylor expansion <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref>, etc. Low-rank decomposition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b70">71]</ref> are also used to induce structured sparsity in terms of channels or filters. Most of the above-mentioned pruning methods require many pruning and re-training cycles to achieve the desired performance.</p><p>During-Training Pruning. Instead of inheriting weights from a pre-trained model, some works attempt to discover well-performing sparse neural networks with one single training process.</p><p>Gradual Magnitude Pruning (GMP), introduced in <ref type="bibr" target="#b76">[77]</ref> and studied further in <ref type="bibr" target="#b12">[13]</ref>, gradually sparsifies the neural network during the training process until the desired sparsity is reached. Besides, <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b67">[68]</ref> are prior works that enforce the network to sparse during training via L 0 and L 1 regularization, respectively. <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b27">28]</ref> moved further by introducing trainable sparsity heuristics to learn the sparse masks and weights simultaneously. These methods are all classified as dense-to-sparse training as they start from a dense network.</p><p>Dynamic Sparse Training (DST) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25]</ref> is another class of methods that prune models during training. The key factor of DST is that it starts from a random initialized sparse network and optimizes the sparse topology as well as the weights simultaneously during training (sparse-to-sparse training). Without an extended training time <ref type="bibr" target="#b36">[37]</ref>, sparse-to-sparse training usually falls short of dense-to-sparse training in terms of the prediction accuracy. For further details, see the survey of <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Before-Training Pruning. Motivated by SNIP <ref type="bibr" target="#b30">[31]</ref>, many works <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b5">6]</ref> have emerged recently to explore the possibility of obtaining a trainable sparse neural network before the main training process. <ref type="bibr" target="#b10">[11]</ref> demonstrates that the existing methods for pruning at initialization perform equally well when the unpruned weights are randomly shuffled, which reveals that what these methods discover is the layer-wise sparsity ratio, rather than the indispensable weight values and positions. Our analysis shows that both the mask positions and weight values are crucial for GraNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology for Pruning Plasticity</head><p>The primary goal of this paper is to study the effect of pruning as well as neuroregeneration on neural networks during the standard training process. Therefore, we do not consider post-training pruning and before-training pruning. Below, we introduce in detail the definition of pruning plasticity and the experimental design that we used to study pruning plasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metrics</head><p>Let us denote W t ? R d as the weights of the network and m t ? {0, 1} d as the binary mask yielded from the pruning method at epoch t. Thus, the pruned network can be denoted as W t m t . Let T be the total number of epochs the model should be trained. Let CONTRAIN k (W t m t , a) refers to the function that continues to train the pruned model for k epochs with the learning rate schedule a.</p><p>Definition of Pruning plasticity. We define pruning plasticity as t CONTRAIN k (Wt mt,at) ? t PRE , where t PRE is the test accuracy measured before pruning and t CONTRAIN k (Wt mt,at) is the test accuracy measured after k epoch of continued training CONTRAIN k (W t m t , a t ). Specifically, to better understand the effect of pruning on the current model status and to avoid the effect of learning rate decay, we fix the learning rate as the one when the model is pruned, i.e, a t . This setting is also appealing to GMP <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref> and DST <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b36">37]</ref> in which most of the pruned models are continually trained with the current learning rate for some time.</p><p>Final performance gap. Nevertheless, we also investigate the effect of pruning on the final performance, that is, continually training the pruned networks to the end with the remaining learning rate schedule CONTRAIN T ?t (W t m t , a [t+1:T ] ). In this case, we report t CONTRAIN T ?t (Wt mt,a [t+1:T ] ) ? t FINAL , where t FINAL is the final test accuracy of the unpruned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures and Datasets</head><p>We choose two commonly used architectures to study pruning plasticity, VGG-19 <ref type="bibr" target="#b57">[58]</ref> with batch normalization on CIFAR-10 <ref type="bibr" target="#b26">[27]</ref>, and ResNet-20 <ref type="bibr" target="#b19">[20]</ref> on CIFAR-10.</p><p>We share the summary of the networks, data, and hyperparameters of dense-to-sparse training in <ref type="table" target="#tab_0">Table 1</ref>. We use standard implementations and hyperparameters available online, with the exception of the small batch size for the ResNet-50 on ImageNet due to the limited hardware resources (2? Tesla V100). All accuracies are in line with the baselines reported in the references <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How to Prune, and How to Regenerate</head><p>Structured and Unstructured Pruning. We consider unstructured and structured pruning in this paper. Structured pruning prunes weights in groups, or removes the entire neurons, convolutional filters, or channels, enabling acceleration with the off-the-shelf hardware. In particular, we choose the filter pruning method used in Li et al. <ref type="bibr" target="#b31">[32]</ref>. Unstructured sparsity is a more promising direction not only due to its outstanding performance at extreme sparsities but the increasing support for sparse operation in the practical hardware <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b21">22]</ref>. For example, Liu et al. <ref type="bibr" target="#b34">[35]</ref> illustrated for the first time the true potential of DST, demonstrating significant training/inference efficiency improvement over the dense training. Different from prior conventions <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2]</ref> where values of the pruned weights are kept, we set the pruned weights to zero to eliminate the historical information for all implementations in this paper.</p><p>Magnitude pruning. We prune the weights with the smallest magnitude, as it has evolved as the standard method when pruning happens during training, e.g., GMP <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref> and DST <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref>. We are also aware of other pruning criteria including but not limited to Hessian <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b58">59]</ref>, Taylor expansion <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref>, connection sensitivity <ref type="bibr" target="#b30">[31]</ref>, Gradient Flow <ref type="bibr" target="#b66">[67]</ref>, Neural Tangent Kernel <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>One-shot pruning. To isolate the pruning effect at different training stages and to avoid the interaction between two iterations of pruning, we focus on one-shot pruning. Please note that iterative pruning can also be generalized in our setting, as our experimental design includes neural networks trained at various sparsities and each of them is further pruned with various pruning rates.</p><p>Layer-wise pruning and global pruning. We study both the layer-wise magnitude pruning and global magnitude pruning for pruning plasticity. Global magnitude pruning prunes different layers together and leads to non-uniform sparsity distributions; layer-wise pruning operates layer by layer, resulting in uniform distributions.</p><p>Gradient-based regeneration. The simplest regeneration scheme is to randomly activate new connections <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44]</ref>. However, it would take a lot of time for random regeneration to discover the important connections, especially for the very extreme sparsities. Alternatively, gradients, including those for the connections with zero weights, provide good indicators for the connection importance. For this reason, we focus on gradient-based regeneration proposed in Rigged Lottery ( RigL) <ref type="bibr" target="#b8">[9]</ref>, i.e., regenerating the same number of connections as pruned with the largest gradient magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>We study pruning plasticity during training with/without regeneration, for both dense training and sparse training. We report the results of ResNet-20 on CIFAR-10 with unstructured global pruning in the main body of the paper. The rest of the experiments are given in Appendix A. Unless otherwise stated, results are qualitatively similar across all networks. Concretely, we first pre-train networks at four sparsity levels, including 0, 0.5, 0.9, and 0.98. The sparse neural networks are trained with uniform distribution (i.e., all layers have the same sparsity). We further choose four pruning rates, e.g., 0.2, 0.5, 0.9, and 0.98, to measure the corresponding pruning plasticity of the pre-trained networks.</p><p>Pruning plasticity. We continue to train the pruned model for 30 epochs and report pruning plasticity in <ref type="figure" target="#fig_3">Figure 2</ref>. Overall, the learning rate schedule, the pruning rate, and the sparsity of the original models all have a big impact on pruning plasticity. Pruning plasticity decreases as the learning rate decays for all models with different sparsity levels. The models trained with a large learning rate 0.1 can easily recover, or exceed the original performance except for the extremely large pruning rate 0.98. However, the models obtained during the later training phases can recover only with the mild pruning rate choices, e.g., 0.2 (orange lines) and 0.5 (green lines).</p><p>We next demonstrate the effect of connection regeneration on pruning plasticity in the bottom row of <ref type="figure" target="#fig_3">Figure 2</ref>. It is clear to see that connection regeneration significantly improves pruning plasticity of all the cases, especially for the models that are over-pruned (purple lines). Still, even with connection regeneration, pruning plasticity suffers from performance degradation when pruning occurs after the learning rate drops.  <ref type="bibr" target="#b19">20</ref> 40 <ref type="table" target="#tab_0">60  80  90  100  110  120  130  140  150  160  40   20   0   20  40  60  80  90  100  110  120  130  140  150  160  40   20   0   20  40  60  80  90  100  110  120  130  140  150  160</ref>   The vertical red lines refer to the points when the learning rate is decayed. "Pre-trained Sparsity" refers to the original sparsity of the pre-trained networks before pruning. The pruning method is the magnitude global pruning.</p><p>Final performance gap. Compared with the current model status, people might be more interested in the effect of pruning on the final performance. We further measure the performance gap between the original test accuracy of the unpruned models and the final test accuracy of the pruned model under a full continued training CONTRAIN T ?t (W t m t , a [t+1:T ] ) in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>We observe that, in this case, large learning rates do not enjoy large performance improvement, but still, the performance gap increases as the learning rate drops. It is reasonable to conjecture that the accuracy improvement of pruning plasticity with the large learning rate, 0.1, is due to the  unconverged performance during the early phase of training. Besides, it is surprising to find that the final performance of extreme sparse networks (e.g., the third column and the fourth column) significantly benefits from mild pruning. Again, the ability of the pruned model to recover from pruning remarkably improves after regenerating the connections back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gradual Pruning with Zero-Cost Neuroregeneration</head><p>So far, we have known that regenerating the important connections to the pruned models during training substantially improves pruning plasticity as well as the final performance. However, naively regenerating extra connections increases the parameter count and conflicts with the motivation of gradual pruning.</p><p>Inspired by the mechanism of neuroregeneration in the nervous system, we propose a novel sparse training method which we call gradual pruning with zero-cost neuroregeneration (GraNet). GraNet consults the information produced throughout training and regenerates important connections during training in a parameter-efficient fashion. See Appendix B.1 for the pseudocode of GraNet. We introduce the main components of GraNet below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gradual Pruning</head><p>We follow the gradual pruning scheme used in <ref type="bibr" target="#b76">[77]</ref> and gradually sparsifies the dense network to the target sparsity level over n pruning iterations. Let us define s i is the initial sparsity, s f is the target sparsity, t 0 is is the starting epoch of gradual pruning, t f is the end epoch of gradual pruning, and ?t is the pruning frequency. The pruning rate of each pruning iteration is:</p><formula xml:id="formula_0">s t = s f + (s i ? s f ) 1 ? t ? t 0 n?t 3 , t ? {t 0 , t 0 + ?t, ..., t 0 + n?t} .<label>(1)</label></formula><p>We choose global pruning for our method as it generally achieves better performance than uniform pruning. We also report the performance of the uniform sparsity as used in <ref type="bibr" target="#b12">[13]</ref> in Appendix C.3.</p><p>The conventional gradual pruning methods <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref> change the mask (not the weight values) to fulfill the pruning operation, so that the pruned connections have the possibility to be reactivated in the later training phases. Despite this, since the weights of the pruned connections are not updated, they have a small chance to receive sufficient updates to exceed the pruning threshold. This hinders the regeneration of the important connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-Cost Neuroregeneration</head><p>The main difference between GraNet and the conventional GMP methods <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref> is the Zero-Cost Neuroregeneration. Imitating the neuroregeneration of the peripheral nervous system <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b72">73]</ref> where new neurons and connections are synthesized to replace the damaged ones, we first detect and eliminate the "damaged" connections, and then regenerate the same number of new connections. By doing this, we can achieve connection regeneration without increasing the number of connections.</p><p>Concretely, we identify the "damaged" connections as the ones with the smallest weight magnitudes. Small magnitude indicates that either the weight's gradient is small or a large number of oscillations occur to the gradient direction. Therefore, these weights have a small contribution to the training loss and can be removed. Again, we use the gradient as the importance score for regeneration, same as the regrow method as used in RigL <ref type="bibr" target="#b8">[9]</ref>.</p><p>Why we call it "Zero-Cost Neuroregeneration"? In addition to not increasing the connection (parameter) count, the backward pass of our method is sparse most of the time even though our regeneration utilizes the dense gradient to identify the important connections. We perform neuroregeneration immediately after each gradual pruning step, meaning that the regeneration occurs only once every several thousand iterations. The extra overhead to calculate the dense gradient can be amortized compared with the whole training costs. Compared with the methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b68">69]</ref> that require updating all the weights in the backward pass, our method is much more training efficient, as around 2/3 of the training FLOPs is owing to the backward pass <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>Let us denote r as the ratio of the number of the regenerated connections to the total number of connections; W is the network weight. We first remove r proportion of "damaged" weights with the smallest magnitude by:</p><formula xml:id="formula_1">W = TopK (|W |, 1 ? r) .</formula><p>(2) Here TopK(v, k) returns the weight tensor retaining the top k-proportion of elements from v. Immediately after that, we regenerate r proportion of new connections based on the gradient magnitude:</p><formula xml:id="formula_2">W = W + TopK (|g i / ?W |, r) ,<label>(3)</label></formula><p>where |g i / ?W | are the gradient magnitude of the zero weights. We perform Zero-Cost Neuroregeneration layer by layer from the beginning of the training to the end.</p><p>GraNet can naturally generalize to the dense-to-sparse training scenario and the sparse-to-sparse training scenario by setting the initial sparsity level s i = 0 and s i &gt; 0 in Eq. (1), respectively. For simplicity, we set s i = 0.5, t 0 = 0, and t f as the epoch when performing the first learning rate decay for the sparse-to-sparse training. Different from the existing sparse-to-sparse training methods, i.e., SET <ref type="bibr" target="#b43">[44]</ref>, RigL <ref type="bibr" target="#b8">[9]</ref>, and ITOP <ref type="bibr" target="#b36">[37]</ref>, in which the sparsity is fixed throughout training, GraNet starts from a denser yet still sparse model and gradually prunes the sparse model to the desired sparsity. Although starting with more parameters, the global pruning technique of gradual pruning helps GraNet quickly evolve to a better sparsity distribution than RigL with lower feedforward FLOPs and higher test accuracy. What's more, GraNet sparsifies all layers including the first convolutional layer and the last fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>We conduct various experiments to evaluate the effectiveness of GraNet. We compare GraNet with various dense-to-sparse methods and sparse-to-sparse methods. The results of Rigged Lottery (RigL) and GMP with CIFAR-10/100 were reproduced by our implementation with PyTorch so that the only difference between GraNet and GMP is the Zero-Cost Neuroregeneration. For each model, we divide the results into three groups from top to bottom: pruning at initialization, dynamic sparse training and dense-to-sparse methods. See Appendix B for more implementation details used in the experiments. GraNet (s i = 0.5) refers to the sparse-to-sparse version and the and GraNet (s i = 0) refers to the dense-to-sparse version.</p><p>CIFAR-10/100. The results of CIFAR-10/100 are shared in <ref type="table">Table 2</ref>. We can observe that performance differences among different methods on CIFAR-10 are generally small, but still, GraNet (s i = 0) consistently improves the performance over GMP except for the sparsity 95%, and achieves the highest accuracy in 4 out of 6 cases. In terms of the more complex data CIFAR-100, the performance differences between the during-training pruning methods and before-training pruning methods are much larger. GraNet (s i = 0) again consistently outperforms GMP with all sparsities, highlighting the benefits of Zero-Cost Neuroregeneration. It is maybe more interesting that GraNet (s i = 0) even outperforms the post-training method, subdifferential inclusion for sparsity (SIS), by a large margin.</p><p>In terms of sparse-to-sparse training, our proposed GraNet (s i = 0.5) has a dominant performance over other methods. Especially at the very extreme sparsity 0.98, our method outperforms RigL by 1.40% and 2.22% with VGG-19 on CIFAR-10 and CIFAR-100, respectively.</p><p>ImageNet. Due to the small data size, the experiments with CIFAR-10/100 may not be sufficient to draw a solid conclusion. We further evaluate our method with ResNet-50 on ImageNet in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="table">Table 2</ref>: Test accuracy of pruned VGG-19 and ResNet-50 on CIFAR-10/100. We mark the best sparse-to-sparse training results in blue and the best dense-to-sparse training results in bold. The results reported with (mean ? std) are run with three different random seeds by us. The rest are obtained from <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b66">[67]</ref>. Note that the accuracy of RigL is higher than the ones reported in <ref type="bibr" target="#b65">[66]</ref>, as we choose a large update interval following the In-Time Over-Parameterization strategy <ref type="bibr" target="#b36">[37]</ref>. s i refers to the initial sparsity of GraNet.  We only run this experiment once due to the limited resources. We set t 0 = 0 and t f = 30 for both GraNet (s i = 0) and GraNet (s i = 0.5) on ImageNet. Again, GraNet (s i = 0) outperforms GMP consistently with only half training FLOPs and achieves the highest accuracy among all the dense-to-sparse methods at sparsity of 0.9. Surprisingly, GraNet (s i = 0.5) significantly boosts the sparse-to-sparse training performance, even over the dense-to-sparse training. Concretely, GraNet (s i = 0.5) outperforms RigL by 0.9% and 1.5% at sparsity 0.8 and 0.9, respectively. To the best of our knowledge, this is the first time in the literature that sparse-to-sparse training reaches a test accuracy of 76% with ResNet-50 on ImageNet at sparsity 0.8, without extension of training time. It is reasonable for GraNet (s i = 0.5) to achieve better accuracy than RigL, since the denser models at the beginning help GraNet explore more the parameter space. According to the In-Time Over-Parameterization hypothesis <ref type="bibr" target="#b36">[37]</ref>, the performance of sparse training methods is highly correlated with the total number of parameters that the sparse model has visited. We increase the training time of GraNet by 2.5? (250 epochs) and compare with the baselines with longer training time. With 50% fewer training time and 60% fewer training FLOPs, GraNet can match the performance of RigL trained with 500 epochs.</p><p>We further report the training/inference FLOPs required by all pruning methods. Compared with other dense-to-sparse methods, the final networks learned by GraNet (s i = 0) require more FLOPs to test, whereas the overall training FLOPs required by GraNet (s i = 0) are smaller than others. Even though starting from a denser model, GraNet (s i = 0.5) requires less training and inference FLOPs than the state-of-the-art method, i.e., RigL. The sparsity budgets for 0.9 sparse ResNet-50 on ImageNet-1K learned by our methods are reported in Appendix D. We also report how FLOPs of the pruned ResNet-50 evolve during the course of training in Appendix E. As we mentioned earlier, the denser initial network is the key factor in the success of GraNet. We conducted experiments to study the effect of the initial sparsity on GraNet with ResNet-50 on Ima-geNet. The initial sparsity is chosen from [0.0, 0.5, 0.6, 0.7, 0.8, 0.9] and the final sparsity is fixed as 0.9. The results are shared in <ref type="table" target="#tab_3">Table 4</ref>. We can see the training FLOPs of GraNet are quite robust to the initial sparsity. Surprisingly yet reasonably, it seems that the the smaller the initial sparsity is (up to 0.5), the better final sparsity distribution GraNet finds, with higher test accuracy and fewer feedforward FLOPs. The lower feedforward FLOPs of the final network perfectly balance the overhead caused by the denser initial network. In this section, we share the results of GraNet and RigL at extreme sparsities. The initial sparsity is set as 0.5. When the final sparsity is relatively smaller (e.g., 0.8, 0.9), GraNet requires a lower (or the same) number of training FLOPs than RigL, whereas GraNet requires more training FLOPs than RigL when the final sparsity is extremely high (e.g., 0.95, 0.965). This makes sense since when the sparsity is extremely high, the saved FLOPs count of the distribution discovered by GraNet is too small to amortize the overhead caused by denser initial models. Yet, the increased number of training FLOPs of GraNet leads to substantial accuracy improvement (&gt; 2%) over RigL. The efficiency of GraNet (s i = 0.5) comes from two important technical differences compared with RigL: (1) better final sparse distribution discovered by global pruning; (2) a shorter period of gradual pruning time (the first 30 epochs for ResNet-50 on ImageNet). Although starting with more parameters, the global pruning enables GraNet to quickly (first 30 epochs) evolve to a better sparsity distribution with lower test FLOPs than ERK. After 30 epochs of gradual pruning, the network continues to be trained with this better distribution for 70 epochs, so that the overhead in the early training phase with larger training FLOPs is amortized by the later and longer training phase with fewer training FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of the Initial Sparsity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance of GraNet at Extreme Sparsities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study of Random Reinitialization</head><p>Next, we ask whether what GraNet learned are the specific sparse connectivity or the sparse connectivity together with the weight values. We randomly reinitialize the pruned network with the same mask and retrain it. The results are given in <ref type="figure" target="#fig_6">Figure 4</ref>. The performance of the reinitialized networks falls significantly short of the performance achieved by GraNet (s i = 0), indicating that what was learned by GraNet is the sparse connectivity together with the weight values. Besides, we find that the retraining performance of GraNet is higher than GMP. This further confirms that Zero-Cost Neuroregeneration helps the gradual pruning find more accurate mask positions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparison between Re-training and Extended Training</head><p>In this section, we study if re-training techniques can further improve the performance of the subnetworks discovered by GraNet. The authors of Lottery Ticket Hypothesis (LTH) <ref type="bibr" target="#b9">[10]</ref> introduced a retraining technique, even if they did not evaluate it as such, where the subnetworks discovered by iterative magnitude pruning can be re-trained in isolation to full accuracy with the original initializations. Later on, learning rate rewinding (LRR) <ref type="bibr" target="#b53">[54]</ref> was proposed further to improve the re-training performance by only rewinding the learning rate. Since GraNet also utilizes magnitude pruning to discover subnetworks, it is natural to test if these re-training techniques can bring benefits to GraNet. As shown in <ref type="table" target="#tab_6">Table 6</ref>, both re-training techniques do not bring benefits to GraNet. Instead of re-training the subnetworks, we find that simply extending the training time significantly boosts the performance of GraNet with similar computational costs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion, and Reflection of Broader Impacts</head><p>In this paper, we re-emphasize the merit of during-training pruning. Compared with the recently proposed works, i.e., LTH and SNIP, during-training pruning is an efficient yet performant class of pruning methods that have received much less attention. We quantitatively study pruning during training from the perspective of pruning plasticity. Inspired by the findings from pruning plasticity and the mechanism of neuroregeneration in the nervous system, we further proposed a novel sparse training method, GraNet, that performs the cost-free connection regeneration during training. GraNet advances the state of the art in both dense-to-sparse training and sparse-to-sparse training.</p><p>Our paper re-emphasizes the great potential of during-training pruning in reducing the training/inference resources required by ML models without sacrificing accuracy. It has a significant environmental impact on reducing the energy cost of the ML models and CO2 emissions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This project is partially financed by the Dutch Research Council (NWO). We thank the reviewers for the constructive comments and questions, which improved the quality of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Remaining Experimental Results of Pruning Plasticity</head><p>We also studied pruning plasticity on structured pruning. In particular, we choose the filter pruning method used in Li et al. <ref type="bibr" target="#b31">[32]</ref>. The pruning criterion is the absolute weight sum of each nonzero filter and the regeneration criterion is the absolute gradient sum of each zero filter. We first pre-train four sets of neural networks from scratch with various structured sparsity, including 0, 0.10, 0.50, and 0.70, noted as "Pre-trained Sparsity" in the figure title. To measure the plasticity of these pre-trained models, we choose four different pruning rates noted as "Pruning rate" to remove filters from these pre-trained models. The results of ResNet-20 and VGG-19 are shown as below.</p><p>A.1 ResNet-20 on CIFAR-10 with Structured Filter Pruning  <ref type="bibr" target="#b19">20</ref> 40 <ref type="table" target="#tab_0">60  80  90  100  110  120  130  140  150  160  40   20   0   20  40  60  80  90  100  110  120  130  140  150  160  40   20   0   20  40  60  80  90  100  110  120  130  140  150  160</ref>             </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details of GraNet</head><p>In this appendix, we share in detail the pseudocode and implementation details of GraNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Algorithm</head><p>The pseudocode of GraNet is shared in Algorithm 1. The only difference between sparse-to-sparse training and dense-to-sparse training is the choices of initial sparsity s i . For dense-to-sparse training, we need to set the initial sparsity of the model s i = 0. To perform sparse-to-sparse training, we need to make sure the model is sparse at the beginning by setting the initial sparsity larger than 0, i.e., s i &gt; 0. We share the hyperparameter choices in our experiments in <ref type="table" target="#tab_9">Table 7</ref>.  <ref type="bibr" target="#b8">[9]</ref>, where the FLOPs of the backward pass are around twice the ones of the forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details of GMP</head><p>In this appendix, we share in detail the pseudocode and implementation details of GMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Algorithm</head><p>Gradual Magnitude Pruning (GMP), introduced in <ref type="bibr" target="#b76">[77]</ref> and studied further in <ref type="bibr" target="#b12">[13]</ref>, gradually sparsifies the neural network during the training process until the desired sparsity is reached. The pruning rate of each pruning iteration is:</p><formula xml:id="formula_3">s t = s f + (s i ? s f ) 1 ? t ? t 0 n?t 3 t ? {t 0 , t 0 + ?t, ..., t 0 + n?t}<label>(4)</label></formula><p>The pseudocode of GMP is shown in Algorithm 2. To demonstrate the effectiveness of Zero-Cost Neuroregeneration, we reproduce GMP with our implementation for CIFAR-10/100 so that the only difference between GMP and GraNet is the Zero-Cost Neuroregeneration. Hence, all the hyperparameters of GMP on CIFAR-10/100 are the same as GraNet.</p><p>It is surprising that the number of training FLOPs required by GraNet is smaller than GMP reported in <ref type="bibr" target="#b12">[13]</ref>. Since the authors of <ref type="bibr" target="#b12">[13]</ref> did not share the specific hyperparameters that used to produce the results of GMP, we guess the pruning of GMP happens late in training. Thus, it makes sense that GraNet requires fewer training FLOPs than GMP, as the dense training time of GraNet is much shorter than GMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Implementation</head><p>We reproduce GMP only for the results of CIFAR-10/100 for a fair comparison with GraNet. The results of GMP with ResNet-50 on ImageNet are obtained directly from <ref type="bibr" target="#b12">[13]</ref>.</p><p>We also test our implemented GMP with ResNet-50 on ImageNet. However, the performance is much worse than the results in <ref type="bibr" target="#b12">[13]</ref> as shown below. We are aware that our GMP implementation has several differences from the original Tensorflow implementation used by <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b12">13]</ref>. Firstly, since our implementation reset the weight values to zero once the weights are pruned, the pruned weights of GMP are also set to zero. However, in the original GMP implementation, only the masks are set to zero and the weight values are kept, leading to a situation where the pruned weights can be regenerated back in a natural way. Secondly, the original GMP uses uniform pruning and keeps the first layer dense and the sparsity of the last layer no larger than 0.8. Same as GraNet, our implementation of GMP prune all the layers including the first layer and the last layer.</p><p>We also compare GMP and GraNet with uniform pruning as used in <ref type="bibr" target="#b12">[13]</ref>, as shown below. While the results with CIFAR-10 in unclear, GraNet outperforms GMP with CIFAR-100 consistently. As we expected, the performance using uniform pruning is generally worse than global pruning.  <ref type="table" target="#tab_0">Table 10</ref> summarizes the final sparsity budgets for 90% sparse ResNet50 on ImageNet-1K obtained by various methods. Backbone represents the sparsity budgets for all the CNN layers without the last fully-connected layer. VD refers to Variational Dropout <ref type="bibr" target="#b44">[45]</ref> and GS refers to iterative magnitude pruning using a global threshold for global sparsity <ref type="bibr" target="#b17">[18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E FLOPs Dynamics During Training with ResNet-50 on ImageNet</head><p>To have an overview of how the FLOPs of the pruned model evolves during training, we share the FLOPs dynamics (inference on single sample) of the pruned ResNet-50 during the course of training in <ref type="figure" target="#fig_9">Figure 15</ref>. While starting from a model with a higher number of FLOPs compared with GraNet (s i = 0.5), GraNet (s i = 0) is gradually sparsified towards a sparse structure with lower FLOPs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Unstructured Pruning: Pruning plasticity (see Section 3.1 for definition) under a 30epoch continued training with and without connection regeneration for ResNet-20 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Unstructured Pruning: Final performance gap between the unpruned models and the pruned models for ResNet-20 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. "Pre-trained Sparsity" refers to the original sparsity of the pre-trained networks before pruning. The pruning method is the magnitude global pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Reinitialization ablation on subnetworks discovered by GMP and GraNet (s i = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Structured Pruning: Pruning plasticity of under a 30-epochs continued training with and without connection regeneration for ResNet-20 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is uniform pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Structured Pruning: Final performance gap between the unpruned models and the pruned models for ResNet-20 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed.A.2 VGG-19 on CIFAR-10 with Structured Filter Pruning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Structured Pruning: Pruning plasticity of under a 30-epochs continued training with and without connection regeneration for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is uniform pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Structured Pruning: Final performance gap between the unpruned models and the pruned models for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed.A.3 ResNet-20 on CIFAR-10 with Unstructured Uniform Pruning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Unstructured Pruning: Pruning plasticity under a 30-epochs continued training with and without connection regeneration for ResNet-20 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Unstructured Pruning: Final performance gap between the unpruned models and the pruned models for ResNet-20 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is uniform pruning.A.4 VGG-19 on CIFAR-10 with Unstructured Global Pruning Unstructured Pruning: Pruning plasticity under a 30-epochs continued training with and without connection regeneration for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is global pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Unstructured Pruning: Final performance gap between the unpruned models and the pruned models for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is global pruning.A.5 VGG-19 on CIFAR-10 with Unstructured Uniform Pruning Unstructured Pruning: Pruning plasticity under a 30-epochs continued training with and without connection regeneration for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is uniform pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Unstructured Pruning: Final performance gap between the unpruned models and the pruned models for VGG-19 on CIFAR-10. The vertical red lines refer to the points when the learning rate is decayed. The pruning method is uniform pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 :</head><label>15</label><figDesc>FLOPs dynamics (inference on single sample) of GraNet with ResNet-50 on ImageNet during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the architectures and hyperparameters we study in this paper.</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell cols="2">#Epoch Batch Size</cell><cell>LR</cell><cell cols="3">LR Decay, Epoch Weight Decay Test Accuracy</cell></row><row><cell cols="2">ResNet-20 CIFAR-10</cell><cell>160</cell><cell>128</cell><cell>0.1 (? = 0.9)</cell><cell>10?, [80, 120]</cell><cell>0.0005</cell><cell>92.41?0.04</cell></row><row><cell>VGG-19</cell><cell>CIFAR-10 CIFAR-100</cell><cell>160 160</cell><cell>128 128</cell><cell>0.1 (? = 0.9) 0.1 (? = 0.9)</cell><cell>10?, [80, 120] 10?, [80, 120]</cell><cell>0.0005 0.0005</cell><cell>93.85?0.05 73.43?0.08</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>160</cell><cell>128</cell><cell>0.1 (? = 0.9)</cell><cell>10?, [80, 120]</cell><cell>0.0005</cell><cell>94.75?0.01</cell></row><row><cell>ResNet-50</cell><cell>CIFAR-100</cell><cell>160</cell><cell>128</cell><cell>0.1 (? = 0.9)</cell><cell>10?, [80, 120]</cell><cell>0.0005</cell><cell>78.23?0.18</cell></row><row><cell></cell><cell>ImageNet</cell><cell>100</cell><cell>64</cell><cell cols="2">0.1 (? = 0.9) 10?, [30, 60, 90]</cell><cell>0.0004</cell><cell>76.80?0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>38?0.11 93.06?0.09 91.98?0.09 73.13?0.28 72.14?0.15 69.82?0.09 GraNet (s i = 0.5) (ours) 93.73?0.08 93.66?0.07 93.38?0.15 73.30?0.13 73.18?0.31 72.04?0.13 45?0.43 93.86?0.25 93.26?0.22 76.50?0.33 76.03?0.34 75.06?0.27 GraNet (s i = 0.5) (ours) 94.64?0.27 94.38?0.28 94.01?0.23 77.89?0.33 77.16?0.52 77.14?0.45</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Pruning ratio</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell></row><row><cell>VGG-19 (Dense)</cell><cell>93.85?0.05</cell><cell>-</cell><cell>-</cell><cell>73.43?0.08</cell><cell>-</cell><cell>-</cell></row><row><cell>SNIP [31]</cell><cell>93.63</cell><cell>93.43</cell><cell>92.05</cell><cell>72.84</cell><cell>71.83</cell><cell>58.46</cell></row><row><cell>GraSP [67]</cell><cell>93.30</cell><cell>93.04</cell><cell>92.19</cell><cell>71.95</cell><cell>71.23</cell><cell>68.90</cell></row><row><cell>SynFlow [63]</cell><cell>93.35</cell><cell>93.45</cell><cell>92.24</cell><cell>71.77</cell><cell>71.72</cell><cell>70.94</cell></row><row><cell>Deep-R [3]</cell><cell>90.81</cell><cell>89.59</cell><cell>86.77</cell><cell>66.83</cell><cell>63.46</cell><cell>59.58</cell></row><row><cell>SET [44]</cell><cell>92.46</cell><cell>91.73</cell><cell>89.18</cell><cell>72.36</cell><cell>69.81</cell><cell>65.94</cell></row><row><cell cols="2">RigL [9] 93.STR [28] 93.73</cell><cell>93.27</cell><cell>92.21</cell><cell>71.93</cell><cell>71.14</cell><cell>69.89</cell></row><row><cell>SIS [66]</cell><cell>93.99</cell><cell>93.31</cell><cell>93.16</cell><cell>72.06</cell><cell>71.85</cell><cell>71.17</cell></row><row><cell>GMP [13]</cell><cell cols="6">93.26</cell></row><row><cell>ResNet-50 (Dense)</cell><cell>94.75?0.01</cell><cell>-</cell><cell>-</cell><cell>78.23?0.18</cell><cell>-</cell><cell>-</cell></row><row><cell>SNIP [31]</cell><cell>92.65</cell><cell>90.86</cell><cell>87.21</cell><cell>73.14</cell><cell>69.25</cell><cell>58.43</cell></row><row><cell>GraSP [67]</cell><cell>92.47</cell><cell>91.32</cell><cell>88.77</cell><cell>73.28</cell><cell>70.29</cell><cell>62.12</cell></row><row><cell>SynFlow [63]</cell><cell>92.49</cell><cell>91.22</cell><cell>88.82</cell><cell>73.37</cell><cell>70.37</cell><cell>62.17</cell></row><row><cell cols="2">RigL [9] 94.STR [28] 92.59</cell><cell>91.35</cell><cell>88.75</cell><cell>73.45</cell><cell>70.45</cell><cell>62.34</cell></row><row><cell>SIS [66]</cell><cell>92.81</cell><cell>91.69</cell><cell>90.11</cell><cell>73.81</cell><cell>70.62</cell><cell>62.75</cell></row><row><cell>GMP [13]</cell><cell>94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>59?0.10 93.58?0.07 93.52?0.03 73.10?0.12 72.30?0.15 72.07?0.37 GraNet (s i = 0) (ours) 93.80?0.10 93.72?0.11 93.63?0.08 73.74?0.30 73.10?0.04 72.35?0.34?0.09 94.52?0.08 94.19?0.04 76.91?0.23 76.42?0.51 75.58?0.20 GraNet (s i = 0) (ours) 94.49?0.08 94.44?0.01 94.34?0.17 77.29?0.45 76.71?0.26 76.10?0.20</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy of pruned ResNet-50 on ImageNet dataset. The best results of DST methods are marked as blue and the best results of pruning during training methods are marked in bold. The training/test FLOPs are normalized with the FLOPs of a dense model. s i refers to the initial sparsity of GraNet.</figDesc><table><row><cell>Method</cell><cell>Top-1</cell><cell>FLOPs</cell><cell>FLOPs</cell><cell>TOP-1</cell><cell>FLOPs</cell><cell>FLOPs</cell></row><row><cell></cell><cell>Accuracy</cell><cell>(Train)</cell><cell>(Test)</cell><cell>Accuracy</cell><cell>(Train)</cell><cell>(Test)</cell></row><row><cell>Dense</cell><cell cols="6">76.8?0.09 1x (3.2e18) 1x (8.2e9) 76.8?0.09 1x (3.2e18) 1x (8.2e9)</cell></row><row><cell>Pruning ratio</cell><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell>90%</cell><cell></cell></row><row><cell>Static (ERK)</cell><cell>72.1?0.04</cell><cell>0.42?</cell><cell>0.42?</cell><cell>67.7?0.12</cell><cell>0.24?</cell><cell>0.24?</cell></row><row><cell>Small-Dense</cell><cell>72.1?0.06</cell><cell>0.23?</cell><cell>0.23?</cell><cell>67.2?0.12</cell><cell>0.10?</cell><cell>0.10?</cell></row><row><cell>SNIP [31]</cell><cell>72.0?0.06</cell><cell>0.23?</cell><cell>0.23?</cell><cell>67.2?0.12</cell><cell>0.10?</cell><cell>0.10?</cell></row><row><cell>SET [44]</cell><cell>72.9?0.39</cell><cell>0.23?</cell><cell>0.23?</cell><cell>69.6?0.23</cell><cell>0.10?</cell><cell>0.10?</cell></row><row><cell>DSR[48]</cell><cell>73.3</cell><cell>0.40?</cell><cell>0.40?</cell><cell>71.6</cell><cell>0.30?</cell><cell>0.30?</cell></row><row><cell>RigL (ERK) [9]</cell><cell>75.1?0.05</cell><cell>0.42?</cell><cell>0.42?</cell><cell>73.0?0.04</cell><cell>0.25?</cell><cell>0.24?</cell></row><row><cell>SNFS (ERK) [8]</cell><cell>75.2?0.11</cell><cell>0.61?</cell><cell>0.42?</cell><cell>72.9?0.06</cell><cell>0.50?</cell><cell>0.24?</cell></row><row><cell>GraNet (s i = 0.5) (ours)</cell><cell>76.0</cell><cell>0.37?</cell><cell>0.35?</cell><cell>74.5</cell><cell>0.25?</cell><cell>0.20?</cell></row><row><cell>STR [28]</cell><cell>76.1</cell><cell>n/a</cell><cell>0.17?</cell><cell>74.0</cell><cell>n/a</cell><cell>0.08?</cell></row><row><cell>DPF [33]</cell><cell>75.1</cell><cell>0.71?</cell><cell>0.23?</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>GMP [13]</cell><cell>75.6</cell><cell>0.56?</cell><cell>0.23?</cell><cell>73.9</cell><cell>0.51?</cell><cell>0.10?</cell></row><row><cell>GraNet (s i = 0) (ours)</cell><cell>75.8</cell><cell>0.34?</cell><cell>0.28?</cell><cell>74.2</cell><cell>0.23?</cell><cell>0.16?</cell></row><row><cell>Small-Dense 5?</cell><cell>73.9</cell><cell>1.01 ?</cell><cell>0.20 ?</cell><cell>71.3</cell><cell>0.60 ?</cell><cell>0.12 ?</cell></row><row><cell>RigL-ITOP 2? [37]</cell><cell>76.9</cell><cell>0.84?</cell><cell>0.42?</cell><cell>75.5</cell><cell>0.50?</cell><cell>0.24?</cell></row><row><cell>RigL 5? [9]</cell><cell>77.1</cell><cell>2.09?</cell><cell>0.42?</cell><cell>76.4</cell><cell>1.23?</cell><cell>0.24?</cell></row><row><cell>GraNet 2.5? (s i = 0.5) (ours)</cell><cell>77.1</cell><cell>0.85?</cell><cell>0.34?</cell><cell>76.4</cell><cell>0.49?</cell><cell>0.20?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>si</cell><cell>s f</cell><cell>Top-1 [%]</cell><cell>FLOPs</cell><cell>FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>(Train)</cell><cell>(Test)</cell></row><row><cell>GraNet</cell><cell>0.0</cell><cell>0.9</cell><cell>74.2</cell><cell>0.23?</cell><cell>0.16?</cell></row><row><cell>GraNet</cell><cell>0.5</cell><cell>0.9</cell><cell>74.5</cell><cell>0.25?</cell><cell>0.20?</cell></row><row><cell>GraNet</cell><cell>0.6</cell><cell>0.9</cell><cell>74.4</cell><cell>0.25?</cell><cell>0.22?</cell></row><row><cell>GraNet</cell><cell>0.7</cell><cell>0.9</cell><cell>74.2</cell><cell>0.24?</cell><cell>0.22?</cell></row><row><cell>GraNet</cell><cell>0.8</cell><cell>0.9</cell><cell>74.1</cell><cell>0.25?</cell><cell>0.24?</cell></row><row><cell>RigL</cell><cell>0.9</cell><cell>0.9</cell><cell>73.0</cell><cell>0.25?</cell><cell>0.24?</cell></row></table><note>Effect of the initial sparsity on GraNet with ResNet-50 on ImageNet. The training/test FLOPs are normalized with the FLOPs of a dense model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between GraNet and RigL at extreme sparsities with ResNet-50 on Ima-geNet. The training/test FLOPs are normalized with the FLOPs of a dense model.</figDesc><table><row><cell>Method</cell><cell>si</cell><cell>s f</cell><cell>Top-1 [%]</cell><cell>FLOPs</cell><cell>FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>(Train)</cell><cell>(Test)</cell></row><row><cell>RigL</cell><cell>0.8</cell><cell>0.8</cell><cell>75.1</cell><cell>0.42?</cell><cell>0.42?</cell></row><row><cell>GraNet</cell><cell>0.5</cell><cell>0.8</cell><cell>76.0</cell><cell>0.37?</cell><cell>0.35?</cell></row><row><cell>RigL</cell><cell>0.9</cell><cell>0.9</cell><cell>73.0</cell><cell>0.25?</cell><cell>0.24?</cell></row><row><cell>GraNet</cell><cell>0.5</cell><cell>0.9</cell><cell>74.5</cell><cell>0.25?</cell><cell>0.20?</cell></row><row><cell>RigL</cell><cell>0.95</cell><cell>0.95</cell><cell>69.7</cell><cell>0.12?</cell><cell>0.12?</cell></row><row><cell>GraNet</cell><cell>0.5</cell><cell>0.95</cell><cell>72.3</cell><cell>0.17?</cell><cell>0.12?</cell></row><row><cell>RigL</cell><cell>0.965</cell><cell>0.965</cell><cell>67.2</cell><cell>0.11?</cell><cell>0.11?</cell></row><row><cell>GraNet</cell><cell>0.5</cell><cell>0.965</cell><cell>70.5</cell><cell>0.15?</cell><cell>0.09?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Effects of LTH and LRR on the subnetworks learned by GraNet. Methods with 2 ? refer to extending the training steps by 2 times. The results are reported with top-1 test accuracy [%]. 80?0.10 93.72?0.11 93.63?0.08 73.74?0.30 73.10?0.04 72.35?0.26 + Lottery Ticket Hypothesis 93.63?0.04 93.29?0.05 92.46?0.08 72.97?0.25 71.76?0.22 69.28?0.36 + Learning Rate Rewinding 93.84?0.14 93.72?0.06 93.53?0.04 73.71?0.08 73.24?0.24 72.50?0.26 GraNet 2? (s i = 0) 94.17?0.03 93.98?0.07 93.94?0.11 74.80?0.29 73.65?0.32 73.63?0.05 49?0.08 94.44?0.01 94.34?0.17 77.29?0.45 76.71?0.26 76.10?0.20 + Lottery Ticket Hypothesis 93.96?0.10 93.70?0.15 92.94?0.14 75.74?0.19 74.31?0.10 71.99?0.08 + Learning Rate Rewinding 94.55?0.13 94.39?0.13 94.20?0.25 77.40?0.14 76.90?0.19 75.75?0.25 GraNet 2? (s i = 0) 95.09?0.15 94.84?0.11 94.69?0.24 78.18?0.20 78.17?0.20 77.15?0.29</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Pruning ratio</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell></row><row><cell>VGG-19 (Dense)</cell><cell>93.85?0.05</cell><cell>-</cell><cell>-</cell><cell>73.43?0.08</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">GraNet (s i = 0) 93.ResNet-50 (Dense) 94.75?0.01</cell><cell>-</cell><cell>-</cell><cell>78.23?0.18</cell><cell>-</cell><cell>-</cell></row><row><cell>GraNet (s i = 0)</cell><cell>94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Algorithm 1The pseudocode of GraNet.Require: Model weights W ? R d , initial sparsity s i , target sparsity s f , gradual pruning starting point t 0 , gradual pruning end point t f , gradual pruning frequency ?T . 1: W ? randomly initialize W with initial sparsity s i 2: for each training step t do ? t ? t f and (t mod ?T) == 0 then</figDesc><table><row><cell>3:</cell><cell>training W ? SGD(W )</cell></row><row><cell cols="2">4: if t o 5: gradual pruning with the pruning rate produced by Eq. 1</cell></row><row><cell>6:</cell><cell>zero-cost neuroregeneration with Eq. 2 and Eq. 3</cell></row><row><cell>7:</cell><cell>end if</cell></row><row><cell cols="2">8: end for</cell></row><row><cell cols="2">B.2 Hyperparameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Experiment hyperparameters of GraNet used in this paper. Learning Rate (LR), Batch Size (BS), Epochs, Learning Rate Drop (LR Drop), Weight Decay (WD), Sparse Initialization (Sparse Init), Gradual Pruning Frequency (?T ), Initial Sparsity (s i ), Starting Epoch of Gradual Pruning (t 0 ), End Epoch of Gradual Pruning (t f ), Initial Neuroregeneration Ratio (r), Neuroregeneration Ratio (r Sche), etc. The implementation used in the paper is modified based on the open-source code of Sparse Momentum repository 2 introduced by [8]. We added VGG-19 with batchnorm from the GraSP repository 3 . The code for calculating the inference FLOPs of ResNet-50 on ImageNet is modified based on the opensource code provided in the rethinking-network-pruning repository 4 . For the training FLOPs, we follow the way of approximating the training FLOPs of RigL</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell>Methods</cell><cell cols="5">LR BS Epochs LR Drop, Epochs WD Sparse Init</cell><cell></cell><cell>Gradual Pruning</cell><cell>Neuroregeneration</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?T</cell><cell>si</cell><cell>t0</cell><cell>tf</cell><cell>r</cell><cell>r Sche</cell></row><row><cell>VGG-19</cell><cell cols="3">CIFAR-10/100 dense-to-sparse 0.1 128 CIFAR-10/100 sparse-to-sparse 0.1 128</cell><cell>160 160</cell><cell>10x, [80, 120] 10x, [80, 120]</cell><cell>5e-4 5e-4</cell><cell>Dense ERK</cell><cell cols="2">1000 1000 0.5 0 epoch 80 epoch 0.5 0 0 epoch 110 epoch 0.5</cell><cell>Cosine Cosine</cell></row><row><cell></cell><cell cols="3">CIFAR-10/100 dense-to-sparse 0.1 128</cell><cell>160</cell><cell>10x, [80, 120]</cell><cell>5e-4</cell><cell>Dense</cell><cell>1000</cell><cell>0</cell><cell>0 epoch 110 epoch 0.5</cell><cell>Cosine</cell></row><row><cell>ResNet-50</cell><cell cols="3">CIFAR-10/100 sparse-to-sparse 0.1 128 ImageNet dense-to-sparse 0.1 64</cell><cell>160 100</cell><cell cols="2">10x, [80, 120] 10x, [30, 60, 90] 1e-4 5e-4</cell><cell>ERK Dense</cell><cell cols="2">1000 0.5 0 epoch 80 epoch 0.5 4000 0 0 epoch 30 epoch 0.5</cell><cell>Cosine Cosine</cell></row><row><cell></cell><cell>ImageNet</cell><cell cols="2">sparse-to-sparse 0.1 64</cell><cell>100</cell><cell cols="2">10x, [30, 60, 90] 1e-4</cell><cell>ERK</cell><cell cols="2">4000 0.5 0 epoch 30 epoch 0.5</cell><cell>Cosine</cell></row><row><cell cols="3">B.3 Implementation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Algorithm 2The pseudocode of GMP.Require: Model weights W ? R d , initial sparsity s i , target sparsity s f , gradual pruning starting point t 0 , gradual pruning end point t f , gradual pruning frequency ?T . 1: W ? randomly initialize W with initial sparsity s i 2: for each training step t do ? t ? t f and (t mod ?T) == 0 then</figDesc><table><row><cell>3:</cell><cell>training W ? SGD(W )</cell></row><row><cell cols="2">4: if t o 5: gradual pruning with the pruning rate produced by Eq. 1</cell></row><row><cell>6:</cell><cell>end if</cell></row><row><cell cols="2">7: end for</cell></row><row><cell cols="2">C.2 Hyperparameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Test accuracy of GMP ResNet-50 on ImageNet dataset with our own implementation.</figDesc><table><row><cell>Method</cell><cell>Top-1</cell><cell>FLOPs</cell><cell>FLOPs</cell><cell>TOP-1</cell><cell cols="2">FLOPs FLOPs</cell></row><row><cell></cell><cell>Accuracy</cell><cell>(Train)</cell><cell>(Test)</cell><cell cols="3">Accuracy (Train) (Train)</cell></row><row><cell>Dense</cell><cell cols="3">76.8?0.09 1x (3.2e18) 1x (8.2e9)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>S = 0.8</cell><cell></cell><cell></cell><cell>S = 0.9</cell><cell></cell></row><row><cell>GMP [13]</cell><cell>75.6</cell><cell>0.56?</cell><cell>0.23?</cell><cell>73.9</cell><cell>0.51?</cell><cell>0.10?</cell></row><row><cell>GMP (our implementation)</cell><cell>74.6</cell><cell>0.34?</cell><cell>0.28?</cell><cell>73.3</cell><cell>0.23?</cell><cell>0.16?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy of pruned VGG-19 and ResNet32 on CIFAR-10 and CIFAR-100 datasets using uniform pruning. 28?0.04 92.76?0.10 91.64?0.18 71.88?0.29 71.02?0.27 66.16?0.23 GraNet (s i = 0) 93.12?0.03 92.88?0.08 91.87?0.06 72.37?0.01 71.48?0.25 70.14?0.18 08?0.14 94.20?0.24 93.66?0.44 77.30?0.27 76.77?0.02 75.38?0.24 GraNet (s i = 0) 94.19?0.23 94.16?0.26 93.64?0.25 77.57?0.12 77.15?0.18 76.17?0.15 D ResNet-50 Learnt Budgets and Backbone Sparsities</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Pruning ratio</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell><cell>90%</cell><cell>95%</cell><cell>98%</cell></row><row><cell cols="2">VGG-19 (Dense) 93.85?0.05</cell><cell>-</cell><cell>-</cell><cell>73.43?0.08</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">GMP [13] 93.ResNet-50 (Dense) 94.75?0.01</cell><cell>-</cell><cell>-</cell><cell>78.23?0.18</cell><cell>-</cell><cell>-</cell></row><row><cell>GMP [13]</cell><cell>94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>ResNet-50 Learnt Budgets and Backbone Sparsities at Sparsity 0.9</figDesc><table><row><cell>Metric</cell><cell>Fully Dense Params</cell><cell>Fully Dense FLOPs</cell><cell cols="6">Sparsity (%) GraNet (si = 0) GraNet (si = 0.5) STR Uniform ERK SNFS</cell><cell>VD</cell><cell>GS</cell></row><row><cell>Overall</cell><cell cols="2">25502912 8178569216</cell><cell>89.99</cell><cell>89.98</cell><cell>90.23</cell><cell>90.00</cell><cell cols="4">90.07 90.06 90.27 89.54</cell></row><row><cell>Backbone</cell><cell cols="2">23454912 8174272512</cell><cell>89.89</cell><cell>90.65</cell><cell>92.47</cell><cell>90.00</cell><cell cols="4">89.82 89.44 91.41 90.95</cell></row><row><cell>Layer 1 -conv1</cell><cell>9408</cell><cell>118013952</cell><cell>53.50</cell><cell>40.60</cell><cell>59.80</cell><cell>90.00</cell><cell>58.00</cell><cell>2.50</cell><cell cols="2">31.39 35.11</cell></row><row><cell>Layer 2 -layer1.0.conv1</cell><cell>4096</cell><cell>236027904</cell><cell>54.60</cell><cell>43.40</cell><cell>83.28</cell><cell>90.00</cell><cell>0.00</cell><cell>2.50</cell><cell cols="2">39.50 56.05</cell></row><row><cell>Layer 3 -layer1.0.conv2</cell><cell>36864</cell><cell>231211008</cell><cell>78.80</cell><cell>64.50</cell><cell>89.48</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell cols="2">67.87 75.04</cell></row><row><cell>Layer 4 -layer1.0.conv3</cell><cell>16384</cell><cell>102760448</cell><cell>78.00</cell><cell>67.40</cell><cell>85.80</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">64.87 70.31</cell></row><row><cell>Layer 5 -layer1.0.downsample.0</cell><cell>16384</cell><cell>102760448</cell><cell>79.30</cell><cell>72.90</cell><cell>83.34</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">60.38 66.88</cell></row><row><cell>Layer 6 -layer1.1.conv1</cell><cell>16384</cell><cell>102760448</cell><cell>76.60</cell><cell>67.30</cell><cell>89.89</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">61.35 75.09</cell></row><row><cell>Layer 7 -layer1.1.conv2</cell><cell>36864</cell><cell>231211008</cell><cell>76.70</cell><cell>62.10</cell><cell>90.60</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell cols="2">64.38 80.42</cell></row><row><cell>Layer 8 -layer1.1.conv3</cell><cell>16384</cell><cell>102760448</cell><cell>74.10</cell><cell>54.50</cell><cell>91.70</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">65.83 80.00</cell></row><row><cell>Layer 9 -layer1.2.conv1</cell><cell>16384</cell><cell>102760448</cell><cell>72.20</cell><cell>58.80</cell><cell>88.07</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">68.75 75.21</cell></row><row><cell>Layer 10 -layer1.2.conv2</cell><cell>36864</cell><cell>231211008</cell><cell>72.70</cell><cell>58.50</cell><cell>87.03</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell cols="2">70.86 74.95</cell></row><row><cell>Layer 11 -layer1.2.conv3</cell><cell>16384</cell><cell>102760448</cell><cell>73.20</cell><cell>57.30</cell><cell>90.99</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell cols="2">54.05 79.28</cell></row><row><cell>Layer 12 -layer2.0.conv1</cell><cell>32768</cell><cell>205520896</cell><cell>68.30</cell><cell>49.30</cell><cell>85.95</cell><cell>90.00</cell><cell>43.00</cell><cell>2.50</cell><cell cols="2">57.10 70.89</cell></row><row><cell>Layer 13 -layer2.0.conv2</cell><cell>147456</cell><cell>231211008</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/TimDettmers/sparse_learning 3 https://github.com/alecwangcq/GraSP 4 https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/ weight-level/compute_flops.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00560</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The generalization-stability tradeoff in neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bartoldson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20852" to="20864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep rewiring: Training very sparse deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05136</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="16306" to="16316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive skeletonization: Trimming more fat from a network at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<idno>arXiv:cs.CV/2006.09081</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.0736</idno>
	</analytic>
	<monogr>
		<title level="m">Twenty-eighth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2943" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pruning neural networks at initialization: Why are we missing the mark?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08576</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01611</idno>
		<title level="m">The lottery ticket hypothesis at scale</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10901</idno>
		<title level="m">Sparse gpu kernels for deep learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of energy consumption in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garc?a-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="75" to="88" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A unified paths perspective for pruning at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schrater</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10552</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05517</idno>
		<title level="m">Network decoupling: From regular to depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">241</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Island</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08124</idno>
		<title level="m">Accelerated sparse neural training: A provable and efficient method to find n: M transposable masks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pruning versus clipping in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Janowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6600</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Top-kast: Top-k always sparse training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20744" to="20754" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Principles of neural science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Jessell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siegelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hudspeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5544" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A signal propagation perspective for pruning neural networks at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06307</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Snip: Single-shot network pruning based on connection sensitivity. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R R</forename><surname>Matavalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2589" to="2604" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selfish sparse rnn training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6893" to="6904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Do we actually need dense overparameterization? in-time over-parameterization in sparse training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6989" to="7000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding trainable sparse networks through neural tangent transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6336" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<title level="m">Learning sparse neural networks through l_0 regularization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Intrinsic mechanisms of neuronal axon regeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cavalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="323" to="337" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recovering from random pruning: On the plasticity of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="848" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Curci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Vale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01636</idno>
		<title level="m">Sparse training theory for scalable and efficient agents. International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2383</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2498" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Skeletonization: A technique for trimming the fat from a network via relevance assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="107" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using relevance to reduce network size automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neuroregeneration and plasticity: a review of the physiological mechanisms for achieving functional recovery postinjury</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Military Medical Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Nvidia a100 tensor core gpu architecture</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-M</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02389</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green Ai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10597</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10451</idno>
		<title level="m">Cpot: Channel pruning via optimal transport</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Woodfisher: Efficient second-order approximation for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scalable distributed dnn training using commodity gpu cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Forr?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00896</idno>
		<title level="m">Pruning via iterative ranking of sensitivity statistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparsifying networks via subdifferential inclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>M. Meila and T. Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Discovering neural wirings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Autoprune: Automatic network pruning by regularizing auxiliary parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02402</idno>
		<title level="m">Trained rank pruning for efficient deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Procrustes: a dataflow and accelerator for sparse deep neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghasemazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="711" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Glial inhibition of cns axon regeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="617" to="627" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11957</idno>
		<title level="m">Drawing early-bird tickets: Towards more efficient training of deep networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A block decomposition algorithm for sparse optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning n:m fine-grained structured sparse neural networks from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: Exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno>2018. 77.50 69.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
