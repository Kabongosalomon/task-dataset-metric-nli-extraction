<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DKMA-ULD: Domain Knowledge augmented Multi-head Attention based Robust Universal Lesion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Sheoran</surname></persName>
							<email>manu.sheoran@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghal</forename><surname>Dani</surname></persName>
							<email>dani.meghal@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Sharma</surname></persName>
							<email>monika.sharma1@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
							<email>lovekesh.vig@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DKMA-ULD: Domain Knowledge augmented Multi-head Attention based Robust Universal Lesion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>DKMA-ULD: ROBUST UNIVERSAL LESION DETECTOR 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Incorporating data-specific domain knowledge in deep networks explicitly can provide important cues beneficial for lesion detection and can mitigate the need for diverse heterogeneous datasets for learning robust detectors. In this paper, we exploit the domain information present in computed tomography (CT) scans and propose a robust universal lesion detection (ULD) network that can detect lesions across all organs of the body by training on a single dataset, DeepLesion. We analyze CT-slices of varying intensities, generated using heuristically determined Hounsfield Unit (HU) windows that individually highlight different organs and are given as inputs to the deep network. The features obtained from the multiple intensity images are fused using a novel convolution augmented multi-head self-attention module and subsequently, passed to a Region Proposal Network (RPN) for lesion detection. In addition, we observed that traditional anchor boxes used in RPN for natural images are not suitable for lesion sizes often found in medical images. Therefore, we propose to use lesion-specific anchor sizes and ratios in the RPN for improving the detection performance. We use self-supervision to initialize weights of our network on the DeepLesion dataset to further imbibe domain knowledge. Our proposed Domain Knowledge augmented Multi-head Attention based Universal Lesion Detection Network DMKA-ULD produces refined and precise bounding boxes around lesions across different organs. We evaluate the efficacy of our network on the publicly available DeepLesion dataset which comprises of approximately 32K CT scans with annotated lesions across all organs of the body. Results demonstrate that we outperform existing state-of-the-art methods achieving an overall sensitivity of 87.16%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>detection solutions that focus on specific organs such as liver, kidney, and lungs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b36">36]</ref>. However, to address the clinical necessity where radiologists are required to locate different types of lesions present in various organs of the body to diagnose patients and determine treatment, developing a universal lesion detection (ULD) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38]</ref> model has become an active area of research. Tang et. al <ref type="bibr" target="#b22">[23]</ref> proposed ULDor based on Mask-RCNN for lesion detection and a hard negative mining (HNEM) strategy to reduce false positives. However, the proposed HNEM technique may not enhance detection performance due to missing annotations as the mined negatives may actually contain positives. There are a few more impressive RCNN based ULD networks that use weights pre-trained on Imagenet for detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref>. We also found from earlier methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b32">32]</ref> that utilizing neighboring slice information is essential for providing 3D context to the network which gives a lift in lesion detection accuracy. This is due to the fact that clinicians look at multiple slices of a patient's CT scan to confirm the final diagnosis.</p><p>There also exist attention-based ULD networks where attention has been shown to improve the lesion detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b37">37]</ref> by enabling the network to focus on important regions of CT-scans. MVP-Net <ref type="bibr" target="#b12">[13]</ref> proposed to use a position-aware attention module to aggregate features from a multi-view feature pyramid network. Another work on ULD by Wang et al. <ref type="bibr" target="#b29">[29]</ref>, proposed volumetric attention which exploits 3D-context from multi-slice image inputs and a 2.5D network for improving the detection performance. The multi-task universal lesion analysis network (MULAN) <ref type="bibr" target="#b34">[34]</ref> utilizes 27 slices as input and proposes a 3D feature fusion strategy with Mask-RCNN backbone for lesion detection. In addition, they jointly train the network to perform lesion segmentation and tagging.</p><p>Typically, deep networks are reliant on high-volume datasets for automatically discovering relevant features for a learning task. However, due to the very similar appearance of lesions and other internal structures in CT scans, lesion detection is quite a challenging problem. Yan et al. <ref type="bibr" target="#b31">[31]</ref> proposed a Lesion ENSemble (LENS) network for lesion detection that can efficiently learn from heterogeneous lesion datasets and address the issue of missing annotations by exploiting clinical prior knowledge and cross-dataset knowledge transfer. In another paper <ref type="bibr" target="#b30">[30]</ref>, authors have proposed a MELD network for lesion detection which learns from multiple heterogeneous diverse datasets and uses missing annotation matching (MAM) and negative region mining (NRM) for achieving state-of-the-art lesion detection performance on DeepLesion <ref type="bibr" target="#b33">[33]</ref> dataset. In summary, previous works on ULD have made use of 3D-context in the form of multi-slice inputs, incorporation of attention mechanisms, multi-task learning, hard negative mining techniques, and multiple heterogeneous datasets to enhance the lesion detection sensitivity performance.</p><p>Rather than using a variety of heterogeneous datasets to learn robust representations for ULD, we claim that significant improvements to learning can be obtained by incorporating task-specific domain knowledge in the network. This motivates us to extract as many domain-specific features as possible from a minimal number of CT-slices for a particular patient, to come up with a computationally efficient ULD with enhanced prediction performance. Therefore, in our proposed lesion detector, we utilize only 3 slices from a patient's CT scan to incorporate 3D context in the network. Next, taking cues from MVP-Net <ref type="bibr" target="#b12">[13]</ref>, we utilize the information of tissue density from CT-scans represented as HU-values in terms of window width and window length. During manual analysis, radiologists adjust these windows to focus on organs/tissues of interest <ref type="bibr">[2]</ref>. Despite having critical importance in lesion detection, the use of multiple HU windows of CT-slices, representing different organs of the body, has been overlooked in the literature. To this end, we introduce 5 novel heuristically determined HU windows for CT-slices and feed them as multi-intensity input to the detec-  of a patient, is pre-processed and used for generating 5 multi-intensity images using 5 different HU windows (U 1 to U 5 ). Using a shared convolutional feature extractor (having domain-specific weight initialization) with 5 FPN levels (P 2 to P 6 ), we obtain 5 feature map blocks F Ui , (b) Using our proposed attention based feature fusion module inspired from transformer's multi-head self-attention <ref type="bibr" target="#b27">[27]</ref>, feature map sub-levels (P j ? F Ui ) with same resolution of (H,W ) are fused into a single sub-level for obtaining the final fused feature map block (F ). Here V, K, Q and N h represent value, key, query matrix and number of attention heads and finally, (c) RPN with custom lesion-specific anchors is applied over F for improved lesion detection. tion network to make it organ-agnostic. Further, the computed features are combined using our novel convolution augmented multi-head attention-based fusion architecture. We use transformers <ref type="bibr">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">27]</ref> based self-attention mechanism for feature-fusion of multi-intensity images that effectively combines multi-organ information efficiently. This is analogous to the radiologists' way of paying attention to different organs at different HU windows of CTslices simultaneously while detecting lesions. Additionally, we have observed that default anchor sizes and ratios used in general object detection networks do not perform satisfactorily for lesions of different sizes, particularly for very small (&lt; 10mm) and medium-sized (10 ? 30mm) lesions. Therefore, we propose new anchor sizes and ratios for RPN that enable the network to detect lesions of varied sizes mostly found in medical imaging datasets. We named our network DKMA-ULD (Domain-knowledge augmented multi-attention based universal lesion detection).</p><p>Furthermore, observing that self-supervised learning (SSL) techniques have recently become the cornerstone for learning improved representations in data-scarce scenarios which can, subsequently, be used for efficient learning on downstream tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">26]</ref>. We utilize Bootstrap Your Own Latent (BYOL) <ref type="bibr" target="#b8">[9]</ref>, a SSL technique to learn weights of the backbone network of DKMA-ULD on the DeepLesion <ref type="bibr" target="#b33">[33]</ref> dataset. The DeepLesion dataset consists of approximately 32K CT scans with annotated lesions across different organs of the body. Subsequently, the weights learned via SSL are used for initializing our DKMA-ULD so that it is able to learn robust domain-specific representations resulting in improved sensitivity. To summarize, we make the following contributions in the paper:</p><p>? We propose a domain-knowledge augmented deep network with multi-head self-attention based feature-fusion named DKMA-ULD which performs robust detection of lesions across all organs of the body and is trained on DeepLesion <ref type="bibr" target="#b33">[33]</ref> dataset. ? We introduce 5 novel HU windows, computed in an unsupervised manner, for highlighting the different organs of the body in CT -scans which make our network organagnostic. ? We propose a novel convolution augmented multi-head self-attention mechanism for the fusion of the features obtained from multiple intensity CT-slices for subsequent detection by an RPN. ? We propose lesion-specific new anchor sizes and ratios for detection that cover various sizes of lesions present in medical images. We also illustrate that these new anchors can detect very small and medium-sized lesions effectively. Hence, giving a boost to the overall detection sensitivity. ? We demonstrate that initializing DKMA-ULD with self-supervised domain-specific weights is beneficial for learning improved representations over Imagenet weights. ? We evaluate DKMA-ULD on DeepLesion dataset and show improvement over existing state-of-the-methods of ULD such as MULAN <ref type="bibr" target="#b34">[34]</ref>, 3DCE <ref type="bibr" target="#b32">[32]</ref>, MVP-Net <ref type="bibr" target="#b12">[13]</ref>, MELD <ref type="bibr" target="#b30">[30]</ref> and improved RetinaNet <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Details</head><p>DeepLesion <ref type="bibr" target="#b33">[33]</ref> is the largest publicly available repository of CT-slices with annotated lesions across different organs of the body released by the National Institutes of Health (NIH). It consists of data from 4, 427 unique patients based on markings performed by radiologists during their routine work. There are approximately 32, 120 axial CT slices from 10, 594 CT studies of the patients having around 1-3 lesions annotated per CT -scan. The lesions in each image have annotations such as bounding-box coordinates and size measurements, etc. which add up to 32, 735 lesions altogether from eight different body organs including bone, abdomen, mediastinum, liver, lung, kidney, soft-tissue, and pelvis. We use the official split of the DeepLesion dataset for training and evaluation of our proposed DKMA-ULD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method: DKMA-ULD</head><p>DKMA-ULD method, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, consists of different modules such as preprocessing, multi-intensity image generation using five different HU windows, convolutional feature extraction backbone, multi-head self-attention based feature-fusion, lesion-specific anchors, and self-supervision. These modules are discussed in detail as follows:</p><p>? Pre-processing: Typically, clinicians observe multiple adjacent slices of a patient's CT scan to confirm the diagnosis of a pathology. In order to provide 3D context of a patient's CT-scan to the network, we utilize its 3 slices (key slice with one superior  and one inferior neighboring slice) to generate 3-channel image. First, we remove black borders of the CT-slices for computational efficiency and to focus on region-ofinterest. Next, we normalize and clip the 12-bit intensity values of a CT slice using a HU window and then, re-scale them to floating-point values in the range [0.0, 255.0]. Subsequently, we re-sample all the CT-slices to a common resolution of 0.8 ? 0.8 ? 2 mm 3 . In addition, since the DeepLesion dataset does not have segmentation masks for the lesions, we generate pseudo masks using provided RECIST diameter measurements <ref type="bibr" target="#b7">[8]</ref>. These pseudo masks boost the performance of Mask-RCNN <ref type="bibr" target="#b9">[10]</ref> by adding a branch for predicting segmentation masks on region proposals generated by RPN. We also augment the data during training using random affine transformations such as horizontal and vertical flips, resizing with a ratio of 0.8 to 1.2, and translation of (?8, 8) pixels in x and y direction.</p><p>? Multiple Intensity Image Generation: In general, the intensity of a CT-slice is rescaled using a certain HU window, U (e.g., a single and wide window of [1024, 4096]) in order to include gray-scale intensities of different organs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">32]</ref>. However, using a single window suppresses organ-specific information resulting into a degenerated image-contrast, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), which in turn makes it hard for the network to learn to focus on various organs present in the given CT volume. During manual detection of lesions, radiologists adjust these intensity values to focus on organs/tissue of interest <ref type="bibr">[2]</ref>. We exploit this domain knowledge and propose to feed it to the deep network explicitly in the form of CT-slices having multiple intensities which highlight different organs of the body. In a previous method by Zihao Li et al. <ref type="bibr" target="#b12">[13]</ref>, a clustering algorithm is used to determine three HU windows. In this paper, we incorporate this multi-organ information in input CT-slices by introducing five novel HU windows which are determined in such a way that the major body organs are covered. The proposed HU windows, as inspired by Masoudi et al. <ref type="bibr" target="#b16">[17]</ref>, which cover almost all organs of interest for radiologists are: ? Convolution Feature Extraction Backbone: Now, for a given patient, 5 multiple intensity images each having 3 slices/channels, are passed as input to the ResNeXt-152 shared backbone with feature pyramid network (FPN) <ref type="bibr" target="#b13">[14]</ref> based convolutional feature extractor. The fact that applying pooling layers of CNN repeatedly on an image can filter out information of small objects due to downsampling, hence, resulting in missing small and medium-sized lesions in radiological scans. Therefore, we utilize FPN where shallow and deeper feature maps are more suitable for detecting small/medium and larger lesions, respectively. As a result, for a given input, we obtain 5 feature-map blocks (F Ui ) corresponding to 5 FPN levels, each having 5 feature map sub-levels (P j ) of dimension (256, H,W ) j , where H and W represent height and width of the featuremap and j = 2, ..., 6 are the pyramid-levels. These extracted feature maps at different FPN levels, each having a different resolution allows RPN to effectively focus on lesions of different sizes.</p><formula xml:id="formula_0">U 1 = [400,</formula><p>? Convolution augmented Multi-head Attention for Feature Fusion: Earlier ULD techniques such as MULAN <ref type="bibr" target="#b34">[34]</ref> incorporated information from multiple slices in their network by fusing the feature maps of all 3-channel images/slices with a convolution layer to obtain a 3D-context-enhanced feature map for the central slice. In this work, we propose a novel convolution augmented multi-head attention-based feature fusion module. Recently, vision transformers <ref type="bibr">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">25]</ref> have achieved state-of-theart results on various machine vision tasks via a focus on self-attention <ref type="bibr" target="#b27">[27]</ref>. The use of multi-head self-attention enables the model to attend jointly to both spatial and feature sub-spaces. As shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, we first concatenate sub-level feature maps (256, H,W ) j of 5 different intensities to obtain feature-vectors of shape (256 * 5, H,W ) j . These features are, subsequently, passed to two parallel branches namely, the 2D convolution layer and transformer's multi-head self-attention <ref type="bibr" target="#b6">[7]</ref>. Finally, their outputs are fused using concatenation. Since the output depth of the attention module is dependent on the depth of its "values" matrix (dv), the output depth of 2D convolution branch is kept such that the depth of the final feature vector obtained after concatenation of both the outputs is 256. Similar attention-based feature fusion is used at all 5 feature-map sub-levels and finally, we obtain a fused feature map block (F , with 5 feature-maps sub-levels) for later processing. To minimize computation overhead for attention, we use 2 attention heads (N h ) and keep the depth of values matrix as 4. In addition, we use 20 dimensions per head for key and query matrix <ref type="bibr">[3]</ref>.</p><p>? Lesion-specific Anchors: Next, for extracting Regions of Interest (ROI) from the obtained feature maps from 5 FPN levels, anchor boxes play a very crucial role. We observed that the small lesions are hard to detect using default anchor sizes and ratios <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>  map sub-levels, respectively. These lesion-specific anchors are used in RPN for RoI extraction, which are combined with feature maps using RoI pooling layer and further used, for predicting bounding-boxes around lesions along with probability values, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We demonstrate that the custom anchors allow us to cover variedsized lesions and more specifically, improve the detection of small-sized (&lt; 10mm) and medium-sized (10 ? 30mm) lesions considerably, as evident in <ref type="figure">Figure 3</ref>(a).</p><p>? Self supervision: The idea behind self-supervised learning (SSL) is that the learned intermediate representations can carry better semantic and structural meanings and can prove to be beneficial for a variety of downstream tasks. In order to make our DKMA-ULD more robust, we utilize a state-of-the-art SSL technique called BYOL <ref type="bibr" target="#b8">[9]</ref>. It relies on two neural networks, referred to as online and target networks, that interact and learn from each other. The target network (parameterized by ? ) has the same architecture as the online one (parameterized by ? ), but with polyak averaged weights, ? ? ?? + (1 ? ?)? . The goal is to learn a representation y that can be used in downstream tasks. Generally, the detection networks are initialized using weights pre-trained on Imagenet consisting of natural images and may not be effective for the medical imaging domain. Therefore, we propose domain-specific weights, obtained by training the backbone using SSL over train-split (23K images) of the DeepLesion dataset, for initializing DKMA-ULD to obtain enhanced performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results and Discussions</head><p>We use the official data split of DeepLesion <ref type="bibr" target="#b33">[33]</ref> dataset which consists of 70%, 15%, 15% for training, validation, and test, respectively. Please note that the DeepLesion test-set includes only key CT-slices and may contain missing annotations. The lesion detection is classified as true positive (TP) when the IoU between the predicted and the ground-truth bounding-box is larger than 0.5. We report average sensitivity, computed at 0.5, 1, 2, and 4 false-positives (FP) per image, as the evaluation metric on the standard test-set split for the fair comparison. Training: The proposed DKMA-ULD is trained on 3 channel CT images of size 512 ? 512 with a batch size of 4 on a single NVIDIA Tesla V100 having 32GB GPU-memory. We use cross-entropy and smooth 1 loss for classification and bounding-box regression, respectively. The model is trained until convergence using SGD optimizer with a learning rate (LR) and decay-factor of 0.02 and 10, respectively. The SSL model is trained using cross-entropy loss with a batch size of 64, Adam optimizer <ref type="bibr" target="#b10">[11]</ref>, and LR of 3e ? 4 for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions:</head><p>We evaluate the performance of our DKMA-ULD against previous methods in the literature, as shown in <ref type="table" target="#tab_7">Table 1</ref>. Our experiments demonstrate that by using only 3 slices per patient, the proposed method DKMA-ULD outperforms all the previous state-of-the-art ULD methods at different FP per image and achieves an average sensitivity of 86.88% when Imagenet pre-trained weights are used for the backbone initialization.</p><p>Here, one important point to note is that the base model of MELD <ref type="bibr" target="#b30">[30]</ref> (refer row 6 of Table 1) achieves an average sensitivity of 85.90% by training on 4 heterogeneous datasets, namely LUNA <ref type="bibr" target="#b19">[20]</ref>, LiTS <ref type="bibr">[4]</ref>, NIH-Lymph <ref type="bibr">[1]</ref> and DeepLesion <ref type="bibr" target="#b33">[33]</ref>. On the other hand, our proposed DKMA-ULD base model w/o self-supervision, despite being trained only on DeepLesion train-set, beats MELD on Deeplesion test-set by achieving an average sensitivity of 86.88%. Therefore, it is evident that the introduction of domain-specific features and multi-head attention-based feature fusion have enabled DKMA-ULD to learn robust representations. Hence, it validates our claim that domain knowledge can alleviate the requirement of a set of diverse datasets for learning good representations in medical imaging analysis. Furthermore, we observe that previous methods such as MULAN <ref type="bibr" target="#b34">[34]</ref> and MELD <ref type="bibr" target="#b30">[30]</ref> improved the performance of their base models by incorporating techniques such as the use of tagging in MULAN; Missing Annotation Matching (MAM) and Negative Region Mining (NRM) in MELD (refer row 5 and 7 of <ref type="table" target="#tab_7">Table 1</ref>). Hence, we also experimented by initializing our network with self-supervised weights. As shown in <ref type="table" target="#tab_7">Table 1</ref>, it leads to a gain in performance and achieves a final average sensitivity of 87.16%. For more results on organ-wise sensitivity, refer supplementary material. Next, we show a comparison of sensitivity at FP = 4 for different lesion sizes and average sensitivity (over FP = {0.5, 1, 2, 4}) for different organs. We observe from the <ref type="figure">Figure 3(a)</ref> that DKMA-ULD improves the detection of very small (&lt; 10mm) and medium-sized (10 ? 30mm) lesions over 3DCE <ref type="bibr" target="#b32">[32]</ref>, improved RetinaNet <ref type="bibr" target="#b37">[37]</ref>, and MULAN <ref type="bibr" target="#b34">[34]</ref>. Unlike 3DCE <ref type="bibr" target="#b32">[32]</ref> and improved RetinaNet <ref type="bibr" target="#b37">[37]</ref>, sensitivity values of MULAN <ref type="bibr" target="#b34">[34]</ref> for different lesion sizes and organs are not mentioned in their paper. Moreover, as the best trained model of MULAN with tags is not available publicly, we use the official released base model of MULAN <ref type="bibr" target="#b34">[34]</ref> (average sensitivity of 84.33%) for computing lesion size-wise and organwise sensitivity values on DeepLesion test-set. For a fair comparison, we use the base model of our proposed DKMA-ULD without self-supervision (average sensitivity of 86.88%) in <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref>. Further, in <ref type="figure">Figure 3</ref>(b), we observe that our proposed method of including domain-specific information in the lesion detection network improves the average  sensitivity across all organs. Furthermore, we provide a comparison of average sensitivity of MULAN and DKMA-ULD for different lesion-sizes and organs in <ref type="figure">Figure 4</ref> and demonstrate that DKMA-ULD substantially improves over MULAN in all the cases. For all the above experiments, we use cropped CT slices by clipping the black border region, as mentioned in previous state-of-the-art methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b34">34]</ref>, to focus on the region of interest. Now, we present the ablation study on the introduction of different modules in the proposed lesion detection pipeline, as shown in <ref type="table" target="#tab_5">Table 2</ref>. Our proposed 5 HU windows to give organ-specific domain knowledge results in an improvement of approximately 2% in the average sensitivity (82.37%), as shown in row 3 of <ref type="table" target="#tab_5">Table 2</ref>. Subsequent to this, we experiment with the inclusion of our novel convolution augmented multi-head attention module for feature fusion and custom anchors to detect varied-sized lesions effectively. We observe a performance boost by achieving an average sensitivity of 84.23%. All the ablation experiments are performed on CT-slices without applying cropping during the pre-processing step. Later in our experiments, we replace our feature extraction backbone with ResNeXt-152 and clip black borders in CT slices enabling the network to focus only on the region of interest. This resulted in a quantitative improvement by achieving a state-of-the-art average sensitivity of 86.88%. Finally, we show a qualitative comparison of lesion detection performance of our proposed DKMA-ULD in the form of reduction of FP in <ref type="figure" target="#fig_3">Figure 5</ref>. For more detailed experimental results, please refer supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we demonstrate the potential of exploiting domain knowledge in medical imaging data for developing a robust universal lesion detection network named DKMA-ULD that detects lesions across multiple organs of interest with improved sensitivity as compared to the state-of-the-art methods. We also prove that domain-specific weight initialization of ULD using self-supervision on the DeepLesion dataset gives a boost in lesion detection performance. Further, we provide evidence for our choices in using multiple HU windows, lesion-specific custom anchors, multi-head attention-based feature fusion and surpass the state-of-the-art ULD methods by achieving a sensitivity of 87.16% with only 3 slices per patient. This idea of exploiting maximum information in the dataset and learning self-supervised weights can prove useful for any medical image analysis task. In the future, we intend to extend this work by developing an anchor-less lesion detection network and making it robust to shifts in the domain, acquisition protocols, etc. using domain adaptation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Ablation for Feature Extraction Backbone</head><p>We use 5 multiple intensity images having 3 slices/channels each for a given patient in DeepLesion <ref type="bibr">[4]</ref> dataset. These images are passed as input to the shared convolutional feature extractor with feature pyramid network (FPN) <ref type="bibr">[3]</ref>. To determine the best feature extractor network that can learn the most relevant features for 3D CT-scans, we performed experiments with different ResNet <ref type="bibr">[2]</ref> variants. We found that ResNeXt-152 performs the best, as it is evident in <ref type="table" target="#tab_7">Table 1</ref>. Therefore, in our proposed DKMA-ULD method, we utilize the ResNeXt-152 backbone for feature extraction with self-supervised weights using BYOL technique <ref type="bibr">[1]</ref> and achieve state-of-the-art average sensitivity of 87.16% on the test-set of the DeepLesion dataset.  2 Organ-wise sensitivity DeepLesion dataset <ref type="bibr">[4]</ref> consists of approx. 32K annotated lesions across 8 different organs of the body. It is the largest dataset available right now which contains lesions in a variety of organs and hence, the best candidate for developing a universal lesion detection network. Our proposed method DKMA-ULD uses multiple-HU windows and lesion-specific custom anchors with a novel multi-head self attention-based feature fusion module. The inclusion of domain-knowledge specific features in the proposed DKMA-ULD has led to an improved overall and organ-wise performance as evident in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Organ Type FP@0.5 FP@1.0 FP@2.0 FP@0.  Next, we provide detailed comparison of our proposed DKMA-ULD network with MU-LAN <ref type="bibr">[5]</ref>. Since, the best trained model of MULAN with tags is not available publicly, we use the official released base model of MULAN <ref type="bibr">[5]</ref> for computing organ-wise sensitivity values on DeepLesion test-set for comparison. For a fair comparison, we use the base model of our proposed DKMA-ULD without self-supervision in <ref type="table" target="#tab_9">Table 3</ref>. It is clearly visible that our proposed method DKMA-ULD outperforms MULAN in lesion detection across all organs. Further, We present a qualitative comparison of our proposed DKMA-ULD with MULAN <ref type="bibr">[5]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref> and demonstrate that the detection of false positives is substantially reduced using domain-knowledge. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our DKMA-ULD architecture. (a) An input image I, consisting of 3 CT-slices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The top, middle, and bottom rows have CT-slices of chest-region, abdomen-region, and pelvic-region, respectively. The column (a) illustrates images with commonly used HU window (U = [1024, 4096]) while columns ranging from (b) to (f) have images with our new 5 HU windows. The figure clearly demonstrates that by using more number of windows, different organs of a particular body-region present in a CT volume, can be highlighted more efficiently.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Sensitivity comparison for different lesion sizes and organs. a) Sensitivity (at FP= 4) for lesions with 3 different size ranges is compared with existing methods [32, 34, 37] and our proposed DKMA-ULD network w/o self-supervision. b) Average sensitivity comparison per organ computed using our proposed lesion detector without and with domain knowledge &amp; multi-head attention. Here, BN, LNG, MDT, LVR, KDY, ABM, PLS and ST represent different organs such as bones, lungs, mediastinum, liver, kidney, abdomen, pelvis and soft-tissues, respectively. Average sensitivity comparison for different lesion sizes and organs. a) Average sensitivity (FP = {0.5, 1, 2, 4}) for lesions with 3 different size ranges is compared with MULAN [34] and our proposed DKMA-ULD. b) Organ-wise average sensitivity of MULAN and DKMA-ULD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison of DKMA-ULD and MULAN<ref type="bibr" target="#b34">[34]</ref> (at FP =2) on CT-scans of different body regions. The green, magenta, and red color boxes represent ground-truth, true-positive (TP), and false-positive (FP) lesion detection, respectively. Please note that ULD w/o DKMA represents when 3 slices with only one HU window ([1024, 4096]), default anchors, and without convolution augmented multi-head attention feature fusion are used. We can observe that after incorporating domain knowledge in the form of multi-intensity CT slices, custom anchors, and multi-head attention (i.e., DKMA-ULD), the number of FP reduced drastically resulting in improved lesion detection performance as compared to MULAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative comparison of DKMA-ULD and MULAN[5]  (at FP =2) on CT-scans of different body regions. The green, magenta, and red color boxes represent ground-truth, true-positive (TP), and false-positive (FP) lesion detection, respectively. Please note that ULD w/o DKMA represents when 3 slices with only one HU window ([1024, 4096]), default anchors, and without convolution augmented multi-head attention feature fusion are used. We can observe that after incorporating domain knowledge in the form of multi-intensity CT slices, custom anchors, and multi-head attention (i.e., DKMA-ULD), the number of FP reduced drastically resulting in improved lesion detection performance as compared to MULAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Concat Concat Detected Lesion (a) Multi-Level Feature Map extraction (b) Attention Based Feature Fusion F' Fused Multi-Level Feature Map Block RoI Pooling Detection Branch Weight Initialization With Self-Supervision</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">P j -F Ui ; ? ( , )</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(256 x 5, H, W) j</cell></row><row><cell></cell><cell></cell><cell>Convolutional Feature Extractor</cell><cell>F U1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>osition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">( q+ k+ v, H, W)</cell><cell>Embedding</cell></row><row><cell></cell><cell>I U1</cell><cell>P6</cell><cell>F U2</cell><cell cols="3">V Multi-Head Self-Attention K Q</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Linear</cell><cell></cell></row><row><cell>Ki-1</cell><cell>I U2</cell><cell>P4 P5</cell><cell>F U3</cell><cell>L V</cell><cell>MatMul L Q L K</cell><cell>X N h</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scale</cell><cell></cell></row><row><cell></cell><cell>I U3</cell><cell>P3</cell><cell></cell><cell></cell><cell>SoftMax</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MatMul</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>F U4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>P2</cell><cell></cell><cell></cell><cell cols="2">( v, H, W)</cell></row><row><cell></cell><cell>I U4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>F U5</cell><cell cols="3">( v, H, W) (256 ? , H, W)~P</cell></row><row><cell></cell><cell>I U5</cell><cell>I Ui</cell><cell>? ( , )</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sub-Levels of a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Feature Map Block</cell><cell>(256, H, W) j</cell><cell></cell><cell></cell></row></table><note>j</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>used in RPN for real-world object detection. To circumvent this, we propose new custom anchors which are well suited for detecting lesions of all sizes in CT scans. Let's say, H and W are the image height and width, respectively. We generate anchor boxes of different sizes centered on each image pixel such that it has maximum</figDesc><table><row><cell>Method 3DCE(27 slices) [32] improved RetinaNet(3 slices) [37] MVP Net(9 slices) [13] MULAN(27 slices, w/o tags) [34] MULAN(27 slices, w/ 171 tags) [34] MELD(9 slices)) [30] MELD+MAM+NRM(9 slices) [30]</cell><cell cols="3">Windows FP@0.5 FP@1.0 FP@2.0 FP@4.0 Average 1 62.48 73.37 80.70 85.65 75.55 1 72.18 80.07 86.40 90.77 82.36 3 73.83 81.82 87.60 91.30 83.64 1 76.10 82.50 87.50 90.90 84.33 1 76.12 83.69 88.76 92.30 85.22 1 77.80 84.80 89.00 91.80 85.90 1 78.60 85.50 89.60 92.50 86.60</cell></row><row><cell>(a) DKMA-ULD* (b)+Self-Supervision</cell><cell>5 5</cell><cell>78.10 78.75</cell><cell>85.26 90.48 93.48 86.88 85.95 90.48 93.48 87.16</cell></row><row><cell cols="4">Table 1: Comparison of DKMA-ULD with previous state-of-the-art ULD methods. Sensitivity(%) at different false-positives (FP) per sub-volume on the volumetric test-set of DeepLesion [33] dataset.</cell></row><row><cell cols="4">Here, we feed CT-slices, after performing cropping of black-borders during pre-processing, to the</cell></row><row><cell cols="3">network. *training with Imagenet pre-trained weights.</cell><cell></cell></row></table><note>IoU with lesion bounding box. If anchor sizes and ratios are in sets {s 1 , s 2 , ..., s n } and {r 1 , r 2 , ..., r m }, respectively for each r &gt; 0, we will have a total of W H(n + m ? 1) anchor boxes [35]. Consider w b and h b to be the width and height of anchor boxes, [w b , h b ] = [W s n ? rm , Hs n / ? rm ] s.t. n, m ? [1, 5] (1) We employ a differential evolution search algorithm [22] and find 5 best anchor sizes [16, 24, 64, 128, 256] and ratios [3.27, 1.78, 1, 0.56, 0.30] for P2, P3, P4, P5, P6 feature</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Sr. No. HU windows Backbone Attention Custom Anchors Avg.</figDesc><table><row><cell>1 2 3 4 5 6 7*</cell><cell>1 3 5 5 5 5 5</cell><cell>x101 x101 x101 x101 x101 x152 x152</cell><cell>Sensitivity 77.59 80.66 82.37 83.30 84.23 84.85 86.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies and average sensitivity comparison (%) of introducing different modules in the proposed lesion detection (DKMA-ULD) on the test-set of the DeepLesion dataset. *CT-slices after cropping of outer black-region are used for this experiment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>Sensitivity(%) for DKMA-ULD using different backbone networks and weight</figDesc><table /><note>initialization via self-supervised learning (SSL), at different false-positives (FP) per sub- volume on the volumetric test-set of DeepLesion [4] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Organ-wise sensitivity in % (at different FP and average) for DKMA-ULD having BYOL initialized ResNeXt-152 backbone. 89.86 88.37 88.75 76.83 79.69 83.84 71.80 DKMA-ULD(ours w/o byol) 79.16 94.30 89.32 89.64 84.37 80.81 85.97 76.31</figDesc><table><row><cell>5 Average</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Organ-wise average sensitivity (%) comparison (over FP = {0.5, 1, 2, 4}) of base MULAN [5] model w/o tags and our proposed base DKMA-ULD model (without byol selfsupervision) on test-set of DeepLesion [4] dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Manu Sheoran* manu.sheoran@tcs.com Meghal Dani* dani.meghal@tcs.com Monika Sharma monika.sharma1@tcs.com Lovekesh Vig lovekesh.vig@tcs.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. * Equal contribution in paper arXiv:2203.06886v1 [cs.CV] 14 Mar 2022</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: DKMA-ULD: Domain-Knowledge augmented Multi-head Attention based Robust Universal Lesion Detection</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The cancer imaging archive (TCIA) public access</title>
		<ptr target="https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CT Lymph Nodes dataset</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CT depiction of pulmonary emboli: display window settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kyongtae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">Ferdinand</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chlebus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Hesser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (lits)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
		<ptr target="https://arxiv.org/abs/2002.05709" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">New response evaluation criteria in solid tumours: revised RECIST guideline (version 1.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elizabeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Eisenhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Therasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dancey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Arbuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Gwyther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of cancer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Liver lesion detection from weakly-labeled multi-phase CT volumes with a grouped single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Gil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Seok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MVP-Net: Multi-view FPN with position-aware attention for deep universal lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A survey on deep learning in medical image analysis. Med. Image Anal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-DermDiagnosis: Few-Shot Skin Disease Identification using Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushagra</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00373</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3142" to="3151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quick guide on radiology image pre-processing for deep learning applications in prostate cancer research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Masoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10901</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting and classifying lesions in mammograms with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezs?</forename><surname>Ribli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsuzsa</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?ter</forename><surname>Pollner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>Csabai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">De</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Moira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cas</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piergiorgio</forename><surname>Bogaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cerello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Evelina</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipo</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukrit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haveesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parasuraman Padmanabhan, and Bal?zs Guly?s. 3D deep learning on medical images: a review</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">5097</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of global optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ULDor: a universal lesion detector for CT scans with pseudo masks and hard negative example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Bao</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISBI 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving deep lesion detection using 3D contextual and spatial attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding. CoRR, abs/1807.03748</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated pulmonary nodule detection: High sensitivity with few candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="759" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Volumetric attention for 3D medical image segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dashan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13753</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13753</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D context enhanced region-based convolutional neural network for endto-end lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36501</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MULAN: multitask universal lesion analysis network for joint lesion detection, tagging, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dive into Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="https://d2l.ai" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MSS U-Net: 3D segmentation of kidneys and tumors from CT images with a multi-scale supervised U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">Pe?a</forename><surname>Queralta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomi</forename><surname>Westerlund</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imu.2020.100357.URLhttps:/www.sciencedirect.com/science/article/pii/S2352914820301969</idno>
		<idno>2352-9148</idno>
		<ptr target="https://doi.org/10.1016/j.imu.2020.100357.URLhttps://www.sciencedirect.com/science/article/pii/S2352914820301969" />
	</analytic>
	<monogr>
		<title level="j">Informatics in Medicine Unlocked</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">100357</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving RetinaNet for CT lesion detection with dense masks from weak RECIST labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zlocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Universal Lesion Detector: Deep Learning for Analysing Medical Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zlocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36501</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mulan: multitask universal lesion analysis network for joint lesion detection, tagging, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
