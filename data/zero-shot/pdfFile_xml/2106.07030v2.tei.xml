<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alpha</forename><surname>Renner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Neuroinformatics</orgName>
								<orgName type="institution">University of Zurich and ETH Zurich</orgName>
								<address>
									<postCode>8057</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Sheldon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Physics of Condensed Matter</orgName>
								<orgName type="institution">Los Alamos National Laboratory</orgName>
								<address>
									<addrLine>Complex Systems (T-4)</addrLine>
									<postCode>87545</postCode>
									<settlement>Los Alamos</settlement>
									<region>New Mexico</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Zlotnik</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Applied Mathematics &amp; Plasma Physics (T-5)</orgName>
								<orgName type="institution">Los Alamos National Laboratory</orgName>
								<address>
									<postCode>87545</postCode>
									<settlement>Los Alamos</settlement>
									<region>New Mexico</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Tao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Center for Bioinformatics</orgName>
								<orgName type="department" key="dep2">School of Life Sciences</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Protein Engineering and Plant Genetic Engineering</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Center for Quantitative Biology</orgName>
								<orgName type="department" key="dep2">Academy for Advanced Interdisciplinary Studies</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sornborger</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Information Sciences (CCS-3)</orgName>
								<orgName type="institution">Los Alamos National Laboratory</orgName>
								<address>
									<postCode>87545</postCode>
									<settlement>Los Alamos</settlement>
									<region>New Mexico</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">(Dated: August 27, 2021)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The capabilities of natural neural systems have inspired new generations of machine learning algorithms as well as neuromorphic very large-scale integrated (VLSI) circuits capable of fast, low-power information processing. However, it has been argued that most modern machine learning algorithms are not neurophysiologically plausible. In particular, the workhorse of modern deep learning, the backpropagation algorithm, has proven difficult to translate to neuromorphic hardware. In this study, we present a neuromorphic, spiking backpropagation algorithm based on synfire-gated dynamical information coordination and processing, implemented on Intel's Loihi neuromorphic research processor. We demonstrate a proof-of-principle three-layer circuit that learns to classify digits from the MNIST dataset. To our knowledge, this is the first work to show a Spiking Neural Network (SNN) implementation of the backpropagation algorithm that is fully on-chip, without a computer in the loop. It is competitive in accuracy with off-chip trained SNNs and achieves an energy-delay product suitable for edge computing. This implementation shows a path for using in-memory, massively parallel neuromorphic processors for low-power, low-latency implementation of modern deep learning applications. * alpren@ini.uzh.ch ? fsheldon@lanl.gov ? azlotnik@lanl.gov ? taolt@mail.cbi.pku.edu.cn ? sornborg@lanl.gov; Corresponding Author 2. The Grand Challenge of Spiking Backpropagation (sBP)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I. Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Dynamical Coordination in Cognitive Processes.</head><p>Spike-based learning in plastic neuronal networks is playing increasingly key roles in both theoretical neuroscience and neuromorphic computing. The brain learns in part by modifying the synaptic strengths between neurons and neuronal populations. While specific synaptic plasticity or neuromodulatory mechanisms may vary in different brain regions, it is becoming clear that a significant level of dynamical coordination between disparate neuronal populations must exist, even within an individual neural circuit <ref type="bibr" target="#b0">[1]</ref>. Classically, backpropagation (BP, and other learning algorithms) <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> has been essential for supervised learning in artificial neural networks (ANNs). Although the question of whether or not BP operates in the brain is still an outstanding issue <ref type="bibr" target="#b4">[5]</ref>, BP does solve the problem of how a global objective function can be related to local synaptic modification in a network. It seems clear, however, that if BP is implemented in the brain, or if one wishes to implement BP in a neuromorphic circuit, some amount of dynamical information coordination is necessary to propagate the correct information to the correct location such that appropriate local synaptic modification may take place to enable learning.</p><p>There has been a rapid growth of interest in the reformulation of classical algorithms for learning, optimization, and control using event-based informationprocessing mechanisms. Such spiking neural networks (SNNs) are inspired by the function of biophysiological neural systems <ref type="bibr" target="#b5">[6]</ref>, i.e., neuromorphic computing <ref type="bibr" target="#b6">[7]</ref>. The trend is driven by the advent of flexible computing architectures such as Intel's neuromorphic research processor, codenamed Loihi, that enable experimentation with such algorithms in hardware <ref type="bibr" target="#b7">[8]</ref>. There is particular interest in deep learning, which is a central tool in modern machine learning. Deep learning relies on a layered, feedforward network similar to the early layers of the visual cortex, with threshold nonlinearities at each layer that resemble mean-field approximations of neuronal integrate-and-fire models. While feedforward networks are readily translated to neuromorphic hardware <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, the far more computationally intensive training of these networks 'on chip' has proven elusive as the structure of backpropagation makes the algorithm notoriously difficult to implement in a neural circuit <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. A feasible neural implementation of the backpropagation algorithm has gained renewed scrutiny with the rise of new neuromorphic computational architectures that feature local synaptic plasticity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Because of the well-known difficulties, neuromorphic systems have relied to date almost entirely on conventional off-chip learning, and used on-chip computing only for inference <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. It has been a long-standing challenge to develop learning systems whose function is realized exclusively using neuromorphic mechanisms.</p><p>Backpropagation has been claimed to be biologically arXiv:2106.07030v2 [cs.NE] <ref type="bibr" target="#b25">26</ref> Aug 2021 implausible or difficult to implement on spiking chips because of several issues: (a) Weight transport -Usually, synapses in biology and on neuromorphic hardware cannot be used bidirectionally, therefore, separate synapses for the forward and backward pass are employed. However, correct credit assignment, i.e. knowing how a weight change affects the error, requires feedback weights to be the same as feedforward weights <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>; (b) Backwards computation -Forward and backward passes implement different computations <ref type="bibr" target="#b17">[18]</ref>. The forward pass requires only weighted summation of the inputs, while the backward pass operates in the opposite direction and additionally takes into account the derivative of the activation function; (c) Gradient storage -Error gradients must be computed and stored separately from activations; (d) Differentiability -For spiking networks, the issue of non-differentiability of spikes has been discussed and solutions have been proposed <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>; and (e) Hardware constraints -For the case of neuromorphic hardware, there are often constraints on plasticity mechanisms, which allow for adaptation of synaptic weights. On some hardware, no plasticity is offered at all, while in some cases only specific spike-timing dependent plasticity (STDP) rules are allowed. Additionally, in almost all available neuromorphic architectures, information must be local, i.e., information is only shared between neurons that are synaptically connected, in particular, to facilitate parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Previous Attempts at sBP</head><p>The most commonly used approach to avoiding the above issues is to use neuromorphic hardware only for inference using fixed weights, obtained by training of an identical network offline and off-chip <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. This has recently achieved state-of-the-art performance <ref type="bibr" target="#b10">[11]</ref>, but it does not make use of neuromorphic hardware's full potential, because offline training consumes significant power. Moreover, to function in most field applications, an inference algorithm should be able to learn adaptively after deployment, e.g., to adjust to a particular speaker in speech recognition, which would enable autonomy and privacy of edge computing devices. So far, only last layer training, without backpropagation, and using variants of the delta rule, has been achieved on spiking hardware <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. Other on-chip learning approaches use alternatives to backpropagation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, bio-inspired non-gradient based methods <ref type="bibr" target="#b36">[37]</ref>, or hybrid systems with a conventional computer in the loop <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. Several promising alternative approaches for actual on-chip spiking backpropagation have been proposed recently <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr">but</ref> have not yet been implemented in hardware.</p><p>To avoid the backwards computation issue (b) and because neuromorphic synapses are not bidirectional, a separate feedback network for the backpropagation of errors has been proposed <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> (see <ref type="figure">Fig. 1</ref>). This leads to the weight transport problem (a) which has been solved by using symmetric learning rules to maintain weight symmetry <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref> or with the Kolen-Pollack algorithm <ref type="bibr" target="#b46">[47]</ref>, which leads to symmetric weights automatically. It has also been found that weights do not have to be perfectly symmetric, because backpropagation can still be effective with random feedback weights (random feedback alignment) <ref type="bibr" target="#b47">[48]</ref>, although symmetry in the sign between forward and backward weights matters <ref type="bibr" target="#b48">[49]</ref>.</p><p>The backwards computation issue (b) and the gradient storage issue (c) have been addressed by approaches that separate the function of the neuron into different compartments and use structures that resemble neuronal dendrites for the computation of backward propagating errors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr">50]</ref>. The differentiability issue (d) has been circumvented by spiking rate-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> that use the ReLU activation function as done in ANNs. The differentiability issue has also been addressed more generally using surrogate gradient methods <ref type="bibr">[9, 19-22, 25, 28, 53]</ref> and methods that use biologically-inspired STDP and reward modulated STDP mechanisms <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref>. For a review of SNN-based deep learning, see <ref type="bibr" target="#b57">[58]</ref>. For a review of backpropagation in the brain, see <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Contribution</head><p>In this study, we describe a hardware implementation of the backpropagation algorithm that addresses each of the issues (a)-(e) introduced above using a set of mechanisms that have been developed and tested in simulation by the authors during the past decade, synthesized in our recent study <ref type="bibr" target="#b58">[59]</ref>, and simplified and adapted here to the features and constraints of the Loihi chip. These neuronal and network features include propagation of graded information in a circuit composed of neural populations using synfire-gated synfire chains (SGSCs) <ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref>, control flow based on the interaction of synfire-chains <ref type="bibr" target="#b60">[61]</ref>, and regulation of Hebbian learning using synfire-gating <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. We simplify our previously proposed network architecture <ref type="bibr" target="#b58">[59]</ref>, and streamline its function. We demonstrate our approach using a proof-of-principle implementation on Loihi <ref type="bibr" target="#b7">[8]</ref>, and examine the performance of the algorithm for learning and inference of the MNIST test data set <ref type="bibr" target="#b65">[66]</ref>. The sBP implementation is shown to be competitive in clock time, sparsity, and power consumption with state-of-the-art algorithms for the same tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The Binarized sBP model</head><p>We extend our previous architecture <ref type="bibr" target="#b58">[59]</ref> using several new algorithmic and hardware-specific mechanisms. Each unit of the neural network is implemented as a single spiking neuron, using the current-based leaky integrate-and-fire (CUBA) model (see Eq. <ref type="bibr" target="#b18">(19)</ref> in Section IV) that is built into Loihi. The time constants of the CUBA model are set to 1, so that the neurons are memoryless. Rather than using rate coding, whereby spikes are counted over time, we consider neuron spikes at every algorithmic time step, so we can regard our implementation as a binary neural network. The feedforward component of our network is a classic multilayer perceptron (MLP) with 3-layers, a binary activation function, and discrete (8 bit) weights. Our approach may, however, be extended to deeper networks and different encodings. In the following equations, each lowercase letter corresponds to a Boolean vector that represents a layer of spiking neurons on the chip (a spike corresponds to a 1). The inference (forward) pass through the network is computed as:</p><formula xml:id="formula_0">o = f (W 2 f (W 1 x)) , (1) f (x) = H(x ? 0.5) ,<label>(2)</label></formula><formula xml:id="formula_1">H(x) = 0, x &lt; 0, 1, x ? 0 ,<label>(3)</label></formula><p>where W i is the weight matrix of the respective layer, f is a binary activation function with a threshold of 0.5, and H denotes the Heaviside function. The forward pass thus occurs in 3 time steps as spikes are propagated through layers. The degree to which the feedforward network's output (o) deviates from a target value (t) is quantified by the squared error, E = 1 2 o ? t 2 , which we would like to minimize. Performing backpropagation to achieve this requires the calculation of weight updates, which depend on the forward activations, and backward propagated lo-cal gradients d l , which represent the amount by which the loss changes when the activity of that neuron changes, as:</p><formula xml:id="formula_2">d 2 = (o ? t) ? f (W 2 h) , (4) d 1 = sgn(W T 2 d 2 ) ? f (W 1 x) , (5) ?E ?W l = d l (a l?1 ) T ,<label>(6)</label></formula><formula xml:id="formula_3">W new l = W old l ? ? ?E ?W l , l = 1, 2 .<label>(7)</label></formula><p>Here, ? denotes a Hadamard product, i.e. the elementwise product of two vectors, T denotes the matrix transpose, sgn(x) is the sign function, and a l denotes the activation of the lth layer, f (W l a l?1 ), with a 0 = x, a 1 = h, a 2 = o. Here, ? denotes the learning rate, and is the only hyperparameter of the model apart from the weight initialization. Here denotes the derivative, but because f is a binary thresholding function (Heaviside), the derivative would be the Dirac delta function, which is zero everywhere apart from at the threshold. Therefore, we use a common method <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, and represent the thresholding function using a truncated (between 0 and 1) ReLU (Eq. (8)) as a surrogate or straight-through estimator when back-propagating the error. The derivative of the surrogate is a box function (Eq. (9)):</p><formula xml:id="formula_4">f surrogate (x) = min(max(x, 0), 1) , (8) f (x) = H(x) ? H(x ? 1) .<label>(9)</label></formula><p>The three functions (Equations (2), <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_4">(9)</ref>) are plotted in the inset in <ref type="figure" target="#fig_0">Fig. 2</ref>. When performed for each target (t) in the training set, the model may be thought of as a stochastic gradient descent algorithm with a fixed step size update for each weight in the direction of the sign of the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">sBP on Neuromorphic Hardware</head><p>On the computational level, Equations (1)-(9) fully describe our model exactly as it is implemented on Loihi, excluding the handling of bit precision constraints that affect integer discreteness and value limits and ranges. In the following, we describe how these equations are translated from the computational to the algorithmic neural circuit level, thereby enabling implementation on neuromorphic hardware. Further details on the implementation can be found in Section IV.</p><p>a. Hebbian weight update Equation <ref type="formula" target="#formula_3">(7)</ref> effectively results in the following weight update per single synapse from presynaptic index i in layer l ? 1 to postsynaptic index j in layer l:</p><formula xml:id="formula_5">?w ij = ?? ? a l?1,i ? d l,j ,<label>(10)</label></formula><p>where ? is the constant learning rate. To accomplish this update, we use a Hebbian learning rule <ref type="bibr" target="#b68">[69]</ref> implementable on the on-chip microcode learning engine (for the exact implementation on Loihi, see Section IV B). Hebbian learning means that neurons that fire together, wire together, i.e., the weight update ?w is proportional to the product of the simultaneous activity of the presynaptic (source) neuron and the postsynaptic (target) neuron. In our case, this means that the values of the 2 factors of Equation (10) have to be propagated simultaneously, in the same time step, to the pre-(a l?1,i ) and postsynaptic (d l,j ) neurons while the pre-and postsynaptic neurons are not allowed to fire simultaneously at any other time. For this purpose, a mechanism to control the information flow through the network is needed. b. Gating controls the information flow As our information control mechanism, we use synfire gating <ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b69">70]</ref>. A closed chain of 12 neurons containing a single spike perpetually sent around the circle is the backbone of this flow control mechanism, which we call the gating chain. The gating chain controls information flow through the controlled network by selectively boosting layers to bring their neurons closer to the threshold and thereby making them receptive to input. By connecting particular layers to the gating neuron that fires in the respective time steps, we lay out a path that the activity through the network is allowed to take. For example, to create the feedforward pass, the input layer x is connected to the first gating neuron and therefore gated 'on' in time step 1, the hidden layer h is connected to the second gating neuron and gated 'on' in time step 2, and the output layer o is connected to the third gating neuron and gated 'on' in time step 3. A schematic of this path of the activity can be found in <ref type="figure" target="#fig_0">Fig. 2</ref>. To speak in neuroscience terms, we are using synfire gating to design functional connectivity through the network anatomy shown in <ref type="figure">Supplementary Fig. 4</ref> . Using synfire gating, the local gradient d l,j is brought to the postsynaptic neuron at the same time as the activity a l?1,i is brought back to the presynaptic neuron effecting a weight update. However, in addition to bringing activity at the right time to the right place for Hebbian learning, the gating chain also makes it possible to calculate and back-propagate the local gradient.</p><p>c. Local gradient calculation For the local gradient calculation, according to Equation (5), the error o ? t and the box function derivative of the surrogate activation function (Equation <ref type="formula" target="#formula_4">(9)</ref>) are needed. Because there are no negative (signed) spikes, the local gradient is calculated and propagated back twice for a Hebbian weight update in two phases with different signs. The error o ? t is calculated in time step 4 in a layer that receives excitatory (positive) input from the output layer o and inhibitory (negative) input from the target layer t, and vice versa for t ? o.</p><p>The box function has the role of initiating learning when the presynaptic neuron receives a non-negative input and of terminating learning when the input exceeds 1, which is why we call the two conditions 'start' and 'stop' learning (inspired by the nomenclature of <ref type="bibr" target="#b70">[71]</ref>). This inherent feature of backpropagation avoids weight updates that have no effect on the current output as the neuron is saturated by the nonlinearity with the current input. This regulates learning by protecting weights that are trained and used for inference when given different inputs.</p><p>To implement these two terms of the box function (9), we use two copies of the output layer that receive the same input (W 2 h) as the output layer. Using the abovedescribed gating mechanism, one of the copies (start learning, o &lt; ) is brought exactly to its firing threshold when it receives the input, which means that it fires for any activity greater than 0 and the input is not in the lower saturation region of the ReLU. The other copy (stop learning, o &gt; ) is brought further away from the threshold (to 0), which means that if it fires, the upper saturation region of the ReLU has been reached and learning should cease.</p><p>d. Error backpropagation Once the local gradient d 2 is calculated as described in the previous paragraph, it is sent to the output layer as well as to its copies to bring about the weight update of W 2 and its 4 copies in time steps 5 and 9. From there, the local gradient is propagated back through the transposed weight matrices W T 2 and ?W T 2 , which are copies of W 2 connected in the opposite direction and, in the case of ?W T 2 , with opposite sign. Once propagated backwards, the back-propagated error is also combined with the 'start' and 'stop' learning conditions, and then it is sent to the hidden layer and its copies in time steps 7 and 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm Performance</head><p>Our implementation of the sBP algorithm on Loihi achieves an inference accuracy of 95.7% after 60 epochs (best: 96.2%) on the MNIST test data set, which is comparable with other shallow, stochastic gradient descent (SGD) trained MLP models without additional allowances. In these computational experiments, the sBP model is distributed over 81 neuromorphic cores. Processing of a single sample takes 1.48 ms (0.169 ms for inference only) on the neuromorphic cores, including the time required to send the input spikes from the embedded processor, and consumes 0.653 mJ of energy on the neuromorphic cores (0.592 mJ of which is dynamic energy, i.e. energy used by our neural circuit in addition to the fixed background energy), resulting in an energy-delay product of 0.878 ?Js. All measurements were obtained using NxSDK version 0.9.9 on the Nahuku32 board nclghrd-01. <ref type="table" target="#tab_2">Table I</ref> shows a comparison of our results with published performance metrics for other neuromorphic learning architectures that were also tested on MNIST. <ref type="table" target="#tab_2">Table III</ref> in the Supplementary Material shows a breakdown of energy consumption and a comparison of different conditions and against a GPU. Switching off the learning engine after training reduces the dynamic energy per inference to 0.0204 mJ, which reveals that the on-chip learning engine is responsible for most of the power consumption. Because the learning circuit is not necessary for performing inference, we also tested a reduced architecture that is able to do inference within four time steps using the previously trained weights. This architecture uses 0.00249 mJ of dynamic energy and 0.169 ms per inference. Layers are only shown when they are gated 'on' and synapses are only shown when their target is gated on. Plastic connections are all-to-all (fully connected), i.e. all neurons are connected to all neurons of the next layer. The gating connections from the gating chain are one-to-all, and all other connections are one-to-one, which means that a firing pattern is copied directly to the following layer. The names of the neuron layers are given on the left margin so that the row corresponds to layer identity. The columns correspond to time steps of the algorithm, which are the same as the time steps on Loihi. <ref type="table" target="#tab_2">Table II</ref> shows the information contained in each layer in each respective time step. The red background in time steps 5 and 7 indicates that in these steps, the sign of the weight update is inverted (positive), as r = 1 in Eq. <ref type="bibr" target="#b20">(21)</ref>. A detailed step-by-step explanation of the algorithm is given in Section IV C 2 and in <ref type="table" target="#tab_2">Table II</ref> in the supplementary material. The plot in the top left corner illustrates our approach to approximate the activation function f by a surrogate with the box function as derivative, fsurr = H(x)H(1 ? x), where f is the rectified linear map (ReLU) (see Equations 2, 8 and 9).</p><p>The sBP algorithm trains the network without explicit sparsity constraints, and yet it exhibits sparsity because of its binary (spiking) nature. After applying the binary threshold of 0.5 to the MNIST images, one image is encoded using on average 100 spikes in the input layer, which corresponds to a sparsity of 0.25 spikes per neuron per inference. This leads to a typical number of 110 spikes in the hidden layer (0.28 spikes per neuron per inference) and 1 spike in the output layer (0.1 spikes per neuron per inference). The spikes of the input and hidden layer are repeated in the two learning phases (see <ref type="figure" target="#fig_0">Fig. 2</ref>) independent of the training progress. However, the error-induced activity from the local gradient layer d 1 starts with 0.7 spikes per neuron per sample (during the first 1000 samples) and goes down to approximately 0 spikes in the trained network as the error goes towards 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Summary</head><p>As we have demonstrated here, by using a well-defined set of neuronal and neural circuit mechanisms, it is possible to implement a spiking backpropagation algorithm on contemporary neuromorphic hardware. Previously proposed methods to address the issues outlined in Section I were not on their own able to offer a straightforward path to implement a variant of the sBP algorithm on current hardware. In this study, we avoided or solved these previously encountered issues with spiking backpropagation by combining known solutions with synfire-gated synfire chains (SGSC) as a dynamical information coordination scheme that was evaluated on the MNIST test data set on the Loihi VLSI hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Solutions of Implementation Issues</head><p>The five issues (a)-(e) listed in Section I were addressed using the following solutions: (a) The weight transport issue was avoided via the use of a deterministic, symmetric learning rule for the parts of the network that im-plement inference (feed-forward) and error propagation (feedback) as described by <ref type="bibr" target="#b85">[86]</ref>. This approach is not biologically plausible because of a lack of developmental mechanisms to assure the equality of corresponding weights <ref type="bibr" target="#b86">[87]</ref>. It would, however, without modifications to the architecture be feasible to employ weight decay as described by Kolen and Pollack <ref type="bibr" target="#b86">[87]</ref> to achieve selfalignment of the backward weights to the forward weights or to use feedback alignment to approximately align the feedforward weights to random feedback weights <ref type="bibr" target="#b47">[48]</ref>; (b) The backwards computation issue was solved by using a separate error propagation network through which activation is routed using an SGSC; (c) The gradient storage issue was solved by routing activity through the inference and error propagation circuits within the network in separate stages, thereby preventing the mixing of inference and error information. There are alternatives that would not require synfire gated routing, but are more challenging to implement on hardware <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> as also described in a more comprehensive review <ref type="bibr" target="#b4">[5]</ref>; (d) The differentiability issue was solved by representing the step activation function by a surrogate in the form of a (truncated) ReLU activation function with an easily implementable box function derivative; and (e) The hardware constraint issue was solved by the proposed mechanism's straightforward implementation on Loihi because it only requires basic integrate-and-fire neurons and Hebbian learning that is modulated by a single factor which is the same for all synapses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Encoding and Sparsity</head><p>While neural network algorithms on GPUs usually use operations on dense activation vectors and weight matrices, and therefore do not profit from sparsity, spiking neuromorphic hardware only performs an addition operation when a spike event occurs, i.e., adding the weight to the input current as in Equation <ref type="formula" target="#formula_4">(19)</ref>. This means that the power consumption directly depends on the number of spikes. Therefore sparsity, which refers to the property of a vector to have mostly zero elements, is important for neuromorphic algorithms <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b87">88]</ref>, and it is also observed in biology <ref type="bibr" target="#b88">[89]</ref>. Consequently, significant effort has been made to make SNNs sparse to overcome the classical rate-based approach based on counting spikes <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref>. The binary encoding used here could be seen as a limit case of the rate-based approach allowing only 0 or 1 spike. Even without regularization to promote sparse activity, it yields very sparse activation vectors that are even sparser than most timing codes. The achievable encoded information per spike is unquestionably lower, however. In a sense, we already employ spike timing to route spikes through the network because the location of a spike in time within the 12 time steps determines if and where it is sent and if the weights are potentiated or depressed. However, usage of a timing code for activations could be enabled by having more than one Loihi time step per algorithm time step. Therefore the use of SGSCs is not limited to this particular binary encoding, and in fact, SGSCs were originally designed for a population rate code.</p><p>Similarly, the routing method we use in this work is not limited to backpropagation, but it could serve as a general method to route information in SNNs where autonomous activity (without interference from outside the chip) is needed. That is, our proposed architecture can act in a similar way as or even in combination with neural state machines <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b92">93]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithmically-Informed Neurophysiology</head><p>Although our implementation of sBP here was focused primarily on a particular hardware environment, we point out that the synfire-gated synfire chains and other network and neuronal structures that we employ could all potentially have relevance to the understanding of computation in neurophysiological systems. Many of these concepts that we use, such as spike coincidence, were originally inspired by neurophysiological experiments <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b93">94]</ref>. Experimental studies have shown recurring sequences of stereotypical neuronal activation in several species and brain regions <ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref> and particularly replay in hippocampus <ref type="bibr" target="#b97">[98]</ref>. Recent studies also hypothesize <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b99">100]</ref> and show <ref type="bibr" target="#b100">[101]</ref> that a mechanism like gating by synfire chains may play a role in memory formation. Additional evidence <ref type="bibr" target="#b101">[102]</ref> shows that large-scale cortical activity has a stereotypical, packet-like character that can convey information about the nature of a stimulus, or be ongoing or spontaneous. This type of apparently algorithmically-related activity has a very similar form to the SGSC controlled information flow found previously <ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref>. Additionally, this type of sequential activation of populations is evoked by the sBP learning architecture, as seen in the raster plot in <ref type="figure">Fig. 5</ref> in the Supplementary Material.</p><p>Other algorithmic spiking features, such as the backpropagated local gradient layer activity decreasing from 0.7 spikes per neuron to 0 by the end of training, could be identified and used to generate qualitative and quantitative hypotheses concerning network activity in biological neural systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Directions</head><p>Although the accuracy we achieve is similar to early implementations of binary neural networks in software <ref type="bibr" target="#b67">[68]</ref>, subsequent approaches now reach 98.5% <ref type="bibr" target="#b102">[103]</ref>, and generally include binarized weights. However, networks that achieve such accuracy typically employ either a convolutional structure or multiple larger hidden layers. Additional features such as dropout, softmax final layers, gain terms, and others could in principle be included in spiking hardware and may also account for this 3% gap. So, while we show that it is possible to efficiently implement backpropagation on neuromorphic hardware, several non-trivial steps are still required to make it usable in practical applications:</p><p>1) The algorithm needs to be scaled to deeper networks. While the present structure is in principle scalable to more layers without major adjustments, investigation is needed to determine whether gradient informa-tion remains intact over many layers, and to what extent additional features such as alternatives to batch normalization may need to be developed.</p><p>2) Generalization to convolutional networks is compelling, in particular for application to image processing. The Loihi hardware presents an advantage in this setting because of its weight-sharing mechanisms.</p><p>3) Although our current implementation demonstrates on-chip learning, we train on the MNIST images in an offline fashion by iterating over the training set in epochs. Further research is required to develop truly continual learning mechanisms such that additional samples and classes can actually be learned without losing previously trained synaptic weights and without retraining on the whole dataset.</p><p>Additionally, the proposed algorithmic methodology can be used to inform hardware adjustments to promote efficiency for learning applications. Although our algorithm is highly efficient in terms of power usage, in particular for binary encoding, the Loihi hardware is not specifically designed for implementing standard deep learning models, but rather as general-purpose hardware for exploring different SNN applications <ref type="bibr" target="#b74">[75]</ref>.</p><p>This leads to a significant computational overhead for functions that are not needed in our model (e.g. neuronal dynamics), or that could have been realized more efficiently if integrated directly on the chip instead of using network mechanisms. Our results provide a potential framework to guide future hardware modifications to facilitate more efficient learning algorithm implementations. For example, in an upgraded version of the algorithm, it would be preferable to replace relay neurons with presynaptic (eligibility) traces to keep a memory of the presynaptic activity for the weight update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Significance</head><p>To our knowledge, this work is the first to show an SNN implementation of the backpropagation algorithm that is fully on-chip, without a computer in the loop. Other onchip learning approaches so far either use feedback alignment <ref type="bibr" target="#b35">[36]</ref>, forward propagation of errors <ref type="bibr" target="#b34">[35]</ref> or single layer training <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73]</ref>. Compared to an equivalent implementation on a GPU, there is no loss in accuracy, but there are about two orders of magnitude power savings in the case of small batch sizes which are more realistic for edge computing settings. So, this implementation shows a path for using in-memory, massively parallel neuromorphic processors for low-power, low-latency implementation of modern deep learning applications. The network model we propose offers opportunities as a building block that can, e.g. be integrated into larger SNN architectures that could profit from a trainable on-chip processing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Methods</head><p>In this section, we describe our system on three different levels <ref type="bibr" target="#b103">[104]</ref>. First, we describe the computational level by fully stating the equations that result in the intended computation. Second, we describe the spiking neural network (SNN) algorithm. Third, we describe the details of our hardware implementation that are necessary for exact reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Binarized Backpropagation Model</head><p>Network Model. Backpropagation is a means of optimizing synaptic weights in a multi-layer neural network. It solves the problem of credit assignment, i.e., attributing changes in the error to changes in upstream weights, by recursively using the chain rule. The inference (forward) pass through the network is computed as</p><formula xml:id="formula_6">o = f (W N f (W N ?1 (. . . f (W 1 x)))) ,<label>(11)</label></formula><p>where f is an element-wise nonlinearity and W i is the weight matrix of the respective layer. The degree to which the network's output (o) deviates from the target values (t) is quantified by the squared error, E = 1 2 o ? t 2 , which we aim to minimize. The weight updates for each layer are computed recursively by</p><formula xml:id="formula_7">d l = (o ? t) ? f (n l ), l = N W T l+1 d l+1 ? f (n l ), l &lt; N ,<label>(12)</label></formula><formula xml:id="formula_8">?E ?W l+1 = d l+1 (a l ) T ,<label>(13)</label></formula><formula xml:id="formula_9">W new l = W old l ? ? ?E ?W l ,<label>(14)</label></formula><p>where n l is the network activity at layer l (i.e., n l = W l f (W l?1 n l?1 )). Here, denotes the derivative, ? denotes a Hadamard product, i.e. the element-wise product of two vectors, T denotes the matrix transpose, and a l denotes f (W l a l?1 ), with a 0 = x. The parameter ? denotes the learning rate. These general equations (12)-(14) are realized for two layers in our implementation as given by (5)- <ref type="bibr" target="#b6">(7)</ref> in Section II. Below, we relate these equations to the neural Hebbian learning mechanism used in the neuromorphic implementation of sBP. Although in theory the derivative f of the activation function is applied in <ref type="bibr" target="#b11">(12)</ref>, in the case that f is a binary thresholding (Heaviside) function, the derivative is the Dirac delta function, which is zero everywhere apart from at the threshold. We use a common approach <ref type="bibr" target="#b67">[68]</ref> and represent the activation function by a surrogate (or straight-through estimator <ref type="bibr" target="#b66">[67]</ref>), in the form of a rectified linear map (ReLU) truncated between 0 and 1 (Eq. (16)) in the part of the circuit that affects error backpropagation. The derivative of this surrogate (f ) is of box function form, i.e.</p><formula xml:id="formula_10">f (x) = H(x ? 0.5) ,<label>(15)</label></formula><p>f surrogate (x) = min(max(x, 0), 1) ,</p><formula xml:id="formula_11">f (x) = H(x) ? H(x ? 1) ,<label>(16)</label></formula><p>where H denotes the Heaviside function:</p><formula xml:id="formula_13">H(x) = 0, x &lt; 0 , 1, x ? 0 .<label>(18)</label></formula><p>In the following section, we describe how we implement Equations (11)-(16) in a spiking neural network. Weight Initialization. Plastic weights are initialized by sampling from a Gaussian distribution with mean of 0 and a standard deviation of 1/ 2/(N fanin + N fanout ) (He initialization <ref type="bibr" target="#b104">[105]</ref>). N fanin denotes the number of neurons of the presynaptic layer and N fanout the number of neurons of the postsynaptic layer. Input Data. The images of the MNIST dataset <ref type="bibr" target="#b65">[66]</ref> were cropped by a margin of 4 pixels on each side to remove pixels that are never active and avoid unused neurons and synapses on the chip. The pixel values were thresholded with 0.5 to get a black and white picture for use as input to the network. In the case of the 100 ? 300 ? 10 architecture, the input images were downsampled by a factor of 2. The dataset was presented in a different random order in each epoch. Accuracy Calculation. The reported accuracies are calculated on the full MNIST test data set. A sample was counted as correct when the index of the spiking neuron of the output layer in the output phase (time step 3 in <ref type="figure" target="#fig_0">Fig. 2)</ref> is equal to the correct label index of the presented image. In fewer than 1% of cases, there was more than one spike in the output layer, and in that case, the lowest spiking neuron index was compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Spiking Backpropagation Algorithm</head><p>Spiking Neuron Model. For a generic spiking neuron element, we use the current-based linear leaky integrateand-fire model (CUBA). This model is implemented on Intel's neuromorphic research processor, codenamed Loihi <ref type="bibr" target="#b7">[8]</ref>. Time evolution in the CUBA model as implemented on Loihi is described by the discrete-time dynamics with t ? N, and time increment ?t ? 1:</p><formula xml:id="formula_14">V i (t + 1) = V i (t) ? 1 ? V V i (t) + U i (t) + I const , U i (t + 1) = U i (t) ? 1 ? U U i (t) + j w ij ? j (t) ,<label>(19)</label></formula><p>where i identifies the neuron. The membrane potential V (t) is reset to 0 upon exceeding the threshold V thr and remains at its reset value 0 for a refractory period, T ref . Upon reset, a spike is sent to all connecting synapses. Here, U (t) represents a neuronal current and ? represents time-dependent spiking input. I const is a constant input current. Parameters and Mapping. In our implementation of the backpropagation algorithm, we take ? V = ? U = 1, T ref = 0, and I const = ?8192 (except in gating neurons, where I const = 0). This leads to a memoryless point neuron that spikes whenever its input in the respective time step exceeds V thr = 1024. This happens, when the neuron receives synaptic input larger than 0.5?V thr and in the same timestep, a gating input overcomes the strong inhibition of the I const , i.e. it is gated 'on'. This is how the Heaviside function in Equation <ref type="formula" target="#formula_10">(15)</ref> is implemented. For the other activation functions, a different gating input is applied.</p><p>There is a straightforward mapping between the weights and activations in the spiking neural network (SNN) described in this section and the corresponding artificial neural network (ANN) described in Section IV A:</p><formula xml:id="formula_15">w SNN = w ANN ? V thr , a SNN = a ANN ? V thr .<label>(20)</label></formula><p>So, a value of V thr = 1024 allows for a maximal ANN weight of 0.25, because the allowed weights on Loihi are the even integers from -256 to 254.</p><p>Feed-forward Pass. The feedforward pass can be seen as an independent circuit module that consists of 3 layers. An input layer x with 400 (20x20) neurons that spikes according to the binarized MNIST dataset, a hidden layer h of 400 neurons, and an output layer o of 10 neurons. The 3 layers are sequentially gated 'on' by the gating chain so that activity travels from the input layer to the hidden layer through the plastic weight matrix W 1 and then from the hidden to the output layer through the plastic weight matrix W 2 . Learning Rule. The plastic weights follow the standard Hebbian learning rule with a global third factor to control the sign. Note, however, that here, unlike other work with Hebbian learning rules, due to the particular activity routed to the layers, the learning rule implements a supervised mechanism (backpropagation). Here we give the discrete update equation as implemented on Loihi:</p><formula xml:id="formula_16">?w = 4r(t) ? x(t) ? y(t) ? 2x(t) ? y(t) (21) = (2r(t) ? 1) ? 2x(t)y(t) ,</formula><formula xml:id="formula_17">r(t) = 1, if (t mod T ) = 5, 7 , 0, otherwise .<label>(22)</label></formula><p>Above, x, y, and r represent time series that are available at the synapse on the chip. The signals x and y are the pre-and postsynaptic neuron's spike trains, i.e., they are equal to 1 in time steps when the respective neuron fires, and 0 otherwise. The signal r is a third factor that is provided to all synapses globally and determines in which phase (potentiation or depression) the algorithm is in. T denotes the number of phases per trial, which is 12 in this case. So, r is 0 in all time steps apart from the 5 th and 7 th of each iteration, where the potentiation of the weights happens. This regularly occurring r signal could thus be generated autonomously using the gating chain. On Loihi, r is provided using a so-called "reinforcement channel". Note that the reinforcement channel can only provide a global modulatory signal that is the same for all synapses. The above learning rule produces a positive weight update in time steps in which all three factors are 1, i.e., when both pre-and post-synaptic neurons fire and the reinforcement channel is active. It produces a negative update when only the pre-and post-synaptic neurons fire, and the weight stays the same in all other cases.</p><p>To achieve the correct weight update according to the backpropagation algorithm (see <ref type="bibr" target="#b9">(10)</ref>), the spiking network has to be designed in a way that the presynaptic activity a l?1,i and the local gradient d l,j are present in neighboring neurons at the same time step. Furthermore, the sign of the local gradient has to determine if the simultaneous activity happens in a time step with active third factor r or not.</p><p>This requires control over the flow of spikes in the network, which is achieved by a mechanism called synfire gating <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>, which we adapt and simplify here. Gating Chain. Gating neurons are a separate structure within the backpropagation circuit and are connected in a ring. That is, the gating chain is a circular chain of neurons that, once activated, controls the timing of information processing in the rest of the network. This allows information routing throughout the network to be autonomous to realize the benefits of neuromorphic hardware without the need for control by a classical sequential processor. Specifically, the neurons in the gating chain are connected to the relevant layers of the network, which allows them to control when and where information is propagated. All layers are inhibited far away from their firing threshold by default, as described in Section IV B, and can only transmit information, i.e., generate spikes, if their inhibition is neutralized via activation by the gating chain. Because a neuron only fires if it is gated 'on' AND gets sufficient input, such gating corresponds to a logical AND or coincidence detection with the input.</p><p>In our implementation, the gating chain consists of 12 neurons, which induce 12 algorithmic (Loihi) time steps that are needed to process one sample. Each neuron is connected to all layers that must be active in each respective time step. The network layers are connected in the manner shown in <ref type="figure">Supplementary Fig. 4</ref>, but the circuit structure, which is controlled by the gating chain, results in a functionally connected network as presented in <ref type="figure" target="#fig_0">Fig. 2</ref>, where the layers are shown according to the timing of when they become active during one iteration.</p><p>The weight of the standard gating synapses (from one of the gating neurons to each neuron in a target layer) is w gate = ?I const + 0.5V thr , i.e. each neuron that is gated 'on' is brought to half of its firing threshold, which effectively implements Eq. <ref type="bibr" target="#b14">(15)</ref>. In four cases, i.e., for the synapses to the start learning layers in time step 2 (h &lt; ) and 3 (o &lt; ) and to the backpropagated local gradient layer d 1 in time steps 6 and 10, the gating weight is w gate = ?I const + V thr . In two cases, i.e., for the synapses to the stop learning layer in time step 2 (h &gt; ) and 3 (o &gt; ), the gating weight is w gate = ?I const . These different gating inputs lead to step activation functions with different thresholds, as required for the computations explained below, in Section IV B 3. Backpropagation Network Modules. In the previous sections, we have explained how the weight update happens and how to bring the relevant values (a l?1 and d l according to <ref type="bibr" target="#b9">(10)</ref>) to the correct place at the correct time. In this section, we discuss how these values are actually calculated. The signal a l?1 , which is the layer activity from the feedforward pass, does not need to be calculated but only remembered. This is done using a relay layer with synaptic delays, as explained in Section IV B 1. The signal d 2 , the last layer local gradient, consists of 2 parts according to <ref type="bibr" target="#b4">(5)</ref>. The difference between the output and the target o ? t (see Section IV B 2) and the box function f must be calculated. We factorize the latter into two terms, a start and stop learning signal (see Section IV B 3). The signal d 1 , the backpropagated local gradient, also consists of 2 parts, according to Eq. (6). In addition to another 'start' and 'stop' learning signal, we need sgn(W T 2 d 2 ), whose computation is explained in Section IV B 4.</p><p>In the following equation, the weight update is annotated with the number of the paragraph in which the calculating module is described:</p><formula xml:id="formula_18">?W 2 = ? 2. (o ? t) ? 3. f (W 2 h) 1. h T ,<label>(23)</label></formula><formula xml:id="formula_19">?W 1 = ? sgn(W T 2 d 2 )</formula><p>4.</p><p>? f (W 1 x)</p><p>3.</p><p>x T 1.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Relay Neurons</head><p>The memory used to store the activity of the input and the hidden layer is a single relay layer that is connected both from and to the respective layer in a one-to-one manner with the proper delays. The input layer x sends its activity to the relay layer m x so that the activity can be restored in the W 1 update phases in time steps 7 and 11. The hidden layer x sends its activity to the relay layer m h so that the activity can be restored in the W 2 update phases in time steps 5 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Error Calculation</head><p>The error calculation requires a representation of signed quantities, which is not directly possible in a spiking network because there are no negative spikes. This is achieved here by splitting the error evaluation into two parts, t ? o and o ? t, to yield the positive and negative components separately. Similarly, the calculation of back-propagated local gradients, d 1 , is performed using a negative copy of the transpose weight matrix, and it is done in 2 phases for the positive and negative local gradient, respectively. In the spiking neural network, t ? o is implemented by an excitatory synapse from t and an inhibitory synapse of the same strength from o, and vice versa for o ? t. Like almost all other nonplastic synapses in the network, the absolute weight of the synapses is just above the value that makes the target neuron fire, when gated on. The difference between the output and the target is, however, just one part of the local gradient d 2 . The other part is the derivative of the activation function (box function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Start and Stop Learning Conditions</head><p>The box function <ref type="bibr" target="#b16">(17)</ref> can be split in two conditions: a 'start' learning and a 'stop' learning condition. These two conditions are calculated in parallel with the feedforward pass. The feedforward activation f (x) = H(x ? 0.5V thr ) corresponding to Eq. (1) is an application of the spiking threshold to the layer's input with an offset of 0.5V thr , which is given by the additional input from the gating neurons. The first term of the box function <ref type="bibr" target="#b8">(9)</ref>, H(x), is also an application of the spiking threshold, but this time with an offset equal to the firing threshold so that any input larger than 0 elicits a spike. We call this first term the 'start' learning condition, and it is represented in h &lt; for the hidden and in o &lt; for the output layer. The second term of the box function in Eq. (9), ?H(x ? 1V thr ), is also an application of the spiking threshold, but this time without an offset so that only an input larger than the firing threshold elicits a spike. We call this second term the 'stop' learning condition, and it is represented in h &gt; and o &gt; for the hidden and output layers, respectively. For the W 1 update, the two conditions are combined in a box function layer b h = h &lt; ?h &gt; that then gates the d 1 local gradient layer. For the W 2 update, the two conditions are directly applied to the d 2 layers because an intermediate b o layer would waste one time step. The function is however the same: the stop learning o &gt; inhibits the d 2 layers and the 'start' learning signal o &lt; gates them. In our implementation, the two conditions are obtained in parallel with the feedforward pass, which requires two additional copies of each of the two weight matrices. An alternative method to avoid these copies but takes more time steps, would do this computation sequentially and use the synapses and layers of the feedforward pass three times per layer with different offsets, and then route the outputs to their respective destinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Error Backpropagation</head><p>Error calculation and gating by the start learning signal and inhibition by the stop learning signal are combined in time step 4 to calculate the last layer local gradients d + 2 and d ? 2 . From there, the local gradients are routed into the output layer and its copies for the last layer weight update. This happens in 2 phases: The positive local gradient d + 2 is connected without delay so that it leads to potentiation of the forward and backward last layer weight matrices in time step 5. The negative local gradient is connected with a delay of 4 time steps so that it arrives in the depression phase in time step 9. For the connections to the o T ? layer which is connected to the negative copy ?W T 2 , the opposite delays are used to get a weight update with the opposite sign. See <ref type="figure" target="#fig_0">Fig. 2</ref> for a visualization of this mechanism. Effectively, this leads to the last layer weight update</p><formula xml:id="formula_21">?W 2 = ?(H(t ? o) ? f (W 2 h))h T ??(H(o ? t) ? f (W 2 h))h T ,<label>(25)</label></formula><p>where the first term on the right hand side is non-zero when the local gradient is positive, corresponding to an update happening in the potentiation phase, and the second term is nonzero when the local gradient is negative, corresponding to an update happening in the depression phase. The functions f and f are as described in Equations <ref type="bibr" target="#b14">(15)</ref> and <ref type="bibr" target="#b8">(9)</ref>. The local gradient activation in the output layer does not only serve the purpose of updating the last layer weights, but it is also directly used to backpropagate the local gradients. Propagating the signed local gradients backwards through the network layers requires a positive and negative copy of the transposed weights, W T 2 and ?W T 2 , which are the weight matrices of the synapses between the output layer o and the back-propagated local gradient layer d 1 , and between o T ? and d 1 , respectively.</p><p>Here o T ? is an intermediate layer that is created exclusively for this purpose. The local gradient is propagated backwards in two phases. The part of the local gradient that leads to potentiation is propagated back in time steps 5 to 7, and the part of the local gradient that leads to depression of the W 1 weights is propagated back in time steps 9 to 11. In time step 6, the potentiating local gradient is calculated in layer d 1 as</p><formula xml:id="formula_22">d + 1 = H(W T 2 d + 2 + (?W T 2 )d ? 2 ) ? b h ,<label>(26)</label></formula><p>and in timestep 10 the depressing local gradient is calculated in layer d 1 as</p><formula xml:id="formula_23">d ? 1 = H((?W T 2 )d + 2 + W T 2 d ? 2 ) ? b h .<label>(27)</label></formula><p>Critically, this procedure does not simply separate the weights by sign, but rather maintains a complete copy of the weights that is used to associate appropriate sign information to the back-propagated local gradient values. Note that here the Heaviside activation function H(x) is used rather than the binary activation function f = H(x ? 0.5V thr ), so that any positive gradient will induce an update of the weights. Any positive threshold in this activation will lead to poor performance by making the learning insensitive to small gradient values. The transposed weight copies must be kept in sync with their forward counterparts, so the updates in the potentiation and depression phases are also applied to the forward and backward weights concurrently. So in total, after each trial, the actual first layer weight update is the sum of four different parts:</p><formula xml:id="formula_24">?W 1 = ?((H(W T 2 d + 2 + (?W T 2 )d ? 2 )) ? b h )x T ? ?((H((?W T 2 )d + 2 + W T 2 d ? 2 )) ? b h )x T .<label>(28)</label></formula><p>These four terms are necessary because, e.g., a positive error can also lead to depression if backpropagated through a negative weight matrix and the other way round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The sBP Implementation on Loihi</head><p>Partitioning on the Chip. To distribute the spike load over cores, neurons of each layer are approximately equally distributed over 96 cores of a single chip. This distribution is advantageous because only a few layers are active at each time step, and Loihi processes spikes within a core in a sequential manner. In total, the network as presented here needs 2N in +6N hid +7N out +12N gat neurons. With N in = 400, N hid = 400, N out = 10, and N gat = 1, these are 3282 neurons and about 200k synapses, most of which are synapses of the 3 plastic all-to-all connections between the input and the hidden layer.</p><p>Learning Implementation. The learning rule on Loihi is implemented as given in Eq. <ref type="bibr" target="#b20">(21)</ref>. Because the precision of the plastic weights on Loihi is maximally 8 bits with a weight range from ?256 to 254, we can only change the weight in steps of 2 without making the update nondeterministic. This is necessary for keeping the various copies of the weights in sync (hence the factor of 2 in Eq. <ref type="formula" target="#formula_0">(21)</ref>). With V thr = 1024, this corresponds to a learning rate of ? = 2 1024 ? 0.002. The learning rate can be changed by increasing the factor in the learning rule, which leads to a reduction in usable weight precision, or by changing V thr , which changes the range of possible weights according to Eq. <ref type="bibr" target="#b19">(20)</ref>. Several learning rates (settings of V thr ) were tested with the result that the final accuracy is not very sensitive to small changes. The learning rate that yielded the best accuracy is reported here. In the NxSDK API, the neuron traces that are used for the learning rule are x0, y0, and r1 for x(t), y(t) and r(t) in 21 respectively. r1 was used with a decay time constant of 1, so that it is only active in the respective time step, effectively corresponding to r0. To provide the r signal, a single reinforcement channel was used and was activated by a regularly firing spike generator in time steps 5 and 7.</p><p>Weight Initialization. After the He initialization as described in Section IV A, the weights are mapped to Loihi weights according to <ref type="bibr" target="#b19">(20)</ref>. Then, the weights are truncated to [?240, 240] and discretized to 8 bits resolution, i.e, steps of 2, by rounding them to the next valid number towards 0.</p><p>Power Measurements. All Loihi power measurements are obtained using NxSDK version 0.9.9 on the Nahuku32 board ncl-ghrd-01. Both software API and hardware were provided by Intel Labs. All other probes, including the output probes, are deactivated. For the inference measurements, we use a network that only consists of the three feedforward layers with non-plastic weights and the gating chain of four neurons. The power was measured for the first 10000 samples of the training set for the training measurements and all 10000 samples of the test set for the inference measurements. <ref type="bibr">1,7,11 2,7,11 2,7,11 2,5,7,9,11 3,5,9 3,5,9 3,5,9 5,9 5,6,9,</ref> (1) error (target but no output spike) leads to potentiation of the W2 synaptic weight and the positive transpose; (2) the same error leads to depression of the negative transpose via activity of d1; (3) no error because o and t fire at the same location, i.e. there is no update in this iteration; <ref type="bibr" target="#b3">(4)</ref> there is an error (t fires at index 4, but o at index 7), but the local gradient is 0 because it is gated 'off' at index 7 because the derivative of the activation function is 0, i.e. both o &lt; and o &gt; fire. Also, it is not gated 'on' at index 4, because o &lt; does not fire; (5) local gradient (output but not target), leads to potentiation of the weight of the synapses from o T ? to d1 (red), and (6) depression of h ? o and d1 ? o synaptic weights; <ref type="bibr" target="#b6">(7)</ref> The orange spikes show the back-propagated local gradient from (1) which leads to potentiation of the x ? h weights. Note that for visualization purposes, the gating from b h is applied one time step later directly to h, h &lt; and h &gt; . That is, the orange spikes in time step 7 are the full backpropagated error, but only the neurons that are also gated 'on' by the combination of h &lt; and h &gt; are actually active in the potentiation phase in time step 8. (8) Same as <ref type="bibr" target="#b6">(7)</ref>, but the error from (2) leads to depression of the x ? h weights.  <ref type="table" target="#tab_2">TABLE III</ref>. Breakdown of power consumption of the chip for different cases. 'ds100-300-10', means that the network is run using 100 input and 300 hidden layer neurons and with the input downsampled by 2. Here, 'start' means that the measurement is taken in the first 10000 iterations of the first training epoch, and 'end' means that it has been taken with the fully trained model. We use 'learning engine off' to say that the weights are set to nonplastic weights after training so that the on-chip circuitry that handles learning is not active. The inference net, which uses the same weights as the fully trained network, consists only of the 3 feedforward layers and the gating chain.</p><formula xml:id="formula_25">Supplementary Material b h b o -W T 2 W 1 W 2 W T 2 h &lt; h &gt; h x m x m h t d 1 o &lt; o &gt; o o T? d + 2 d ? 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Power Measurements on GPU</head><p>The power on the GPU was measured using nvidia-smi while running the TensorFlow implementation of our algorithm. The dynamic power was calculated by taking the difference between the power reading before the start of the run and during the run. The system is running Tensorflow version 2.3 on Windows 10 with an Intel i5-9600K processor and 16 GB of RAM. The GPU is an NVIDIA GeForce GTX 1600S (driver version: 461.92, CUDA version: 11.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Detailed Description of the Algorithm</head><p>Here we go through all steps of the algorithm as shown in <ref type="figure" target="#fig_0">Fig. 2:</ref> 1. MNIST images are sent to the x layer in binarized form 2. W 1 is applied three times with different offsets (received from the gating chain) before application of the nonlinear activation function. This yields the hidden layer activity and start and stop learning conditions for the W 1 update according to Eq. <ref type="formula" target="#formula_4">(9)</ref> 3. The MNIST labels are sent as a one-hot vector to the target layer and W 2 is applied three times with different offsets. This yields the output layer activity and start and stop learning conditions for the W 2 update according to Eq. <ref type="bibr" target="#b8">(9)</ref>. Furthermore, the input and hidden layer activities are stored into the m x and m h relay layers for later usage in the learning phases. In the box function layer (b h ), the start and stop learning conditions of W 1 are combined using an excitatory and inhibitory connection. This corresponds to the multiplication in Eq. <ref type="formula" target="#formula_4">(9)</ref> as in this case, multiplication is the same as taking the difference with the inverted term. 7. This time step realizes the potentiation of the W 1 weights. The hidden layer and its two siblings receive the local gradient that was propagated back in the previous time step, and the x layer restores its previous activity from the relay layer. Because the third factor is switched on in this time step, potentiation happens according to Eq. (21). This time step implements the first term of Eq. (28).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>To avoid interference of the input to the output layer from the previously active hidden layer (layer h is connected to o, o &lt; and o &gt; ), no layer is gated on in this time step.</p><p>9. The steps 9 to 11 are the same as 5 to 7, but with input of the opposite local gradient and without third factor active and therefore the opposite sign of learning (depression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Synapse Types</head><p>There are five types of synapses (see different colors in <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="figure">Supplementary Fig. 4</ref>):</p><p>? Plastic all-to-all synapses (red), that change weights according to a learning rule (Eq. (21)) and can be both positive or negative (w p = [?256, 254]).</p><p>? Excitatory (black) one-to-one synapses with a fixed weight (w e = V thr = 1024) to copy the firing pattern of the source layer to the target layer.</p><p>? Inhibitory (blue) one-to-one synapses with a fixed weight (w i = ?V thr = ?1024) to always inhibit the target neurons.</p><p>? Excitatory one-to-all gating (green) synapses from a single gating chain neuron to all neurons of a layer with a fixed weight of usually w g = ?I const + 0.5V thr = 8704. The usual gating synapses are used to gate 'on' a layer that is supposed to have the feedforward activation function f . Different weights w g&lt; = ?I const = 8192 and w g&gt; = ?I const + V thr = 9216 are used to gate 'on' the start and stop learning conditions to get the two step functions with which the box function is built.</p><p>? Excitatory one-to-one gating (yellow) synapses from the start learning layer o &lt; and the box function layer b h to the respective local gradient layers. The w g weight is used for gating of the d 2 layers in time step 4, and the w g&gt; weight is used for gating of the d 1 layer in time steps 6 and 10.</p><p>To a certain extent within the precision boundaries of the chip, the scaling of the weights is arbitrary. All plastic synapses and most other synapses have a synaptic delay of 0 time steps. Some synapses, specifically the ones originating from the relay layers, the d 2 layers, and the b h layer, have delays up to 6 time steps, i.e. their output spikes affect the target neuron several time steps later. The delays, corresponding to the horizontal arrow length, can be read from <ref type="figure" target="#fig_0">Fig. 2</ref>. <ref type="table" target="#tab_2">Table III</ref> above refers to network structures with input sizes of 400 and 100. The 400-400-10 network used a set of trimmed images, where whitespace was removed to reduce the required size of the input layer. MNIST images for this had a border of width 4 removed. For the smaller 100-300-10 network, these images were then downsampled by averaging over 2 ? 2 pixel grids and binarized by thresholding at 0.5. This gave a set of 10 ? 10 binary images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Downsampled MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tensorflow Implementation</head><p>For the feedforward network, our GPU tensorflow implementation is similar to the binary network presented in <ref type="bibr" target="#b67">[68]</ref> but with continuous weights, and simplified to correspond with the neuromorphic design. The loss function was modified to mean squared error and minimized with stochastic gradient descent with a fixed learning rate. Dropout and batch normalization were removed and the batch size was set to 1.</p><p>The network was initialized with Glorot parameters for each layer l, i.e., ? l = 1.5 N l?1 +N l where N l is the number of neurons in layer l. For each layer, weights were generated uniformly on the interval [?? l , ? l ] and clipped to these values during learning. While these initializations do not take into account the binary activation functions of our network, tests with alternative initializations showed little or no improvement, likely due to the shallow depth of the networks considered.</p><p>Gradients on the binary network in <ref type="bibr" target="#b67">[68]</ref> were calculated with a 'straight-through' estimator in which the derivative of the binary activation function ?(x) = 1 x ? 0 0 otherwise <ref type="bibr" target="#b28">(29)</ref> was approximated as</p><formula xml:id="formula_26">h l (x) = ? ? ? ? ? 0 x ? ? l 1 2? l ?? l &lt; x &lt; ? l 0 x &lt; ? l .<label>(30)</label></formula><p>This same gradient calculation is maintained, but to mimic the binary gradient signal in the neuromorphic implementation, the final gradient with respect to each weight is thresholded. If the gradient calculated with respect to weight i is g i , the applied gradient in the learning step is sgn(g i )?(|g i | ? t) for a threshold t representing the threshold of signal propagation in the neuromorphic implementation and ? as defined above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 2 .</head><label>2</label><figDesc>Functional connectivity of the 2 layer backpropagation circuit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 3 .</head><label>3</label><figDesc>Accuracy and loss (mean squared error) over epochs. Note separate axis scaling for accuracy (left) and loss (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 4 .FIG. 5 .</head><label>45</label><figDesc>Anatomical connectivity of the 2 layer backpropagation circuit. While in the Loihi implementation the o layers are connected directly go to the d2 layers, here an intermediate fictional bo layer is added for easier understanding. Arrows that end on the border of a box that encompasses several layers go to each of the layers. The gating chain is not shown, but the small numbers on top of each layer indicate when it is gated on. Colors are the same as in Fig. 2. Example raster plot of the spikes over six gating cycles. All populations of the same size are plotted in the same plot and only the first 50 neurons are plotted per layer. To avoid occlusion, a small offset in time is added to the time step of some layers. Refer to Tab. II for a detailed explanation of the spike propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 . 5 .</head><label>45</label><figDesc>The positive and negative errors are calculated by excitation from the o layer and inhibition from the t layer (and vice versa). The local gradient layers are not gated on by the gating chain but by the start learning condition o &lt; , and furthermore, they are inhibited by the stop learning condition o &gt; . This way, the Hadamard product from Eq. (5) is calculated. The positive local gradient is now sent to the output layer (and its two siblings) while at the same time the hidden layer activity from time step 2 is reactivated from the relay layer. This leads to a positive weight update of W 2 , and because also the d 1 layer is activated, to updates of W T 2 and W T ?2 according to the learning rule in Eq. (21). Weights W T ? 2 are however potentiated oppositely because layer o T ? receives input from layer d ? in this time step. This time step implements the first term of Eq. (25). 6. The actual backpropagation happens in this time step. By way of the transposed weight matrices, the d 1 layer receives activity from the output layer and the o T ? layer, which contains the positive and negative local gradients. The layer is gated by the box function layer b h , which calculates the Hadamard product from Eq. (6).This gating includes an offset so that even the smallest possible nonzero input sum leads to a spike in the neurons of the d 1 layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are calculated from the output and the training signal (t). Errors are backpropagated through a feedback module with the same weights W2 for synapses between h and o, but in the opposite direction (mathematically expressed as the transpose, W T</figDesc><table><row><cell>Feedback</cell><cell>Error</cell></row><row><cell></cell><cell>Target</cell></row><row><cell>Feed-forward</cell><cell></cell></row><row><cell cols="2">FIG. 1. Overview of conceptual circuit architecture.</cell></row><row><cell cols="2">Feedforward activations of input (x), hidden (h) and output</cell></row><row><cell cols="2">(o) layers are calculated by a feedforward module. Errors (e =</cell></row><row><cell>t ? o)</cell><cell></cell></row></table><note>2 ). Local gradients (d1, d2) are gated back into the feedforward circuit at appropriate times to accomplish potentiation or depression of appropriate weights.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>On-chip single layer training or BP alternatives [36] Shrestha et al. (2021) 400 (20x20) corresponds to 784 (28x28) after cropping of the empty image margin of 4 pixels</figDesc><table><row><cell>Publication</cell><cell>Hardware</cell><cell>Learning Mode</cell><cell>Network</cell><cell cols="3">Energy per Latency per Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Structure</cell><cell cols="3">Sample (mJ) Sample (ms) Accuracy (%)</cell></row><row><cell>On-chip backpropagation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>This study</cell><cell>Loihi</cell><cell>on-chip sBP</cell><cell>400-400-10 a</cell><cell>0.592</cell><cell>1.48</cell><cell>96.2</cell></row><row><cell></cell><cell>Loihi</cell><cell cols="2">EMSTDP FA/DFA CNN-CNN-100-10</cell><cell>8.4</cell><cell>20</cell><cell>94.7</cell></row><row><cell>[35] Frenkel et al. (2020)</cell><cell>SPOON</cell><cell>DRTP</cell><cell>CNN-10</cell><cell>0.000366 b</cell><cell>0.12</cell><cell>95.3</cell></row><row><cell>[33] Park et al. (2019)</cell><cell>unnamed</cell><cell>mod. SD</cell><cell>784-200-200-10</cell><cell>0.000253 b</cell><cell>0.01</cell><cell>98.1</cell></row><row><cell>[72] Chen et al. (2018)</cell><cell>unnamed</cell><cell>S-STDP</cell><cell>236-20 c</cell><cell>0.017</cell><cell>0.16</cell><cell>89</cell></row><row><cell>[30] Frenkel et al. (2018)</cell><cell>ODIN</cell><cell>SDSP</cell><cell>256-10</cell><cell>0.000015</cell><cell>-</cell><cell>84.5</cell></row><row><cell>[73] Lin et al. (2018)</cell><cell>Loihi</cell><cell>S-STDP</cell><cell>1920-10 c</cell><cell>0.553</cell><cell>-</cell><cell>96.4</cell></row><row><cell>[32] Buhler et al. (2017)</cell><cell>unnamed</cell><cell>LCA features</cell><cell>256-10</cell><cell>0.000050</cell><cell>0.001 b</cell><cell>88</cell></row><row><cell>On-chip inference only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>This study</cell><cell>Loihi</cell><cell>inference</cell><cell>400-400-10 a</cell><cell>0.00249</cell><cell>0.169</cell><cell>96.2</cell></row><row><cell>[36] Shrestha et al. (2021)</cell><cell>Loihi</cell><cell>inference</cell><cell>CNN-CNN-100-10</cell><cell>2.47</cell><cell>10</cell><cell>94.7</cell></row><row><cell>[35] Frenkel et al. (2020)</cell><cell>SPOON</cell><cell>inference</cell><cell>CNN-10</cell><cell>0.000313</cell><cell>0.12</cell><cell>97.5</cell></row><row><cell>[74] G?ltz et al. (2019)</cell><cell cols="2">BrainScaleS-2 inference</cell><cell>256-246-10</cell><cell>0.0084</cell><cell>0.048</cell><cell>96.9</cell></row><row><cell>[73] Lin et al. (2018)</cell><cell>Loihi</cell><cell>inference</cell><cell>1920-10 c</cell><cell>0.0128 d</cell><cell>-</cell><cell>96.4</cell></row><row><cell>[72] Chen et al. (2018)</cell><cell>unnamed</cell><cell>inference</cell><cell>784-1024-512-10</cell><cell>0.0017</cell><cell>-</cell><cell>97.9</cell></row><row><cell>[76] Esser et al. (2015)</cell><cell>True North</cell><cell>inference</cell><cell cols="2">CNN (512 neurons) 0.00027</cell><cell>1</cell><cell>92.7</cell></row><row><cell>[76] Esser et al. (2015)</cell><cell>True North</cell><cell>inference</cell><cell cols="2">CNN (3840 neurons) 0.108</cell><cell>1</cell><cell>99.4</cell></row><row><cell>[77] Stromatias et al. (2015)</cell><cell>SpiNNaker</cell><cell>inference</cell><cell>784-500-500-10</cell><cell>3.3</cell><cell>11</cell><cell>95</cell></row><row><cell cols="2">Neuromorphic sBP in simulated SNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[78] Jin et al. (2018)</cell><cell>Simulation</cell><cell>BP</cell><cell>784-800-10</cell><cell>-</cell><cell>-</cell><cell>98.8</cell></row><row><cell>[79] Neftci et al. (2017)</cell><cell>Simulation</cell><cell>BP</cell><cell>784-500-10</cell><cell>-</cell><cell>-</cell><cell>97.7</cell></row><row><cell>[80] Shrestha et al. (2019)</cell><cell>Simulation</cell><cell>EM-STDP</cell><cell>784-500-10</cell><cell>-</cell><cell>-</cell><cell>97</cell></row><row><cell>[81] Tavanaei and Maida (2019)</cell><cell>Simulation</cell><cell>BP-STDP</cell><cell>784-500-150-10</cell><cell>-</cell><cell>-</cell><cell>97.2</cell></row><row><cell>[82] Mostafa (2017)</cell><cell>Simulation</cell><cell>BP</cell><cell>784-800-10</cell><cell>-</cell><cell>-</cell><cell>97.55</cell></row><row><cell>[83] Lee et al. (2016)</cell><cell>Simulation</cell><cell>BP</cell><cell>784-800-10</cell><cell>-</cell><cell>-</cell><cell>98.64</cell></row><row><cell cols="2">[84] O'Connor and Welling (2016) Simulation</cell><cell>BP</cell><cell>784-300-300-10</cell><cell>-</cell><cell>-</cell><cell>96.4</cell></row><row><cell>[85] Diehl and Cook (2015)</cell><cell>Simulation</cell><cell>STDP</cell><cell>784-1600-10</cell><cell>-</cell><cell>-</cell><cell>95</cell></row></table><note>ab Calculated from given valuesc Off-chip preprocessingd Dynamic energy reported in the Supplementary Material of [75]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I .</head><label>I</label><figDesc>Review of the MNIST Literature in SNN and on neuromorphic hardware. The table includes 4 relevant classes of literature. Studies that used ANN-SNN conversion purely in software are not reviewed here. Note that the energy-delay product may be computed from the Energy per Sample and Latency per Sample columns. Abbreviations: EMSTDP: Error-modulated spike-timing dependent plasticity; DFA: Direct feedback alignment; DRTP: Direct random target projection; SD: Segregated dendrites; SDSP: Spike-driven synaptic plasticity; LCA: Locally competitive algorithm.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>The authors contributed equally to the methodology presented here. Alpha Renner adapted and implemented the algorithm on Intel's Loihi chip. Alpha Renner, Forrest Sheldon, and Anatoly Zlotnik formalized neuromorphic information-processing mechanisms, implemented the algorithm in simulation and hardware, and developed figures. All authors wrote the manuscript with Renner, Sheldon, and Zlotnik doing the bulk of the writing. Andrew Sornborger and Anatoly Zlotnik supervised the research and Sornborger and Louis Tao developed the concepts and algorithmic basis of the neuromorphic backpropagation algorithm and circuit structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Financial Interests</head><p>The authors declare that they have no competing financial interests.</p><p>Step</p><p>Step 6 7 8 9 10 11</p><p>backpropagation potentiation depression backpropagation depression w in,hid w hid,out w in,hid TABLE II. Information flow through the backpropagation network (see <ref type="figure">Fig. 2</ref>). Gating pulses (g1 ? g11) are sent from the gating chain. The triplet notation (a, b, c) denotes information b in population a gated by population c.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Control of synaptic plasticity in deep cortical networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtmaat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="166" to="180" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Linnainmaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Helsinki</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
	<note>in Finnish</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beyond regression:&quot; new tools for prediction and analysis in the behavioral sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="http://www.dtic.mil/docs/citations/{ADA164453}" />
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Backpropagation and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using goal-driven deep learning models to understand sensory cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="356" to="365" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neuromorphic electronic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1629" to="1636" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Loihi: A neuromorphic manycore processor with on-chip learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2018.112130359</idno>
		<ptr target="https://doi.org/10.1109/MM.2018.112130359" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks for fast, energyefficient neuromorphic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Esser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="11441" to="11446" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conversion of continuous-valued deep networks to efficient event-driven networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rueckauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-A</forename><surname>Lungu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">682</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training deep neural networks for binary communication with the whetstone method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Severa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Vineyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dellana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Verzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Aimone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Competitive learning: From interactive activation to adaptive resonance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=54455.54464" />
	</analytic>
	<monogr>
		<title level="m">Connectionist Models and Their Implications: Readings from Cognitive Science</title>
		<editor>Waltz, D. &amp; Feldman, J. A.</editor>
		<meeting><address><addrLine>Norwood, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Ablex Publishing Corp</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="243" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The recent excitement about neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Crick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SpiNNaker: A multi-core systemon-chip for massively-parallel neural net simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Painkras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2012 Custom Integrated Circuits Conference</title>
		<meeting>the IEEE 2012 Custom Integrated Circuits Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A wafer-scale neuromorphic hardware system for large-scale neural modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2010 IEEE International Symposium on Circuits and Systems</title>
		<meeting>the IEEE 2010 IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Competitive learning: From interactive activation to adaptive resonance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="63" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How important is weight symmetry in backpropagation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Errorbackpropagation in temporally encoded networks of spiking neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>La Poutr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toyoizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.6.1318</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.2006.18.6.1318" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1318" to="1348" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Supervised learning in multilayer spiking neural networks. Neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Superspike</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1514" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient descent for spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nengodl: Combining deep learning and neuromorphic modelling methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="611" to="628" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper in spiking neural networks: VGG and residual architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">https:/www.frontiersin.org/article/10.3389/fnins.2019.00095/full</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fnins.2019.00095/full" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slayer: Spike layer error reassignment in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mapping high-performance rnns to in-memory neuromorphic chips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10692</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nxtf: An api and compiler for deep spiking neural networks on intel loihi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rueckauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04261</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On-chip few-shot learning with surrogate gradient descent on a neuromorphic processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="223" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nengo and low-power ai hardware for robust, embedded neurorobotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A 0.086-mm 2 12.7-pj/sop 64k-synapse 256-neuron onlinelearning digital spiking neuromorphic processor in 28-nm cmos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Legat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical circuits and systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A 640m pixel/s 3.65 mw sparse event-driven neuromorphic object recognition processor with on-chip learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Knag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Symposium on VLSI Circuits (VLSI Circuits)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="50" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A 3.43 tops/w 48.9 pj/pixel 50.1 nj/classification 512 analog neuron sparse coding neural network with on-chip learning and classification in 40nm cmos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Buhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Symposium on VLSI Circuits</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="30" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">.6 a 65nm 236.5 nj/classification neuromorphic processor with 7.5% energy overhead on-chip learning using direct spike-only feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Solid-State Circuits Conference-(ISSCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="140" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">experimental demonstration of supervised learning in spiking neural networks with phase-change memory synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nandakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A 28-nm convolutional neuromorphic processor enabling online learning with spike-based retinas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Legat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">In-hardware learning of multilayer spiking neural networks on a neuromorphic processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>In to appear in 2021 58th ACM/ESDA/IEEE Design Automation Conference (DAC</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rapid online learning and robust recall in a neuromorphic olfactory circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Cleland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Demonstrating hybrid learning in a flexible neuromorphic hardware system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical circuits and systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="128" to="142" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mixed-precision deep learning based on computational memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nandakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">406</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On-chip error-triggered learning of multi-layer memristive spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Payvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Fouda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kurdahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Eltawil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="522" to="535" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Payeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerguiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Naud</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.03.30.015511</idno>
		<ptr target="https://doi.org/10.1101/2020.03.30.015511" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A solution to the learning dilemma for recurrent networks of spiking neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">738385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dendritic cortical microcircuits approximate the backpropagation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8721" to="8732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is backpropagation biologically plausible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The neurobiological significance of the new learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Neuroscience</title>
		<editor>Schwartz, E. L.</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="192" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<idno type="DOI">http:/www.mitpressjournals.org/doi/10.1162/neco.1996.8.5.895</idno>
		<ptr target="http://www.mitpressjournals.org/doi/10.1162/neco.1996.8.5.895" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="895" to="938" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Backpropagation without weight transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pollack</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/374486/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN&apos;94)</title>
		<meeting>1994 IEEE International Conference on Neural Networks (ICNN&apos;94)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1375" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Random synaptic feedback weights support error backpropagation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How important is weight symmetry in backpropagation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3016100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1837" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dendritic solutions to the credit assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2018.08.003</idno>
		<ptr target="http://dx.doi.org/10.1016/j.conb.2018.08.003" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="28" to="36" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-time classification and sensor fusion with a spiking deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2013.00178</idno>
		<ptr target="http://dx.doi.org/10.3389/fnins.2013.00178" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple framework for constructing functional spiking recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="22811" to="22820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Surrogate gradient learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Neftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Solving the distal reward problem through linkage of STDP and dopamine signaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhl152</idno>
		<ptr target="http://dx.doi.org/10.1093/cercor/bhl152" />
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2443" to="2452" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Supervised learning in multilayer spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sporea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gr?ning</surname></persName>
		</author>
		<idno type="DOI">10.1162/{NECO_a_00396}</idno>
		<ptr target="http://dx.doi.org/10.1162/{NECO_a_00396}" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="473" to="509" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A learning theory for reward-modulated spike-timing-dependent plasticity with application to biofeedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pecevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1000180</idno>
		<ptr target="http://dx.doi.org/10.1371/journal.pcbi.1000180" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1000180</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neuromodulated spiketiming-dependent plasticity, and theory of three-factor learning rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fr?maux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncir.2015.00085</idno>
		<ptr target="http://dx.doi.org/10.3389/fncir.2015.00085" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neural Circuits</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A pulse-gated, neural implementation of the backpropagation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zlotnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual Neuroinspired Computational Elements Workshop</title>
		<meeting>the 7th Annual Neuroinspired Computational Elements Workshop</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A mechanism for graded, dynamically routable current propagation in pulse-gated synfire chains and implications for information coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Neurosci</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graded, dynamically routable information processing with synfire-gated synfire chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comp Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A fokker-planck approach to graded information propagation in pulse-gated feedforward neuronal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00520</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mutual information and information gating in synfire chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A pulse-gated, predictive neural circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Asilomar Conference on Signals</title>
		<meeting>the 50th Asilomar Conference on Signals</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A mechanism for synaptic copy between neural circuits. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://www.biorxiv.org/content/early/2018/06/20/351114" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The Organization of Behavior: A Neuropsychological Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Exact, dynamically routable current propagation in pulse-gated synfire chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sornborger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<idno>ArXiv:1410.1115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning only when necessary: better memories of correlated patterns in networks with bounded synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2106" to="2138" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A 4096-neuron 1m-synapse 3.8-pj/sop spiking neural network with on-chip stdp learning and sparse weights in 10-nm finfet cmos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Knag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="992" to="1002" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Programming spiking neural networks on intel&apos;s loihi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Fast and deep: energy-efficient neuromorphic learning with first-spike times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>G?ltz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11443v4</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Advancing neuromorphic computing with loihi: A survey of results and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Backpropagation for energy-efficient neuromorphic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Scalable energy-efficient, lowlatency implementations of trained spiking deep belief networks on spinnaker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stromatias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Hybrid macro/micro level backpropagation for training deep spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07866</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Event-driven random back-propagation: Enabling neuromorphic deep learning machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Neftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Detorakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">324</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Approximating back-propagation for a biologically plausible local learning rule in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neuromorphic Systems</title>
		<meeting>the International Conference on Neuromorphic Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bp-stdp: Approximating backpropagation using spike timing dependent plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Supervised learning based on temporal coding in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3227" to="3235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Training deep spiking neural networks using backpropagation. Frontiers in neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">508</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Deep spiking networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08323</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Neurobiological significance of new learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Neuroscience</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Backpropagation without weight transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN&apos;94)</title>
		<meeting>1994 IEEE International Conference on Neural Networks (ICNN&apos;94)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1375" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>St?ckl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Responses of neurons in primary and inferior temporal visual cortices to natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B: Biological Sciences</title>
		<meeting>the Royal Society of London. Series B: Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="1775" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Temporal coding in spiking neural networks with alpha synaptic function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Comsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8529" to="8533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Conversion of analog to spiking neural networks using sparse temporal coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rueckauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Visual pattern recognition with on on-chip learning: towards a fully neuromorphic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Neural state machines for robust learning and control of neuromorphic agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="679" to="689" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Spike synchronization and rate modulation differentially involved in motor cortical function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Riehle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gr?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diesmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aertsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="page" from="1950" to="1953" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Spatiotemporal firing patterns in the frontal cortex of behaving monkeys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abeles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Margalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vaadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1629" to="1638" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An ultra-sparse code underliesthe generation of neural sequences in a songbird</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Fee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Synfire chains and cortical songs: temporal modules of cortical activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ikegaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page" from="559" to="564" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Reverse replay of behavioural sequences in hippocampal place cells during the awake state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="680" to="683" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Recurrent Network Models of Sequence Generation and Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="128" to="142" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Fast and flexible sequence induction in spiking neural networks via rapid excitability changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Fairhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">44324</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Awake hippocampal reactivations project onto orthogonal neuronal assemblies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malvache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reichinnek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="1280" to="1283" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Packetbased communication in the cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="745" to="755" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A review of binarized neural networks. Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">661</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">From understanding computation to understanding neural circuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
