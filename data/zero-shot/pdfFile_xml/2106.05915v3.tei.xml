<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anatomy-XNet: An Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification in Chest X-rays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-12">12 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kamal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Nusrat</roleName><forename type="first">Mohammad</forename><surname>Zunaed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binta</forename><surname>Nizam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Taufiq</forename><surname>Hasan</surname></persName>
						</author>
						<title level="a" type="main">Anatomy-XNet: An Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification in Chest X-rays</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-12">12 Aug 2022</date>
						</imprint>
					</monogr>
					<note>arXiv:2106.05915v3 [eess.IV] IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anatomy-aware attention</term>
					<term>chest radiography</term>
					<term>semi-supervised learning</term>
					<term>anatomical segmentation</term>
					<term>thoracic dis- ease classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thoracic disease detection from chest radiographs using deep learning methods has been an active area of research in the last decade. Most previous methods attempt to focus on the diseased organs of the image by identifying spatial regions responsible for significant contributions to the model's prediction. In contrast, expert radiologists first locate the prominent anatomical structures before determining if those regions are anomalous. Therefore, integrating anatomical knowledge within deep learning models could bring substantial improvement in automatic disease classification. Motivated by this, we propose Anatomy-XNet, an anatomy-aware attention-based thoracic disease classification network that prioritizes the spatial features guided by the pre-identified anatomy regions. We adopt a semisupervised learning method by utilizing available small-scale organ-level annotations to locate the anatomy regions in largescale datasets where the organ-level annotations are absent. The proposed Anatomy-XNet uses the pre-trained DenseNet-121 as the backbone network with two corresponding structured modules, the Anatomy Aware Attention (A 3 ) and Probabilistic Weighted Average Pooling (PWAP), in a cohesive framework for anatomical attention learning. We experimentally show that our proposed method sets a new state-of-the-art benchmark by achieving an AUC score of 85.78%, 92.07%, and, 84.04% on three publicly available large-scale CXR datasets-NIH, Stanford CheXpert, and MIMIC-CXR, respectively. This not only proves the efficacy of utilizing the anatomy segmentation knowledge to improve the thoracic disease classification but also demonstrates the generalizability of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C HEST radiography (CXR) is the most commonly used primary screening tool for assessing thoracic diseases <ref type="bibr" target="#b0">[1]</ref>. Each year a massive number of CXRs are produced, and the diagnosis is performed mainly by radiologists. With the severe shortage of expert radiologists, especially in developing countries, computer-aided disease detection from chest radiographs is considered the future of medical diagnosis <ref type="bibr" target="#b1">[2]</ref>, <ref type="figure">Fig. 1</ref>: Overview of the proposed semi-supervised anatomyaware attention-based thoracic disease classification framework. A semi-supervised technique is utilized to generate anatomy masks for unannotated CXR images. Then, with the help of our proposed novel anatomy-aware attention module, anatomical information is integrated into the classification network for pathology detection. <ref type="bibr" target="#b2">[3]</ref>. Advancement in deep learning and artificial intelligence offers several ways of rapid, accurate, and reliable screening techniques <ref type="bibr" target="#b3">[4]</ref>. These techniques can significantly impact the health systems in the resource-constrained regions of the world where there is a high prevalence of thoracic diseases and a shortage of expert radiologists.</p><p>Driven by many publicly accessible large-scale CXR datasets, a significant amount of research efforts have been carried out for the automatic diagnosis of thoracic diseases. Wang et al. <ref type="bibr" target="#b4">[5]</ref> first announced the ChestX-ray14 dataset and proposed a unified weakly-supervised classification network by introducing various multi-label DCNN losses based on ImageNet pre-trained deep CNN models. LLAGnet <ref type="bibr" target="#b5">[6]</ref> is a novel lesion location attention guided network containing two corresponding attention modules which focus on the discriminative features from lesion areas for multi-label thoracic disease classification in CXRs. Wang et al. <ref type="bibr" target="#b6">[7]</ref> proposed a DenseNet-121 based triple learning approach that integrates three attention modules which are unified for channel-wise, element-wise, and scale-wise attention learning.</p><p>In medical practice, interpretation of chest X-rays, or any other medical imaging modalities for that matter, requires an understanding of the relevant human anatomy that is being imaged. For example, fundamental analysis of chest X-rays involves the radiologist determining if the trachea is central, the lungs are uniformly expanded, the lung fields are clear, and the heart size is normal <ref type="bibr" target="#b7">[8]</ref>. These and other similar observations form the basis of CXR interpretation by human vision, where it is clear that knowledge of anatomical structures is vital. Real-world radiologists tend to locate the vital anatomy regions first and then determine if those regions have abnormalities. Similarly, successful implementation of deep learning-based thoracic disease classification approaches requires not only higher accuracy but also interpretability. However, most previous research works in automated analysis of CXRs do not consider this aspect and address the problem as any other computer vision problem. Most previous methods employed a global learning strategy <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or relied on attention mechanisms <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, that try to determine the spatial regions that are more responsible for model prediction. In <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> methods have been proposed to integrate segmentation masks into the backbone framework. However, proper contour-level annotations for large-scale datasets <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> are unavailable. Generating segmentation masks from a minimal amount of annotated datasets (e.g., Japanese Society of Radiological Technology (JSRT) <ref type="bibr" target="#b17">[18]</ref>) for these large-scale datasets lead to imperfect segmentation masks. However, the approaches in <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> did not consider the effect of the imperfect segmentation masks in their proposed frameworks. These imperfections of the segmentation masks lead to difficulty for the backbone model to properly identify the anatomy regions.</p><p>In this work, we propose an anatomy-aware attention-based architecture named Anatomy-XNet that utilizes the anatomy segmentation information along with CXRs frames to classify thoracic diseases. A significant challenge to integrate anatomy information into the framework is the lack of proper contourlevel anatomy region annotations for large-scale datasets such as NIH <ref type="bibr" target="#b4">[5]</ref>, CheXpert <ref type="bibr" target="#b15">[16]</ref>, and MIMIC-CXR <ref type="bibr" target="#b16">[17]</ref>. To solve this problem, we leverage a semi-supervised learning technique <ref type="bibr" target="#b18">[19]</ref>, requiring only a handful of annotated instances that enables us to utilize small scale dataset like JSRT <ref type="bibr" target="#b17">[18]</ref> to train the segmentation network and generate the anatomy segmentation masks for the NIH, CheXpert, and MIMIC-CXR datasets. However, one downside of this method is that it doesn't guarantee similar performance compared to any supervised learning method <ref type="bibr" target="#b18">[19]</ref>. In order to mitigate this problem, we incorporate a novel structured module called Anatomy Aware Attention (A 3 ) on top of the backbone feature extractor, Densenet-121, in a united framework. The A 3 module not only reinforces the sensitivity of the different stages of the model to prioritize the anatomical location responsible for a thoracic disease, but also retains information outside the masks through the residual attention vector and thus is less affected by the imperfect anatomy masks. In addition, we propose a novel pooling operation layer, named Probabilistic Weighted Average Pooling (PWAP), that explicitly leverages the probability attention map derived from the feature activation map to enhance the salient regions of the feature space. An overview of our proposed framework is presented in <ref type="figure">Fig. 1</ref>. The contributions of this paper are summarized as follows:</p><p>? We propose novel hierarchical feature-fusion-based A <ref type="bibr" target="#b2">3</ref> modules that learn to re-calibrate the feature maps in different stages of the model based on anatomical knowledge to improve the classification performance and the model's robustness to imperfection in anatomy masks.</p><p>? We incorporate novel PWAP modules that utilizes a learnable re-weighting mechanism based on spatial feature importance before performing spatial feature aggregation. ? Our proposed Anatomy-XNet achieves new state-of-theart performances with AUC scores of 85.78%, 92.07%, and, 84.04% on three publicly available large-scale CXR datasets, NIH, Stanford CheXpert, and, MIMIC-CXR, respectively. These extensive experiments demonstrate the effectiveness of utilizing prior anatomy knowledge and prove the generalizability of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Organ Segmentation from Chest Radiographs</head><p>There are several methods for organ segmentation from a CXR image. Among the classical signal processing based methods, a hybrid approach by Shao et al. <ref type="bibr" target="#b19">[20]</ref> combining active shape and appearance models, a combined approach of landmark-based segmentation and a random forest classifier by Ibragimov et al. <ref type="bibr" target="#b20">[21]</ref>, an active shape framework addressing the initialization dependency of these active shape models proposed by Xu et al. <ref type="bibr" target="#b21">[22]</ref> are noteworthy. In the advent of deep learning, Convolutional neural network (CNN) based segmentation of medical images has attracted wider attention of researchers. An end-to-end contour-aware CNN-based segmentation method is shown to provide organ contour information and improve the segmentation accuracy <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, lung segmentation is performed from CXRs using Generative adversarial networks. However, this model is not generalizable to new datasets. Two-stage deep learning techniques such as patch classification and reconstruction of lung fields can be used for lung segmentation from CXR images <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Disease Classification from Chest Radiographs 1) Methods without Utilizing Segmentation Masks:</head><p>Many signal processing and deep learning approaches have been proposed to classify thoracic diseases in recent years. Tang et al. <ref type="bibr" target="#b9">[10]</ref> identified the disease category and localized the lesion areas through an attention-guided curriculum learning method. In <ref type="bibr" target="#b25">[26]</ref>, multiple feature integration is presented using shallow handcrafted techniques and a pre-trained deep CNN model. DualCheXNet <ref type="bibr" target="#b26">[27]</ref> is an approach that enables two different feature fusion operations, such as feature-level fusion and decision level fusion, which form the complementary feature learning embedded in the network. LLAGnet <ref type="bibr" target="#b5">[6]</ref> is a novel lesion location attention guided network containing two corresponding attention modules which focus on the discriminative features from lesion locations for multi-label thoracic disease classification in CXRs. Guan et al. <ref type="bibr" target="#b27">[28]</ref> proposed a category-wise residual attention learning framework for multi-label thoracic disease classification. Rajpurkar et al. <ref type="bibr" target="#b8">[9]</ref> exploited a modified 121-layer DenseNet named CheXNet, for diagnosis of all 14 pathologies in the ChestXray14 dataset, especially for pneumonia. In <ref type="bibr" target="#b6">[7]</ref>, a triple learning approach integrating a unified channel-wise, element-wise, and scalewise attention modules are used. They can simultaneously learn disease-discriminative channels, locations, and scales for  <ref type="bibr" target="#b28">[29]</ref> fused semantic features from radiology reports along with encoded X-ray features to feed into transformer encoder to utilize both CXR images and metadata related to them. Zhang et al. <ref type="bibr" target="#b29">[30]</ref> proposed a medical concept graph, based on prior knowledge, to diagnose CXR images. Seyyed-Kalantar et al. <ref type="bibr" target="#b30">[31]</ref> examined the extent to which state-of-the-art deep learning classifiers show true positive rate disparity among different protected attributes. Allaouzi et al. <ref type="bibr" target="#b31">[32]</ref> explored binary relevance (BR), label powerset, and classifier chain in terms of label dependencies. Yan et al. <ref type="bibr" target="#b10">[11]</ref> proposed a weakly supervised deep learning framework equipped with squeeze and excitation blocks, multi-map transfer, and max-min pooling for classifying and localizing suspicious lesion regions. Luo et al. <ref type="bibr" target="#b32">[33]</ref> adopted task-specific adversarial training and an uncertainty-aware temporal ensemble of model predictions to address the domain and label discrepancies across different datasets. To handle label uncertainty on the CheXpert dataset, Irvin et al. <ref type="bibr" target="#b15">[16]</ref> trained a DenseNet-121 on CheXpert with various labeling policies such as U-Ignore, U-Ones, and U-Zeros policies. Pham et al. <ref type="bibr" target="#b33">[34]</ref> exploited dependencies among abnormality labels and utilized label smoothing technique for better handling of uncertain samples in the CheXpert dataset. However, a systematic exploration of the potential of integrating anatomical prior to improve the classification performance was absent in all the above mentioned methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Methods Utilizing Segmentation Masks:</head><p>Xu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a dual-stage approach (segmentation and classification) to utilize mask-attention-mechanism as spatial attention to adjust salient features of the CNN. Their attention mask suppresses the receptive field of the CNN based on their overlapping rates with the segmentation masks. Keidar et al. <ref type="bibr" target="#b12">[13]</ref> proposed a deep learning-based model, along with the segmentation masks as additional input, for the detection of COVID-19 from CXRs. Segmentation-based Deep Fusion Network (SDFN) <ref type="bibr" target="#b13">[14]</ref> is a method that leverages the domain knowledge and the higher-resolution information of local lung regions. The local lung regions are identified using Lung Region Generator, and discriminative features are extracted using two CNN models. Then these features are fused by the feature fusion module for the disease classification process. Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref> proposed a two-stage method where the surrounding area around anatomy regions are removed from the CXR image based on the segmentation masks to remove any classification bias towards the extraneous (i.e., non-anatomy) regions of the image. Afterward, they fed the CXR image constrained by the segmentation mask to a CNN model. Overall, the methods described in <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> utilized small-scale annotated datasets in a supervised training setting to generate segmentation masks for large-scale datasets used in their approaches. However, in these methods, the effect of imperfect segmentation masks was not considered, which naturally arises from supervised training of the segmentation network using out-of-distribution data resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semi-supervised Anatomy Segmentation Network</head><p>For semi-supervised segmentation of anatomy regions, we adopted the method from <ref type="bibr" target="#b18">[19]</ref> which is based on the popular CycleGAN architecture <ref type="bibr" target="#b34">[35]</ref>. The CycleGAN architecture comprises of four interconnected blocks, two conditional generators, and two discriminators as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The first generator (G CM ), corresponding to the segmentation network that we want to obtain, learns a mapping from a CXR image to its anatomy segmentation mask. The first discriminator (D M ) takes either the generated mask from G CM or the real segmentation mask as input, and learns to differentiate one from another. Conversely, the second generator (G MC ) learns to map a segmentation mask back to its CXR image. The second discriminator (D C ) receives a CXR image as input (either a real CXR image or a generated CXR from G MC ) and predicts whether this image is real or generated. To enforce cycle consistency criterion, the segmentation network is trained in a way so that feeding the segmentation mask generated by G CM for a CXR image into G MC returns the same CXR image. Similarly, passing back the CXR image generated by G MC to G CM for a segmentation mask returns the same mask.</p><p>1) Loss functions: The segmentation setting contains two distinct subsets: subset L, containing annotated CXR images ? L and their corresponding ground-truth masks ? L , and subset U , which contains unannotated CXR images ? U . We train the generator module G CM to generate segmentation mask by imposing the following loss function,</p><formula xml:id="formula_0">L M gen (G CM ) = E ?,???L,?L H(?, G CM (?))<label>(1)</label></formula><formula xml:id="formula_1">H(?,?) = ? N j=1 K k=1 ? j,k log? j,k<label>(2)</label></formula><p>Here, H is the pixel-wise cross-entropy, ? j,k and? j,k are the annotated segmentation mask and predicted probabilities that pixel j ? {1, ..., N } has label k ? {1, ..., K}. We employ a pixel-wise L2 norm between an annotated CXR and the CXR generated from its corresponding segmentation mask as a supervised loss to train the CXR generator G MC :</p><formula xml:id="formula_2">L C gen (G MC ) = E ?,???L,?L G MC (?) ? ? 2 2<label>(3)</label></formula><p>Two additional losses, adversarial and cycle consistency losses, are incorporated to exploit unannotated CXR images. We use the adversarial losses to train the generators and discriminators in a competing fashion and help the generators produce realistic CXR image and anatomy segmentation mask. Suppose that D M (?) is the predicted probability that segmentation mask ? correspond to an annotated CXR's segmentation mask. We define the adversarial loss for D M as,</p><formula xml:id="formula_3">L M disc (G CM , D M ) = E ???L (D M (?) ? 1) 2 + E ? ? ??U (D M (G CM (? ? ))) 2<label>(4)</label></formula><p>Let D C (?) be the predicted probability that a CXR ? is real. We get the adversarial loss for the CXR discriminator by,</p><formula xml:id="formula_4">L C disc (G MC , D C ) = E ? ? ??U (D C (? ? ) ? 1) 2 + E ???L (D C (G MC (?))) 2<label>(5)</label></formula><p>The first cycle consistency loss measures the difference between an unannotated CXR and the regenerated CXR after passing through generators G CM and G MC sequentially.</p><formula xml:id="formula_5">L C cycle (G CM , G MC ) = E ? ? ??U G MC (G CM (? ? )) ? ? ? 1 (6)</formula><p>We use cross-entropy to evaluate the difference between an annotated and regenerated segmentation mask after passing through generators G MC and G CM in sequence:</p><formula xml:id="formula_6">L M cycle (G CM , G MC ) = E ???L H(?, G CM (G MC (?))) (7)</formula><p>Finally, the total loss is obtained by combining all loss terms:</p><formula xml:id="formula_7">L total = L M gen + L C gen + L M cycle + L C cycle ? L M disc ? L C disc (8)</formula><p>We perform the learning in an alternating fashion. The parameters of the generators are optimized while considering those of the discriminators as fixed and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anatomy-XNet</head><p>The proposed Anatomy-XNet architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. We utilize transfer learning on DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> architecture pre-trained on the ImageNet and use it as our backbone model. The A 3 modules operate on the high-level feature space encoded by the dense blocks (DB) to enforce attention supervision guided by the anatomy masks. We perform downsampling and upsampling on the anatomy masks and feature space to an intermediate shape before passing them to an A 3 module. The different components of the proposed Anatomy-XNet are described in the following sub-sections.</p><p>1) Probabilistic Weighted Average Pooling (PWAP) Module: The traditional global average pooling or max-pooling layer provides the same weight to all spatial regions of the input. However, in many cases, the object of interest may reside in a salient region that is more important than others. Usually, thoracic diseases are often characterized by an anatomy region and lesion areas that constitute much smaller portions than the entire image. Thus, to further enhance the attention mechanism, we use a PWAP module in conjunction with the A 3 block and a PWAP module within the A 3 block. This module explicitly leverages the probability attention map, derived from the input feature activation space, to enhance the most discriminative regions of the feature map before applying the pooling operation. In this module, we learn the weight of each spatial position to guide Anatomy-XNet towards lesion localization during training through a 1?1 convolutional filter. This 1?1 filter has been chosen as we aim to learn the weight at a single spatial position; surrounding information is unwanted. First, we get the probability map P ? R H?W ?1 from a input feature map F inp ? R H?W ?C by,</p><formula xml:id="formula_8">P i,j,1 = S C c=1 K 1,1,c * F inp i,j,c<label>(9)</label></formula><p>Here, i ? {1, ..., H}, j ? {1...., W }, S(?) denotes the sigmoid function, and K is the learnable convolutional filter. Afterward, we elementwise multiply the probability map P with the input feature map F inp to obtain the weighted feature space X. Then we normalize the feature space X and finally, obtain the pooled feature vector V of size 1?1?C by, 2) Anatomy Aware Attention (A 3 ) Module: The Anatomy-XNet consists of two A 3 modules. The first A 3 module is connected to the third dense block (DB-3), and the other A 3 module works with the fourth dense block (DB-4). Each one of them takes the upsampled high-level feature map F US DB ? R H?W ?C generated by their corresponding DB block, and the downsampled anatomy segmentation masks M ? R H?W ?2 as inputs. Using a PWAP module, the feature map is pooled to a feature vector V ? R 1?1?C . This feature vector V is then passed through three different Attention Vector Encoder (AVE) modules to get the three feature vectors module is described in <ref type="table" target="#tab_1">Table I</ref>. The architecture is designed to introduce the bottleneck mechanism in the AVE module, which is inspired by the Squeeze-and-Excitation block <ref type="bibr" target="#b36">[37]</ref>. To introduce bottleneck, feature vector V is first squeezed into dimension 1 ? 1 ? (C/r) and later excited back to 1 ? 1 ? C.</p><formula xml:id="formula_9">X i,j,c = F inp i,j,c ? P i,j,c<label>(10)</label></formula><formula xml:id="formula_10">V 1,1,c = H i=1 W j=1 X i,j,c H i=1 W j=1 P i,j,c<label>(11)</label></formula><formula xml:id="formula_11">A 1 , A 2 , A 3 ? R 1?1?C . The detailed architecture of an AVE</formula><p>The value of C is 512 and 1024 for A 3 modules connected to DB-3, and DB-4, respectively. In both cases, r is 0.5. ?2022 IEEE. This article has been accepted for publication in IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS. See http://www.ieee.org/publications standards/publications/rights/index.html for copyright information.</p><p>We aim to give relevant importance to lung and heart anatomy compared to background regions. One straightforward way is to apply softmax operation across all the attention vectors to get that relevancy scores. But one drawback of this approach is that it will make the attention scores for lung and heart mask attention vectors dependent on each other. However, pathologies related to the heart are independent of whether the CXR contains lung pathologies or not. This motivates us to design the softmax operations across the attention vectors in such a way that the lung attention vector and heart attention vector are independent of each other, but the residual attention vector is jointly dependent on both of them. First, we apply softmax function between A 1 and A 2 :</p><formula xml:id="formula_12">?(A k ) i = exp (A k ) i 2 j=1 exp (A j ) i , i ? {1, ..., C}, k ? {1, 2}<label>(12)</label></formula><p>Here, ?(?) represents softmax operation, j and k represent feature vector indices, and i represents the i th channel value of a feature vector. Thus, we obtain two attention vectors where each feature value across the channel dimension depends on each other. We name these two attention vectors as the lung and lung-complementary attention vectors denoted by A L and A L , respectively. These quantities are related by,</p><formula xml:id="formula_13">A L i + A L i = 1, i ? {1, ..., C}<label>(13)</label></formula><p>Similarly, we apply softmax on A 2 and A 3 by,</p><formula xml:id="formula_14">?(A k ) i = exp (A k ) i 3 j=2 exp (A j ) i , i ? {1, ..., C}, k ? {2, 3}<label>(14)</label></formula><p>Here, j and k represent feature vector indices, and i represents the i th channel value of a feature vector. Similarly, we obtain two attention vectors where each feature value across the channel dimension depends on each other. We name these two attention vectors as the heart-complementary and heart attention vectors denoted by A H and A H , respectively. These quantities are related by,</p><formula xml:id="formula_15">A H i + A H i = 1, i ? {1, ..., C}<label>(15)</label></formula><p>The lung-complementary regions and heart-complementary regions have considerable overlap between them. For this </p><formula xml:id="formula_16">A R = ?A L + ?A H<label>(16)</label></formula><p>Here, the value of hyperparameters are: ?=0.5 and ?=0.5, which are inferred from a grid search with a cross-validation. Next, we downsample and broadcast the lung and heart masks to the dimension of H ?W ?C by repeating them in the channel (C) axis. We denote the downsampled and broadcasted lung and heart masks, respectively, as M L ? R H?W ?C and M H ? R H?W ?C . Afterward, we element-wise multiply the attention vectors, A L with F US DB and M L , A H with F US DB and M H , and A R with F US DB to get three feature spaces, R L , R H , and R R , respectively.</p><formula xml:id="formula_17">R L = A L ? M L ? F US DB (17) R H = A H ? M H ? F US DB (18) R R = A R ? F US DB<label>(19)</label></formula><p>where ? represents the element-wise multiplication operation. Thus, we obtain two anatomy attentive feature space R L , R H ? R H?W ?C , and the residual attentive feature space, R R ? R H?W ?C . For faster convergence and removal of any internal covariate shift among R L , R H , and R R , batch normalization operation is applied individually. Next, we sum all of the three feature spaces and apply batch normalization to obtain the final feature space R ? R H?W ?C by,</p><formula xml:id="formula_18">R = bn bn(R L ) + bn(R H ) + bn(R R )<label>(20)</label></formula><p>Here, bn(?) denotes the batch normalization operation. Since A L is multiplied by M L and F US DB , A L provides attention to the spatial regions responsible for respiratory diseases. To verify this, let us define the loss function score L and take the gradient of L with respect to the lung attention vector A L .</p><formula xml:id="formula_19">?L ?(A L ) k i,j = i j ?L ?(R L ) k i,j ? ?(R L ) k i,j ?(A L ) k i,j<label>(21)</label></formula><p>where,</p><formula xml:id="formula_20">A L ? R 1?1?C , R L ? R H?W ?C , and i ? {1, 2, ...H}, j ? {1, 2, ...W }, k ? {1, 2, ...C}. From equation (17) we get that, R L = A L ? M L ? F US DB , where M L ? R H?W ?C . Hence, ?L ?(A L ) k i,j = i j ?L ?(R L ) k i,j ? ? (A L ) k i,j ?(M L ) k i,j ?(F US DB ) k i,j ?(A L ) k i,j = i j (M L ) k i,j ? ? (A L ) k i,j ?(F US DB ) k i,j ?(A L ) k i,j ? ?L ?(R L ) k i,j<label>(22)</label></formula><p>The value of (M L ) k i,j is 1 at any spatial position if it is the lung region, otherwise is 0. As a result, the gradient for the lung attention vector (A L ) is weighted according to the lung-mask region. Similarly, the gradient for the heart attention vector (A H ) is weighted according to the heart-mask region, making A H to provide attention to the heart-related (cardiac) diseases.</p><p>The residual feature space (R R ) contains feature activation values responsible for the whole input feature map that includes predicted anatomy mask regions, as well as areas other than predicted anatomy mask regions. The areas other than the predicted anatomy mask regions include any left-out anatomy regions from the predicted segmentation masks. The residual attentive feature space (R R ) is responsible for attention to these regions, enabling the Anatomy-XNet a relaxed view constraint on the imperfect segmentation masks and thus making it less affected by the imperfect anatomy masks. ?2022 IEEE. This article has been accepted for publication in IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS. See http://www.ieee.org/publications standards/publications/rights/index.html for copyright information.</p><p>3) Classifier: Let V 1 ? R 1?1?C1 , V 2 ? R 1?1?C2 be the pooled feature vectors from the PWAP modules connected to the A 3 modules that work on the DB-3 and DB-4, respectively. Here, C 1 = 512 and C 2 = 1024. We concatenate V 1 , V 2 together and pass them through a fully connected (FC) layer. The output f I i from this FC layer is then passed through a sigmoid layer and normalized by,</p><formula xml:id="formula_21">p I i = 1 1 + exp ?f I i<label>(23)</label></formula><p>where I is a CXR image and p I i represents the probability score of I belonging to the i th class, where i ? {1, 2, . . . , n}. n represents the number of pathologies presented in each dataset.</p><p>4) Loss functions: The pathological labels of each CXR are expressed as an n-dimensional label vector, L = [l 1 , . . . , l i , . . . , l n ], where l i ? {0, 1}. l i denotes whether there is any pathology, i.e., 1 for presence and 0 for absence. We employ binary cross-entropy loss for optimization, defined by:</p><formula xml:id="formula_22">L cls = ? 1 n n i=1 l i log p I i + (1 ? l i ) log 1 ? p I i<label>(24)</label></formula><p>IV. TRAINING</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIH:</head><p>The NIH chest X-ray dataset <ref type="bibr" target="#b4">[5]</ref> consists of 112,120 X-rays from 30,805 unique patients with 14 diseases. We strictly follow the official split of NIH, 70% for training, 10% for validation, and 20% for testing, for conducting experiments and fair comparison with previous works.</p><p>CheXpert: The CheXpert dataset <ref type="bibr" target="#b15">[16]</ref> consists of 224,316 X-rays of 65,240 patients. The official specific validation and test datasets consist of 200, and 500 studies respectively.</p><p>MIMIC-CXR: The MIMIC-CXR dataset <ref type="bibr" target="#b16">[17]</ref> contains 377,111 X-rays with 14 diseases. We combine all non-positive labels (negative, not mentioned, and uncertain) into an aggregate negative label <ref type="bibr" target="#b30">[31]</ref> for experimenting on this dataset.</p><p>JSRT: We use the JSRT <ref type="bibr" target="#b17">[18]</ref> as annotated dataset to train the segmentation model. The segmentation annotations for JSRT, including heart and lung, are obtained from <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details 1) Training Scheme for Segmentation:</head><p>We follow the procedure outlined in <ref type="bibr" target="#b18">[19]</ref> for preprocessing, output binarization, and hyper-parameter settings to utilize the semi-supervised training pipeline. We utilize the large-scale datasets as the    unannotated subset, i.e., for training the model on the NIH dataset, the NIH dataset is used as an unannotated subset. Thus, we get three separate segmentation models, where NIH, CheXpert, and MIMIC-CXR datasets are used as an unannotated subset, respectively. We use the JSRT dataset as the annotated subset in all three cases. Calculating accuracy in external datasets such as NIH, CheXpert, or MIMIC-CXR is impossible due to the unavailability of the ground truths for them. For this reason, the validation dataset for the semisupervised setting in all three cases is comprised of CXR images from the JSRT dataset. We choose the checkpoint with the highest dice score on this validation dataset as the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Training Scheme for Classification:</head><p>In terms of image size, for a fair comparison with others, we follow <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[34]</ref> and resize the CXR images to 256?256, and then randomly crop 224?224 patches as inputs <ref type="bibr" target="#b10">[11]</ref>. However, 224?224 is too small to predict small and subtle diseases like nodules, pneumonia in CXR. For that reason, we train our model on a larger image size as well. To utilize the larger input image dimension, we resize the CXR images to 586?586, and then randomly crop 512?512 patches as inputs. We normalize the input images with the mean and standard deviation of the ImageNet training set. We follow <ref type="bibr" target="#b10">[11]</ref> and take advantage of flipping to increase the variation and the diversity of training samples. For validation and inference, we use a centrally cropped sub-image of 512?512 for 586?586 and 224?224 for 256?256 dimensions as input. The anatomy masks from the segmentation network are resized to 56?56 before passing to the A 3 modules. We use Adam optimizer with an initial learning rate of 0.0001 and set the batch size to 120. Following previous studies, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we employ the percentage area under the receiver operating characteristic curve (AUC) for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison With State-of-the-Arts 1) Performance on NIH-dataset:</head><p>We compare our proposed Anatomy-XNet with previously published state-ofthe-art methods including: Category-wise Residual Attention Learning (CRAL) <ref type="bibr" target="#b27">[28]</ref>, CheXNet <ref type="bibr" target="#b8">[9]</ref>, DualCheXNet <ref type="bibr" target="#b26">[27]</ref>, Lesion Location Attention Guided Network (LLAGNet) <ref type="bibr" target="#b5">[6]</ref>, the methods of Ho et al. <ref type="bibr" target="#b25">[26]</ref>, Wan et al. <ref type="bibr" target="#b6">[7]</ref>, Yan et al. <ref type="bibr" target="#b10">[11]</ref>, Luo et al. <ref type="bibr" target="#b32">[33]</ref>, Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref>, Keidar et al. <ref type="bibr" target="#b12">[13]</ref>, and MANet <ref type="bibr" target="#b11">[12]</ref>. We have implemented the methods of Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref>, Keidar et al. <ref type="bibr" target="#b12">[13]</ref>, and MANet <ref type="bibr" target="#b11">[12]</ref>. For the methods of Ho et al. <ref type="bibr" target="#b25">[26]</ref> and DualCheXNet <ref type="bibr" target="#b26">[27]</ref>, results have been reported from their implementations. The results for the rest of the methods are quoted from <ref type="bibr" target="#b32">[33]</ref>. As shown in <ref type="table" target="#tab_1">Table II</ref>, the method proposed by Luo et al. <ref type="bibr" target="#b32">[33]</ref> is the previous state-of-the-art yielding an AUC of 83.49%, while our proposed Anatomy-XNet exceeds all the compared models and achieves a new state-of-the-art performance of 85.05% AUC. With a higher input image dimension of 512?512, our proposed framework boosts performance to an AUC score of 85.78%. Specifically, our classification results outperform others in 12 out of 14 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Performance on CheXpert-dataset:</head><p>The results on CheXpert are compared in <ref type="table" target="#tab_1">Table III</ref>. In this paper, we focus on comparing the results achieved by a single model architecture.</p><p>We quote the single model performance for Pham et al. <ref type="bibr" target="#b33">[34]</ref>, and Allaouzi et al. <ref type="bibr" target="#b31">[32]</ref> from their implementations. We report the ensemble result of Irvin et al. <ref type="bibr" target="#b15">[16]</ref> as single checkpoint performance is not given in their paper. To compare with approaches that have utilized segmentation masks, we have implemented the methods of Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref>, Keidar et al. <ref type="bibr" target="#b12">[13]</ref>, and MANet <ref type="bibr" target="#b11">[12]</ref>. The results from <ref type="table" target="#tab_1">Table III</ref> show that our model achieves an AUC of 91.50% and 92.07% with input image dimensions of 224?224 and 512?512, respectively, surpassing the previous state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Performance on MIMIC-CXR-dataset:</head><p>We compare our proposed Anatomy-XNet with previously published state-ofthe-art methods including: Densenet-KG <ref type="bibr" target="#b29">[30]</ref>, VSE-GCN <ref type="bibr" target="#b28">[29]</ref>, CheXclusion <ref type="bibr" target="#b30">[31]</ref>, the methods of Keidar et al. <ref type="bibr" target="#b12">[13]</ref>, MANet <ref type="bibr" target="#b11">[12]</ref>, and Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref>. We adopt the same data split procedure outlined in <ref type="bibr" target="#b30">[31]</ref>. The results of Densenet-KG <ref type="bibr" target="#b29">[30]</ref>, VSE-GCN <ref type="bibr" target="#b28">[29]</ref> have been quoted from the implementation of VSE-GCN <ref type="bibr" target="#b28">[29]</ref>. We report the result of CheXclusion <ref type="bibr" target="#b30">[31]</ref> from their implementation. We have implemented the methods of Arias-Garz?n et al. <ref type="bibr" target="#b14">[15]</ref>, Keidar et al. <ref type="bibr" target="#b12">[13]</ref>, and MANet <ref type="bibr" target="#b11">[12]</ref>. The results are shown in <ref type="table" target="#tab_1">Table IV</ref>. Our proposed model with both input image dimensions of 224?224 and 512?512 has achieved higher performance than the compared models. <ref type="figure">Fig. 5</ref>: Investigation of the performance of the segmentationmask-based classification methods, with masks generated with the semi-supervised segmentation setting compared to masks generated without the semi-supervised segmentation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of Semi-supervised Segmentation</head><p>The selected models for NIH, CheXpert, and MIMIC-CXR datasets achieve validation dice scores of 0.7437, 0.7395, and 0.7417, respectively. These models are used to generate anatomy masks for their corresponding datasets. The visualizations of predicted segmentation results on the NIH test dataset are given in the second row of <ref type="figure" target="#fig_3">Fig. 4</ref>. To verify the impact of the quality of the semi-supervised segmentation masks on the classification performance, we train the segmentation network, only on the labeled dataset (JSRT), without the semi-supervised setting. Next, we train the segmentation-based methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, including Anatomy-XNet, on the NIH dataset with masks generated from this segmentation network (without the semi-supervised setting), and measure their performance on the NIH test dataset. The results are reported in <ref type="figure">Fig. 5</ref>, where we observe that the classification performance of all the methods improves by utilizing masks generated from the semi-supervised setting. In addition, we also observe that the drop in performances of other methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> is larger compared to Anatomy-XNet due to their lack of robustness to counter imperfection in segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Visualization and Analysis</head><p>We generate attention heatmaps using Gradient-weighted Class Activation Mappings (Grad-CAMs) <ref type="bibr" target="#b38">[39]</ref> to visualize the most indicative pathology areas on CXRs from the NIH test dataset to interpret the representational power of Anatomy-XNet. These attention heatmaps, along with the CXRs, anatomy masks predicted from the semi-supervised segmentation network, and classification results, are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. A visual evaluation of the Grad-CAMs confirms the module's anatomy awareness. Thus, similar to the process followed by a radiologist, the A 3 module integrates the anatomy information responsible for a particular pathology within the model. In cases of imperfect mask segmentation (due to semi-supervised training setting), our proposed method still manages to capture the pathology relevant areas and give attention to them. Column (e) of <ref type="figure" target="#fig_3">Fig. 4</ref> demonstrates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effectiveness of A 3 Modules</head><p>For evaluating the impact of A 3 modules on classification performance, we cascade multiple A 3 modules with different dense blocks (DB). First, we use an A 3 module with DB-4. We denote this experiment by anatomy aware attention level-1 (A 3 -L1). Afterward, we use A 3 modules with DB-3,4 and indicate this by anatomy aware attention level-2 (A 3 -L2). Finally, we apply the A 3 modules with DB-2,3,4 and refer to it as anatomy aware attention level-3 (A 3 -L3). The experimental results are provided in part-1 of <ref type="table" target="#tab_5">Table V</ref>. Our experiments find that classification performance improves from the baseline when we cascade a A 3 module with a DB. The baseline denotes the backbone model, DenseNet-121, without any integrated A 3 modules. The results show that performance improves when going from A 3 -L1 to A 3 -L2 but decreases if A 3 -L3 is used. Because low-level spatial features from DB-2 might have outlier information which deteriorates the performance by causing the model to give attention to noisy information. Again, applying A 3 only on the highest level of features, in our case DB-4, does not guarantee the best performance. Because due to subsequent pooling in these DBs, some salient information presented in the previous DBs, may be lost in the later stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effectiveness of PWAP Modules</head><p>To demonstrate the effectiveness of PWAP modules, we replace all the PWAP layers in Anatomy-XNet with average pooling, max pooling, and generalized mean pooling <ref type="bibr" target="#b39">[40]</ref> layers, respectively, and run the experiment while keeping all <ref type="figure">Fig. 6</ref>: Visualization of the impact of the PWAP module. The first, second, and third row depict the Grad-CAMs of the input feature spaces (F inp ), probability attention maps (P), and recalibrated feature spaces (X) of the PWAP module, which is inside the A 3 module connected with the fourth dense block. the other hyperparameters the same. The results across all the datasets, shown in part-2 of Table V, depict the effectiveness of the proposed module. In <ref type="figure">Fig. 6</ref>, the Grad-CAMs of the input feature spaces (F inp ), probability attention maps (P), and recalibrated feature spaces (X) of the PWAP module, that is inside the A 3 module connected with the fourth dense block, are shown. The heatmaps are resized to the dimensions of the CXR images and overlaid on the CXR images. A visual examination of the probability attention maps and the recalibrated feature space shows that the PWAP module modulates the feature space to focus more prominently on the lesion areas by removing unwanted attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effect of Different Anatomy Mask Dimensions</head><p>To demonstrate the effect of the dimension of anatomy masks, we vary the intermediate dimensions of the anatomy masks, chosen from the set {28?28, 42?42, 56?56}, and evaluate the performances of Anatomy-XNet. Part-3 of <ref type="table" target="#tab_5">Table V</ref> presents performance numbers across all datasets. Here, we observe that the performance of Anatomy-XNet improves as we increase the dimension of the anatomy masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effect of Different Input Image Sizes</head><p>We perform experiments to investigate the effect of varying input image dimensions on classification performance. We resize the CXR images into three different sizes: 256?256, 438?438, and 586?586 and crop patches of 224?224 for 256?256, 384?384 for 438?438, and 512?512 for 586?586 to use as input images. The classification performances on all three datasets for different input sizes are given in the part-4 of <ref type="table" target="#tab_5">Table V</ref>. The classification results show that enlarging the input image size increases the average AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Investigation of the Impact of Imperfect Segmentation</head><p>To simulate the resilience of the proposed Anatomy-XNet towards imperfect segmentation masks, we randomly apply cutout operations <ref type="bibr" target="#b40">[41]</ref> on the predicted anatomy segmentation regions with different window sizes and measure the AUC score. The NIH, CheXpert, and MIMIC-CXR datasets do not contain pixel-level ground truth annotations. Due to the lack of pixel-level ground truth annotations and the sheer size of the datasets, it is very challenging to ensure that the cutout window will always be on the lesion area. Instead, we make sure that the regions on which the cutout windows are applied always overlap with the predicted anatomy masks. We perform the cutout operation three times for each window size, measure AUC each time, and take the average as the final AUC score for that particular cutout window. Next, we apply the exact same cutout operations at exactly the same locations of the anatomy masks and use them to evaluate the AUC of segmentation mask-based approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> and compare their drop in classification performance with our method. The AUC scores against different cutout window sizes are shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. The proposed Anatomy-XNet shows only around 0.2% performance degradation against cutout operations and maintains stable performance against increasing window sizes. On the other hand, the methods of <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> show a larger degradation in classification performance, around 2.41-3.13%, against increasing cutout window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose Anatomy-XNet, an anatomyaware convolutional neural network for thoracic disease classification. Departing from the previous works that rely on the chest X-ray image only or attention mechanisms guided by the model prediction, the proposed network is guided by prior anatomy segmentation information to act similar to a radiologist by focusing on relevant anatomical regions associated with the thoracic disease. Extensive experiments demonstrate that combining our novel A 3 and PWAP modules within a backbone Densenet-121 model in a unified framework yields state-of-the-art performance on the NIH chest X-ray, Stanford CheXpert, and MIMIC-CXR datasets. The Anatomy-XNet achieves an average AUC score of 85.78% on the official NIH test set, 92.07% on the official validation split of the Stanford CheXpert dataset, and 84.04% on the MIMIC-CXR dataset, surpassing the former best-performing methods published on these datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the semi-supervised anatomy segmentation architecture. effective diagnosis. Hou et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The architecture of the proposed Anatomy-XNet. The anatomy-aware attention (A 3 ) modules operate on the upsampled feature spaces from dense block-3 and dense block-4 with the help of downsampled anatomy masks. These anatomy masks are derived from the segmentation network in a semi-supervised manner. The feature spaces calibrated with the supervision of anatomy knowledge from each of the A 3 modules are pooled by the PWAP layers and concatenated. The classifier module outputs pathology class scores by taking these concatenated pooled features as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>b</head><label></label><figDesc>The 14 pathologies for the MIMIC-CXR datasets are Atelectasis (Atel), Cardiomegaly (Card), Consolidation (Cons), Edema (Edem), Enlarged Cardiomediastinum (E.C.), Fracture (Frac), Lung Lesion (L.L.), Lung Opacity (L.O.), No Finding (N.F.), Pleural Effusion (Effu), Pleural Other (P.O.), Pneumonia (Pne1), Pneumothorax (Pne2), Support Devices (S.D.). c Indicates that the result is obtained by the ensemble of 5 checkpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative visualization of the Anatomy-XNet's output on the NIH test dataset. The first and second rows depict the CXR images and their corresponding anatomy masks predicted from the semi-supervised segmentation network, respectively. The color red in the segmentation masks indicates the lung regions, while the color green indicates the heart regions. The third row shows the Grad-CAMs of the Anatomy-XNet for the target classes. The color red in the Grad-CAMs means the most indicative regions with abnormalities, while the color blue indicates regions without abnormalities. The contours of the anatomy segmentation masks are marked with white color on top of the heatmaps. The final row shows top-6 predicted findings and their corresponding prediction scores. The ground truth labels are highlighted in red color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>(a) Visualization of cutout window of two different sizes. (b) Simulation of classification performance drop against increasing imperfection in segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>ATTENTION VECTOR ENCODER STRUCTURE. weighting between the lung-complementary and heart-complementary attention vectors is needed. Finally, we get the residual attention vector by,</figDesc><table><row><cell>reason, proper</cell><cell></cell><cell></cell></row><row><cell>Layer (Type)</cell><cell cols="3">Input Shape Output Shape</cell></row><row><cell>FC-1 (Fully Connected)</cell><cell></cell><cell>(C)</cell><cell>(C/r)</cell></row><row><cell>ReLU-1 (ReLU)</cell><cell></cell><cell>(C/r)</cell><cell>(C/r)</cell></row><row><cell>BN-1 (Batch Normalization)</cell><cell></cell><cell>(C/r)</cell><cell>(C/r)</cell></row><row><cell>FC-2 (Fully Connected)</cell><cell></cell><cell>(C/r)</cell><cell>(C)</cell></row><row><cell>ReLU-2 (ReLU)</cell><cell></cell><cell>(C)</cell><cell>(C)</cell></row><row><cell>BN-2 (Batch Normalization)</cell><cell></cell><cell>(C)</cell><cell>(C)</cell></row><row><cell cols="2">* C: Channel Dimension</cell><cell cols="2">r: Reduction ratio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>PATHOLOGY-WISE PERFORMANCE COMPARISON OF THE PROPOSED METHOD WITH STATE-OF-THE-ART SYSTEMS ON THE NIH DATASET a . THE TWO BEST RESULTS ARE SHOWN IN RED AND BLUE.</figDesc><table><row><cell>Method</cell><cell>Emph</cell><cell>Fibr</cell><cell>Hern</cell><cell>Infi</cell><cell>PT</cell><cell>Mass</cell><cell>Nodu</cell><cell>Atel</cell><cell>Card</cell><cell>Cons</cell><cell>Edem</cell><cell>Effu</cell><cell>Pne1</cell><cell>Pne2</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Methods without Utilizing Segmentation Masks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ho et al. [26]</cell><cell>87.50</cell><cell>75.60</cell><cell>83.60</cell><cell>70.30</cell><cell>77.40</cell><cell>83.50</cell><cell>71.60</cell><cell>79.50</cell><cell>88.70</cell><cell>78.60</cell><cell>89.20</cell><cell>87.50</cell><cell>74.20</cell><cell>86.30</cell><cell>80.97</cell></row><row><cell>CRAL [28]</cell><cell>90.80</cell><cell>83.00</cell><cell>91.70</cell><cell>70.20</cell><cell>77.80</cell><cell>83.40</cell><cell>77.30</cell><cell>78.10</cell><cell>88.00</cell><cell>75.40</cell><cell>85.00</cell><cell>82.90</cell><cell>72.90</cell><cell>85.70</cell><cell>81.59</cell></row><row><cell>CheXNet [9]</cell><cell>92.49</cell><cell>82.19</cell><cell>93.23</cell><cell>68.94</cell><cell>79.25</cell><cell>83.07</cell><cell>78.14</cell><cell>77.95</cell><cell>88.16</cell><cell>75.42</cell><cell>84.96</cell><cell>82.68</cell><cell>73.54</cell><cell>85.13</cell><cell>81.80</cell></row><row><cell>DualCheXNet [27]</cell><cell>94.20</cell><cell>83.70</cell><cell>91.20</cell><cell>70.50</cell><cell>79.60</cell><cell>83.80</cell><cell>79.60</cell><cell>78.40</cell><cell>88.80</cell><cell>74.60</cell><cell>85.20</cell><cell>83.10</cell><cell>72.70</cell><cell>87.60</cell><cell>82.30</cell></row><row><cell>LLAGNet [6]</cell><cell>93.90</cell><cell>83.20</cell><cell>91.60</cell><cell>70.30</cell><cell>79.80</cell><cell>84.10</cell><cell>79.00</cell><cell>78.30</cell><cell>88.50</cell><cell>75.40</cell><cell>85.10</cell><cell>83.40</cell><cell>72.90</cell><cell>87.70</cell><cell>82.37</cell></row><row><cell>Wang et al. [7]</cell><cell>93.30</cell><cell>83.80</cell><cell>93.80</cell><cell>71.00</cell><cell>79.10</cell><cell>83.40</cell><cell>77.70</cell><cell>77.90</cell><cell>89.50</cell><cell>75.90</cell><cell>85.50</cell><cell>83.60</cell><cell>73.70</cell><cell>87.80</cell><cell>82.60</cell></row><row><cell>Yan et al. [11]</cell><cell>94.22</cell><cell>83.26</cell><cell>93.41</cell><cell>70.95</cell><cell>80.83</cell><cell>84.70</cell><cell>81.05</cell><cell>79.24</cell><cell>88.14</cell><cell>75.98</cell><cell>84.70</cell><cell>84.15</cell><cell>73.97</cell><cell>87.59</cell><cell>83.02</cell></row><row><cell>Luo et al. [33]</cell><cell>93.96</cell><cell>83.81</cell><cell>93.71</cell><cell>71.84</cell><cell>80.36</cell><cell>83.76</cell><cell>79.85</cell><cell>78.91</cell><cell>90.69</cell><cell>76.81</cell><cell>86.10</cell><cell>84.18</cell><cell>74.19</cell><cell>90.63</cell><cell>83.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Methods Utilizing Segmentation Masks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Arias-Garz?n et al. [15]</cell><cell>85.72</cell><cell>81.68</cell><cell>82.48</cell><cell>70.10</cell><cell>77.67</cell><cell>83.63</cell><cell>78.92</cell><cell>80.43</cell><cell>88.93</cell><cell>80.17</cell><cell>87.71</cell><cell>86.89</cell><cell>75.07</cell><cell>85.59</cell><cell>81.79</cell></row><row><cell>MANet [12]</cell><cell>85.23</cell><cell>82.82</cell><cell>92.10</cell><cell>70.04</cell><cell>76.82</cell><cell>83.36</cell><cell>77.76</cell><cell>81.43</cell><cell>89.35</cell><cell>80.23</cell><cell>88.56</cell><cell>86.30</cell><cell>75.29</cell><cell>85.46</cell><cell>82.48</cell></row><row><cell>Keidar et al. [13]</cell><cell>90.87</cell><cell>81.47</cell><cell>91.80</cell><cell>70.60</cell><cell>78.02</cell><cell>83.93</cell><cell>77.07</cell><cell>80.64</cell><cell>90.88</cell><cell>80.43</cell><cell>89.20</cell><cell>86.94</cell><cell>76.53</cell><cell>85.54</cell><cell>83.14</cell></row><row><cell>Anatomy-XNet (224)</cell><cell>92.85</cell><cell>84.42</cell><cell>96.36</cell><cell>71.71</cell><cell>79.79</cell><cell>86.04</cell><cell>80.37</cell><cell>83.06</cell><cell>91.37</cell><cell>80.91</cell><cell>89.90</cell><cell>88.58</cell><cell>77.09</cell><cell>88.21</cell><cell>85.05</cell></row><row><cell>Anatomy-XNet (512)</cell><cell>94.33</cell><cell>85.91</cell><cell>94.57</cell><cell>72.07</cell><cell>79.90</cell><cell>86.80</cell><cell>83.78</cell><cell>83.69</cell><cell>91.38</cell><cell>81.54</cell><cell>90.25</cell><cell>89.12</cell><cell>77.48</cell><cell>90.09</cell><cell>85.78</cell></row><row><cell cols="16">a The 14 findings for NIH datasets are Emphysema (Emph), Fibrosis (Fibr), Hernia (Hern), Infiltration (Infi), Pleural Thickening (PT), Mass, Nodule (Nodu), Atelectasis</cell></row><row><cell cols="13">(Atel), Cardiomegaly (Card), Consolidation (Cons), Edema (Edem), Effusion (Effu), Pneumonia (Pne1), and Pneumothorax (Pne2).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>PATHOLOGY-WISE PERFORMANCE COMPARISON OF THE PROPOSED METHOD WITH STATE-OF-THE-ART SYSTEMS ON THE CHEXPERT DATASET. THE TWO BEST RESULTS ARE SHOWN IN RED AND BLUE.</figDesc><table><row><cell>Method</cell><cell>Atelectasis</cell><cell>Cardiomegaly</cell><cell>Edema</cell><cell>Consolidation</cell><cell>Pleural Effusion</cell><cell>Average</cell></row><row><cell></cell><cell cols="4">Methods without Utilizing Segmentation Masks</cell><cell></cell><cell></cell></row><row><cell>Allaouzi et al. [32] BR</cell><cell>72.00</cell><cell>88.00</cell><cell>87.00</cell><cell>77.00</cell><cell>90.00</cell><cell>82.80</cell></row><row><cell>Irvin et al. [16] U-Ones</cell><cell>85.80</cell><cell>83.20</cell><cell>94.10</cell><cell>89.90</cell><cell>93.40</cell><cell>89.30</cell></row><row><cell>Pham et al. [34] U-Ones+CT+LSR</cell><cell>82.50</cell><cell>85.50</cell><cell>93.00</cell><cell>93.70</cell><cell>92.30</cell><cell>89.40</cell></row><row><cell></cell><cell cols="4">Methods Utilizing Segmentation Masks</cell><cell></cell><cell></cell></row><row><cell>MANet [12]</cell><cell>81.35</cell><cell>86.61</cell><cell>92.22</cell><cell>91.59</cell><cell>89.86</cell><cell>88.33</cell></row><row><cell>Arias-Garz?n et al. [15]</cell><cell>81.74</cell><cell>84.24</cell><cell>94.06</cell><cell>90.74</cell><cell>94.31</cell><cell>89.02</cell></row><row><cell>Keidar et al. [13]</cell><cell>86.42</cell><cell>87.39</cell><cell>91.97</cell><cell>88.23</cell><cell>91.73</cell><cell>89.15</cell></row><row><cell>Anatomy-XNet (224)</cell><cell>86.55</cell><cell>87.86</cell><cell>95.28</cell><cell>93.13</cell><cell>94.66</cell><cell>91.50</cell></row><row><cell>Anatomy-XNet (512)</cell><cell>86.72</cell><cell>89.54</cell><cell>95.73</cell><cell>93.31</cell><cell>95.04</cell><cell>92.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>PATHOLOGY-WISE PERFORMANCE COMPARISON OF THE PROPOSED METHOD WITH STATE-OF-THE-ART SYSTEMS ON THE MIMIC-CXR DATASET b . THE TWO BEST RESULTS ARE SHOWN IN RED AND BLUE.</figDesc><table><row><cell>Method</cell><cell>Atel</cell><cell>Card</cell><cell>Cons</cell><cell>Edem</cell><cell>E.C.</cell><cell>Frac</cell><cell>L.L.</cell><cell>L.O.</cell><cell>N.F.</cell><cell>Effu</cell><cell>P.O.</cell><cell>Pne1</cell><cell>Pne2</cell><cell>S.D.</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Methods without Utilizing Segmentation Masks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Densenet-KG [30]</cell><cell>69.40</cell><cell>74.60</cell><cell>64.00</cell><cell>79.00</cell><cell>65.10</cell><cell>60.50</cell><cell>57.40</cell><cell>60.90</cell><cell>77.80</cell><cell>80.90</cell><cell>65.00</cell><cell>57.20</cell><cell>68.90</cell><cell>78.10</cell><cell>68.50</cell></row><row><cell>VSE-GCN [29]</cell><cell>72.20</cell><cell>73.00</cell><cell>72.80</cell><cell>79.90</cell><cell>76.70</cell><cell>56.00</cell><cell>62.30</cell><cell>65.40</cell><cell>81.70</cell><cell>86.30</cell><cell>65.30</cell><cell>58.80</cell><cell>79.70</cell><cell>78.90</cell><cell>72.10</cell></row><row><cell>Chexclusion [31] c</cell><cell>83.70</cell><cell>82.80</cell><cell>84.40</cell><cell>90.40</cell><cell>75.70</cell><cell>71.80</cell><cell>77.20</cell><cell>78.20</cell><cell>86.80</cell><cell>93.30</cell><cell>84.80</cell><cell>74.80</cell><cell>90.30</cell><cell>92.70</cell><cell>83.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Methods Utilizing Segmentation Masks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Arias-Garz?n et al. [15]</cell><cell>82.61</cell><cell>81.57</cell><cell>83.16</cell><cell>90.01</cell><cell>73.71</cell><cell>65.36</cell><cell>74.57</cell><cell>77.40</cell><cell>85.83</cell><cell>91.50</cell><cell>81.96</cell><cell>72.79</cell><cell>87.47</cell><cell>90.64</cell><cell>81.33</cell></row><row><cell>MANet [12]</cell><cell>82.77</cell><cell>81.86</cell><cell>83.66</cell><cell>90.03</cell><cell>74.52</cell><cell>69.56</cell><cell>75.43</cell><cell>77.24</cell><cell>85.90</cell><cell>91.53</cell><cell>83.05</cell><cell>73.01</cell><cell>88.02</cell><cell>90.24</cell><cell>81.92</cell></row><row><cell>Keidar et al. [13]</cell><cell>83.24</cell><cell>82.59</cell><cell>84.19</cell><cell>90.40</cell><cell>74.71</cell><cell>71.33</cell><cell>76.66</cell><cell>77.67</cell><cell>86.39</cell><cell>92.93</cell><cell>84.18</cell><cell>74.51</cell><cell>89.70</cell><cell>92.05</cell><cell>82.90</cell></row><row><cell>Anatomy-XNet (224)</cell><cell>83.79</cell><cell>82.67</cell><cell>85.25</cell><cell>90.83</cell><cell>75.45</cell><cell>74.30</cell><cell>77.08</cell><cell>78.79</cell><cell>86.90</cell><cell>93.37</cell><cell>86.55</cell><cell>75.98</cell><cell>90.87</cell><cell>92.75</cell><cell>83.90</cell></row><row><cell>Anatomy-XNet (512)</cell><cell>83.93</cell><cell>82.59</cell><cell>84.84</cell><cell>90.76</cell><cell>75.12</cell><cell>74.95</cell><cell>78.78</cell><cell>78.90</cell><cell>86.97</cell><cell>93.43</cell><cell>86.21</cell><cell>75.81</cell><cell>91.20</cell><cell>93.12</cell><cell>84.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>INVESTIGATION OF THE CLASSIFICATION PER-FORMANCE IN THE NIH, MIMIC-CXR, AND CHEXPERT DATASETS WITH DIFFERENT SETTINGS. THE BEST RESULT IS SHOWN IN RED. 1: Investigation of the effectiveness of A 3 modules.</figDesc><table><row><cell>Part Dataset</cell><cell>Baseline</cell><cell>A 3 -L1</cell><cell>A 3 -L2</cell><cell>A 3 -L3</cell></row><row><cell>NIH</cell><cell>82.44</cell><cell>84.67</cell><cell>85.05</cell><cell>84.72</cell></row><row><cell>MIMIC-CXR</cell><cell>82.76</cell><cell>83.72</cell><cell>83.90</cell><cell>83.67</cell></row><row><cell>CheXpert</cell><cell>89.22</cell><cell>90.93</cell><cell>91.50</cell><cell>91.07</cell></row><row><cell cols="5">Part 2: Investigation of the effectiveness of PWAP modules.</cell></row><row><cell>Dataset</cell><cell>PWAP</cell><cell>Gem</cell><cell>Average</cell><cell>Max</cell></row><row><cell>NIH</cell><cell>85.05</cell><cell>84.85</cell><cell>84.45</cell><cell>84.30</cell></row><row><cell>MIMIC-CXR</cell><cell>83.90</cell><cell>83.68</cell><cell>83.46</cell><cell>83.32</cell></row><row><cell>CheXpert</cell><cell>91.50</cell><cell>91.18</cell><cell>91.03</cell><cell>90.42</cell></row><row><cell cols="5">Part 3: Investigation of different anatomy mask sizes.</cell></row><row><cell>Dataset</cell><cell>28?28</cell><cell>42?42</cell><cell>56?56</cell><cell>-</cell></row><row><cell>NIH</cell><cell>84.86</cell><cell>84.90</cell><cell>85.05</cell><cell>-</cell></row><row><cell>MIMIC-CXR</cell><cell>83.38</cell><cell>83.71</cell><cell>83.90</cell><cell>-</cell></row><row><cell>CheXpert</cell><cell>91.21</cell><cell>91.21</cell><cell>91.50</cell><cell>-</cell></row><row><cell cols="5">Part 4: Investigation of different input image sizes.</cell></row><row><cell>Dataset</cell><cell>224?224</cell><cell>384?384</cell><cell>512?512</cell><cell>-</cell></row><row><cell>NIH</cell><cell>85.05</cell><cell>85.44</cell><cell>85.78</cell><cell>-</cell></row><row><cell>MIMIC-CXR</cell><cell>83.90</cell><cell>83.97</cell><cell>84.04</cell><cell>-</cell></row><row><cell>CheXpert</cell><cell>91.50</cell><cell>91.70</cell><cell>92.07</cell><cell>-</cell></row><row><cell cols="5">an example where the lung mask fails to contain the mass</cell></row><row><cell cols="5">area. Nevertheless, our model localizes its attention in that</cell></row><row><cell cols="5">area, demonstrating the efficacy of the proposed architecture's</cell></row><row><cell cols="4">resilience towards imperfect segmentation.</cell><cell></cell></row></table><note>?2022 IEEE. This article has been accepted for publication in IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS. See http://www.ieee.org/publications standards/publications/rights/index.html for copyright information.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">?2022 IEEE. This article has been accepted for publication in IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS. See http://www.ieee.org/publications standards/publications/rights/index.html for copyright information.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interpretation of plain chest roentgenogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feigin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Irugulpati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Rosenow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chest</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An automatic computer-aided detection scheme for pneumoconiosis on digital chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="393" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic tuberculosis screening using chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lesion location attention guided network for multi-label thoracic disease classification in chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Biomed. and Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2016" to="2027" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Triple attention learning for classification of 14 thoracic diseases using chest radiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101846</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Chest X-rays for medical students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>abs/1711.05225</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in MLMI@MICCAI</title>
		<imprint>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised deep learning for thoracic disease classification and localization on chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>ACM Int. Conf. on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Manet: A two-stage deep learning method for classification of covid-19 from chest x-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="page" from="96" to="105" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Covid-19 classification of x-ray images using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keidar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9654" to="9663" />
			<date type="published" when="2021-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sdfn: Segmentation-based deep fusion network for thoracic disease classification in chest x-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Covid-19 detection in x-ray images using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arias-Garz?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLWA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">100138</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Artif. Intell</title>
		<meeting>AAAI Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno>abs/1901.07042</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists&apos; detection of pulmonary nodules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AJR Am J Roentgenol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="75" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Revisiting cyclegan for semi-supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical lung field segmentation with joint shape and appearance sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1761" to="1780" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate landmarkbased segmentation by incorporating landmark misdetections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vrtovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 13th ISBI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An edge-region force guided active shape approach for automatic lung field detection in chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="452" to="463" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contour-aware multi-label chest x-ray organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kholiavchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segmentation of lungs in chest x-ray image using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azmat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gr?nlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="153" to="535" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An automatic method for lung segmentation and reconstruction in chest x-ray using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Bandeira Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Fran?a Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>De Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="285" to="296" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple feature integration for classification of thoracic disease in chest radiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">4130</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dualchexnet: dual asymmetric feature learning for thoracic disease classification in chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. and Control</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">101554</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-label chest x-ray image classification via category-wise residual attention learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Letters</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>IUVA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-label learning with visual-semantic embedded knowledge graph for diagnosis of radiology imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="15" to="720" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When radiology report generation meets knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Conf AAAI Artif Intell</title>
		<meeting>Conf AAAI Artif Intell</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chexclusion: Fairness gaps in deep chest x-ray classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. Symp. Biocomput., Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="232" to="243" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel approach for multi-label chest x-ray classification of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ben</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="64" to="279" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep mining external imperfect data for chest x-ray disease screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3583" to="3594" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpreting chest x-rays via cnns that exploit hierarchical disease dependencies and uncertainty labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">437</biblScope>
			<biblScope unit="page" from="186" to="194" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="40" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno>abs/1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
