<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
							<email>yanghuan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-ofthe-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment, applied to facial landmark detection, has experienced tremendous improvement by means of Convolutional Neural Networks, and provides a continuous impetus for improvements in many computer vision techniques for face such as face recognition <ref type="bibr" target="#b29">[29]</ref>, face synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">17]</ref> and face 3D reconstruction <ref type="bibr" target="#b23">[23]</ref>.</p><p>Error-bias can be treated as a special kind of AI-bias that could be resulted from the prejudiced assumptions made in the process of algorithm development or prejudices in the * Corresponding author   <ref type="bibr" target="#b36">[36]</ref>, the ellipses is colored by standard deviation normalized by face size. (b) The prediction error distribution of baseline model, each point represent the relevant position between predicted landmark and its corresponding ground truth, colored by the average NME interpupil / 2 of that landmark, in general face size is twice of the inter pupil. (c) The prediction error distribution of ADNet model. training data thus is an anomaly in the output of machine learning algorithms. Common AI-bias, such as race-bias and gender-bias, always causes negative ethical effects, especially in the case of gender shades <ref type="bibr" target="#b4">[5]</ref>. Different from them, the error-bias considered in this paper is more like a location uncertainty, as mentioned by <ref type="bibr" target="#b26">[26]</ref>.</p><p>In our research, error-bias is regarded as the nature of error direction and proves to be highly conducive for model understanding, thus deserving a thorough investigation to fill in the research gap. <ref type="figure" target="#fig_6">Figure 7</ref>.(a) demonstrates the labeling-bias of landmarks location on 300W dataset, deduced by the anisotropic standard deviation of each point, especially the points located on the boundary. By the observation, we trained a common face alignment model with 300W dataset, whose setting is the same as the baseline model described at the end of Section 4.2, and rendered the error distribution on the whole test dataset in <ref type="figure" target="#fig_6">Figure 7</ref>.(b), in which the error means the relative offset from predicted position to ground truth. It is found that the error-bias exists on common face alignment model and reaches 33% relative rate, which is highly consistent with labeling-bias in <ref type="figure" target="#fig_6">Figure 7</ref>.(a). Based on the finding, we speculate that model easier converges the error in normal than in tangent direction because of noisy label and semantic confusion in tangent direction. Inspired by this, in order to test the applicability, stronger and weaker constraint are respectively imposed in normal and tangent direction by the proposed ADNet instead of isotropic loss. Similar to <ref type="figure" target="#fig_6">Figure 7.(b)</ref>, the corresponding error distribution is shown in <ref type="figure" target="#fig_6">Figure 7</ref>.(c), in which the error-bias becomes higher at 48%, but the error distribution becomes compact. To conclude, these three figures support our guideline that imposing stronger constraint to normal than to tangent.</p><p>In order to improve the localization capability of face alignment and fully leverage the error-bias, this paper proposes an end-to-end training framework involving devised Symmetric Direction Loss and Anisotropic Attention Module. Specifically, Anisotropic Direction Loss disentangles the landmark errors into normal error and tangent error, and imposes strong constraint in normal error and weak constraint in tangent error for coordinate regression, which is an improved L n loss. Anisotropic Attention Module combines point heatmap and edge heatmap into one heatmap, which contains both landmarks information and local boundary information. Applying the combined attention heatmap as a mask to the landmarks heatmap, the model has high tolerance to tangent direction and low tolerance to normal direction. These two modules are highly aligned with the proposed guideline. Some previous works, such as LAB <ref type="bibr" target="#b47">[47]</ref>, PropNet <ref type="bibr" target="#b22">[22]</ref>, incorporates boundary information into CNN by attention and can be treated as special cases for the proposed guideline. ADNet enables both heatmap regression and coordinate regression optimization, and the anisotropic attention mask in it acts like a gabor filter supervised by both point and edge information.</p><p>The method is evaluated on several academic datasets, 300W <ref type="bibr" target="#b37">[37]</ref>, WFLW <ref type="bibr" target="#b47">[47]</ref> and COFW <ref type="bibr" target="#b5">[6]</ref>. All of them achieve the start-of-the-art performance, which demonstrates the effectiveness and robustness of the method.</p><p>In summary, the main contributions of this paper is as follows:</p><p>? Unveiling error-bias on error directions of face alignment, which is highly consistent with labeling-bias by human, strong in tangent direction and weak in normal direction, and based on this firstly proposing the guideline to leverage error-bias in face alignment by magnification instead of suppression.</p><p>? Devising Anisotropic Direction Loss to assign uneven loss weights to the disentangled normal and tangent error for each landmarks coordinate and magnify the error-bias by the proposed guideline.</p><p>? Proposing Anisotropic Attention Module to generate anisotropic attention mask for each landmark heatmap and magnify the error-bias again by the proposed guideline.</p><p>? Constructing an advanced end-to-end training pipeline and implementing extensive experiments on various datasets, the result outperforms other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the course of face alignment development, some classic approaches of face alignment were proposed in the 1990s, e.g. AAM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b24">24]</ref>, ASM <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b32">32]</ref> and cascade regression <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b51">51]</ref>. In recent years, driven by vigorous development of Deep Convolutional Neural Network (DCNN), CNN-based face alignment methods have achieved state-of-the-art performance and have drawn close attention from researchers. Currently, the spotlight is centered on two main branches of face alignment, that are coordinate regression and heatmap regression. Coordinate regression methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">56]</ref> directly regress facial landmarks based on the input without postprocessing. To solve this problem, Zhang <ref type="bibr" target="#b55">[55]</ref> involves multiple tasks, learning landmarks and facial attributes into the model concurrently. Then MDM <ref type="bibr" target="#b43">[43]</ref> designs a pipeline to train the model from coarse to fine by focusing more on detailed local information. The basic combination, ResNet <ref type="bibr" target="#b18">[18]</ref>, DenseNet <ref type="bibr" target="#b20">[20]</ref> with L1, L2, Smooth L1 or wing loss <ref type="bibr" target="#b15">[15]</ref> are commonly used in this type of methods. Heatmap regression methods <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b1">2]</ref> predict an intermediate heatmap for each landmark, and then the highest response point of each heatmap or near it is the final detected coordinate of landmark, where UNet <ref type="bibr" target="#b35">[35]</ref> and stacked HG <ref type="bibr" target="#b33">[33]</ref> are applied frequently. Adaptive wing loss <ref type="bibr" target="#b45">[45]</ref> and focal wing loss <ref type="bibr" target="#b22">[22]</ref> are proposed to balance the weight of easy sample and hard sample. Boundary information is introduced into face alignment by LAB <ref type="bibr" target="#b47">[47]</ref>, PropNet <ref type="bibr" target="#b22">[22]</ref> and ACENet <ref type="bibr" target="#b21">[21]</ref>, which supplies more structure information and helps network to know which region should be paid more attention to. Usually, heatmap regression methods outperform coordinate regression methods with the effort of complex and tricky postprocessing.</p><p>Apart from the popular models, there are techniques that can further advance face alignment performance, such as CoordConv <ref type="bibr" target="#b27">[27]</ref>, Anti-aliased CNN <ref type="bibr" target="#b53">[53]</ref>, Multi-view CNN block <ref type="bibr" target="#b2">[3]</ref> and attention mechanism <ref type="bibr" target="#b47">[47]</ref>. Among them, Co-ordConv proves to be instrumental for coordinate transformation problem in the vision task, such as object detection and generative modeling. Anti-aliased CNN with a special pooling layer, has advantages in translation invariance or ranging degrees of translation dependence. The multi-view CNN block is proven to be beneficial for landmark localization on account of multiple receptive fields and the various scale of images those fields bring about. Furthermore, the attention mechanism is able to guide a CNN to study valuable features and focus on salient regions, thus gaining great popularity among scholars nowadays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ADNet</head><p>We design a network that employs stacked 4 hourglasses (HGs) <ref type="bibr" target="#b3">[4]</ref> as backbone. In each HG structure, three heatmaps are generated, respectively corresponding to a landmarks heatmap, an edge heatmap and a point heatmap. All heatmaps contribute losses to model training in different ways. Based on these heatmaps, an anisotropic attention mask is generated from the point and edge heatmaps. The attention mask can then impose anisotropic supervision upon the landmarks training, where an anisotropic direction loss is applied to the predicted landmark coordinates that are formed through soft-argmax operation. We also leverage components such as coordconv <ref type="bibr" target="#b27">[27]</ref> and anti-aliased blocks <ref type="bibr" target="#b53">[53]</ref> to improve performance further, as is also proposed by <ref type="bibr" target="#b22">[22]</ref>. We name this network the ADNet, with AD standing for anisotropic direction. The overview structure of ADNet is detailedly illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Anisotropic Direction Loss (ADL)</head><p>Before proposing the new loss, we would like to review the L n loss in Equation 1. Two of its special cases are L 1 and L 2 , which are commonly used in machine learning tasks, e.g. regression, detection and GAN.</p><formula xml:id="formula_0">L n (p i ,p i ) = |p i ?p i | n<label>(1)</label></formula><p>where p i andp i are respectively the predicted value and ground truth of coordinate in ith landmarks. However, when utilizing L1 loss, model learns much noise instead of valid information from easy samples and when applying L 2 to outliers, model easily gets to gradient explosion. With the development of L n , Smooth L 1 is proposed to solve these two problems by piecewise-defining as Equation 2.</p><formula xml:id="formula_1">SmoothL 1 (p i ,p i ) = 0.5L 2 (p i ,p i ), |p i ?p i | &lt; 1 L 1 (p i ,p i ) ? 0.5, otherwise</formula><p>(2) The L n series losses treat the error isotropically, even if there is error-bias on error direction of model or labelingbias on annotations, such as in landmarks regression. In other words, they ignore the anisotropy of errors in face alignment and give the same weight to loss in all directions.</p><p>According to the guideline of imposing strong constraint in normal direction and weak constraint in tangent direction, we propose the anisotropic direction loss (ADL), shown in <ref type="figure" target="#fig_3">Figure 3</ref>, which disentangles the error into two mutually orthogonal directions, namely normal error and tangent error, and put anisotropic loss weight to them, defined as follows:</p><formula xml:id="formula_2">ADL n (p i ,p i ) = ( 2? 1 + ? |N (p i ,p i ) ? (p i ?p i )| 2 + 2 1 + ? |T (p i ,p i ) ? (p i ?p i )| 2 ) n 2<label>(3)</label></formula><p>Where N (p i ,p i ) and T (p i ,p i ) are unit vectors to disentangle error into two sub-errors, when thep i locates on the edge, they are unit normal and tangent vectors respectively, otherwise, they would be unit error vectors; and ? refers to the hyper parameter to adjust constraint strength in these two sub-errors. Refer to the equation below for the corresponding Smooth ADL 1 loss:</p><formula xml:id="formula_3">SmoothADL 1 (p i ,p i ) = 0.5 ADL 2 (p i ,p i ), |p i ?p i | &lt; 1 ADL 1 (p i ,p i ) ? 0.5, otherwise<label>(4)</label></formula><p>The ADL n loss can be treated as a more general form of the L n loss. When ? = 1 orp i does not subordinate to any edge, ADL n exactly degenerates into L n .</p><p>Generally, most landmarks locate on edges of either face contour or five sense organs. Hence, for these landmarks, we get the normal direction by calculating the slope from its directly adjacent points on the edge, namelyp prei and p nexti , which can be obtained from a pre-defined template of landmarks. And as for other landmarks not associating any edges, we still put isotropic constraint on all the directions, thus, both normal direction and tangent direction are equal to error direction. Based on this, normal and tangent direction can respectively be defined as below:</p><formula xml:id="formula_4">N (p i ,p i ) = ppre i +pnext i ?2pi |ppre i +pnext i ?2pi| ,p i in edge pi?pi, |pi?pi| otherwise (5) T (p i ,p i ) = S ? N (p i ,p i ),p i in edge N (p i ,p i ), otherwise<label>(6)</label></formula><p>where S is a skew-symmetric matrix 0 1 ?1 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Anisotropic Attention Module (AAM)</head><p>As introduced in Section 2, coordinate regression and heatmap regression are widely applied in landmarks detection. Inspired by the proposed guideline, ADL loss is devised to deal with coordinate regression task under the bias of error direction. And as for heatmap regression task, in order to enhance the localization capability on normal direction, anisotropic attention module (AAM) is designed through supervising the network to focus more on tangent direction. In previous boundary-aware methods, the attention mask only contains boundary information, thus merely from which it is incapable of getting any point information.</p><p>To leverage the complete semantic information offered by prior information, the points and its directly connected edges, anisotropic attention module outputs the anisotropic heatmap by combining point heatmap and edge heatmap in  target point with its partial edge has high response, which means it contains both edge and point information and can guide model training in normal and tangent anisotropically. Therefore, using soft argmax operation with the ability of mapping heatmap to coordinate, model accumulates landmarks coordinates mainly on the local region of attentioned landmarks heatmap with tangent information instead of on whole region of the landmarks heatmap.</p><formula xml:id="formula_5">H point?edge = H point ? H edge<label>(7)</label></formula><p>Where H point is the point heatmap and H edge is the edge heatmap. Both of them are generated from the feature map of each HG and are supervised on two ground truth concurrently, point and edge heatmaps, by Awing <ref type="bibr" target="#b45">[45]</ref> loss in Equation 8. Then the combined H point?edge serves as attention mask in landmarks coordinates regression by</p><p>Smooth ADL 1 loss in Equation <ref type="bibr" target="#b3">4</ref>. In this process, it is interesting that the shape of salient region in point heatmap H point is more like a rough edge than a point region in gaussian distribution. This is caused by the supervisory signal from Smooth ADL 1 loss and is conducive to generate point-edge heatmap with strong tangent information.</p><formula xml:id="formula_6">AWing(y,?) = ? ln(1 + | y?? ? | ??y ), |y ??| &lt; ? A|y ??| ? C, otherwise<label>(8)</label></formula><p>The detailed description and default values of hyper parameters could refer to AWing <ref type="bibr" target="#b45">[45]</ref> loss, from which both H point and H edge adopt the same setting to learn the ground truth of respective heatmaps.</p><formula xml:id="formula_7">P = soft argmax(H landmarks ? H point?edge ) (9)</formula><p>Where H landmarks is also one of the feature maps from each HG, and P is the predicted landmarks value by applying soft argmax operation to H landmarks . Then network could learn P by annotatedP for each training sample under the constraint of Smooth ADL 1 .</p><p>Finally, the holistic loss of our architecture is:</p><formula xml:id="formula_8">L ADNet = SmoothADL 1 + ? ? AWing edge + ? ? AWing point<label>(10)</label></formula><p>Where ? and ? are the loss weights to balance different tasks, we empirically set both as 10 in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>To generate the inputs of our model, we cropped face regions and resized them into 256?256. We applied augmentation techniques to the training data by random rotation (18 ? ), random scaling (?10%), random crop (?5%), random gray (20%), random blur (30%), random occlusion (40%) and random horizontal flip (50%). Regarding the architecture, we adopt four stacked hourglass modules. Each hourglass outputs three 64 ? 64 feature maps (landmark, edge, and point heatmaps). To train our model, we empirically set the hyper-parameters of Gaussian sigma as 1.5 in point heatmap generation, line width as 1.0 in edge heatmap generation, and ? = 2.0 in the anisotropic direction loss function. We employed Adam optimizer with the initial learning rate of 1 ? 10 ?3 and reduced the learning rate by 1/10 at each epoch of 200, 350, and 450. The model was trained on four GPUs (16GB NVIDIA Tesla P100), where the batch size of each GPU is 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Normalized Mean Error (NME) is a widely-used standard metric to evaluate landmark accuracy for face alignment algorithms, which is defined as</p><formula xml:id="formula_9">NME(P,P ) = 1 N P N P i=1 ?p i ?p i ? 2 d<label>(11)</label></formula><p>Where P andP denote the predicted and ground-truth coordinates of landmarks, respectively, N P is the number of landmarks, and d is the reference distance to normalize the absolute errors. Usually, d could be inter-ocular (distance between outer eye corners) or inter-pupils (distance between pupil centers) distance. This paper uses interocular distance as the normalization factor for 300W <ref type="bibr" target="#b37">[37]</ref>, WFLW <ref type="bibr" target="#b47">[47]</ref>, and inter-pupils for 300W <ref type="bibr" target="#b37">[37]</ref> and COFW <ref type="bibr" target="#b5">[6]</ref>.</p><p>Failure Rate (FR) is a metric to evaluate the robustness of algorithms in terms of NME. Samples having larger NME than a pre-defined threshold are regarded as failed prediction. FR is defined by the percentage of failed examples over the whole dataset. We set the thresholds by 5% for 300W <ref type="bibr" target="#b37">[37]</ref>, and 10% for COFW <ref type="bibr" target="#b5">[6]</ref> and WLFW <ref type="bibr" target="#b47">[47]</ref>.</p><p>Area Under Curve (AUC) is another widely-adopted metric for face alignment task. It can be calculated by using Cumulative Error Distribution (CED) curve. The horizontal axis of CED plot indicates the target NME (ranging between 0 and the pre-defined threshold), and the CED value at the specific point means the rate of samples whose NME are smaller than the specific target NME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>NME FR 10% AUC 10% Human <ref type="bibr" target="#b5">[6]</ref> 5.60 --RCPR <ref type="bibr" target="#b5">[6]</ref> 8.50 20.00 -TCDCN <ref type="bibr" target="#b54">[54]</ref> 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Method Comparison</head><p>To compare the performance of ADNet with the state-ofthe-art competitors, we conduct evaluation on three public datasets: COFW <ref type="bibr" target="#b5">[6]</ref>, 300W <ref type="bibr" target="#b37">[37]</ref> and WFLW <ref type="bibr" target="#b47">[47]</ref>.</p><p>COFW <ref type="bibr" target="#b5">[6]</ref> contains a wide range of head poses and heavy occlusions, with 1,345 training images and 507 testing images. Each face has 29 annotated landmarks.</p><p>The experimental results on the COFW dataset are shown in <ref type="table">Table 1</ref>, where all the models were tested under the same condition without any additional annotations. As tabulated in the table, our method outperforms the competitors by a wide margin in NME and FR 10% . Compared with the second leading method, ADNet decreases NME by 5.3% in NME, which implies that the model is robust to large head poses and heavy occlusion despite the sparse landmarks definition.</p><p>300W <ref type="bibr" target="#b37">[37]</ref> is a widely-adopted dataset for face alignment, with 3,148 images for training and 689 images for testing. The testing data is divided into two sub-categories: 554 for the common subset, and 135 for the challenging subset. All the data are manually labelled with 68 landmarks. <ref type="table">Table 2</ref> compares our methods with the other approaches using different normalization factors: inter-ocular and interpupils distances. In both settings, our method yields the competitive accuracy, usually one of the best two. In particular, for inter-ocular normalization, ADNet improves the metric by 4.6% and 7.0% in NME over the previous SOTA method on the fullset and the common subset, respectively. As shown in <ref type="table">Table 2</ref>, ADNet achieves a larger improvement on the common subset than the challenging subset, which could be due to the inaccurate labels in the challenging subset. For instance, in the challenging subset of 300W, we observed some samples having inaccurate landmark labels on the chin regions due to wrong bounding boxes, which could lead to the domain gap issue and degraded performance.</p><p>WFLW <ref type="bibr" target="#b47">[47]</ref> is a challenging dataset constructed from inthe-wild face images. It also provides rich attribute labels such as occlusion, pose, make-up, illumination, blur and expression. And it consists of 7,500 faces for training and 2,500 faces for testing, with 98 annotated landmarks. Using the WFLW dataset, we aim to evaluate the models in various specific scenarios by making use of the annotated labels such as pose, expression, illumination and occlusion. In addition, in order to demonstrate the stability of landmark localization, we employ three metrics: NME, FR (10%) and AUC (10%). As tabulated in <ref type="table" target="#tab_4">Table 3</ref>, ADNet outperforms the other state-of-the-art methods significantly in most of subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>To explore the efficacy and the contribution of each component of our proposed model, we conduct comprehensive ablation experiments.</p><p>Evaluation on different ? for ADL. To find the optimal hyper-parameters for ADL, we trained our model with different ? on the 300W dataset. And AAM is not included so as to avoid ambiguity. Instead of seeking the entire parameter space, we choose a few candidates of ? as shown in   <ref type="table">Table 4</ref>. It appears that, compared with the model without ADL (? = 1.0), the optimal model gained about 4% improvement in NME when ? = 2.0. In addition, when ? is set to 0.5 (more weight for the tangent direction), the alignment accuracy dropped, which aligns with our assumption about error bias in face alignment. However, when ? ? 2.0, the performance is not sensitive to the value of ?. In our experiments, we chose ? = 2 as the default setting.</p><p>Evaluation on different attention modules for AAM. AAM plays a vital role in ADNet, which contributes the largest improvement in alignment accuracy. In <ref type="table" target="#tab_5">Table 5</ref>, 'w/ PAM' and 'w/ EAM' indicate that AAM is replaced with point attention module (PAM) and edge attention module (EAM), respectively. As revealed in the table, anisotropic attention leads to the best performance (12% improvement in NME over the second leading method) among the three ones that attempt to make the model focus on salient regions. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the distribution of generated heatmap from AAM is aligned with the tangent direction rather than normal direction, which accounts for the effectiveness of AAM in our method.</p><p>Evaluation on different backbones in our method. To investigate the impact of backbones on face alignment per-   for the network architecture. The results are tabulated in <ref type="table">Table 6</ref> where baseline denotes the model trained without ADL and AAM, in other words, baseline model feed  <ref type="table">Table 6</ref>. NME(%) in different backbones on the 300W dataset.</p><p>Evaluation with and without facial attributes labels. It is shown that additional labeling information, such as face attributes, can help the model learn face alignment effectively. For example, PropNet <ref type="bibr" target="#b22">[22]</ref> proposed a focal factor which dynamically adjusts the loss weight by utilizing class labels. We also test ADNet equipped with the focal factor, which is denoted as ADNet * . As shown in <ref type="table" target="#tab_4">Table 3</ref> and 7, it is apparent that both PropNet <ref type="bibr" target="#b22">[22]</ref> and ADNet * benefit from facial attribute labels.  <ref type="table">Table 7</ref>. NME(%) with and without attribute labels. The result of ADNet * on 300W is missing because the annotations for 300W was labelled by PropNet and the data is not published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Evaluation on different error directions. Our fundamental assumption regarding the error-bias is that we should impose a strong constraint to the error along normal direction, while a relaxed constraint for the tangent-direction error. Similar ideas were also discussed in previous studies, LAB <ref type="bibr" target="#b47">[47]</ref> and PropNet <ref type="bibr" target="#b22">[22]</ref>. To further investigate the error direction, we report NME along normal and tangent directions independently as shown in <ref type="table">Table 8</ref> The baseline indicates the model without ADL and AAM. It can be observed that there is the significantly stronger NME bias along the tangent direction than tangent direction, which supports our assumption. In addition, ADL and AAM appear to conduce to model accuracy enhancement, especially in normal direction, while allowing flexibility along the tangent direction to some degree, resulting in the increased bias rate of 48.05%. In other words, it is reasonable to impose a strong  <ref type="table">Table 8</ref>. NME(%) in normal and tangent directions of landmarks on the 300W dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth Error in normal direction</head><p>Error in tangent direction <ref type="figure">Figure 5</ref>. Visualization results on different error direction. Green points are ground truth, red points are mocked predictions. In the middle figure, all the mocked predictions have error in normal direction, and tangent direction in the right <ref type="figure">figure.</ref> Obviously, errors in tangent direction is more acceptable than normal direction which is aligned with labeling bias and human perception.</p><p>constraint to the error along the normal direction, which allows us to achieve better holistic performance in the end. In addition, as demonstrated in <ref type="figure">Figure 5</ref>, errors along the tangent direction (the third image) are more acceptable to human perception than the same amount of the errors along the normal direction (the second image).</p><p>Bias Rate = NME tangent ? NME normal NME normal</p><p>Where NME tangent and NME normal are respectively the NME in tangent and normal directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we point out the significant bias of error distribution of each facial landmark, which is caused by ambiguity in the process of labeling. Inspired by this knowledge, anisotropic direction loss is proposed to handle error in normal and tangent directions independently. Moreover, anisotropic attention module is put forward to efficiently combine point and edge heatmap information. By combining and integrating the two modules, the proposed model imposes more constraint on normal direction than on tangent direction for the learning of landmarks coordinates. And rigorous experiments show that the proposed method outperforms other state-of-the-art models on multiple datasets. Finally, ablation studies verify the usefulness of the method comprehensively. Additionally, we realized that the current metrics do not reflect the actual error bias issue, and result in the same error values for the 2nd and 3rd images in <ref type="figure">Figure 5</ref>. In future work, the new metric considering human perception shown in <ref type="figure">Figure 5</ref> and the error-bias should be studied to provide more meaningful insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Model Architecture Tables 9, 10 and 11 fully demonstrate the architecture of the proposed ADNet. For detailed introduction of our experimental setting, please refer to Section 4 of our manuscript. In the table, P * , H * point and H * edge denote the inputs of smooth ADL1 loss, AWing loss and AWing loss, respectively. N point and N edge indicate the number of points and edges, which varies according to each dataset. The loss weights of Hour Glass (HG) for stacked 4 HGs are respectively 1/8, 1/4, 1/2, and 1. The fourth head branch outputs P 3 is the final predicted coordinate of each landmark, which is derived from the soft argmax operation.</p><p>In <ref type="table">Table 10</ref>, the goal of E2P Transform is to convert H edge (N edge channels) into H edge (N point channels) by considering the adjacency relationship as E2P Transform(? edge (x, y)) = MatE2P ?? edge (x, y) <ref type="bibr" target="#b13">(13)</ref> where? edge (x, y) is a column vector at the position of (x, y), and Mat E2P is a N point ?N edge binary matrix describing the adjacency relationship between each point and each edge. More specifically, if the ith point is connected to the jth edge, Mat E2P (i, j) = 1, otherwise, Mat E2P (i, j) = 0. Note that Mat E2P is a constant variable, and is derived based on the landmark definition of each dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Edge Definition</head><p>We categorize the landmarks into two groups: edge landmarks and point landmarks. If the landmarks locate on edges, they belong to the former group, conversely, landmarks not on edges belong to the latter group. For several well-known face alignment datasets such as COFW, 300W, and WFLW, most of the landmarks belong to edge landmarks. We show our definition of edges in 300W dataset in <ref type="table">Table 12</ref> and <ref type="figure" target="#fig_5">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Comparison of Inference Time</head><p>To show the computational complexity of ADL and AAM, we compare the inference time of the baseline model and ADNet. Note that the baseline model is almost identical to ADNet except that AAM and ADL are removed from the baseline. To estimate the time, we repeated the experiment 10 times on the 300W fullset and averaged the measured times. We used one NVIDIA v100 GPU with a batch size of 1. As tabulated in <ref type="table" target="#tab_4">Table 13</ref>, ADNet takes only 6% longer time than the baseline method, which indicates the high efficiency of ADL and AAM. Moreover, ADL and AAM take small FLOPs and require a small number of parameters as shown in the table.  <ref type="table">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Evaluation of Individual Edges on 300W</head><p>Apart from evaluating the whole face on the test dataset, we also provide the NME of each edge in the 300W fullset dataset to fully demonstrate the effectiveness of the proposed method. The detailed results are shown in <ref type="table" target="#tab_5">Table 15</ref>. The bias rate is defined as Bias Rate = NME tangent ? NME normal NME normal <ref type="bibr" target="#b14">(14)</ref> where NME tangent and NME normal are respectively the NME in tangent and normal directions. For both normal NME and tangent NME, ADNet outperforms the baseline method for every edge. In addition, ADNet has always larger bias rate than the baseline, which means that ADNet is leveraging the bias towards normal direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Exploration of ? Settings</head><p>We investigate three ? settings in <ref type="table">Table 14</ref>: i) All landmarks have the same value ? i = 2: (c)(f). Other ? i can be found in <ref type="table">Table 14</ref>. ii) ? i = 4 for the outer face contour (denoted by O in <ref type="table">Table 14</ref>), and ? i = 2 for the rest: (d)(g). iii) Independent ? i for each landmark: (e)(h). Each was computed by ? i = a i /b i , where a i and b i are long and short radius of each fitted ellipse by error distribution in <ref type="figure" target="#fig_6">Figure 7</ref>(a). It can be observed that: i) though a more flexible ? i leads to better performance, the improvement is marginal; ii) the significant improvement comes from AAM rather than ADL.  <ref type="figure" target="#fig_5">Figure 6</ref> with the same color.  <ref type="table">Table 14</ref>. Evaluating different ? strategies on 300W in terms of interocular NME. The Baseline in (a) removes both AAM and ADL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Demonstration of Error Distribution on 300W</head><p>To demonstrate the error-bias in error distribution with realworld data, in <ref type="figure" target="#fig_6">Figure 7</ref>, we provide the empirical error distribution of chin point obtained by using an off-the-shelf face alignment algorithm on the 300W dataset trained by baseline method. It is obvious that the error distribution along tangent direction (tangent distribution in <ref type="figure">figure)</ref> is broader than that along the normal direction (normal distribution in <ref type="figure">figure)</ref>, which is consistent with our assumption, error-bias towards normal direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 Visualized Examples of ADNet</head><p>To verify the robustness of ADNet, we additionally show the landmark inference on the extended test data in <ref type="figure">Figure 9</ref>, 10 and 11. For each image, the first row (red landmarks) is the inference result by ADNet and the second row (green landmarks) is the corresponding ground-truth provided by the dataset. As can be seen, our method yields stable and reasonable prediction of landmarks even for dif- ficult cases such as extreme occlusion, large pose, extreme expression, blur and bad illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Extra Description of AAM</head><p>As described in the manuscript, the anisotropic attention module outputs an anisotropic mask per landmarks. By design, the anisotropic mask has a strong response in tangent direction and a weak response in normal direction. Consequently, each predicted landmark has a large tolerance for tangent error, but small tolerance for normal error. This can be confirmed in the visualized example in <ref type="figure" target="#fig_7">Figure 8</ref>, where the AAM mask has broad distribution along tangent direction (ranging between t 0 to t 1 ) while the distribution along normal direction is limited (ranging between n 0 to n 1 ). In other words, the guideline imposes strong constraints along the normal direction of each landmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edges</head><p>Methods Overall NME Normal NME Tangent NME Bias Rate  <ref type="table" target="#tab_5">Table 15</ref>. Evaluation of individual edges on 300W.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Manual annotations variance v.s. Prediction error distribution on 300W dataset. (a) The variance of the manual annotations for each landmarks cited from 300W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Equation 7, named point-edge heatmap. When utilizing the point-edge heatmap as attention mask, the local region of Overview of our ADL training framework.The backbone is constructed by stack four hourglass modules, and each hourglass module connected with three head branches, point attention, edge attention and final landmark regression branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Diagram of Anisotropic Direction Loss (ADL). In the right figure, the green point (pi) attracts red point (pi) under force filed, which is strong in normal direction and weak in tangent directions. According to the proposed guideline, the model studies more in normal direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Samples from 300W fullset. (Red indicates NME, blue indicates normal NME, and purple indicates tangent NME.) (a) is the baseline result without ADL and AAM, (b) is the result of AD-Net, (c) is the output of a sample from H point?edge and (d) is the accumulated output of half H point?edge set for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualized example of edges in 300W. Each colored line corresponds to each edge defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Error distribution of chin landmark (the 8th point in Figures 6) on the 300W fullset dataset obtained by off-the-shelf face alignment model. Each sub-figure (up/right) shows the projected error distribution along (tangent/normal) direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Error tolerance in different direction by applying AAM mask. The orange segment indicates the predicted coordinate range in normal direction, and green segment indicates the predicted coordinate range in tangent direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .Figure 10 .Figure 11 .</head><label>91011</label><figDesc>Visualized examples in COFW test dataset. (Red denotes predicted values by ADNet model and Green denotes ground truth.) Visualized examples in the 300W test dataset. (Red denotes predicted values by ADNet model and Green denotes ground truth.) Visualized examples in the WFLW test dataset. (Red denotes predicted values by ADNet model and Green denotes ground truth.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.09 / 5.76 7.18 / 5.41 / 3.63 8.53 / 5.30 / 6.14 5.68 / 3.14 / 4.42 2.09 / 0.98 / 1.74 5.01 / 3.24 / 3.43 3.81 / 2.49 / 2.45 6.68 / 3.81 / 5.09 3.97 / 1.92 / 3.27</figDesc><table><row><cell>3.49 / 1.38 / 3.03</cell><cell>9.73 / 7</cell></row><row><cell></cell><cell cols="2">(a) Baseline</cell></row><row><cell></cell><cell cols="2">(b) ADNet</cell></row><row><cell></cell><cell>(c)</cell><cell>_</cell></row><row><cell></cell><cell>(d) ?</cell><cell>_</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparing with state-of-the-art methods on WFLW testing set. PropNet and ADNet * (Ours) employ focal wing loss<ref type="bibr" target="#b22">[22]</ref> by using the attribute labels provided by WFLW.</figDesc><table><row><cell>Method</cell><cell cols="2">NME(%) FR 5% AUC 5%</cell><cell>Method</cell><cell cols="3">NME(%) FR 5% AUC 5%</cell></row><row><cell>?=0.5</cell><cell>3.43</cell><cell>12.01 0.3504</cell><cell>w/o AAM</cell><cell>3.38</cell><cell cols="2">11.72 0.3653</cell></row><row><cell>?=1.0(w/o ADL)</cell><cell>3.38</cell><cell>11.72 0.3653</cell><cell>w/ PAM</cell><cell>3.12</cell><cell>9.56</cell><cell>0.4095</cell></row><row><cell>?=2.0</cell><cell>3.23</cell><cell>10.45 0.4006</cell><cell>w/ EAM</cell><cell>3.09</cell><cell>9.13</cell><cell>0.4124</cell></row><row><cell>?=4.0</cell><cell>3.24</cell><cell>11.03 0.4027</cell><cell>w/ AAM</cell><cell>2.98</cell><cell>8.72</cell><cell>0.4318</cell></row><row><cell>?=8.0</cell><cell>3.27</cell><cell>11.47 0.3950</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 4. The contribution of ADL under different ?. AAM is not used in these settings.formance, we trained models with two additional archi- tectures. All the settings are identical to ADNet except</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The contribution of AAM under different attention modules. ADL is not used in these settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>H landmarks directly to soft-argmax, then apply Smooth L 1 (a.k.a Smooth ADL 1 with ? = 1) loss to supervise landmark coordinates. For the ResNet50 backbone, we add three deconvolutional layers after the last layer to generate 64?64 heatmap like our original model. As shown in the table, all the models are significantly enhanced by equipping them with ADL and AAM, which demonstrates that ADL and AAM are generally adaptable regardless of network architecture. In addition, ADNet with the default backbone (Stacked 4 HGNet) yields the best performance among three models.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Baseline ADNet</cell></row><row><cell>Stacked 4 HGNet [33]</cell><cell>3.38</cell><cell>2.93</cell></row><row><cell>Single HGNet [33]</cell><cell>3.39</cell><cell>2.99</cell></row><row><cell>ResNet50 [18]</cell><cell>3.43</cell><cell>3.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .Table 10 .Table 11 .</head><label>91011</label><figDesc>The architecture of ADNet. x[*] and H[*] indicate intermediate feature maps, and BN indicates batch normalization. The detailed structure of "Head Branch" and "Residual Block" are shown in Tables 10 and 11. The architecture of head branch. The architecture of residual block. "output channels" denotes the channel size of the residual block's output.</figDesc><table><row><cell>Layer</cell><cell>Input of layer</cell><cell>Output of layer</cell><cell>Output Channels</cell><cell>Kernel</cell><cell>Stride</cell><cell>Padding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Size</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>image</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Coord Conv [27]</cell><cell>image</cell><cell>x0</cell><cell>64</cell><cell>7</cell><cell>2</cell><cell>3</cell></row><row><cell>BN-ReLu</cell><cell>x0</cell><cell>x1</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Residual</cell><cell>x1</cell><cell>x2</cell><cell>128</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Block [18]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max Pool</cell><cell>x2</cell><cell>x3</cell><cell>128</cell><cell>2</cell><cell>2</cell><cell>0</cell></row><row><cell>Blur Pool [53]</cell><cell>x3</cell><cell>x4</cell><cell>128</cell><cell>3</cell><cell>2</cell><cell>0</cell></row><row><cell>Residual Block</cell><cell>x4</cell><cell>x5</cell><cell>128</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Residual Block</cell><cell>x5</cell><cell>x6</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Head Branch</cell><cell>x6</cell><cell>(P 0, x7, H0point, H0 edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Head Branch</cell><cell>x7</cell><cell>(P 1, x8, H1point, H1 edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Head Branch</cell><cell>x8</cell><cell>(P 2, x9, H2point, H2 edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Head Branch</cell><cell>x9</cell><cell>(P 3, x10, H3point, H3 edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Output</cell><cell>-</cell><cell>(P  * , H * point, H *  edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer</cell><cell>Input of layer</cell><cell>Output of layer</cell><cell>Output Channels</cell><cell>Kernel</cell><cell>Stride</cell><cell>Padding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Size</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>y0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hour Glass [33]</cell><cell>y0</cell><cell>y1</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv-BN-ReLu</cell><cell>y1</cell><cell>y2</cell><cell>256</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Residual Block</cell><cell>y2</cell><cell>y3</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv-Sigmoid</cell><cell>y3</cell><cell>Hpoint</cell><cell>Npoint</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Conv-Sigmoid</cell><cell cols="2">y3? edge</cell><cell>N edge</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell cols="2">E2P Transform? edge</cell><cell>H edge</cell><cell>Npoint</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Elementwise dot</cell><cell>(Hpoint, H edge )</cell><cell>H point?edge</cell><cell>Npoint</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv-ReLu</cell><cell>y3</cell><cell>H landmarks</cell><cell>Npoint</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Elementwise dot</cell><cell>(H landmarks ,</cell><cell>AH landmarks</cell><cell>Npoint</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>H point?edge )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soft Argmax</cell><cell>AH landmarks</cell><cell>P</cell><cell>Npoint</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv</cell><cell>H landmarks</cell><cell>y4</cell><cell>256</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Conv</cell><cell>Hpoint</cell><cell>y5</cell><cell>256</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Conv</cell><cell>H edge</cell><cell>y6</cell><cell>256</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Elementwise sum</cell><cell>(y3, y4, y5, y6)</cell><cell>y7</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Output</cell><cell>-</cell><cell>(P , y7, Hpoint, H edge )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer</cell><cell>Input of layer</cell><cell>Output of layer</cell><cell>Output Channels</cell><cell>Kernel</cell><cell>Stride</cell><cell>Padding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Size</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>z0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BN-ReLu-Conv</cell><cell>z0</cell><cell>z1</cell><cell>output channels / 2</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>BN-ReLu-Conv</cell><cell>z1</cell><cell>z2</cell><cell>output channels / 2</cell><cell>3</cell><cell>1</cell><cell>1</cell></row><row><cell>BN-ReLu-Conv</cell><cell>z2</cell><cell>z3</cell><cell>output channels</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Skip</cell><cell>z0</cell><cell>z4</cell><cell>output channels</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Elementwise sum</cell><cell>(z3, z4)</cell><cell>z5</cell><cell>output channels</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Output</cell><cell>-</cell><cell>z5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards open-set identity preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6713" to="6722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3706" to="3714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on fairness, accountability and transparency</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Active shape models-&apos;smart snakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Active shape models-their training and application. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decafa: deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3636" to="3648" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2481" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask-guided portrait editing with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3436" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ace-net: Fine-level face alignment through anchors and contours estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Tamrakar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01461</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Propagationnet: Propagate points to curve to learn structure information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiehe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiubao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7265" to="7274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient 3d reconstruction for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="787" to="798" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An active illumination and appearance (aia) model for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8236" to="8246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03247</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 31st SIBGRAPI conference on graphics, patterns and images (SIBGRAPI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5040" to="5049" />
		</imprint>
	</monogr>
	<note>Vassilis Athitsos, and Heng Huang</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Locating facial features with an extended active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A nonlinear discriminative approach to aam fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accurate regression procedures for active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
	<note>Shaoting Zhang, and Dimitris Metaxas</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Leveraging intra and interdataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarseto-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
