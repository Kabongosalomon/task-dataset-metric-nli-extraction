<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. and Tech</orgName>
								<orgName type="department" key="dep2">Tsinghua-Bosch Joint ML Center</orgName>
								<orgName type="institution">Institute for AI</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@idea.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
							<email>yangxiao19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. and Tech</orgName>
								<orgName type="department" key="dep2">Tsinghua-Bosch Joint ML Center</orgName>
								<orgName type="institution">Institute for AI</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. and Tech</orgName>
								<orgName type="department" key="dep2">Tsinghua-Bosch Joint ML Center</orgName>
								<orgName type="institution">Institute for AI</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. and Tech</orgName>
								<orgName type="department" key="dep2">Tsinghua-Bosch Joint ML Center</orgName>
								<orgName type="institution">Institute for AI</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish 91.3% mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-label image classification aims to gain a comprehensive understanding of objects and concepts in an image which has wide applications in realistic scenarios including image search, personal photo organization, digital asset management, medical image recognition <ref type="bibr" target="#b20">[21]</ref>, and scene understanding <ref type="bibr" target="#b40">[41]</ref>. Compared with single label classification, multi-label classification requires special attention on two problems: 1) how to handle the label imbalance problem, and 2) how to extract features from region of interests. The former problem is because of the one-vs-all strategy, i.e., it usually trains a batch of separate binary classifiers  with each designed for recognizing a particular class, which may lead to severely imbalanced numbers of positive and negative samples especially when the number of classes is large . The latter problem is because of the distributed objects, i.e., an image often has multiple objects at different locations -a globally pooled feature as normally used in single label classification may dilute the features and make it hard to identify small objects.</p><p>It has witnessed significant attempts to solve the aforementioned issues. To balance positive and negative samples, many loss functions have been developed, such as focal loss <ref type="bibr" target="#b33">[34]</ref>, distribution-balanced loss <ref type="bibr" target="#b49">[50]</ref>, and asymmetric loss <ref type="bibr" target="#b0">[1]</ref>. Some works have tried to address the second problem by utilizing spatial transformer network <ref type="bibr" target="#b46">[47]</ref>, adopting a global-to-local strategy <ref type="bibr" target="#b18">[19]</ref>, or using semantic label embeddings learned from label graph to discover the locations of discriminative features <ref type="bibr" target="#b52">[53]</ref>. Compared with the first issue, solutions for the second problem are relatively less mature, requiring either specially designed network architectures or additional dependencies on label correlation.</p><p>Motivated by the success of Transformer used in com-puter vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, we present in this paper a simple yet effective solution using Transformer decoder to query the existence of a class label. We show that, without bells and whistles, the proposed solution leads to new state-ofthe-art results and establishes strong baselines for its simple implementation and superior performance. We name the proposed solution as Query2Label and illustrate it in <ref type="figure" target="#fig_1">Fig.  1</ref>. As shown in the figure, we use learnable label embeddings as queries to probe and pool class-related features via the cross-attention module in Transformer encoders. The pooled features are adaptive and more discriminative, leading to a superior multi-label classification performance. The use of Transformer for solving multi-label classification is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. While previous works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b0">1]</ref> show that it is possible to use the globally average-pooled feature from the last layer of a convolutional neural network (CNN) for its simplicity in implementation, we argue that this will lead to inferior performance due to its discard of rich information in the convolutional feature map. The built-in crossattention mechanism, which is called encoder-decoder attention in <ref type="bibr" target="#b44">[45]</ref>, makes Transformer decoder a perfect choice for adaptively extracting desired features. By treating each label class as a query in a Transformer decoder, we can perform cross-attention to pool related features for the subsequent binary classification. The most related work to this idea is developed by You et al. <ref type="bibr" target="#b52">[53]</ref>, which, however, computes attention using cosine similarity with negative value clipping and uses the same feature for both key and value, greatly limiting its capability of learning locally discriminative features.</p><p>Another advantage of Transformer is its multi-head attention mechanism, which can extract features from different parts or different views of an object class and thus is more capable of recognizing objects with occlusions and viewpoint changes. By contrast, the cross-modal attention in <ref type="bibr" target="#b52">[53]</ref> is merely a single-head attention, which is incapable of extracting features by parts or views.</p><p>In this work, we develop a simple two-stage framework called Query2Label for multi-label classification by leveraging multi-layer Transformer decoders. In the first stage, we use an image classification backbone to extract image features. The backbone could be either conventional CNN models such as ResNet <ref type="bibr" target="#b24">[25]</ref> or recently developed Vision Transformer models <ref type="bibr" target="#b16">[17]</ref>. In the second stage, multiple Transformer decoder layers are leveraged, using label embeddings as queries to check the existence of each label by performing multi-head cross-attention to pool object features adaptively for subsequent binary classification to predict the existence of the corresponding label. Unlike <ref type="bibr" target="#b52">[53]</ref> in which the label embeddings are learned separately to take into account label correlations, we learn the label embeddings end-to-end to maximize the model potential and avoid the risk of introducing spurious correlations. The idea of using learnable label embeddings is inspired by DETR <ref type="bibr" target="#b2">[3]</ref>. But the queries in DETR are class agnostic, whereas in our work each query (or label embedding) uniquely corresponds to one label class, making it more effective to extract class-related features. For this reason, in this paper, we will use label embedding and query interchangeably.</p><p>To handle the label imbalance problem, we adopt a simplified asymmetric loss <ref type="bibr" target="#b0">[1]</ref> by using different ? values to weight positive and negative samples differently in focal loss. We found that this simple asymmetric loss works sufficiently well with this Transformer-based framework and leads to new state-of-the-art results on several multi-label benchmark data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome.</p><p>Our contribution can be summarized as follows:</p><p>1. We develop a simple Transformer-based two-stage framework Query2Label for multi-label classification, leading to an effective way to query the existence of a class label. To our best knowledge, this is the first time that the Transformer decoder architecture is used in classification.</p><p>2. We show that, the built-in cross-attention module in Transformer decoders can adaptively extract object features and the multi-head attention further helps to decouple object representations into multiple parts or views, resulting in both improved classification performance and better interpretability.</p><p>3. We verify the effectiveness of the proposed method with comprehensive experiments on several widely used data sets: MS-COCO, PASCAL VOC, NUSWIDE, and Visual Genome, and establish new state-of-the-art results on all these data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Label Classification</head><p>Multi-label classification task has attracted an increasing interest recently. The proposed methods can be categorized into three main directions as follows:</p><p>Improving loss functions. As shown in Sec. 1, one of the key concerns in multi-label classification is the imbalance of samples due to the use of one-vs-rest binary classifier for each category. Wu et al. <ref type="bibr" target="#b49">[50]</ref> proposed a distribution-balanced loss to tackle the long-tailed problem by re-balancing weights and mitigating the oversuppression of negative labels. Ben-Baruch et al. <ref type="bibr" target="#b0">[1]</ref> proposed an asymmetric loss, which uses different ? values to weight positive and negative samples in focal loss <ref type="bibr" target="#b33">[34]</ref>, and discarding easy negative samples by shifting the prediction probability. In our study, we adopt a simplified asymmetric loss which uses different ? values for positive and negative samples without prediction probability shift.</p><p>Modeling label correlations. For its nature of multilabels on one image, the co-occurrence of concepts in a large-scale data set could be mined as prior knowledge for subsequent classification. Chen et al. <ref type="bibr" target="#b6">[7]</ref> proposed to learn category-correlated feature vectors by constructing a graph based on the statistical label co-occurrence and explored their interactions by performing neural graph propagation. Chen et al. <ref type="bibr" target="#b7">[8]</ref> constructed a similar graph but based on class-aware maps, which is calculated by image level feature and classification weights, and constrained the graph by label co-occurrence. Rather than using static graph, Ye et al. <ref type="bibr" target="#b51">[52]</ref> updated static graph to dynamic graph by using a dynamic graph convolutional network(GCN) module for robust representations. While modeling label correlations can introduce additional gains in multi-label classification, it is also arguable that it may learn spurious correlations when the label statistics are insufficient. In our study, we directly learn label embeddings from data and encourage the network to focus on regions of interest to learn better feature representations and capture label relationships implicitly without graph networks.</p><p>Locating regions of interest. As an image normally has multiple objects, how to locate areas of interest becomes a concern in multi-label classification. Early methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51]</ref> found proposals first and treated them as single-label classification. Wang et al. <ref type="bibr" target="#b46">[47]</ref> proposed to locate attentional regions corresponding to semantic labels by using a spatial transformer layer <ref type="bibr" target="#b26">[27]</ref> and predicted their scores with a Long Short-Term Memory (LSTM) sub-network <ref type="bibr" target="#b25">[26]</ref>. Gao et al. <ref type="bibr" target="#b18">[19]</ref> proposed a global-to-local discovery method to find proper regions with objects. All of these methods tried to find local regions to focus, but the discovered bounding boxes were coarse and often contained background information as well. You et al. <ref type="bibr" target="#b52">[53]</ref> computed cosine similarities between a label embedding and the feature map to derive an attention map after clipping negative values for class feature learning, which is a step forward. However, the cosine similarity with negative value clipping is likely to lead to a smoother and none spike attention, making it less effective in extracting desired local features (because it will cover larger areas than needed, see the visualized attention in <ref type="figure" target="#fig_4">Fig.  4</ref> in <ref type="bibr" target="#b52">[53]</ref>). By contrast, we adopt the built-in cross-attention in Transformer as spatial feature selector to extract desired features, which is both simple and effective, thanks to the modularized design of Transformer and its readily available implementations in modern deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer in Vision Tasks</head><p>Transformer <ref type="bibr" target="#b44">[45]</ref> was first proposed to model long-range dependencies in sequence learning problems, and has been widely used in natural language processing tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b54">55]</ref>. Recently, Transformer-based models have also been developed for many vision tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and shown great potentials. Chen et al. <ref type="bibr" target="#b4">[5]</ref> trained a sequence Transformer, named iGPT, to predict pixels auto-regressively. Dosovitskiy et al. <ref type="bibr" target="#b16">[17]</ref> proposed Vision Transformers (ViT), in which they split an image to multiple patches and feed them into a stacked Transformer architecture for classification. Carion et al. <ref type="bibr" target="#b2">[3]</ref> designed an end-to-end object detection framework named DETR with transformer. Yuan et al. <ref type="bibr" target="#b53">[54]</ref> proposed Tokens-To-Token Vision Transformer (T2T-ViT) to address the patch tokenization problem. Srinivas et al. <ref type="bibr" target="#b43">[44]</ref> replaced convolutional layers in last few ResNet Bottleneck <ref type="bibr" target="#b24">[25]</ref> with Multi-Head Self-Attention and capture better global dependencies. More progress of applying Transformer in computer vision can be referred to <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b27">[28]</ref>.</p><p>Our approach also uses Transformers, but we leverage the built-in cross-attention in the Transformer decoder to locate object features for each label, which is largely different from most existing works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10]</ref> using the selfattention mechanism in Transformer encoders to improve feature representation. Our work is inspired by DETR <ref type="bibr" target="#b2">[3]</ref>, but different in that the queries in DETR are class-agnostic and do not have clear semantics, whereas each query in our work uniquely corresponds to a semantic label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Query2Label is a two-stage framework for multi-label classification, which uses Transformer decoders to extract features with multi-head attentions focusing on different parts or views of an object category and learn label embeddings from data automatically. In this section, we will present our framework first (Sec. 3.1), followed with a brief description to the loss function used (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>Given an input image x, among a set of categories of interest, multi-label classification is to predict whether each category is present. A category could be either an object class (e.g. person, dog, table, etc.) or a scene category (grass, sky, etc). Assume there are K categories in total and we denote the corresponding label of x as y = [y 1 , ..., y K ], where y k ? {0, 1}, k = 1, ..., K, is a discrete binary indicator. y k = 1 if image x has the k-th category label, otherwise y k = 0. Using x as input, our model predicts the probabilities of the presence of each category, p = [p 1 , ..., p K ], where y k ? [0, 1], k = 1, ..., K.   <ref type="figure" target="#fig_2">Figure 2</ref>: The framework of our proposed Query2Label (Q2L). After extracting spatial features of an input image, each label embedding is sent to Transformer decoders to query (by comparing the label embedding with features at each spatial location to generate attention maps) and pool the desired feature adaptively (by linearly combining the spatial features based on the attention maps). The pooled feature is then used to predict the existence of the queried label.</p><p>Query2Label (Q2L). For an input image, it firstly feeds it into a backbone in the first stage to extract spatial features. The second stage is composed of two modules: a multilayer Transformer decoder block for query updating and adaptive feature pooling, and a linear projection layer for computing prediction logits for each category. Note that our method is backbone-agnostic. That is, the second stage could be attached to any feature extractor. In this work, we mainly use convolutional neural networks as the feature extraction backbone, but Transformer-based networks such as ViT <ref type="bibr" target="#b16">[17]</ref> can also be used.</p><p>Feature extracting. Given an image x ? R H0?W0?3 as input, we extract its spatial features F 0 ? R H?W ?d0 using the backbone, where H 0 ? W 0 , H ? W represent the height and weight of the input image and the feature map respectively, and d 0 denotes the dimension of features. After that, we add a linear projection layer to project the features from dimension d 0 to d to match with the desired query dimension in the second stage and reshape the projected features to be F ? R HW ?d .</p><p>Query updating. After obtaining the spatial features of the input image in the first stage, we use label embeddings as queries Q 0 ? R K?d and perform cross-attention to pool category-related features from the spatial features using multi-layer Transformer decoders, where K is the number of categories. We use the standard Transformer architecture, which has a self-attention module, a cross-attention module, and a position-wise feed-forward network(FFN). Each Transformer decoder layer i updates the queries Q i?1 from the output of its previous layer as follows:</p><p>self-attn: Q</p><formula xml:id="formula_0">(1) i = MultiHead( Q i?1 , Q i?1 , Q i?1 ), cross-attn: Q (2) i = MultiHead( Q (1) i , F, F), FFN: Q i = FFN(Q (2) i ),<label>(1)</label></formula><p>where the tilde means the original vectors modified by adding position encodings, Q</p><p>(1) i and Q</p><p>(2) i are two intermediate variables. Both the MultiHead(query, key, value) and FFN(x) functions are the same as defined in the standard Transformer decoder <ref type="bibr" target="#b44">[45]</ref> and we omit their parameters for simplicity. As we do not need to perform autoregressive prediction, we do not use attention masks. Thus the M categories can be decoded in parallel in each layer.</p><p>Both the self-attention and cross-attention modules are implemented using the same MultiHead function. The only difference is where key and value are from. In the selfattention module, query, key and value are all from label embeddings, whereas in the cross-attention module, key and value turn into the spatial features. The process of cross-attention can be described more intuitively: each label embedding Q i?1,k ? R d , k = 1, ..., K checks the spatial features F where to attend and selects features of interest to combine. After that, each label embedding obtains a better category-related feature and updates itself. As a result, the label embeddings Q 0 will be updated layer by layer and progressively injected with contextualized information from the input image via cross-attention.</p><p>Inspired by DETR, we treat the label embeddings Q 0 as learnable parameters. In this way, the embeddings can be learned end to end from data and model label correlations implicitly. The difference between our approach and DETR is that our queries are class-specific and have clear semantic meanings, whereas the queries in DETR are class-agnostic and it is hard to predict which query will detect which category of objects.</p><p>Feature Projection. Assuming that we have L layers in total, we will get the queried feature vectors Q L ? R K?d for K categories at the last layer. To perform multi-label classification, we treat each label prediction as a binary classification task and project the feature of each class Q L,k ? R d to a logit value using a linear projection layer followed with a sigmoid function:</p><formula xml:id="formula_1">p k = Sigmoid W T k Q L,k + b k ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">W k ? R d , W = [W 1 , ..., W K ] T ? R K?d , and b k ? R, b = [b 1 , ..., b K ] T ? R K</formula><p>are parameters in the linear layer, and p = [p 1 , ..., p K ] T ? R K are the predicted probabilities for each category. Note that p is a function which maps an input image x to category prediction probabilities.</p><p>x is omitted for notation simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Function</head><p>Thanks to the built-in cross-attention mechanism in Transformer decoders, our framework does not require a new loss function. Both the binary cross entropy loss and focal loss <ref type="bibr" target="#b33">[34]</ref> work well with our framework. To more effectively address the sample imbalance problem, we adopt a simplified asymmetric loss <ref type="bibr" target="#b0">[1]</ref>, which is a variant of focal loss with different ? values for positive and negative values.</p><p>In our experiments, we found it works the best.</p><p>Given an input images x, we can predict its category probabilities p = [p 1 , ..., p K ] T ? R K using our framework. Then we leverage the following asymmetric focal loss to calculate the loss for each training sample x:</p><formula xml:id="formula_3">L = 1 K K k=1 (1 ? p k ) ?+ log(p k ), y k = 1, (p k ) ?? log(1 ? p k ), y k = 0,<label>(3)</label></formula><p>where y k is a binary label to indicate if image x has label k. The total loss is computed by averaging this loss over all samples in the training data set D. And the optimization is performed using stochastic gradient descent. By default, we set ?+ = 0 and ?? = 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the proposed approach, we conduct experiments on several data sets, including MS-COCO <ref type="bibr" target="#b34">[35]</ref>, PAS-CAL VOC <ref type="bibr" target="#b17">[18]</ref>, NUS-WIDE <ref type="bibr" target="#b10">[11]</ref>, and Visual Genome <ref type="bibr" target="#b29">[30]</ref>. Following previous works, we adopt the average precision (AP) on each category and mean average precision (mAP) over all categories for evaluation. To better demonstrate the performance of models, we also present the overall precision (OP), recall (OR), F1-measure (OF1) and per-category precision (CP), recall (CR), F1-measure (CF1) for further comparison. See appendix for a more formal definition of these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Unless otherwise stated, we will use the settings described below for all experiments. Following ASL <ref type="bibr" target="#b0">[1]</ref>, We adopt TResNetL <ref type="bibr" target="#b39">[40]</ref> as our backbone, as it performs better than the standard ResNet101 <ref type="bibr" target="#b24">[25]</ref> for this task under similar efficiency constraints on GPU. We resize all images to H 0 ? W 0 = 448 ? 448 as the input resolution and the size of the output feature from TResNetL is H ? W ? d 0 = 14 ? 14 ? 2432. We set d = d 0 = 2432 in our experiments, hence the size of the final output feature in the first stage is 14 ? 14 ? 2432. The extracted features are fed into the second stage module after adding position encodings and reshaping. For the second stage, we utilize one Transformer encoder layer and two Transformer decoder layers for label feature updating. After the last Transformer decoder, we add a linear projection layer to compute logit predictions for all categories.</p><p>Note that the Transformer encoder is mainly used to further help fuse global context for better feature representation, but it can be removed for more efficient computation. In our experiments, our model works well even with only one Transformer decoder layer. See more ablations in Sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.</head><p>We leverage the ImageNet <ref type="bibr" target="#b13">[14]</ref> pre-trained model as our backbone, and update the whole model on the target multilabel classification data set. We trained the model for 80 epochs using the Adam <ref type="bibr" target="#b28">[29]</ref> optimizer, with True-weightdecay <ref type="bibr" target="#b37">[38]</ref> of 1e ? 2, (? 1 , ? 2 ) = (0.9, 0.9999), and a learning rate of 1 ? 10 ?4 . More implementation and training details are available in the supplementary materials. <ref type="bibr" target="#b34">[35]</ref> is a large-scale data set constructed for object detection and segmentation firstly and has been widely used for evaluating multi-label image classification recently. It contains 122, 218 images and covers 80 common objects, with an average of 2.9 labels per image. Notice that the mAP scores for MS-COCO are highly influenced by the input resolution. Thus we divide the results into two groups: medium resolution (H, W ? 600) and high resolution(H, W &gt; 600) and report them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance on MS-COCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO</head><p>For the medium resolution, we adopt our standard setting and report the comparison between our method and other state-of-the-art methods in <ref type="table" target="#tab_2">Table 1</ref>. All the methods are evaluated in the resolution of 448 ? 448, except for SSGRL in 576 ? 576 and ADD-GCN in 512 ? 512. Our proposed method outperforms all the previous methods in terms of mAP, OF1, and CF1, which are the most important metrics, as other metrics can be affected by the chosen threshold largely. In particular, our Q2L respectively outperforms ADD-GCN by 2.0%, SSGRL by 3.4%, and ASL by 0.6%. That demonstrates the superiority of our approach.</p><p>For high resolution(640 ? 640) experiments, we adopt TResNetXL <ref type="bibr" target="#b39">[40]</ref> as the backbone and remove the selfattention module in Transformer decoders for better training and inference efficiency. The results are shown in <ref type="table" target="#tab_3">Table  2</ref>. Our method outperforms the best result in the literature and establishes a new state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance on PASCAL VOC</head><p>PASCAL VOC 2007 and 2012 <ref type="bibr" target="#b17">[18]</ref> are two frequently used data sets for multi-label classification. Each image in VOC contains one or multiple labels, corresponding to 20 object categories. In order to fairly compare with other methods, we follow the common setting to train our model on the train-val set and then evaluate its performance on the    <ref type="table" target="#tab_5">Table 3</ref>. We can see that our method achieves the best mAP performance among all methods. We also observe the small margin between our results and ADD-GCN <ref type="bibr" target="#b51">[52]</ref>. In addition to the difference in input image resolution (ours 448 ? 448 and ADD-GCN's 512 ? 512), the small increase may indicate the limited data of VOC 07 and its saturated metric. Nevertheless, there might be some other factors that influence the results, as we outperform previous works on VOC 12 with a larger margin as shown in <ref type="table" target="#tab_6">Table 4</ref>, whose results are reported by the evaluation server. We report results with ImageNet-1K pretrained backbone only in the main text for a fair comparison, and results with advanced backbones could be found in the appendix.</p><p>Results on VOC 12. VOC 2012 consists of 11, 540 images as the train-val set and 10, 991 images as the test set. Results on VOC 12 are shown in <ref type="table" target="#tab_6">Table 4</ref>. As all the results are reported by its evaluation server, it is a much fairer comparison than a local test. Our method outperforms all other methods on all metrics with a large margin. <ref type="bibr" target="#b10">[11]</ref> is a real-world web image data set. It contains 269, 648 Flickr images and has been manually annotated with 81 visual concepts. We follow the steps in <ref type="bibr" target="#b0">[1]</ref> for evaluation, and compare the proposed method with previous state-of-the-art models in <ref type="table" target="#tab_7">Table 5</ref>. As the resolutions of NUS-WIDE images are not high enough, we found the improvement of our method is not as significant as on MS-COCO and PASCAL VOC. But we still achieve a new state of the art on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Performance on NUS-WIDE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUS-WIDE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Performance on Visual Genome</head><p>Visual Genome <ref type="bibr" target="#b29">[30]</ref> is a data set that contains 108, 249 images and covers 80, 138 categories. As most categories contain very few samples, <ref type="bibr" target="#b6">[7]</ref> select images with the most fre-    quent 500 categories, and split the data set into train and test sets. They call the new data set VG500. We follow their setting and compare our model with prior methods in <ref type="table" target="#tab_8">Table 6</ref>. For a fair comparison, we set the resolution of input images to 512 ? 512, and evaluate our method using both the ResNet-101 <ref type="bibr" target="#b24">[25]</ref> and TResNetL <ref type="bibr" target="#b39">[40]</ref> backbones. Although the previous state-of-the-art model SSGRL <ref type="bibr" target="#b6">[7]</ref> use larger image resolution (576 ? 576) than ours (512 ? 512), our method outperforms all previous works and achieves a new SOTA on VG500. As the number of categories in VG500 is much larger than other data sets, it becomes more challenging for a simple spatially average-pooled feature to recognize all of the objects. Hence the advantages of our method are more obvious. The results indicate the importance and effectiveness of spatially adaptive feature attention in multilabel classification, particularly when the number of categories is large.</p><p>Method mAP ResNet-101 <ref type="bibr" target="#b24">[25]</ref> 30.9 ResNet-SRN <ref type="bibr" target="#b55">[56]</ref> 33.5 SSGRL(ResNet101) <ref type="bibr" target="#b6">[7]</ref> 36.6 C-Tran(ResNet101) <ref type="bibr">[33] 38.4</ref> Q2L-R101(Ours) 39.5 Q2L-TResL-22k(Ours) 42.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Results on objects of different sizes. To further test Q2L's performance on objects of different sizes, we split the MS-COCO <ref type="bibr" target="#b34">[35]</ref> test set into three subsets for small objects, medium objects, and large objects respectively. Following the common definition, objects occupying areas less than and equal to 32 ? 32 pixels are considered as "small objects", less than and equal to 96 ? 96 pixels are marked as "medium objects", and the others are "large objects". We compare our Q2L with the baseline TResNetL model. The results are listed in <ref type="table" target="#tab_10">Table 7</ref>. Our model outperforms baselines on all three categories, especially on medium objects. The larger improvement on medium objects demonstrates the superiority of the spatially adaptive feature pooling, which helps collect information that may be diluted by average pooling. For small objects, although our method has made a big step forward, it remains a challenging problem to be solved, requiring finer-grained details to be extracted from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>small medium large Baseline(TResNetL) <ref type="bibr" target="#b39">[40]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of Attention Maps</head><p>To further demonstrate the effectiveness of crossattention and adaptive pooling, we visualize some attention maps in <ref type="figure" target="#fig_3">Fig. 3</ref>. The attention map is analogous to the receptive field size in a raw image. We found that our model can locate the specified object approximately, especially on some small or medium objects. We also compare our Q2L model with baseline in <ref type="figure" target="#fig_4">Fig. 4</ref>. It validates the effectiveness of the spatial adaptive pooling built into Transformer decoders and shows great potential for providing better interpretability.</p><p>Beyond single attention maps, we are also interested in finding out the role of multi-head attention in this task. For a given target person, we plot individual attention maps in each head and the mean attention map in <ref type="figure" target="#fig_5">Fig. 5</ref>. We find that different heads are capable of focusing on different parts of targets. For person, head-1, head-3, and head-4 focus on the shoulder, neck, and head respectively. The attention maps of head-2 are less informative, as there is no clear focus, which may indicate that head-2 is not utilized as the other three heads already collect sufficient information for classification. Focusing on different parts makes the model more robust to occlusion and view changes, and provides better interpretability for the superiority of our model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a simple yet effective framework Query2Label (Q2L) for multi-label classification, which is developed based on Transformer decoders attached to an image classification backbone. The built-in cross-attention module in the Transformer decoder architecture offers an effective way to used label embeddings to query the existence of a class label and pool class-related features. The proposed framework consistently outperforms all prior works on several widely used data sets including MS-COCO, PASCAL VOC, NUSWIDE, and Visual Genome. We hope its simple model architecture and outstanding performance will serve as a strong baseline for future research on multi-label image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Implementation Details</head><p>We adopt the official PyTorch implementation for both the backbone and Transformer modules <ref type="bibr" target="#b44">[45]</ref>. Following DETR <ref type="bibr" target="#b2">[3]</ref>, we use 2D sine and cosine encodings to represent spatial positions. Each model was trained for 80 epochs using Adam <ref type="bibr" target="#b28">[29]</ref> and 1-cycle policy <ref type="bibr" target="#b42">[43]</ref>, with a maximal learning rate of 1e ? 4. For regularization, we use Cutout <ref type="bibr" target="#b15">[16]</ref> with a factor of 0.5 and True-wight-decay <ref type="bibr" target="#b37">[38]</ref> of 1e ? 2. Moreover, we normalize input images with mean [0, 0, 0] and std <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b0">1]</ref>, and use RandAugment <ref type="bibr" target="#b11">[12]</ref> for augmentation. Following common practices, we apply exponential moving average (EMA) to model parameters with a decay of 0.9997. To speed up, we use mixed precision during model training. The entire code to reproduce the experiments will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>Beyond the average precision (AP) and mean average precision (mAP), we report more metrics in the experiments, including the overall precision (OP), recall (OR), F1-measure (OF1) and per-category precision (CP), recall (CR), F1-measure (CF1). These metrics are computed as follows: </p><formula xml:id="formula_4">OP = i M i</formula><p>where M i c is the number of images predicted correctly for the i-th category, M i p is the number of images predicted for the i-th category, and M i g is the number of ground truth images for the i-th category. For each image, we assign it a positive label if its prediction probability is greater than a threshold or negative otherwise. Note that these results may be affected by the chosen threshold. The OF1 and CF1 are the primary metrics among them, as they consider both recall and precision and are more comprehensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results on VOC07</head><p>We show the results with ImageNet-1k pretrained backbones only in the maintext for a fair comparison on VOC 07. Additional results are listed in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Visualization Results</head><p>We provide more visualization results of cross-attention maps on MS-COCO <ref type="bibr" target="#b34">[35]</ref>. To visualize the cross-attention maps, we compute the attention values between labels and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Resolution mAP TResNetL 448?448 96.7 Q2L-TResL(Ours) 448?448 96.9 Q2L-CvT(Ours) 384?384 97.3 <ref type="table">Table 8</ref>: Comparison of our method with prior state-of-theart methods on VOC07. Backbones in all models are pretrained on ImageNet-22k dataset. All metrics are in %.</p><p>pixels. Since each matrix adds up to 1 and each value in the matrix is small, we divide the entire matrix by 0.06 and clip it between 0 and 1 to get a better visualization result. Then for a target label, we resize its corresponding attention value matrices (for multiple attention heads) to the same size as raw images and use the resized matrices as the opacity of each pixel in images. We plot the mean-head maps for more images in <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_7">Fig. 7</ref>. These figures provide an intuitive explanation for the effectiveness of our spatial adaptive pooling and the superiority of our method. In addition, we show more multi-head attention maps of the target person in <ref type="figure" target="#fig_8">Fig. 8</ref>. We find that different heads are capable of focusing on different parts of targets. <ref type="figure">Figure 6</ref>: More visualizations of cross-attention maps. We plot the mean of each head's cross-attention maps, which represent similarities of a given query and the extracted spatial features. Texts above images represent the ground truth labels (query) for the raw images. Best view in colors.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of Query2Label. Using cross attention for adaptively feature pooling through focusing on different parts (best view in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>illustrates the framework of the proposed Backbone (CNN/ViT/?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of cross-attention maps. We plot the mean of each head's cross-attention maps, that represent similarities of a given query and extracted spatial features. Texts above images represent the ground truth labels (query) for the raw images. Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Image examples classified correctly by Q2L but wrongly by the baseline TResNetL. The middle two columns are the mean attention maps of Q2L and the enlarged maps on focused regions respectively. The small scale of objects makes it difficult for TResNetL to recognize. Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of multi-head attention maps for the target label person. Each column in the middle represents an attention map for one head and the rightmost column averages the maps of all heads. Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 ?</head><label>2</label><figDesc>CP ? CR CP + CR ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>More visualizations of cross-attention maps (continued). Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>More visualizations of multi-head attention maps for the target label person. Each column in the middle represents an attention map for one head and the rightmost column averages the maps of all heads. Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>81.6 65.4 71.2 82.7 69.9 75.8 85.2 58.8 67.4 87.4 62.5 72.9 ResNet-101 [25] ResNet101 224?224 78.3 80.2 66.7 72.8 83.9 70.8 76.8 84.1 59.4 69.7 89.1 62.8 73.6 CADM [8] ResNet101 448?448 82.3 82.5 72.2 77.0 84.0 75.6 79.6 87.1 63.6 73.5 89.4 66.0 76.0 ML-GCN [9] ResNet101 448?448 83.0 85.1 72.0 78.0 85.8 75.4 80.3 87.2 64.6 74.2 89.1 66.7 76.3</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Resolution mAP</cell><cell>CP</cell><cell cols="6">All CR CF1 OP OR OF1 CP</cell><cell cols="5">Top 3 CR CF1 OP OR OF1</cell></row><row><cell cols="9">SRN [56] 224?224 77.1 KSSNet [36] ResNet101 ResNet101 448?448 83.7 84.6 73.2 77.2 87.8 76.2 81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-CMA [53]</cell><cell>ResNet101</cell><cell cols="13">448?448 83.8 82.9 74.4 78.4 84.4 77.9 81.0 86.7 64.9 74.3 90.9 67.2 77.2</cell></row><row><cell>MCAR [20]</cell><cell>ResNet101</cell><cell cols="13">448?448 83.8 85.0 72.1 78.0 88.0 73.9 80.3 88.1 65.5 75.1 91.0 66.3 76.7</cell></row><row><cell>SSGRL [7]</cell><cell>ResNet101</cell><cell cols="13">576?576 83.8 89.9 68.5 76.8 91.3 70.8 79.7 91.9 62.5 72.7 93.8 64.1 76.2</cell></row><row><cell>C-Trans [33]</cell><cell>ResNet101</cell><cell cols="13">576?576 85.1 86.3 74.3 79.9 87.7 76.5 81.7 90.1 65.7 76.0 92.1 71.4 77.6</cell></row><row><cell>ADD-GCN [52]</cell><cell>ResNet101</cell><cell cols="13">576?576 85.2 84.7 75.9 80.1 84.9 79.4 82.0 88.8 66.2 75.8 90.3 68.5 77.9</cell></row><row><cell>Q2L-R101(Ours)</cell><cell>ResNet101</cell><cell cols="13">448?448 84.9 84.8 74.5 79.3 86.6 76.9 81.5 78.0 69.1 73.3 80.7 70.8 75.4</cell></row><row><cell>Q2L-R101(Ours)</cell><cell>ResNet101</cell><cell cols="13">576?576 86.5 85.8 76.7 81.0 87.0 78.9 82.8 90.4 66.3 76.5 92.4 67.9 78.3</cell></row><row><cell>ASL [1]</cell><cell>TResNetL</cell><cell cols="13">448?448 86.6 87.2 76.4 81.4 88.2 79.2 81.8 91.8 63.4 75.1 92.9 66.4 77.4</cell></row><row><cell>TResNetL [39]</cell><cell cols="2">TResNetL(22k) 448?448 88.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Q2L-TResL(Ours)</cell><cell>TResNetL</cell><cell cols="13">448?448 87.3 87.6 76.5 81.6 88.4 78.5 83.1 91.9 66.2 77.0 93.5 67.6 78.5</cell></row><row><cell cols="15">Q2L-TResL(Ours) TResNetL(22k) 448?448 89.2 86.3 81.4 83.8 86.5 83.3 84.9 91.6 69.4 79.0 92.9 70.5 80.2</cell></row><row><cell>MlTr-l [10]</cell><cell>MLTr-l(22k)</cell><cell cols="7">384?384 88.5 86.0 81.4 83.3 86.5 83.4 84.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-L [37]</cell><cell>Swin-L(22k)</cell><cell cols="13">384?384 89.6 89.9 80.2 84.8 90.4 82.1 86.1 93.6 69.9 80.0 94.3 71.1 81.1</cell></row><row><cell>CvT-w24 [49]</cell><cell>CvT-w24(22k)</cell><cell cols="13">384?384 90.5 89.4 81.7 85.4 89.6 83.8 86.6 93.3 70.5 80.3 94.1 71.5 81.3</cell></row><row><cell>Q2L-SwinL(Ours)</cell><cell>Swin-L(22k)</cell><cell cols="13">384?384 90.5 89.4 81.7 85.4 89.8 83.2 86.4 93.9 70.4 80.5 94.8 71.0 81.2</cell></row><row><cell>Q2L-CvT(Ours)</cell><cell>CvT-w24(22k)</cell><cell cols="13">384?384 91.3 88.8 83.2 85.9 89.2 84.6 86.8 92.8 71.6 80.8 93.9 72.1 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our method with known state-of-the-art models on MS-COCO at medium input resolution. The backbones noted with 22k are pretrained on the ImageNet-22k dataset. Among them, mAP, OF1, and CF1 are the primary metrics (shaded in the table) as the others may be affected by the chosen threshold largely. All metrics are in %.</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell>Input Resolution</cell><cell>mAP</cell></row><row><cell>ASL [1]</cell><cell>TResNetXL</cell><cell>640?640</cell><cell>88.4</cell></row><row><cell cols="2">TResNet [39] TResNetL(22k)</cell><cell>640?640</cell><cell>89.8</cell></row><row><cell>Q2L-TResXL</cell><cell>TResNetXL</cell><cell>640?640</cell><cell>89.0</cell></row><row><cell>Q2L-TResL</cell><cell>TResNetL(22k)</cell><cell>640?640</cell><cell>90.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our method with ASL on MS-COCO for high input resolution of 640 ? 640. All metrics are in %. test set. Following previous works, we also pre-train the model on COCO for better performance. Results on VOC 07. VOC 2007 contains 5, 011 images as the train-val set, and 4, 952 images as the test set. Results on VOC 07 are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP CNN-RNN [46] 96.7 83.1 94.2 92.8 61.2 82.1 89.1 94.2 64.2 83.6 70.0 92.4 91.7 84.2 93.7 59.8 93.2 75.3 99.7 78.6 84.0 VGG+SVM [42] 98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3 93.1 97.2 70.0 92.1 80.3 98.1 87.0 89.7 Fev+Lv [51] 97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7 95.9 98.6 77.6 88.7 78.0 98.3 89.0 90.6 HCP [48] 98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1 94.9 96.3 78.3 94.7 76.2 97.9 91.5 90.9 RDAL [47] 98.6 97.4 96.3 96.2 75.2 92.4 96.5 97.1 76.5 92.0 87.7 96.8 97.5 93.8 98.5 81.6 93.7 82.8 98.6 89.3 91.99.9 98.4 98.9 98.7 86.8 98.2 98.7 98.5 83.1 98.3 89.5 98.8 99.2 98.6 99.3 89.5 99.4 86.8 99.6 95.2 95.8 ADD-GCN [52] (576) 99.8 99.0 98.4 99.0 86.7 98.1 98.5 98.3 85.8 98.3 88.9 98.8 99.0 97.4 99.2 88.3 98.7 90.7 99.5 97.0 96.0 Q2L-TResL(Ours) 99.9 98.9 99.0 98.4 87.7 98.6 98.8 99.1 84.5 98.3 89.2 99.2 99.2 99.2 99.3 90.2 98.8 88.3 99.5 95.5 96.1</figDesc><table><row><cell></cell><cell>9</cell></row><row><cell>RARL [6]</cell><cell>98.6 97.1 97.1 95.5 75.6 92.8 96.8 97.3 78.3 92.2 87.6 96.9 96.5 93.6 98.5 81.6 93.1 83.2 98.5 89.3 92.0</cell></row><row><cell cols="2">SSGRL [7] (576) 99.7 98.4 98.0 97.6 85.7 96.2 98.2 98.8 82.0 98.1 89.7 98.8 98.7 97.0 99.0 86.9 98.1 85.8 99.0 93.7 95.0</cell></row><row><cell>MCAR [19]</cell><cell>99.7 99.0 98.5 98.2 85.4 96.9 97.4 98.9 83.7 95.5 88.8 99.1 98.2 95.1 99.1 84.8 97.1 87.8 98.3 94.8 94.8</cell></row><row><cell>ASL(TResNetL) [1]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of our method with previous state-of-the-art methods on PASCAL VOC 2007, in terms of AP and mAP in %. All results are reported at resolution 448 ? 448 except for the ADD-GCN and SSGRL, whose resolutions are noted in parentheses. Results with advanced backbones could be found in the appendix.</figDesc><table><row><cell>Methods</cell><cell>aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</cell></row><row><cell cols="2">VGG+SVM [42] 99.0 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7 97.1 63.7 93.6 75.2 97.4 87.8 89.3</cell></row><row><cell>Fev+Lv [51]</cell><cell>98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3 97.5 73.1 91.2 75.4 97.0 88.2 89.4</cell></row><row><cell>HCP [48]</cell><cell>99.1 92.8 97.4 94.4 79.9 93.6 89.8 98.2 78.2 94.9 79.8 97.8 97.0 93.8 96.4 74.3 94.7 71.9 96.7 88.6 90.5</cell></row><row><cell>MCAR [19]</cell><cell>99.6 97.1 98.3 96.6 87.0 95.5 94.4 98.8 87.0 96.9 85.0 98.7 98.3 97.3 99.0 83.8 96.8 83.7 98.3 93.5 94.3</cell></row><row><cell cols="2">SSGRL [7](576) 99.7 96.1 97.7 96.5 86.9 95.8 95.0 98.9 88.3 97.6 87.4 99.1 99.2 97.3 99.0 84.8 98.3 85.8 99.2 94.1 94.8</cell></row><row><cell cols="2">ADD-GCN [52](576) 99.8 97.1 98.6 96.8 89.4 97.1 96.5 99.3 89.0 97.7 87.5 99.2 99.1 97.7 99.1 86.3 98.8 87.0 99.3 95.4 95.5</cell></row><row><cell cols="2">Q2L-TResL(Ours) 99.9 98.2 99.3 98.1 90.4 97.7 97.4 99.4 92.7 98.7 89.9 99.4 99.5 99.0 99.4 88.4 98.8 89.3 99.6 96.8 96.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of AP and mAP in % of our model and state-of-the-art methods on PASCAL VOC 2012. All results are reported at resolution 448?448 except for the ADD-GCN and SSGRL, whose resolution is noted in parentheses. Different from VOC 07, results in VOC 12 are reported by the evaluation server.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP CF1 OF1</cell></row><row><cell>MS-CMA [53]</cell><cell>ResNet101</cell><cell>61.4 60.5 73.8</cell></row><row><cell>SRN [56]</cell><cell>ResNet101</cell><cell>62.0 58.5 73.4</cell></row><row><cell>ICME [9]</cell><cell>ResNet101</cell><cell>62.8 60.7 74.1</cell></row><row><cell>Q2L-R101(Ours)</cell><cell>ResNet101</cell><cell>65.0 63.1 75.0</cell></row><row><cell>Baseline [40]</cell><cell>TresNetL</cell><cell>63.1 61.7 74.6</cell></row><row><cell>Focal loss [34]</cell><cell>TresNetL</cell><cell>64.0 62.9 74.7</cell></row><row><cell>ASL [1]</cell><cell>TresNetL</cell><cell>65.2 63.6 75.0</cell></row><row><cell>Q2L-TResL(Ours)</cell><cell>TresNetL</cell><cell>66.3 64.0 75.0</cell></row><row><cell>MlTr-l [10]</cell><cell>MlTr-l(22k)</cell><cell>66.3 65.0 75.8</cell></row><row><cell>Q2L-CvT(Ours)</cell><cell cols="2">CvT-w24(22k) 70.1 67.6 76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our methods to known state-of-theart models on NUS-WIDE. The backbones noted with 22k are pretrained on the ImageNet-22k dataset. All metrics are in %.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of our method with prior state-of-theart methods on VG500. All metrics are in %. All results are reported at the input resolution of 576 ? 576.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison of improvement on objects with different sizes.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent attentional reinforcement learning for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with joint class-aware map disentangling and label correlation embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-Label Image Recognition with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mltr: Multi-label classification with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to discover multiclass attentional regions for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<title level="m">Multi-label image recognition with multi-class attentional regions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Chest x-rays classification: A multi-label and fine-grained problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahil</forename><surname>Garnavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajib</forename><surname>Chakravorty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09688</idno>
		<title level="m">Pct: Point cloud transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Beyond self-attention: External attention using two linear layers for visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gpt2: Empirical slant delay model for radio space geodetic techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lagler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schindelegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kr?sn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1069" to="1073" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14027</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 : International Conference on Learning Representations 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="162" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Enhanced language representation with informative entities</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
