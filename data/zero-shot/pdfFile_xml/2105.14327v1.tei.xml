<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 1 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have been widely applied to hyperspectral image (HSI) classification and have achieved great success. However, the deep neural network model has a large parameter space and requires a large number of labeled data. Deep learning methods for HSI classification usually follow a patchwise learning framework. Recently, a fast patch-free global learning (FPGA) architecture was proposed for HSI classification according to global spatial context information. However, FPGA has difficulty extracting the most discriminative features when the sample data is imbalanced. In this paper, a spectral-spatial dependent global learning (SSDGL) framework based on global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed for insufficient and imbalanced HSI classification. In SSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted softmax loss are proposed to address the imbalanced sample problem. To effectively distinguish similar spectral characteristics of land cover types, the GCL module is introduced to extract the long short-term dependency of spectral features. To learn the most discriminative feature representations, the GJAM module is proposed to extract attention areas. The experimental results obtained with three public HSI datasets show that the SSDGL has powerful performance in insufficient and imbalanced sample problems and is superior to other state-of-the-art methods. Code can be obtained at: https://github.com/dengweihuan/SSDGL. Index Terms-deep learning, patchwise, hyperspectral image classification, imbalanced sample, feature representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ith the development of remote sensing techniques, numerous hyperspectral images (HSIs) can be obtained with abundant spectral information. HSIs are composed of narrow and contiguous spectral bands in the electromagnetic spectrum <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Because of the richness of spectral information, it has a wide range of applications in <ref type="bibr">1 2</ref>  various fields, such as land-cover detection, agricultural development, environmental protection and urban planning <ref type="bibr" target="#b3">[4]</ref>. Hyperspectral image classification, which aims to assign a unique label to each pixel, plays an essential role in the interpretation of hyperspectral remote sensing images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, due to the high dimensionality of HSI and limited labeled data, data redundancy and the Hughes phenomenon often arise and pose a major challenge for HSI classification. To further improve the HSI classification performance, many studies have been undertaken over the years <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Traditional hyperspectral image classification usually carries out feature extraction first and then classifies HSI by various classifiers, such as multinomial logistic regression (MLR), maximum likelihood classification (MLC), and support vector machine (SVM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Numerous studies have shown that introducing spatial information into the classification process can effectively improve the performance of HSI classification. Spectral-spatial based methods, such as simple linear iterative clustering (SLIC), extended morphological profiles (EMP) and the Gabor filter, have been proposed to extract both the spectral and spatial features of HSIs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. However, these spectral-spatial features and hyperparameters are selected based on prior information, and the classification performance is limited by the number of training samples.</p><p>With the development of deep learning, convolutional neural network (CNN)-based methods have attracted great attention for hyperspectral image classification <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. As a data-driven automatic feature learning framework, it can achieve end-to-end training and automatically extract the spectral-spatial features of the images. Many supervised classification methods based on deep learning are used to extract the spectral and spatial features, such as 3D-CNN, multiscale convolutional neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Recurrent neural networks (RNNs) and convolutional recurrent neural networks (CRNNs), which can learn the long short-term spectral dependencies, are widely used in hyperspectral image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. In addition, residual networks, capsule networks, double-branch networks, and other novel networks have been widely applied in HSI classification and have achieved great classification accuracy with sufficient labeled samples <ref type="bibr" target="#b20">[21]</ref>. However, these methods only consider the labeled samples and ignore the spectral-spatial information of unlabeled samples. To make full use of the unlabeled data, many semisupervised classification methods have been proposed. <ref type="bibr">It</ref>  W homogeneity, which means the adjacent pixels are likely the same class <ref type="bibr" target="#b21">[22]</ref>. Therefore, the superpixel-based methods can effectively extract the deep spatial information for HSI classification. Moreover, the spatial size and shape of the superpixels are adaptive <ref type="bibr" target="#b23">[23]</ref>. To further solve the problem of the small samples, the generative adversarial network was proposed to generate pseudolabeled samples and make the distribution of the fake samples closer to real data distribution <ref type="bibr" target="#b24">[24]</ref>- <ref type="bibr" target="#b26">[26]</ref>. However, most existing methods are patch-based learning frameworks, and the neighbor region of each pixel in HSIs is considered as the input data of the network. The high computational complexity is unavoidable since there are large overlapped areas among adjacent patches. To solve the above problems, a fast patch-free global learning (FPGA) framework was proposed to maximize the exploitation of the global spatial information according to the long-range spatial dependency <ref type="bibr" target="#b27">[27]</ref>. FPGA has achieved great classification accuracy on public datasets and reduced the redundant calculations. However, the number of training samples per class is the same, and there are completely different labeled samples in each hierarchical training dataset, which is not suitable for datasets with insufficient and imbalanced samples, such as the Indian Pines dataset. Moreover, the categories with few samples have a small amount of weight in the loss calculation, and it is difficult for FPGA to classify these categories well. Since some land-cover types are difficult to distinguish by visual interpretation, the long-tail distribution issue of hyperspectral image datasets has arisen and seriously limits the classification performance.</p><p>In this paper, to extract the deep spectral-spatial features and solve the sample problem of insufficiency and imbalance, a spectral-spatial dependent global learning (SSDGL) framework combining global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed. Compared with the FPGA sampling strategy, a hierarchically balanced (H-B) sampling strategy is proposed to obtain enough hierarchical data and balance minibatch per class. The weighted softmax with cross entropy loss is used to give each class an equal probability of being selected. The novel sampling strategy and loss strategy can effectively solve the class imbalance problem. The baseline of the proposed framework is an encoder-decoder architecture (SegNet) <ref type="bibr" target="#b28">[28]</ref>, which has achieved good classification performance in image segmentation. Furthermore, the GCL module is introduced to extract the long short-term spectral dependent features and obtain the interrelation among the local pixels. The GJAM module is utilized to extract more discriminative feature representations.</p><p>The main contributions of this paper are as follows. 1) A spectral-spatial dependent global learning (SSDGL) framework is proposed for HSI classification. To solve the insufficient and imbalanced sample problems, a hierarchically balanced sampling strategy is utilized to generate stochastic hierarchical training sample data. The proposed sampling strategy reduces the overall training times and speeds up model convergence. The weighted softmax with cross entropy loss is introduced to reduce the weight of easy-to-classify samples so that the model focuses more on hard-to-classify samples during training. All pixels are used for the convolution operation at the same time, which solves the problem of the limited patch size.</p><p>2) To extract the detailed spectral-spatial information of the whole image, GCL is proposed to capture the long short-term spectral dependent features and leverage convolutional kernel to extract interrelations among the local pixels. GCL is a sequence-to-sequence learning method, and the gated recurrent units are utilized to extract deep spectral and spatial features. This module can effectively distinguish similar land covers by extracting the intrinsic spectral-spatial dependency.</p><p>3) To further extract the most discriminative feature representation, a global joint attention mechanism is designed to reweight and model the extracted features. This module is composed of a spectral attention mechanism and a spatial attention mechanism. The spectral attention mechanism can selectively emphasize informative spectral features and suppress less-useful ones. The spatial attention mechanism is introduced to extract the short-term spatially dependent features and emphasize the key regions.</p><p>The rest of this paper is organized as follows. Section II discusses the related work. Section III provides a detailed description of the SSDGL framework for insufficient and imbalanced HSI classification. A description of the datasets and an analysis of the experimental results are presented in Section IV. The sensitive parameters are discussed in Section V. Finally, the conclusions are drawn in Section ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In recent years, deep learning techniques have achieved great success in the field of remote sensing. The CNN-based classification methods and fully convolutional network (FCN)-based classification methods have been applied to HSI classification successfully. Moreover, insufficient and imbalanced sample problems have become a research hot spot in image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CNN-based classification</head><p>CNN-based classifications are regarded as common feature learning methods, which have a significant advantage in accuracy, and classifications are performed in an end-to-end manner <ref type="bibr" target="#b29">[29]</ref>. To facilitate feature extraction and train the classifiers, HSI pixel patches are first generated from the original image by a sliding window with a fixed size. The spectral-spatial residual network (SSRN) and double branch multi-attention mechanism (DBMA) network were proposed to extract the deep spectral and spatial features. As the layer goes deeper, the features in the model become more abstract and more robust. The extracted spectral-spatial features were flattened into vectors and fused for classification <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. <ref type="bibr">Mou et al. proposed</ref> RNNs extracting the spectral dependency among adjacent wavebands and regarded the HSI as sequential data <ref type="bibr" target="#b15">[16]</ref>. Subsequently, CRNN was proposed to highlight long-term dependency between nonadjacent channel features. To consider both spatial and spectral information, the ConvLSTM was proposed to extract dependent features of the spectrums and geometric <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>Although these patchwise methods have achieved significant classification accuracies, redundant computation on the overlapping areas between adjacent patches is inevitable. The main bottleneck is that the traditional convolutional neural networks first divide HSI into patches and classify each patch into one corresponding label rather than directly classifying the whole image <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FCN-based classification</head><p>Fully convolutional neural networks were first proposed for the task of image segmentation and achieved great success. The fully connected layers were replaced by convolutional layers with a kernel size of 1 ? 1. U-Net is the most representative fully convolutional network. It consists of a contracting path to capture context and a symmetric expanding part to precise localization <ref type="bibr" target="#b36">[36]</ref>. SegNet is a network based on FCN and consists of an encoder network, a corresponding decoder network followed by a pixelwise classification layer. The feature pyramid network (FPN), which improved on U-Net, was proposed to capture multiscale information. It composed of a bottom-up pathway, a top-down pathway and lateral connections <ref type="bibr" target="#b37">[37]</ref>. Inspired by the FPN model, the DeepLab networks combining atrous convolution, spatial pyramid pooling, and fully connected CRFs have been proposed and used to extract feature representations with different scales <ref type="bibr" target="#b38">[38]</ref>.</p><p>The purpose of hyperspectral image classification is the same as semantic segmentation tasks, which is to assign a unique label to each pixel. However, it is not feasible to directly transfer the semantic segmentation networks to HSI classification tasks, since the training sample of HSI datasets is highly sparse and only contains a set of discrete labeled pixels rather than a group of labeled images <ref type="bibr" target="#b39">[39]</ref>. Xu et al. <ref type="bibr" target="#b35">[35]</ref> proposed a spectral-spatial fully convolutional network (SSFCN) to perform feature extraction and semantic segmentation in an end-to-end manner. A novel mask matrix was proposed to deal with the high sparse training samples in HSIs. To solve the problem of model convergence and mine the global spatial context information, Zheng et al. <ref type="bibr" target="#b27">[27]</ref> proposed a fast patch-free global learning framework (FPGA) for HSI classification. FPGA is a deep convolutional encoder-decoder architecture, the global stochastic stratified (GS 2 ) sampling strategy was introduced to obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. The FreeNet model in FPGA was proposed to avoid redundant computation on the overlapping areas between patches. The comparison of the patch-based and patch-free methods is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The green area is the overlapping area. H and W are the spatial sizes of the input data, K is the convolutional kernel size, and S is the pixel patch size. It can be seen that the patchwise classification methods need to generate a patch for each pixel, which will generate redundant calculations in the model inference. However, the input data of global learning methods is the whole image, and there are no pixel patches or center pixels, which speeds up the model inference. Since the convolution operation considers all pixels of the whole image at the same time, the classification performance has been greatly improved <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Insufficient and imbalanced sample problems</head><p>It is well known that the problem of limited training samples is one of the major obstacles that affect the accuracy of HSI classification. Since transfer learning methods using the pretrained deep learning network from the relevant domain to aid learning, few samples are required to fine-tune the model <ref type="bibr" target="#b41">[41]</ref>. Yang et al. pretrained a two-channel CNN network on source HSI, which contains sufficient labeled samples. Then the bottom layers of the pretrained network were transferred to the target network as initialization, and the top layers were randomly initialized <ref type="bibr" target="#b42">[42]</ref>. Pan et al. <ref type="bibr" target="#b8">[9]</ref> proposed a small-scale data-based method, the multi-grained network (MugNet), which can obtain the fined spectral and spatial features. It used all the unlabeled samples to learn convolution kernels and build a lightweight network that does not include many hyperparameters for tuning. Fang et al <ref type="bibr" target="#b5">[6]</ref> designed a lightweight 3D convolutional neural network and a novel clustering strategy for learning the deep discriminative feature and perform semi-supervised classification. Mei et al. <ref type="bibr" target="#b43">[43]</ref> proposed a 3 dimensional (3D) convolutional autoencoder (3D-CAE) to maximally obtain the most discriminative spectral and spatial information for feature extraction. This framework can learn the deep features in an unsupervised mode and network trained without labeled training samples.</p><p>Moreover, the problem of long-tail distribution poses great challenges for deep learning, and it has attracted increasing attention in computer vision. Existing solutions usually have a difference in sampling strategy and classifiers <ref type="bibr" target="#b44">[44]</ref>. For most sampling strategies presented below, the probability j p of sampling a data point from class j is given by:</p><formula xml:id="formula_0">1 q j j C q i i n p n ? ? ? (1) where ? ? 0,1 q ?</formula><p>and C is the number of training classes.</p><p>The instance-balanced sampling, class-balanced sampling, and square-root sampling all achieved good results for long-tailed recognition, where q is set to 1, 0, and 1/2 respectively. Most of the methods trained the classifiers to rectify the decision boundaries on the head-and tail-classes via fine-tuning, and optimize parameters by loss reweighting strategies <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref>. Kang et al. <ref type="bibr" target="#b47">[47]</ref> proposed a decoupled representation learning and classification strategy to compare the performance differences of different sampling strategies and classifiers for classification.</p><p>Several important questions need to be considered. How can the global learning framework consider the relationship between long short-range bands? In addition, long-tail distribution exists in the HSI datasets and limits the performance of HSI classification, so how can we address insufficient and imbalanced training data problems? To overcome the aforementioned issues, we propose the SSDGL framework for HSI classification using small training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SSDGL: SPECTRAL-SPATIAL DEPENDENT GLOBAL LEARNING FRAMEWORK</head><p>To extract the spectral relationship among different bands and the spatial correlation of all pixels, the SSDGL framework is proposed for hyperspectral image classification. This is an ensemble learning method that combines spectral, structural, and semantic features. The most discriminative feature representations are learned by the global convolutional long short-term memory integrated with the global joint attention mechanism (GCLAM). The hierarchically balanced sampling strategy is proposed to divide the training data into a hierarchical sequence of training samples, and the weighted softmax with cross entropy loss is introduced to reweight each class probability. The novel sampling strategy and loss function effectively solve sample imbalanced and insufficient problems. The skip connections are utilized to fuse the spatial features from the encoder and the semantic features in the same stage decoder. The overall architecture of the SSDGL is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchically balanced sampling strategy</head><p>The hierarchically balanced (H-B) sampling strategy was proposed to obtain diverse stochastic gradients, combined with weight decay and learning rate decay to speed up model convergence. In this work, the training sample is the whole image rather than the patches of the local area, and the set of discretely labeled pixels is assigned to a hierarchical sequence to improve the robustness of the model and reduce the training time of SSDGL-Net. Because the labeled pixels of the HSI dataset are insufficient and imbalanced, the labeled samples are divided in a certain ratio and the indices of each category are stored in different lists. The selected sample data are viewed as training data and other sample data as test data. The input data of the SSDGL-Net is a stochastic hierarchical training sample sequence, and the hierarchical training data balanced the number of samples per class. The hyperparameter ? represents the number of hierarchical training data, and the number of labeled samples in hierarchical training dataset will affect the speed of model convergence. Each hierarchical training dataset contains all categories, and the mini batch per class is determined by the parameter ?. Within a certain range, the smaller the value of ? is, the more random gradients can be obtained and the less training time is required. To address the imbalanced sample problem, the weighted softmax loss is introduced to balance the probability of the ground-truth class and focus more on misclassified samples. The category</p><formula xml:id="formula_1">weighting factor ? ? 0 / M ji i q q M ? ? ?</formula><p>is added to the standard cross-entropy criterion to reduce the relative loss for well-classified samples. j p is used to balance the probability that selecting a sample data belong to the class j .</p><p>The number of labeled samples per class in hyperspectral images is different because it is difficult to identify many land-covers by visual interpretation. If using the traditional sampling strategy, the average accuracy and the overall accuracy are greatly limited. Therefore, it is necessary to introduce a novel sampling strategy and a suitable loss function to solve the problem of class long-tail distribution in hyperspectral image datasets. The pseudocode of SSDGL is shown in Algorithm 1.  </p><formula xml:id="formula_2">? ? , ED ? ? ? ? , classification map 1 W H O ?? ? ? , R i j ? [[]] an empty matrix ij I ? {} an empty dict w ? [] an empty list 1-i n i m ? ? , ? ? 1- ii qm ? ? / for j = 0 to M do 0 j j M i i q pM q ? ?? ? ? ? . j w push p end for s = 0 to? do for k = 0 to M do ? ? ? ? ij k I shuffle n ? ?? while ? ? ij len I ? &lt; do fetch all samples from ij I , ? ? , ij R i j I ? ? ? ? ? ,0 0, ,0 1, ij if R i j Mask if R i j ? ? ? ? ? &gt; continue fetch ? samples from ij I , ? ? ? ? ,. ij R i j I pop ? ? ? ? ? ? ,0 0, ,0 1, ij if R i j Mask if R i j ? ? ? ? ? &gt; end</formula><formula xml:id="formula_3">1 w EE E L n ?? ? ? ? ? ? ?? ? ? ? ?? ? ?? 1 w DD D L n ?? ? ? ? ? ? ?? ? ? ? ?? ? ?? end while ? ? 0 , 0 1 W H ij O y i H j W ? ?? ? ? ? | &lt; &lt; B. GCLAM</formula><p>The GCLAM is composed of global convolutional long short-term memory (GCL) and global joint attention mechanisms (GJAM). Since the fully connected LSTM cannot extract local spatial information, it is replaced by ConvLSTM. However, the global spatial context information is ignored with the conventional ConvLSTM, and the importance of extracted features is difficult to emphasize. Hence, GCLAM is proposed to extract the dependency of spectral-spatial features. GCLAM is the most important part of the SSDGL framework, which can extract abundant spectral-spatial features and keep the spatial size of input data unchanged, as shown below. a) The global convolutional long short-term memory (GCL) is utilized to extract the spectral dependency according to the global spatial context information. The flowchart is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, and the input data is the whole hyperspectral image. The channels of the input data are equally distributed to the n group, where n represents the time step of the GCL. In this work, GCL has two hidden layers. The first hidden layer is utilized to extract the interdependency between long-range features. For example, some different types of crops have similar spectral curves, but the correlation between their green band and the near-infrared band is quite different. Therefore, the spectral dependency of long-range bands can effectively distinguish similar land cover types. The second hidden layer is used to enhance the dependency of adjacent channels. GCL is composed of ConvLSTMCells, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. There are four main parts in ConvLSTMCells. a) The forgetting phase selectively forgets information passed from the previous unit and keeps important information. b) The memorizing phase selectively memorizes the input information. c) The output phase determines what information will be output. d) In the convolutional phase, gated units in ConvLSTMCells are equivalent to convolution layers, and it can nonlinearly transform the input features into more discriminative spectral-spatial features.</p><p>b) The global joint attention mechanism (GJAM) is composed of a global spectral attention mechanism and global spatial attention mechanism, and they are utilized to estimate the importance of the extracted features. The spectral attention mechanism is used to reweight the spectral features generated by the GCL module and focus on the most discriminative features. The global spatial attention mechanism is utilized to focus on the important local areas and take full advantage of the global spatial context information.  1) The global spectral attention mechanism can be regarded as a feature detector, which assigns different weights to each channel. The larger weight is assigned to the meaningful channels and the weight of the meaningless channels is smaller. The global spectral attention mechanism is shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>The input feature is the whole image rather than the patches of the local area. The different spectral features are obtained through the global maximum pooling layer and the global average pooling layer. They are represented as c avg F and c max F .</p><p>The input image is passed through the pooling layers, and two spectral vectors are generated. The output features are fed to a three-layer perceptron and two output feature vectors are generated. Then, the output feature vectors are merged using an elementwise summation operation. Finally, the output feature vector is multiplied with the input image. It can be expressed by the following formula:</p><formula xml:id="formula_4">1 x 1 a 0 m 0 ( ) ( ( ( )) ( ( ))) ( ( ( )) ( ( ))) c cc avg M MLP AvgPool MLP MaxPool WW F F F F W F W ? ? ?? ??<label>(2)</label></formula><p>2) In the HSI data cube, the adjacent pixels are likely to form an area and they belong to the same class. The spatial attention mechanism focuses on attention areas and reweights the generated attention areas, as shown in <ref type="figure">Fig. 7</ref>. The global spatial attention also used the maximum pooling layer and average pooling layer to extract different spatial information. The input features are fed to the pooling layers and two spatial feature maps are generated. Connect two feature maps through concatenation operation, and generate a one-dimensional feature map through the activation function and convolution operation. Multiply the one-dimensional feature map with the input feature maps channel by channel. The spatial features are reweighted and the spatial size is unchanged. It is expressed by the following formula:</p><formula xml:id="formula_5">s max ( ) ( ( ( ); ( ))) ( ( ; )) NN N N c c avg AvgPool MaxPo M o F F F F l f fF ? ? ? ? ? ?</formula><p>(3) <ref type="figure">Fig. 7</ref>. The flowchart of the global spatial attention mechanism.</p><p>Generally, the data with same features will gather together to form an area. To highlight these areas, the spatial attention mechanism is necessary. Since some land covers have large spatial variability, the global learning method is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS RESULTS AND ANALYSIS</head><p>To quantitatively and qualitatively analyze the classification performance of the proposed models, it was compared with some state-of-the-art methods for HSI classification, which include support vector machines with radial basis function kernel (RBF-SVM), semisupervised convolutional neural network (SS-CNN) <ref type="bibr" target="#b48">[48]</ref>, spectral-spatial residual network (SSRN) <ref type="bibr" target="#b30">[30]</ref>, double-branch multiattention mechanism network (DBMA) <ref type="bibr" target="#b31">[31]</ref>, MCNN-CONVLSTM <ref type="bibr" target="#b49">[49]</ref>, U-Net <ref type="bibr" target="#b36">[36]</ref> and FPGA <ref type="bibr" target="#b27">[27]</ref>. Extensive experiments were conducted on three datasets: the 16-class Indian Pines dataset, the 9-class Pavia University dataset, and the 15-class Houston University dataset. These datasets are utilized to validate the effectiveness of the proposed method in the cases of imbalanced sample data, high spatial resolution data, and a small number of sample data. All experiments were carried out based on the PyTorch library on a GeForce RTX 2080 Ti graphics card. A. Experimental Settings 1) Model parameters: This is a framework based on encoding and decoding. To make the size of the input image meet the downsampling requirements, the input image was increased to a multiple of 16 and padded with zero. The group number of group normalization was set to 4, so the output channels of each layer must be a multiple of 4. Since this framework employs skip connections between the spatial features in the encoder and semantic features in the decoder, the channel of skip connections was set to 128 for feature fusion.</p><p>2) Optimized parameters: The time step of the global convolutional long short-term memory was set to 8, and the size of the convolutional kernel was set to 5. The optimizer plays an important role in the training processes of the deep CNN model and affected the model convergence <ref type="bibr" target="#b50">[50]</ref>. The proposed framework was trained in 600 epochs training processes for each dataset and using gradient descent with momentum, where the initial learning rate was set to 0.005 and multiplied by 1 _ power iter max iter ?? ? ?? ?? with power = 0.8</p><p>and max_iter = 1,000. The momentum was set to 0.9 and the weight decay rate was set to 0.001. 3) Metrics: To evaluate the performance of the proposed methods, four commonly used quantitative metrics were adopted: the accuracy of each class, the overall accuracy (OA), the average accuracy (AA), and the kappa coefficient (Kappa). To eliminate the deviation introduced by randomly choosing training samples, each experiment was run ten times, and the mean values of each evaluation criterion are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1: Indian Pines Dataset</head><p>The Indian Pines dataset was acquired in 1992 by the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) sensor in Northwestern Indiana. This dataset contains 145?145 pixels, with 220 spectral bands in the wavelength range from 0.4 to 2.5 ?m and is mainly composed of multiple agricultural fields. The spatial resolution is approximately 20 meters per pixel. Since removing bands covering the region of water absorption, 200 bands of the data were retained. After removing the background pixels, 10,249 pixels were reserved, which contain 16 classes representing the different land-cover types. <ref type="figure">Fig. 8</ref> shows the false-color composite of the image and the corresponding ground truth.  <ref type="table" target="#tab_3">Table I</ref> lists the number of training and testing data per class. The training samples were set to 5% of all labeled samples. If the training data of the class was less than 5, the mini-batch per class was set to 5. The training data were obtained by H-B sampling strategy and the remaining data were viewed as test data to evaluate the accuracy.  <ref type="figure">Fig. 9</ref> (a)-(g) illustrates the classification results using RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-Net and FPGA. It can be seen that the classification methods based on CNN had a better visual performance than SVM, and the image is smoother than SVM. This is because the convolutional neural network-based method considered the spatial features of adjacent pixels. The FPGA and SSDGL obtained the complete structure of land covers, and the category boundaries are closer to real images. This is because the FCN-based method makes full use of the global spatial context to extract the most discriminating spatial features. Compared with U-Net, the classification maps of FPGA and SSDGL show that these methods have better classification performance in the categories with similar spectral features, such as corn and soybean, and these methods have better generalization ability. GCL module played an important role in representation learning, and the correlation of the adjacent channels and long-range channels was simultaneously considered. It can be seen from <ref type="figure" target="#fig_0">Fig. 10 (a)</ref> that the large intraclass variations and the small interclass dissimilarity existed in the Indian Pines dataset. The proposed framework can obtain the most discriminative feature representations to reduce the intraclass distance and increase the interclass distance, as shown in <ref type="figure" target="#fig_0">Fig. 10 (b)</ref>.</p><p>For a more detailed verification of the results, the overall accuracies (OA), average accuracy (AA), kappa coefficients, and per-class accuracies are presented in <ref type="table" target="#tab_3">Table II</ref> for all classification methods (RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-Net, and FPGA). The best accuracy is highlighted in bold for each row in the <ref type="table" target="#tab_3">table.  TABLE II  THE CLASSIFICATION RESULTS OF RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-NET, FPGA AND SSDGL ON THE INDIAN</ref> PINES DATASET WITH 5% LABELED SAMPLES. <ref type="figure" target="#fig_0">Fig. 10</ref>. Two-dimensional t-SNE visualization of features from the Indian Pines, Pavia University and Houston University datasets. Data distributions of the labeled samples in the original feature space (the first row) and the convolutional feature space (the second row). Different colors correspond to different classes.</p><p>As shown in <ref type="table" target="#tab_3">Table II</ref>, FCN-based methods obtained better class accuracy, and the overall accuracy was above 90%. It is attributed to the global learning framework, which makes full use of the global spatial context information. Since agricultural land cover types have a large spectral difference between the green band and the near-infrared band, the proposed method can highlight the correlation between these bands. The classification accuracy on soybean using SSDGL achieves a 3%?7% improvement over FPGA and achieved a 6%?10% improvement on corn. Moreover, the hierarchically balanced sampling strategy played a key role in the class imbalance problem. The weighted softmax loss is employed to reduce the weight of easy-to-classify samples so that the model focused more on hard-to-classify samples during training. It can be seen in <ref type="table" target="#tab_3">Table I</ref> that some number of training sample categories were less than 10, and some were more than 50. The categories with a small number of training samples were hard to classify, so the classification accuracy on corn, grass, and soybean was worse than other categories. The category weighting factor was added to the cross-entropy criterion function to balance the relative loss of well-toclassify samples and hard-to-classify samples. Hence, the proposed method achieved great classification performance on datasets with insufficient and imbalanced samples. It can be seen that the FCN-based method had higher accuracy on OA, AA, and kappa coefficient. Compared with FPGA, SSDGL achieved ~3% improvement in OA, AA and kappa coefficient. The novel sampling strategy and loss function can effectively solve the long-tail distribution problem of hyperspectral image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 2: Pavia University Dataset</head><p>The Pavia University dataset was acquired by the Reflective Optics System Imaging Spectrometer (ROSIS) sensor over the University of Pavia in 2001. Since 12 bands covering the region of noise and water absorption were removed, 103 bands of the data were retained, and the spectral range was from 0.43 to 0.86 ?m. This dataset has 610?340 pixels with a resolution of 1.3 meters per pixel. After removing the background pixels, 42,776 pixels were reserved, which contained nine classes representing the different land-cover types. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the false-color composite of the image and the corresponding ground truth.  <ref type="figure" target="#fig_0">. 12</ref> (a)-(g) illustrates the classification results using RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-Net and FPGA. It can be seen that the classification maps of RBF-SVM contain salt-pepper noise because this method only considers the spectral information and ignores the spatial correlation of the adjacent pixels. Therefore, the most discriminative features are difficult to extract, and the classification performance is limited. Due to the different materials of the roof, it is difficult to discriminate the building classes based on spatial context information, so the spectral features need to be emphasized. It can be seen from the classification maps that the building category labels had great differences with different methods, but the proposed method can obtain the complete shape of buildings and accurately discriminated the building materials. The GCL module was utilized to extract the interdependency between the channels according to the spectral information of the whole hyperspectral image. Because the global learning method can model the long-range dependency, the complete structure of the road can be obtained. The sample distribution of Pavia dataset is shown in <ref type="figure" target="#fig_0">Fig. 10 (c)</ref>. There are many labeled samples in this dataset, but similar land cover types have difficulty distinguishing in the original HSI. The most discriminative features can be learned by the SSDGL, and the category boundary was accurately determined with generated feature maps. To evaluate the performance of these methods on this dataset from a quantitative perspective, the overall accuracies (OA), per-class accuracies (AA), and kappa coefficients are presented in <ref type="table" target="#tab_7">Table ?</ref> for RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-Net, and FPGA. The best accuracy is highlighted in bold for each row in the table. As shown in <ref type="table" target="#tab_7">Table ?</ref>, the spatial resolution of this dataset is very high, so the spatial information is important for HSI to discriminate hard-to-classify categories. The classification accuracy varied greatly on gravel with different methods, but the SSDGL framework achieved the best classification accuracy and was ~10% than other methods. This is because the GCL module was introduced to SSDGL to extract the interdependency of channels according to continuous spectral sequence and global spatial context information. The classification accuracy of the FCN-based method was ~ 3% higher than that of the CNN-based method on bare soil and ~10% higher on bitumen. It can be concluded that the high spatial resolution remote sensing image facilitates spatial feature extraction and boosts the classification performance. The classification accuracy of brick reached 99.92% with the SSDGL framework and ~1.5% higher than that of the FPGA. This was attributed to the global joint attention mechanism (GJAM), which extracted fine-grained spatial features and attention areas. It can be observed that SSDGL had higher accuracy in OA, AA, and kappa coefficient. The proposed method was ~3% higher than that of the CNN-based methods and ~1% higher than that of the FPGA. Although the number of training samples was limited, the classification accuracy per class of the SSDGL reached 99% because the proposed method has strong feature learning ability, and it is good at classifying class imbalanced datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3: Houston University Dataset</head><p>The Houston University dataset is a public HSI dataset that was released in the 2013 IEEE GRSS Data Fusion Contest. The Houston University dataset is a more challenging hyperspectral dataset that was captured by the National Center for Airborne Laser Mapping (NCALM) over the Houston University campus and contains 15 complex land-cover classes with 349?1,905 pixels and 144 bands ranging from 0.36 to 1.05 ?m. To further verify the validity of the proposed framework with limited training samples, the ten training samples per class was used to evaluate the performance of the proposed method. <ref type="figure" target="#fig_0">Fig. 13</ref> shows the false-color composite of the image and the corresponding ground truth. <ref type="table" target="#tab_4">Table.</ref> ? lists the number of training and test data for each category. Since the number of training samples per class is ten, the strong performance of the proposed model can be presented in this case.</p><p>The classification performance of SSDGL is compared with seven state-of-the-art methods, which are summarized as follows: SVM-3DG <ref type="bibr" target="#b51">[51]</ref>, SS-CNN, SSRN, MugNet <ref type="bibr" target="#b8">[9]</ref>, AROC-DPNet <ref type="bibr" target="#b5">[6]</ref>, U-Net, and FPGA. SVM-3DG is a SVM-based method with 3D discrete wavelet transform and Markov random field. MugNet is a state-of-the-art deep learning method for small sample HSI classification. AROC-DPNet is a lightweight convolutional neural network with a deep clustering strategy. The weight decay rate was set to 0.001 and trained in 1,000 epochs. It can be seen in <ref type="table">Table ?</ref> that the SSDGL framework achieve best classification performance than other popular methods. The overall accuracy was 10 ~ 20% higher than that of the CNN-based methods. Hence, the global learning framework had a remarkable breakthrough in the small sample classification of hyperspectral images. Compared with U-Net and FPGA, the proposed framework achieved a better classification accuracy on OA, AA, and Kappa. It can be concluded that an appropriate sampling strategy and loss function is necessary to address the insufficient sample problem. It can be seen from <ref type="figure" target="#fig_0">Fig. 10 (e)</ref> that the samples exist a small difference and some categories are difficult to distinguish in the original feature space. However, the proposed method has a strong feature learning ability and increase the gap between the different categories in the convolutional feature space. These categories can be easily distinguished on the generated spectral-spatial features. The confusion matrix is shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> It can be seen that the classification accuracy of each category was higher than 90%, except for the commercial class. The commercial class was easily misclassified as residential, road and parking lot because these categories have similar spectral-spatial features, and the fine-grained spatial difference struggled to extract with a small number of training samples. The SSDGL achieved great classification accuracy for some land-cover categories, so we did not use data augmentation to further improve the classification performance.  To better understand the effectiveness of each component in the spectral-spatial dependent global learning (SSDGL) framework, we conducted extensive analysis experiments of each module. All component analysis experiments were performed on the Indian Pine dataset, which has a low spatial resolution and the sample data is imbalanced. The baseline method is shown in <ref type="table" target="#tab_4">Table.</ref> ? (a), which is an encoder-decoder architecture (SegNet) trained by the global stochastic stratified (GS 2 ) sampling strategy of FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discussion on the GCL module and GJAM module</head><p>The global convolutional long short-term memory (GCL) is described in Section III. <ref type="table" target="#tab_4">Table.</ref> ? (b) presents the classification performance of the baseline method with the GCL module. The GCL module is added to SSDGL, OA increased from 58.39% to 95.69%. The hyperparameter ? is introduced into the GCL module to determine the number of time steps. Where the value of ? is from 4 to 12, and the interval is 2. When ? is set to 8, the best classification accuracy can be obtained by the extracted spectral-dependent  <ref type="table" target="#tab_4">Table.</ref> ? (c) presents the effectiveness of the global joint attention module (GJAM). The addition of GJAM modules to SSDGL results in an OA improvement from 95.69% to 96.97%. This module reweight the feature maps and extracts attention areas to boost the classification performance. The main hyperparameter is a compression factor in the global spectral attention mechanism. Due to the limitation of the space, we do not show the results of various parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion on the H-B sampling strategy and weighted softmax loss</head><p>Table ? (d) presents the results of the SSDGL with the H-B sampling strategy and weighted softmax loss (HB-WL). The OA is improved from 96.77 to 99.63 and AA from 97.65 to 99.79. The H-B sampling strategy is used to balance the number of each class in the hierarchical training samples. The category probability is recalculated by the weighted softmax loss.</p><p>The hyperparameter ? is introduced to the H-B sampling strategy to control the mini-batch per class of hierarchical training samples. The value of ? ranges from 5 to 40 and the interval is 5. The proposed SSDGL has the best classification performance when ? is set to 10. When ? is set to a value larger than 20, the classification accuracy of SSDGL decreases gradually, and the average accuracy is limited. It can be found that the value of ? has little impact on the classification performance because the weighted softmax loss recalculates the category probability, but the H-B sampling strategy still plays a key role in addressing insufficient and imbalanced sample problems.  <ref type="table" target="#tab_10">Table ?</ref> lists the training and testing time of the five methods on the Indian pines (IP), Pavia University (PU), and Houston University (HU) datasets. Although SSRN designed with the deep CNN, the training speed of SSRN was 2 times faster than that of SS-CNN. The semisupervised classification methods may achieve better performance than the supervised classification methods when the training samples are small, but they consume considerable computing memory and time. 27.5 0.63 It can be seen that FCN-based methods reduce memory and time consumption. We set the training iterations of the IP and PU datasets in 600 epochs and set 1,000 epochs on the HU datasets to make the model converge. The training speed of the global learning methods faster than patch-based deep learning methods and the testing speed was 100 times faster than that of patch-based methods. Moreover, the test data of global learning methods is a whole image rather than a patch of each test pixel, so the model inference time is greatly decreased because the global learning method reduces the redundant calculations of the overlapping areas between the adjacent pixel patches. The training times of SSDGL are 2~3 times longer than those of the U-Net and FPGA, but the classification accuracy achieved a significant improvement. Hence, the proposed framework is valuable and has good application prospects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a spectral-spatial dependent global learning (SSDGL) framework is proposed to improve the classification performance of hyperspectral images (HSI) with insufficient and imbalanced samples. In the SSDGL framework, the hierarchically balanced (H-B) sampling strategy is proposed to divide the training data into some hierarchical training samples. The weighted softmax with cross entropy loss is used to recalculate the category probability according to the number of labeled samples per class. The input data of SSDGL is the whole image, and it does not require dividing the HSI dataset into pixel patches. To extract the interdependence between spectral features, the global convolutional long short-term memory (GCL) is added to the SSDGL. The global joint attention mechanism (GJAM) is used to estimate the importance of different spectral-spatial features. It can be seen from the experimental results that SSDGL achieves better classification performance than other state-of-the-art methods on HSI datasets, especially when the training samples are insufficient or imbalanced. Compared with the CNN-based methods, the global learning methods greatly reduce the training and inference time, thereby broadening its application prospects in HSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Convolution operation of patch-based and patch-free.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Algorithm 1</head><label>21</label><figDesc>Flowchart of insufficient and imbalanced HSI classification based on the SSDGL framework. The pseudocode of SSDGL Input: of discrete labeled pixels i n : A set of labeled pixels per class ? : the training sample ratio M : the number of classes ? : the number of stratified training data ? : mini-batch per class ? : compression factor ij Mask : mask matrix n y : predicted scores ij y ? : predicted label Output: The parameters of the whole network,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of the GCLAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of the GCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of the ConvLSTMCells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The flowchart of the global spectral attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>The Indian Pines dataset. (a) Three-band false color composite. (b) Ground-truth map (c) Legend Visualization of the classification maps for the Indian Pines dataset. (a) RBF-SVM. (b) SS-CNN. (c) SSRN. (d) DBMA. (e) MCNN-CONVLSTM. (f) U-Net (g) FPGA. (h) SSDGL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>The Pavia University dataset. (a) Three-band false color composite. (b) Ground-truth map (c) Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of the classification maps for the Pavia University dataset. (a) RBF-SVM. (b) SS-CNN. (c) SSRN. (d) DBMA. (e) MCNN-CONVLSTM. (f) U-Net (g) FPGA. (h) SSDGL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>The Houston University dataset. (a) Three-band false color composite. (b) Ground-truth map (c) Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Confusion matrix of SSDGL on the Houston University dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received September 18, 2020; revised February 15, 2021; accepted March 23, 2021. This work was supported by National Natural Science Foundation of China under Grant No. 41901306, and a Grant from State Key Laboratory of Resources and Environmental Information System. (Corresponding author: Yanfei Zhong). The authors are with the School of Geography and Information Engineering, China University of Geosciences, Wuhan 430079, China, and also with the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China, (e-mail: zhuqq@cug.edu.cn, dengweihuan@cug.edu.cn, guanqf@cug.edu.cn, zhongyanfei@whu.edu.cn, zlp62@whu.edu.cn, drli@whu.edu.cn )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>can be observed that HSIs have strong spatial A Spectral-Spatial Dependent Global Learning Framework for Insufficient and Imbalanced Hyperspectral Image Classification Qiqi Zhu, Weihuan Deng, Zhuo Zheng, Yanfei Zhong*, Senior Member, IEEE, Qingfeng Guan, Weihua Lin, Liangpei Zhang, Fellow, IEEE, Deren Li, Senior Member, IEEE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I THE</head><label>I</label><figDesc>NUMBER OF TRAINING SAMPLES AND TEST SAMPLES FOR THE INDIAN PINES DATASET</figDesc><table><row><cell>No.</cell><cell>Class.</cell><cell cols="2">Train. Test.</cell><cell>Total.</cell></row><row><cell>1</cell><cell>Alfalfa</cell><cell>5</cell><cell>41</cell><cell>46</cell></row><row><cell>2</cell><cell>Corn-notill</cell><cell>72</cell><cell>1356</cell><cell>1428</cell></row><row><cell>3</cell><cell>Corn-mintill</cell><cell>42</cell><cell>788</cell><cell>830</cell></row><row><cell>4</cell><cell>Corn</cell><cell>12</cell><cell>225</cell><cell>237</cell></row><row><cell>5</cell><cell>Grass-pasture</cell><cell>25</cell><cell>458</cell><cell>483</cell></row><row><cell>6</cell><cell>Grass-trees</cell><cell>37</cell><cell>693</cell><cell>730</cell></row><row><cell>7</cell><cell>Grass-pasture-mowed</cell><cell>5</cell><cell>23</cell><cell>28</cell></row><row><cell>8</cell><cell>Hay-windrowed</cell><cell>24</cell><cell>454</cell><cell>478</cell></row><row><cell>9</cell><cell>Oats</cell><cell>5</cell><cell>15</cell><cell>20</cell></row><row><cell>10</cell><cell>Soybean-notill</cell><cell>49</cell><cell>923</cell><cell>972</cell></row><row><cell>11</cell><cell>Soybean-mintill</cell><cell>123</cell><cell>2332</cell><cell>2455</cell></row><row><cell>12</cell><cell>Soybean-clean</cell><cell>30</cell><cell>563</cell><cell>593</cell></row><row><cell>13</cell><cell>Wheat</cell><cell>11</cell><cell>194</cell><cell>205</cell></row><row><cell>14</cell><cell>Woods</cell><cell>64</cell><cell>1201</cell><cell>1265</cell></row><row><cell>15</cell><cell>Buildings-Grass-Trees</cell><cell>20</cell><cell>366</cell><cell>386</cell></row><row><cell>16</cell><cell>Stone-Steel-Towers</cell><cell>5</cell><cell>88</cell><cell>93</cell></row><row><cell></cell><cell>Total</cell><cell>529</cell><cell cols="2">9720 10249</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table . ?</head><label>.</label><figDesc>lists the number of training and test data for each category. We set the training sample number per class as 1% of labeled samples.</figDesc><table><row><cell>Fig</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CNN-based</cell><cell></cell><cell></cell><cell></cell><cell>FCN-based</cell><cell></cell></row><row><cell>Class</cell><cell>RBF-SVM</cell><cell>SS-CNN</cell><cell>SSRN</cell><cell>DBMA</cell><cell>MCNN-CONVLSTM</cell><cell>U-Net</cell><cell>FPGA</cell><cell>Proposed</cell></row><row><cell>1</cell><cell>70.32</cell><cell>72.14</cell><cell>75.57</cell><cell>90.37</cell><cell>94.36</cell><cell>97.67</cell><cell>97.22</cell><cell>100.00</cell></row><row><cell>2</cell><cell>69.63</cell><cell>90.42</cell><cell>90.65</cell><cell>92.72</cell><cell>92.84</cell><cell>92.48</cell><cell>93.07</cell><cell>99.63</cell></row><row><cell>3</cell><cell>58.26</cell><cell>81.48</cell><cell>97.01</cell><cell>95.63</cell><cell>93.02</cell><cell>84.77</cell><cell>89.46</cell><cell>99.24</cell></row><row><cell>4</cell><cell>45.22</cell><cell>71.23</cell><cell>93.36</cell><cell>89.35</cell><cell>95.32</cell><cell>89.33</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>5</cell><cell>75.48</cell><cell>83.62</cell><cell>98.56</cell><cell>96.92</cell><cell>92.13</cell><cell>81.00</cell><cell>95.63</cell><cell>99.56</cell></row><row><cell>6</cell><cell>96.14</cell><cell>97.19</cell><cell>98.94</cell><cell>99.18</cell><cell>98.86</cell><cell>94.08</cell><cell>97.56</cell><cell>100.00</cell></row><row><cell>7</cell><cell>95.79</cell><cell>91.03</cell><cell>84.21</cell><cell>79.57</cell><cell>84.83</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>8</cell><cell>87.72</cell><cell>92.34</cell><cell>98.36</cell><cell>99.11</cell><cell>98.63</cell><cell>98.90</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>9</cell><cell>75.03</cell><cell>96.39</cell><cell>97.61</cell><cell>97.91</cell><cell>92.47</cell><cell>78.95</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>10</cell><cell>66.25</cell><cell>81.75</cell><cell>81.03</cell><cell>92.08</cell><cell>94.76</cell><cell>89.49</cell><cell>96.64</cell><cell>99.68</cell></row><row><cell>11</cell><cell>77.62</cell><cell>87.39</cell><cell>93.02</cell><cell>95.15</cell><cell>96.28</cell><cell>97.81</cell><cell>96.74</cell><cell>99.36</cell></row><row><cell>12</cell><cell>67.28</cell><cell>83.03</cell><cell>95.72</cell><cell>90.71</cell><cell>94.12</cell><cell>86.50</cell><cell>91.65</cell><cell>99.11</cell></row><row><cell>13</cell><cell>96.93</cell><cell>97.42</cell><cell>99.81</cell><cell>99.81</cell><cell>96.95</cell><cell>98.97</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>14</cell><cell>95.07</cell><cell>95.31</cell><cell>95.79</cell><cell>97.11</cell><cell>98.79</cell><cell>98.58</cell><cell>99.91</cell><cell>100.00</cell></row><row><cell>15</cell><cell>35.48</cell><cell>74.04</cell><cell>92.25</cell><cell>88.13</cell><cell>92.83</cell><cell>92.08</cell><cell>99.72</cell><cell>100.00</cell></row><row><cell>16</cell><cell>97.61</cell><cell>94.61</cell><cell>96.57</cell><cell>97.05</cell><cell>87.32</cell><cell>93.18</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>OA</cell><cell>75.31</cell><cell>89.82</cell><cell>92.21</cell><cell>94.43</cell><cell>94.78</cell><cell>93.20</cell><cell>96.18</cell><cell>99.63</cell></row><row><cell>AA</cell><cell>71.12</cell><cell>83.73</cell><cell>93.03</cell><cell>93.81</cell><cell>93.37</cell><cell>92.11</cell><cell>97.33</cell><cell>99.79</cell></row><row><cell>Kappa</cell><cell>0.7173</cell><cell>0.8783</cell><cell>0.9115</cell><cell>0.9365</cell><cell>0.9437</cell><cell>0.9222</cell><cell>0.9564</cell><cell>0.9958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE ? THE</head><label>?</label><figDesc>NUMBER OF TRAINING SAMPLES AND TEST SAMPLES FOR THE</figDesc><table><row><cell></cell><cell cols="3">PAVIA UNIVERSITY DATASET</cell><cell></cell></row><row><cell>No.</cell><cell>Class.</cell><cell>Train.</cell><cell>Test.</cell><cell>Total.</cell></row><row><cell>1</cell><cell>Asphalt</cell><cell>67</cell><cell>6564</cell><cell>6631</cell></row><row><cell>2</cell><cell>Meadows</cell><cell>187</cell><cell>18462</cell><cell>18649</cell></row><row><cell>3</cell><cell>Gravel</cell><cell>21</cell><cell>2078</cell><cell>2099</cell></row><row><cell>4</cell><cell>Trees</cell><cell>31</cell><cell>3033</cell><cell>3064</cell></row><row><cell>5</cell><cell>Metal sheets</cell><cell>14</cell><cell>1331</cell><cell>1345</cell></row><row><cell>6</cell><cell>Bare Soil</cell><cell>51</cell><cell>4978</cell><cell>5029</cell></row><row><cell>7</cell><cell>Bitumen</cell><cell>14</cell><cell>1316</cell><cell>1330</cell></row><row><cell>8</cell><cell>Bricks</cell><cell>37</cell><cell>3645</cell><cell>3682</cell></row><row><cell>9</cell><cell>Shadows</cell><cell>10</cell><cell>937</cell><cell>947</cell></row><row><cell></cell><cell>Total</cell><cell>432</cell><cell>42344</cell><cell>42776</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE ? THE</head><label>?</label><figDesc>NUMBER OF TRAINING SAMPLES AND TEST SAMPLES FOR THE HOUSTON UNIVERSITY DATASET</figDesc><table><row><cell>No.</cell><cell>Class.</cell><cell>Train.</cell><cell>Test.</cell><cell>Total.</cell></row><row><cell>1</cell><cell>Healthy Grass</cell><cell>10</cell><cell>1241</cell><cell>1251</cell></row><row><cell>2</cell><cell>Stressed Grass</cell><cell>10</cell><cell>1244</cell><cell>1254</cell></row><row><cell>3</cell><cell>Synthetic Grass</cell><cell>10</cell><cell>687</cell><cell>697</cell></row><row><cell>4</cell><cell>Tree</cell><cell>10</cell><cell>1234</cell><cell>1244</cell></row><row><cell>5</cell><cell>Soil</cell><cell>10</cell><cell>1242</cell><cell>1252</cell></row><row><cell>6</cell><cell>Water</cell><cell>10</cell><cell>315</cell><cell>325</cell></row><row><cell>7</cell><cell>Residential</cell><cell>10</cell><cell>1258</cell><cell>1268</cell></row><row><cell>8</cell><cell>Commercial</cell><cell>10</cell><cell>1234</cell><cell>1244</cell></row><row><cell>9</cell><cell>Road</cell><cell>10</cell><cell>1242</cell><cell>1252</cell></row><row><cell>10</cell><cell>Highway</cell><cell>10</cell><cell>1217</cell><cell>1227</cell></row><row><cell>11</cell><cell>Railway</cell><cell>10</cell><cell>1225</cell><cell>1235</cell></row><row><cell>12</cell><cell>Parking Lot 1</cell><cell>10</cell><cell>1224</cell><cell>1234</cell></row><row><cell>13</cell><cell>Parking Lot 2</cell><cell>10</cell><cell>459</cell><cell>469</cell></row><row><cell>14</cell><cell>Tennis Court</cell><cell>10</cell><cell>418</cell><cell>428</cell></row><row><cell>15</cell><cell>Running Track</cell><cell>10</cell><cell>650</cell><cell>660</cell></row><row><cell></cell><cell>Total</cell><cell>150</cell><cell>14861</cell><cell>15011</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE ? THE</head><label>?</label><figDesc>CLASSIFICATION RESULTS OF RBF-SVM, SS-CNN, SSRN, DBMA, MCNN-CONVLSTM, U-NET, FPGA AND SSDGL ON THE PAVIA UNIVERSITY DATASET WITH 1% LABELED SAMPLES TABLE ? THE CLASSIFICATION RESULTSOF SVM-3DG, SS-CNN, SSRN, MUGNET, AROC-DPNET, U-NET, FPGA, AND SSDGL ON HOUSTON UNIVERSITY DATASET WITH TEN SAMPLES PER CLASS V. DISCUSSION</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE ? HSI</head><label>?</label><figDesc>CLASSIFICATION RESULTS EVALUATED ON THE INDIAN PINES DATASET WITH 5% LABELED SAMPLES. THE ENCODER-DECODER BASELINE, THE GCL, GJAM, AND HB-WL ARE ADDED IN SSDGL FOR THE MODULE ANALYSIS</figDesc><table /><note>C. Discussion on the running time</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE ? TRAINING</head><label>?</label><figDesc>AND TESTING TIME OF DIFFERENT MODELS ON THREE HSI DATA SETS</figDesc><table><row><cell></cell><cell></cell><cell>IN</cell><cell>PU</cell><cell>HU</cell></row><row><cell>SS-CNN</cell><cell>Train.(m) Test.(s)</cell><cell>25.7 19.6</cell><cell>24.8 35.4</cell><cell>37.3 45.2</cell></row><row><cell>SSRN</cell><cell>Train.(m) Test.(s)</cell><cell>18.5 15.2</cell><cell>16.8 29.6</cell><cell>21.7 36.5</cell></row><row><cell>U-Net</cell><cell>Train.(m) Test.(s)</cell><cell>4.7 0.23</cell><cell>4.2 0.27</cell><cell>15.2 0.41</cell></row><row><cell>FPGA</cell><cell>Train.(m) Test.(s)</cell><cell>4.2 0.16</cell><cell>3.9 0.19</cell><cell>14.6 0.34</cell></row><row><cell>SSDGL</cell><cell>Train.(m) Test.(s)</cell><cell>9.8 0.31</cell><cell>8.9 0.36</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances on spectral-spatial hyperspectral image classification: An overview and new guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1579" to="1597" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with independent component discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4865" to="4876" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Spectral-Spatial Features for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1667" to="1678" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advanced spectral classifiers for hyperspectral images: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="32" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="164" to="178" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised hyperspectral image classification using small sample sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Aydemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bilgin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="621" to="625" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimension Reduction Using Spatial and Spectral Regularized Local Discriminant Embedding for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Philip Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1082" to="1095" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MugNet: Deep learning for hyperspectral image classification using limited samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral and spatial classification of hyperspectral data using SVMs and morphological profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3804" to="3814" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral-spatial hyperspectral image segmentation using subspace multinomial logistic regression and Markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="809" to="823" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From subpixel to superpixel: A novel fusion framework for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4398" to="4411" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 3-D Gabor Phase-Based Coding and Matching Framework for Hyperspectral Imagery Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1176" to="1188" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simultaneous Spectral-Spatial Feature Selection and Extraction for Hyperspectral Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive deep sparse semantic modeling framework for high spatial resolution image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6180" to="6195" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification with Deep Feature Fusion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3173" to="3184" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spectral-spatial hyperspectral image classification via multiscale adaptive sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7738" to="7749" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3-D deep learning approach for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hamida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4420" to="4434" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep recurrent neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3639" to="3655" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for hyperspectral data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">298</biblScope>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Dual Geometric Low-Rank Structure for Semisupervised Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="346" to="358" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Superpixel-Level Weighted Label Propagation for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5077" to="5091" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification via Multitask Joint Sparse Representation and Stepwise MRF Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2966" to="2977" />
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial networks and conditional random fields for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3318" to="3329" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning using pseudo labels for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1259" to="1270" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building Extraction from High Spatial Resolution Remote Sensing Images via Multiscale-Aware and Segmentation-Prior Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">3983</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5612" to="5626" />
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification with Convolutional Neural Network and Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4604" to="4616" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral-spatial residual network for hyperspectral image classification: A 3-D deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Double-branch multi-attention mechanism network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1307</biblScope>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascaded recurrent neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5384" to="5394" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial-Spectral Feature Extraction via Deep ConvLSTM Neural Networks for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4237" to="4250" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going Deeper With Contextual CNN for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4843" to="4855" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond the patchwise classification: Spectral-spatial fully convolutional networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="506" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deeplab-based spatial feature extraction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="255" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep fully convolutional network-based spatial distribution prediction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5585" to="5599" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Low-shot learning for the semantic segmentation of remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6214" to="6223" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hyperspectral classification based on lightweight 3-D-CNN with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5813" to="5828" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning and transferring deep joint spectral-spatial features for hyperspectal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. -Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. C. -W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4729" to="4742" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised spatial-spectral feature learning by 3D convolutional autoencoder for hyperspectral classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6808" to="6820" />
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, Worskhop Track</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A semi-supervised convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="839" to="848" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3-D convolution-recurrent networks for spectral-spatial classification of hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seydgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">883</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Analysis of various optimizers on deep convolutional neural network model in the application of hyperspectral remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2664" to="2683" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification With Markov Random Fields and a Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2354" to="2367" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
