<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Self-Contact and Human Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>M?ller</surname></persName>
							<email>lea.mueller@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
							<email>ahmed.osman@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<email>stang@tuebingen.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao</forename><forename type="middle">P</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Self-Contact and Human Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(Towards Under-standing Contact in Humans). We show that the new self-contact training data significantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research pur-poses at https://tuch.is.tue.mpg.de.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc. While many images of people contain some form of selfcontact, current 3D human pose and shape (HPS) regression methods typically fail to estimate this contact. To address this, we develop new datasets and methods that significantly improve human pose estimation with self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to ensure good contact. Second, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during fitting to create near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild images with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that exploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-contact takes many forms. We touch our bodies both consciously and unconsciously <ref type="bibr" target="#b24">[25]</ref>. For the major limbs, contact can provide physical support, whereas we touch our faces in ways that convey our emotional state. We perform self-grooming, we have nervous gestures, and <ref type="figure">Figure 1</ref>. The first column shows images containing self-contact. In blue (left), results of TUCH, compared to SPIN results in violet (right). When rendered from the camera view, the estimated pose may look fine (column two vs. four). However, when rotated, it is clear that training TUCH with self-contact information improves 3D pose estimation (column three vs. five).</p><p>we communicate with each other through combined face and hand motions (e.g. "shh"). We may wring our hands when worried, cross our arms when defensive, or put our hands behind our head when confident. A Google search for "sitting person" or "thinking pose" for example, will return images, the majority of which, contain self-contact.</p><p>Although self-contact is ubiquitous in human behavior, it is rarely explicitly studied in computer vision. For our purposes, self-contact comprises "self touch" (where the hands touch the body) and contact between other body parts (e.g. crossed legs). We ignore body parts that are frequently in contact (e.g. at the crotch or armpits) and focus on contact that is communicative or functional. Our goal is to estimate 3D human pose and shape (HPS) accurately for any pose. When self-contact is present, the estimated pose should reflect the true 3D contact.</p><p>Unfortunately, existing methods that compute 3D bodies from images perform poorly on images with self-contact; see <ref type="figure">Fig. 1</ref>. Body parts that should be touching generally are not. Recovering human meshes from images typically involves either learning a regressor from pixels to 3D pose and shape <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>, or fitting a 3D model to image features using an optimization method <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>. The learning approaches rely on labeled training data. Unfortunately, current 2D datasets typically contain labeled keypoints or segmentation masks but do not provide any information about 3D contact. Similarly, existing 3D datasets typically avoid capturing scenarios with self-contact because it complicates mesh processing. What is missing is a dataset with in-the-wild images and reliable data about 3D self-contact.</p><p>To address this limitation, we introduce three new datasets that focus on self-contact at different levels of detail. Additionally, we introduce two new optimizationbased methods that fit 3D bodies to images with contact information. We leverage these to estimate pseudo groundtruth 3D poses with self-contact. To make reasoning about contact between body parts, the hands, and the face possible, we represent pose and shape with the SMPL-X <ref type="bibr" target="#b36">[36]</ref> body model, which realistically captures the body surface details, including the hands and face. Our new datasets then let us train neural networks to regress 3D HPS from images of people with self-contact more accurately than state-ofthe-art methods.</p><p>To begin, we first construct a 3D Contact Pose (3DCP) dataset of 3D meshes where body parts are in contact. We do so using two methods. First, we use high-quality 3D scans of subjects performing self-contact poses. We extend previous mesh registration methods to cope with selfcontact and register the SMPL-X mesh to the scans. To gain more variety of poses, we search the AMASS dataset <ref type="bibr" target="#b29">[30]</ref> for poses with self-contact or "near" self-contact. We then optimize these poses to bring nearby parts into full contact while resolving interpenetration. This provides a dataset of valid, realistic, self-contact poses in SMPL-X format.</p><p>Second, we use these poses to collect a novel dataset of images with near ground-truth 3D pose. To do so, we show rendered 3DCP meshes to workers on Amazon Mechanical Turk (AMT). Their task is to Mimic The Pose (MTP) as accurately as possible, including the contacts, and submit a photograph. We then use the "true" pose as a strong prior and optimize the pose in the image by extending SMPLify-X <ref type="bibr" target="#b36">[36]</ref> to enforce contact. A key observation is that, if we know about self-contact (even approximately), this greatly reduces pose ambiguity by removing degrees of freedom. Thus, knowing contact makes the estimation of 3D human pose from 2D images more accurate. The resulting method, SMPLify-XMC (for SMPLify-X with Mimicked Contact), produces high-quality 3D reference poses and body shapes in correspondence with the images.</p><p>Third, to gain even more image variety, we take images from three public datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref> and have them labeled with discrete body-part contacts. This results in the Discrete Self-Contact (DSC) dataset. To enable this, we define a partitioning of the body into regions that can be in contact. Given labeled discrete contacts, we extend SM-PLify to optimize body shape using image features and the discrete contact labels. We call this method SMPLify-DC, for SMPLify with Discrete Self-Contact.</p><p>Given the MTP and DSC datasets, we finetune a re-cent HPS regression network, SPIN <ref type="bibr" target="#b23">[24]</ref>. When we have 3D reference poses, i.e. for MTP images, we use these as though they were ground truth and do not optimize them in SPIN. When discrete contact annotations are available, i.e. for DSC images, we use SMPLify-DC to optimize the fit in the SPIN training loop. Fine-tuning SPIN on MTP and DSC significantly improves accuracy of the regressed poses when there is contact (evaluated on 3DPW <ref type="bibr" target="#b45">[45]</ref>). Surprisingly, the results on non-self-contact poses also improve, suggesting that (1) gathering accurate 3D poses for in-thewild images is beneficial, and (2) that self-contact can provide valuable constraints that simplify pose estimation. We call our regression method TUCH (Towards Understanding Contact in Humans). <ref type="figure">Figure 1</ref> illustrates the effect of exploiting self-contact in 3D HPS estimation. By training with self-contact, TUCH significantly improves the physical plausibility.</p><p>In summary, the key contributions of this paper are: (1) We introduce TUCH, the first HPS regressor for self-contact poses, trained end-to-end. <ref type="bibr" target="#b1">(2)</ref> We create a novel dataset of 3D human meshes with realistic contact (3DCP). (3) We define a "Mimic The Pose" MTP task and a new optimization method to create a novel dataset of in-the-wild images with accurate 3D reference data. (4) We create a large dataset of images with reference poses that use discrete contact labels. <ref type="bibr" target="#b4">(5)</ref> We show in experiments that taking self-contact information into account improves pose estimation in two ways (data and losses), and in turn achieves state-of-the-art results on 3D pose estimation benchmarks. <ref type="bibr" target="#b5">(6)</ref> The data and code are available for research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D pose estimation with contact. Despite rapid progress in 3D human pose estimation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b47">47]</ref>, and despite the role that self-contact plays in our daily lives, only a handful of previous works discuss selfcontact. Information about contact can benefit 3D HPS estimation in many ways, usually by providing additional physical constraints to prevent undesirable solutions such as interpenetration between limbs.</p><p>Body contact. Lee and Chen <ref type="bibr" target="#b25">[26]</ref> approximate the human body as a set of line segments and avoid collisions between the limbs and torso. Similar ideas are adopted in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> where line segments are replaced with cylinders. Yin et al. <ref type="bibr" target="#b50">[50]</ref> build a pose prior to penalize deep interpenetration detected by the Open Dynamics Engine <ref type="bibr" target="#b43">[43]</ref>. While efficient, these stickman-like representations are far from realistic. Using a full 3D body mesh representation, Pavlakos et al. <ref type="bibr" target="#b36">[36]</ref> take advantage of physical limits and resolve interpenetration of body parts by adding an interpenetration loss. When estimating multiple people from an image, Zanfir et al. <ref type="bibr" target="#b51">[51]</ref> use a volume occupancy exclusion loss to prevent penetration. Still, other work has exploited textual and ordinal descriptions of body pose <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>. This includes constraints like "Right hand above the hips". These methods, however, do not consider self-contact.</p><p>Most similar to us is the work of Fieraru et al. <ref type="bibr" target="#b7">[8]</ref>, which utilizes discrete contact annotations between people. They introduce contact signatures between people based on coarse body parts. This is similar to how we collect the DSC dataset. Contemporaneous with our work, Fieraru et al. <ref type="bibr" target="#b8">[9]</ref> extend this to self-contact with a 2-stage approach. They train a network to predict "self-contact signatures", which are used for optimization-based 3D pose estimation. In contrast, TUCH is trained end-to-end to regress body pose with contact information.</p><p>World contact. Multiple methods use the 3D scene to help estimate the human pose. Physical constraints can come from the ground plane <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b51">51]</ref>, an object <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, or contextual scene information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">49]</ref>. Li et al. <ref type="bibr" target="#b26">[27]</ref> use a DNN to detect 2D contact points between objects and selected body joints. Narasimhaswamy et al. <ref type="bibr" target="#b34">[34]</ref> categorize hand contacts into self, person-person, and object contacts and aim to detect them from in-the-wild images. Their dataset does not provide reference 3D poses or shape.</p><p>All the above works make a similar observation: human pose estimation is not a stand-alone task; considering additional physical contact constraints improves the results. We go beyond prior work by addressing self-contact and showing how training with self-contact data improves pose estimation overall.</p><p>3D body datasets. While there are many datasets of 3D human scans, most of these have people standing in an "A" or "T" pose to explicitly minimize self-contact <ref type="bibr" target="#b40">[40]</ref>. Even when the body is scanned in varied poses, these poses are designed to avoid self-contact <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">39]</ref>. For example, the FAUST dataset has a few examples of self-contact and the authors identify these as the major cause of error for scan processing methods <ref type="bibr" target="#b4">[5]</ref>. Recently, the AMASS <ref type="bibr" target="#b29">[30]</ref> dataset unifies 15 different optical marker-based motion capture (mocap) datasets within a common 3D body parameterization, offering around 170k meshes with SMPL-H <ref type="bibr" target="#b41">[41]</ref> topology. Since mocap markers are sparse and often do not cover the hands, such datasets typically do not explicitly capture self-contact. As illustrated in <ref type="table">Table 1</ref>, none of these datasets explicitly addresses self-contact.</p><p>Pose mimicking. Our Mimic-The-Pose dataset uses the idea that people can replicate a pose that they are shown. Several previous works have explored this idea in different contexts. Taylor et al. <ref type="bibr" target="#b44">[44]</ref> crowd-source images of people in the same pose by imitation. While they do not know the true 3D pose, they are able to train a network to match images of people in similar poses. Marinoiu et al. <ref type="bibr" target="#b30">[31]</ref> motion capture subjects reenacting a 3D pose from a 2D image. They found that subjects replicated 3D poses with a mean joint error of around 100mm. This is on par with existing Name</p><p>Meshes Meshes with self-contact 3DCP Scan (ours) 190 188 3D BodyTex <ref type="bibr" target="#b0">[1]</ref> 400 3 SCAPE <ref type="bibr" target="#b1">[2]</ref> 70 0 Hasler et al. <ref type="bibr" target="#b11">[12]</ref> 520 0 FAUST <ref type="bibr" target="#b4">[5]</ref> 100/ 400 20/ 140 <ref type="table">Table 1</ref>. Existing 3D human mesh datasets with the number of poses and the number of contact poses identified by visual inspection. 3DCP Scan is the scan subset of 3DCP (see <ref type="bibr">Section 4)</ref>. FAUST (train/test) includes scans with self-contact, i.e. 20 in the training and 140 in the test set. However, in FAUST the variety is low as each subject is scanned in the same 10/20 poses, whereas in 3DCP Scan each subject does different poses.</p><p>3D pose regression methods, pointing to people's ability to approximately recreate viewed poses. Fieraru et al. <ref type="bibr" target="#b8">[9]</ref> ask subjects to reproduce contact from an image in a lab setting. They manually annotate the contact, whereas our MTP task is done in people's homes and SMPLify-XMC is used to automatically optimize the pose and contact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Contact</head><p>An intuitive definition of contact between two meshes, e.g. a human and an object, is based on intersecting triangles. Self-contact, however, must be formulated to exclude common, but not functional, triangle intersections, e.g. at the crotch or armpits. Intuitively, vertices are in self-contact if they are close in Euclidean distance (near zero) but distant in geodesic distance, i.e. far away on the body surface. </p><formula xml:id="formula_0">v i and v j ? M V to be in self-contact, if (i) v i ? v j &lt; t eucl , and (ii) geo(v i , v j ) &gt; t geo ,</formula><p>where t eucl and t geo are predefined thresholds and geo(v i , v j ) denotes the geodesic distance between v i and v j . We use shape-independent geodesic distances precomputed on the neutral, mean-shaped SMPL and SMPL-X models.</p><p>Following this definition, we denote the set of vertex pairs in self-contact as</p><formula xml:id="formula_1">M C := {(v i , v j )|v i , v j ? M V and v i , v j satisfy Definition 3.1}. M is a self-contact mesh when |M C | &gt; 0.</formula><p>We further define an operator U(?) that returns a set of unique vertices in M C , and an operator f g (?) that takes v i as input and returns the Euclidean distance to the nearest v j that is far enough in the geodesic sense. Formally,</p><formula xml:id="formula_2">U(M C ) = {v 0 , v 1 , v 2 , . . . , v n }, where ?v i ? U(M C ) , ?v j ? U(M C ), such that (v i , v j ) ? M C . f g (v i ) := min vj ?M G (vi) v i ? v j , where M G (v i ) := {v j |geo(v i , v j ) &gt; t geo }.</formula><p>We further cluster self-contact meshes into distinct types. To that end, we define self-contact signatures S ? {0, 1} K?K ; see <ref type="bibr" target="#b8">[9]</ref> for a similar definition. We first segment the vertices of a mesh into K regions R k , where <ref type="figure">Figure 2</ref>. Visualization of the function HD(X), that maps from mesh vertices to mesh surface points. First, a SMPL-X mesh with vertices in contact highlighted. Second, in yellow, all faces containing a vertex in contact are selected. Then, all points lying on a face containing a vertex in contact are selected from MP , denoted as MD. MP is a fixed set of mesh surface points that are regressed from mesh vertices. Note that in image one and two the finger vertices are denser than the arm and chest vertices, in contrast to the more uniform density in images three and four.</p><formula xml:id="formula_3">R k ? R l = ? for k = l and K k=1 R k = M V .</formula><p>We use fine signatures to cluster self-contact meshes from AMASS (see Sup. Mat.) and rough signatures (see <ref type="figure" target="#fig_6">Fig. 18</ref>) for human annotation.</p><formula xml:id="formula_4">Definition 3.2. Two regions R k and R l are in contact if ?(v i , v j ) ? M C , such that v i ? R k and v j ? R l holds. If R k and R l are in contact, S kl = S lk = 1. M S denotes the contact signature for mesh M .</formula><p>To detect self-contact, we need to be able to quickly compute the distance between two points on the body surface. Vertex-to-vertex distance is a poor approximation of this due to the varying density of vertices across the body. Consequently, we introduce HD SMPL-X and HD SMPL to efficiently approximate surface-to-surface distance. For this, we uniformly, and densely, sample points, M P ? R P ?3 with P = 20, 000 on the body. A sparse linear regressor P regresses M P from the mesh vertices M V , M P = PM V . The geodesic distance geo HD (p 1 , p 2 ) between p 1 ? M P and p 2 ? M P is approximated via geo(m, n), where m = arg min v?M V v ? p 1 and n = arg min v?M V v ? p 2 . In practice, we use mesh surface points only when contact is present by following a threestep procedure as illustrated in <ref type="figure">Fig. 2</ref>. First, we use Definition 3.1 to detect vertices in contact, M C . Then we select all points in M P lying on faces that contain vertices</p><formula xml:id="formula_5">in M C , denoted as M D . Last, for p i ? M D we find the closest mesh surface point min pj ?M D p i ? p j , such that geo HD (p i , p j ) &gt; t geo . With HD(X) : X ? M V ? M D ? M P</formula><p>we denote the function that maps from a set of mesh vertices to a set of mesh surface points. As the number of points, P , increases, the point-to-point distance approximates the surface-to-surface distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Self-Contact Datasets</head><p>Our goal is to create datasets of in-the-wild images paired with 3D human meshes as pseudo ground truth. Unlike traditional pipelines that collect images first and then annotate them with pose and shape parameters <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">45]</ref>, we <ref type="figure">Figure 3</ref>. Self-contact optimization. Column 1 and 2: a pose selected from AMASS with near self-contact (between the fingertips and the foot) and interpenetration (thumb and foot). Column 3 and 4: after self-contact optimization, all fingers are in contact with the foot and interpenetration is reduced. take the opposite approach. We first curate meshes with self-contact and then pair them with images through a novel pose mimicking and fitting procedure. We use SMPL-X to create the 3DCP and MTP dataset to better fit contacts between hands and bodies. However, to fine-tune SPIN <ref type="bibr" target="#b23">[24]</ref>, we convert MTP data to SMPL topology, and use SMPLify-DC when optimizing with discrete contact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Contact Pose (3DCP) Meshes</head><p>We create 3D human meshes with self-contact in two ways: with 3D scans and with motion capture data.</p><p>3DCP Scan. We scan 6 subjects (3 males, 3 females) in self-contact poses. We then register the SMPL-X mesh topology to the raw scans. These registrations are obtained using Co-Registration <ref type="bibr" target="#b13">[14]</ref>, which iteratively deforms the SMPL-X template mesh V to minimize the point-to-plane distance between the scan points S ? R N ?3 , where N is the number of scan points and the template points V ? R 10375?3 . However, registering poses with self-contact is challenging. When body parts are in close proximity, the standard process can result in interpenetration. To address this, we add a self-contact-preserving energy term to the objective function. If two vertices v i and v j are in contact according to Definition 3.1, we minimize the point-to-plane distance between triangles including v i and the triangular planes including v j . This term ensures that body parts that are in contact remain in contact; see Sup. Mat. for details.</p><p>3DCP Mocap. While mocap datasets are usually not explicitly designed to capture self-contact, it does occur during motion capture. We therefore search the AMASS dataset for poses that satisfy our self-contact definition. We find that some of the selected meshes from AMASS contain small amounts of self-penetration or near contact. Thus, we perform self-contact optimization to fix this while encouraging contact, as shown in <ref type="figure">Fig. 3</ref>; see Sup. Mat. for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Mimic-The-Pose (MTP) Data</head><p>To collect in-the-wild images with near ground-truth 3D human meshes, we propose a novel two-step process (see   body (the mimicked pose). Mimicking poses may be challenging for people when only a single image of the pose is presented <ref type="bibr" target="#b30">[31]</ref>. Thus, we render each 3DCP mesh from three different views with the contact regions highlighted (the presented pose). We allot 3 hours time for ten poses. Participants also provide their height and weight. All participants gave informed consent for the capture and the use of their imagery. Please see Sup. Mat. for details.</p><p>SMPLify-XMC. The second step applies a novel optimization method to estimate the pose in the image, given a strong prior from the presented pose. The presented pose?, shape?, and gender is not mimicked perfectly. To obtain pseudo ground-truth pose and shape, we adapt SMPLify-X <ref type="bibr" target="#b36">[36]</ref>, a multi-stage optimization method, that fits SMPL-X pose ?, shape ?, and expression ? to image features starting from the mean pose and shape. We make use of the presented pose? in three ways: first, to initialize the optimization and solve for global orientation and camera; second, it serves as a pose prior; and third its contact is used to keep relevant body parts close to each other. We refer to this new optimization method as SMPLify-XMC.</p><p>In the first stage, we optimize body shape ?, camera ? (rotation, translations, and focal length), and body global orientation ? g , while the pose ? is initialized as? and stays constant; see Sup. Mat. for a description of the first stage.</p><p>In the second and third stage, we jointly optimize ?, ?, and ? to minimize</p><formula xml:id="formula_6">L(?, ?, ?) =E J + ? m h E m h + ??L?+ ? M L M + ?CLC + ? S L S .<label>(1)</label></formula><p>E J denotes the same re-projection loss as specified in [36] 1 . We use the standard SMPLify-X priors for left and right hand E m h . While the pose prior in <ref type="bibr" target="#b36">[36]</ref> penalizes deviation from the mean pose, here, L? is an L2-Loss that penalizes deviation from the presented pose. The measurements loss L M takes ground-truth height and weight into account; see Sup. Mat. for details. The term LC acts onM C , the vertices in self-contact on the presented mesh. To ensure the desired self-contact, one could seek to minimize the distances between vertices in contact, e.g.</p><formula xml:id="formula_7">||v i ? v j ||, (v i , v j ) ?M C .</formula><p>However, with this approach, we observe slight mesh distortions, when presented and mimicked contact are different. Instead, we use a term that encourages every vertex i? M C to be in contact. More formally,</p><formula xml:id="formula_8">LC = 1 |U(M C )| vi?U (M C ) tanh(f g (v i )).<label>(2)</label></formula><p>The third stage actives L S for fine-grained self-contact optimization, which resolves interpenetration while encouraging contact. The objective is L S = ? C L C + ? P L P + ? A L A . Vertices in contact are pulled together via a contact term L C ; vertices inside the mesh are pushed to the surface via a pushing term L P , and L A aligns the surface normals of two vertices in contact.</p><p>To compute these terms, we must first find which vertices are inside,</p><formula xml:id="formula_9">M I ? M V , or in contact, M C ? M V . M C is computed following Definition 3.1 with t geo = 30cm</formula><p>and t eucl = 2cm. The set of inside vertices M I is detected by generalized winding numbers <ref type="bibr" target="#b14">[15]</ref>. SMPL-X is not a closed mesh and thus complicating the test for penetration. Consequently, we close it by adding a vertex at the back of the mouth. In addition, neighboring parts of SMPL and SMPL-X often intersect, e.g. torso and upper arms. We identify such common self-intersections and filter them out from M I . See Sup. Mat. for details. To capture fine-grained contact, we map the union of inside and contact vertices onto the HD SMPL-X surface, i.e. M D = HD(M I ? M C ), which is further segmented into an inside M D I and outside M D I subsets by testing for intersections. The self-contact objectives are defined as</p><formula xml:id="formula_10">L C = pi?M D I ? 1 tanh( f g (p i ) ? 2 ) 2 , L P = pi?M D I ? 1 tanh( f g (p i ) ? 2 ) 2 , L A = (pi,pj )?M D C 1 + N (p i ), N (p j ) .</formula><p>f g denotes the function that finds the closest point p j ? M D . M D C is the subset of vertices in contact in M D . We use ? 1 = ? 2 = 0.005, ? 1 = 1.0, and ? 2 = 0.04 and visualize the contact and pushing functions in the Sup. Mat. <ref type="figure" target="#fig_3">Fig. 5</ref> shows examples of our pseudo ground-truth meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discrete Self-Contact (DSC) Data</head><p>Images in the wild collected for human pose estimation normally come with 2D keypoint annotations, body segmentation, or bounding boxes. Such annotations lack 3D information. Discrete self-contact annotation, however, provides useful 3D information about pose. We use K = 24 regions and label their pairwise contact for three publicly available datasets, namely Leeds Sports Pose (LSP), Leeds Sports Pose Extended (LSPet), and DeepFashion (DF). An example annotation is visualized in <ref type="figure" target="#fig_6">Fig. 18</ref>. Of course, such labels are noisy because it can be difficult to accurately determine contact from an image. See Sup. Mat. for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Summary of the Collected Data</head><p>Our 3DCP human mesh dataset consists of 190 meshes containing self-contact from 6 subjects, 159 SMPL-X bodies fit to commercial scans from AGORA <ref type="bibr" target="#b35">[35]</ref>, and 1304 self-contact optimized meshes from mocap data. From these 1653 poses, we collect 3731 mimicked pose images from 148 unique subjects (52 female; 96 male) for MTP and fit pseudo ground-truth SMPL-X parameters. MTP is diverse in body shapes and ethnicities. Our DSC dataset provides annotations for 30K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TUCH</head><p>Finally, we train a regression network that has the same design as SPIN <ref type="bibr" target="#b23">[24]</ref>. At each training iteration, the current regressor estimates the pose, shape, and camera parameters of the SMPL model for an input image. Using groundtruth 2D keypoints, an optimizer refines the estimated pose and shape, which are used, in turn, to supervise the regressor. We follow this regression-optimization scheme for DSC data, where we have no 3D ground truth. To this end, we adapt the in-the-loop SMPLify routine to account for discrete self-contact labels, which we term SMPLify-DC. For MTP images, we use the pseudo ground truth from SMPLify-XMC as direct supervision with no optimization involved. We explain the losses of each routine below.</p><p>Regressor. Similar to SPIN, the regressor of TUCH predicts pose, shape, and camera, with the loss function:</p><formula xml:id="formula_11">L R = E J + ? ? E ? + ? ? E ? + ? C L C + ? P L P . (3)</formula><p>E J denotes the joint re-projection loss. L P and L C are selfcontact loss terms used in L S in SMPLify-XMC, where L P penalizes mesh intersections and L C encourages contact. Further, E ? and E ? are L2-Losses that penalize deviation from the pseudo ground-truth pose and shape.</p><p>Optimizer. We develop SMPLify-DC to fit pose ? opt , shape ? opt , and camera ? opt to DSC data, taking groundtruth keypoints and contact as constraints. Typically, in human mesh optimization methods the camera is fit first, then the model parameters follow. However, we find that this can distort body shape when encouraging contact. Therefore, we optimize shape and camera translation first, using the same camera fitting loss as in <ref type="bibr" target="#b23">[24]</ref>. After that, body pose and global orientation are optimized under the objective</p><formula xml:id="formula_12">L O (?) = E J + ? ? E ? + ? C L C + ? P L P + ? D L D . (4)</formula><p>The discrete contact loss, L D , penalizes the minimum distance between regions in contact. Formally, given a contact signature S where S ij = S ji = 1 if two regions R i and R j are annotated to be in contact, we define</p><formula xml:id="formula_13">L D = K i=1 K j=i+1 S ij min v?Ri,u?Rj ||v ? u|| 2 .</formula><p>Given the optimized pose ? opt , shape ? opt , and camera ? opt , we compute the re-projection error and the minimum distance between the regions in contact. When the re-projection error improves, and more regions with contact annotations are closer than before, we keep the optimized pose as the current best fit. When no ground truth is available, the current best fits are used to train the regressor.</p><p>We make three observations: (1) The optimizer is often able to fix incorrect poses estimated by the regressor because it considers the ground-truth keypoints and contact (see <ref type="figure" target="#fig_5">Fig. 7</ref>). (2) Discrete contact labels bring overall improvement by helping resolve depth ambiguity (see <ref type="figure" target="#fig_6">Fig. 8</ref>).</p><p>(3) Since we have mixed data in each mini-batch, the direct supervision of MTP data improves the regressor, which benefits SMPLify-DC by providing better initial estimates.</p><p>Implementation details. We initialize our regression network with SPIN weights <ref type="bibr" target="#b23">[24]</ref>. For SMPLify-DC, we run 10 iterations per stage and do not use the HD operator to speed up the optimization process. For the 2D re-projection loss, we use ground-truth keypoints when available and, for MTP and DF images, OpenPose detections weighted by confidence. From DSC data we only use images where the full body is visible and ignore annotated region pairs that are connected in the DSC segmentation (see Sup. Mat.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>We evaluate TUCH on the following three datasets: 3DPW <ref type="bibr" target="#b45">[45]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b31">[32]</ref>, and 3DCP Scan. This latter dataset consists of RGB images taken during the 3DCP Scan scanning process. While TUCH has never seen these images or subjects, the contact poses were mimicked in creation of MTP, which is used in training.</p><p>We use standard evaluation metrics for 3D pose, namely Mean Per-Joint Position Error (MPJPE) and the Procrustesaligned version (PA-MPJPE), and Mean Vertex-to-Vertex  Error (MV2VE) for shape and contact. <ref type="table">Tables 2 and 3</ref> summarize the results of TUCH on 3DPW and 3DCP Scan. Interestingly, TUCH is more accurate than SPIN on 3DPW. See Sup. Mat. for results of fine-tuning EFT. We further evaluate our results w.r.t. contact. To this end, we divide the 3DPW test set into subsets, namely for t geo = 50cm: self-contact (t eucl &lt; 1cm), no self-contact (t eucl &gt; 5cm), and unclear (1cm &lt; t eucl &lt; 5cm). For 3DPW we obtain 8752 self-contact, 16752 no self-contact, and 9491 unclear poses. <ref type="table" target="#tab_1">Table 4</ref> shows a clear improvement on poses with contact and unclear poses compared to a smaller improvement on poses without contact.</p><p>To further understand the improvement of TUCH over SPIN, we break down the improved MPJPE in 3DPW selfcontact into the pairwise body-part contact labels defined in the DSC dataset. Specifically, for each contact pair, we search all poses in 3DPW self-contact that have this particular self-contact. We find a clear improvement for a large number of contacts between two body parts, frequently between arms and torso, or e.g. left hand and right elbow, which is common in arms-crossed poses (see <ref type="figure" target="#fig_7">Fig. 9</ref>).</p><p>TUCH incorporates self-contact in various ways: annotations of training data, in-the-loop fitting, and in the re-  gression loss. We evaluate the impact of each in <ref type="table" target="#tab_2">Table 5</ref>. S+ is SPIN but it sees MTP+DSC images in fine-tuning and runs standard in-the-loop SMPLify with no contact information. S++ is S+ but uses pseudo ground truth computed with SMPLify-XMC on MTP images; thus self-contact is used to generate the data but nowhere else. S+ vs. SPIN suggests that, while poses in 3DCP Scan appear in MTP, just seeing similar poses for training and testing does not yield improvement. S+ vs. TUCH is a fair comparison as both see the same images during training. The improved results of TUCH confirm the benefit of using self-contact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we address the problem of HPS estimation when self-contact is present. Self-contact is a natural, com- mon occurrence in everyday life, but SOTA methods fail to estimate it. One reason for this is that no datasets pairing images in the wild and 3D reference poses exist. To address this problem we introduce a new way of collecting data: we ask humans to mimic presented 3D poses. Then we use our new SMPLify-XMC method to fit pseudo ground-truth 3D meshes to the mimicked images, using the presented pose and self-contact to constrain the optimization. We use the new MTP data along with discrete self-contact annotations to train TUCH; the first end-to-end HPS regressor that also handles poses with self-contact. TUCH uses MTP data as if it was ground truth, while the discrete, DSC, data is exploited during SPIN training via SMPLify-DC. Overall, incorporating contact improves accuracy on standard benchmarks like 3DPW, remarkably, not only for poses with selfcontact, but also for poses without self-contact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The Supplementary Material provides additional details about our methods and visualizations of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-Contact Datasets</head><formula xml:id="formula_14">A.1. 3D Contact Pose (3DCP) Meshes A.1.1 3DCP Scan</formula><p>Raw scans have varying topology. To bring a corpus of scans to a common topology is the process of "registration". Most traditional registration methods ignore interpenetration and self-contact. Registering our self-contact scans without modeling self-contact would result in selfpenetration, particularly where the extremities contact the body. We address this by modifying the registrations objective function to encourage self-contact without penetration.</p><p>Specifically, the fitting objective includes a data term E S evaluating the goodness of fit of the the vertices x v on the template V to n randomly sampled points, x s on the surface of the scan S</p><formula xml:id="formula_15">E S (S; V ) = 1 n xs?S ? ( x s ? x v )<label>(5)</label></formula><p>where ? is the Geman-McClure robust penalty function. Additionally, we introduce a self-contact preserving energy term E C to the objective function. The term E C helps to minimize and preserve the point-to-plane distance between body parts that are in contact. E C considers the set of contacting vertex pairs M C defined by Definition 3.1 in the main paper. For each tuple (v i , v j ) in M C , we minimize the point-to-plane distance between triangles including v i and the triangular planes including v j . The contact energy term ensures that body parts that are in contact remain in contact.</p><p>The objective function is minimized in two steps: first a model fitting step, where it is minimized with respect to the SMPL-X model pose parameters ? ? R 55?3 and body shape parameters ? ? R 25 . Following model fitting, a model-free optimization step minimizes point-to-plane distance between the model vertices x v and the scan. A sample of the registrations is shown in <ref type="figure">Figure 10</ref>.</p><formula xml:id="formula_16">A.1.2 3DCP Mocap.</formula><p>Sampling meshes from AMASS. First, each mocap. sequence is sampled at half of its original frame rate. For each sampled mesh, we compute the contact signatures M S with t eucl = 3cm, t geo = 30cm and K = 98. The regions are visualized in <ref type="figure">Fig. 11</ref>. We select only one pose for each unique signature, while ignoring contact when it occurs in more than 1% of the data. We obtain a subset of 20,114 poses with unique self-contact signatures, as shown in <ref type="figure" target="#fig_8">Fig. 12</ref>.</p><p>Self-Contact Optimization. Here we provide details of the self-contact optimization for body meshes from the AMASS dataset. In this optimization, vertex pairs in M C are further pulled together via a contact term L C and vertices inside the mesh are pushed to the surface via a pushing term L P , while L O ensures that vertices far away from contact regions stay in place. Note that L P and L C are slightly different from the loss terms in the main paper. L H is a prior for contact between hand and body and L A aligns the vertex normals when contact happens.</p><p>Given the set of vertices M V of mesh M , M E ? M V denotes the subset of vertices affiliated with extremities, M I ? M V denotes the subset of vertices inside the mesh, and M EI = M E ? M I denotes the vertices of extremities that are inside the mesh itself and M EI its complement. We identify vertices inside the mesh using generalized winding numbers <ref type="bibr" target="#b14">[15]</ref>. M V H ? M V is the subset of hand vertices. Note that we make SMPL-X watertight by closing the back of the mouth. M C is computed following Definition 3.1 in the main paper with t geo = 30cm and t eucl = 3cm and</p><formula xml:id="formula_17">M G (v i ) = {v j |geo(v i , v j ) &gt; t geo }.</formula><p>Given an initial mesh I, we aim to minimize the objective function</p><formula xml:id="formula_18">L(? b , ? h l , ? hr ) =? C L C + ? P L P + ? H L H + ? O L O + ? A L A + ? ? h L ? h (6) ? ? L ? ,<label>(7)</label></formula><p>where ? h denote the hand pose vector of the SMPL-X model. Further,</p><formula xml:id="formula_19">L C = 1 |M EI | vi?M EI a? tanh( f g (v i ) ? ), L P = 1 |M EI | vi?M EI ? 1 tanh( f g (v i ) ? 2 ), and L H = 1 |M V H | vi?M V H ? 1 h vi tanh( f g (v i ) ? 2 ),</formula><p>where f g denotes a function, that for each vertex v i finds the closest vertex in self contact v j , or mathematically f g (v i ) = min vj ?M G (vi) ||v i ? v j || 2 . h vi denotes the weight per hand vertex from the hand-on-body prior L H as explained below, if v i is outside, otherwise h vi = 1. Further, a = (min vj ?U (M C ) geo(v i , v j ) + 1) ?1 is an attraction weight. This weight is higher, for vertices close to vertices in contact of?. L ? is a L 2 prior that penalizes deviation from the initial pose and L ? h defines an L 2 prior on the left and right hand pose using the a low-dimensional hand pose space. ? = 0.04, ? 1 = 0.07, ? 2 = 0.06 define slope and <ref type="figure">Figure 10</ref>. A representative sample from the registrations. A total of 3 male and 3 female subjects were scanned in a diversity of poses that involve self-contact. The 3D scans are registered to a common mesh topology by fitting the SMPL-X template mesh to them using a self-contact preserving energy term that penalizes body part interpenetration. <ref type="figure">Figure 11</ref>. To compute self-contact signatures, we group vertices into distinct regions, shown here with different colors. This is useful for searching our scan datasets for poses with specific types of contact.   offset of the pulling and pushing terms. For the hand-onbody-prior we use ? 1 = 0.023, and ? 2 = 0.02 if v i is inside and ? 1 = ? 2 = 0.01 if v i is outside the mesh. Self-contact optimization aims to correct interpenetration and encourage near-contact vertices to be in contact by slightly refining the poses around the contact regions. Vertices that are not affected should stay as close to the original positions as possible. In L O , the displacement of each vertex from its initial position is weighted by its geodesic distance to a vertex in contact. Given? i denoting the position of vertex i of?, the outside loss term is</p><formula xml:id="formula_20">L O = ? 2 vi?M V min vj ?U (M C ) geo(v i , v j ) 2 ||v i ?? i || 2 ,</formula><p>where min vj ?U (M C ) geo(v i , v j ) = 1 if M C = ? and ? 2 = 4. Lastly, we use a term, L A , that encourages the vertex normals N (v) of vertices in contact to be aligned but in opposite directions:</p><formula xml:id="formula_21">L A = 1 |M C | (vi,vj )?M C 1 + N (v i ), N (v j ) .</formula><p>Hand-on-Body Prior. Hands and fingers play an important role as they frequently make contact with the body. However, they have many degrees of freedom, which makes their optimization challenging. Therefore, we learn a handon-body prior from 1279 self contact registrations. For this, we use only poses where the minimum point-to-mesh distance between hand and body is &lt; 1mm. These are 718 and 701 poses for the right and left hand, respectively. Since left and right hand are symmetric in SMPL-X, we unite left and right hand poses. Across the 1429 poses, the mean distances per hand vertex to the body surface, d m (v i ) ranges per vertex from 1.79 to 5.52 cm, as visualized in <ref type="figure" target="#fig_9">Fig. 13</ref>. To obtain the weights h vi in L H , we normalize d m (v i ) to [0, 1], denoted as s(d m (v i )), and obtain the vertex weight by h vi = ?s(d m (v i )) + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Mimic-The-Pose (MTP) Data</head><p>AMT task details. It can be challenging to mimic a pose precisely. To simplify the process for workers on AMT, we give detailed instructions, add thumbnails to compare the own image with the presented one and, most importantly, highlight the contact areas; see <ref type="figure" target="#fig_1">Fig. 14.</ref> To gain more variety, we also request that participants make small changes in the environment for each image, e.g. by rotating the camera, changing clothes, or turning lights on/off. We also ask participants to mimic the global orientation of the center image. For more variety in global orientation, we vary body roll from ?90 ? to 90 ? in 30 ? steps, resulting in seven different presented global orientations. For example, in the first and third row of <ref type="figure" target="#fig_1">Fig. 14,</ref> the center image shows the presented pose from a frontal view. In the second and fourth row, the center body has different orientations. We also ask participants for their height, weight, and gender (M, F, and Non-Binary).</p><p>SMPLify-XMC. In the first stage, we optimize body shape ? and camera ? (focal length, rotation and translation), and body global orientation ? g , using ground-truth height in meters, h gt , and weight in kg, w gt . The objective function of the first stage is given as <ref type="figure" target="#fig_4">Figure 16</ref>. Body segmented into regions where intersection can happen, since SMPL and SMPL-X are not trained to avoid self intersection. Per segment, we create closed meshes that allow for individual intersection tests. For self-contact, intersections that happen within a segment are not relevant. The hands are not included in any segment, because self intersections within hands or between hands and lower arm are not plausible and need to be resolved.  Here we show a few example images that are annotated as having discrete self-contact between the left upper and lower arm (yellow circle). In the last two images, however, the upper and lower arm are barely touching. We do not consider these to be in self-contact. Another ambiguous case, this time due to occlustion, are the two legs in the first image. An annotator can only assume that the shin and calf are touching, based on semantic knowledge about human pose. L M = e 100|M h ?hgt| + e |Mw?wgt| is the measurements loss, where M h and M w are height and weight of mesh M . We compute height and weight from mesh v-template in a zero pose (T-pose). For height, we compute the distance between the top of the head and the mean point between left and right heel. For weight, we compute the mesh volume and multiply it by 985 kg/m 3 , which approximates human body density. L ?g is a loss that allows rotation around the y-axis, but not around x and z.</p><formula xml:id="formula_22">L(?, ?, ? g ) = ? ?g L ?g + ? M L M + E J .</formula><p>In <ref type="figure" target="#fig_3">Fig. 15</ref> we visualize the pushing and pulling terms used in the SMPLify-XMC objective. We use 6 PCA components for the hand pose space and initialize the fitting with a mean hand pose. In contrast to SMPLify-X we do not ignore hip joints and double the joint weights for knees and elbows. Before optimization, we resize images and keypoints to a maximum height or width of 500 pixel. Similar to SMPLify-X we use the PyTorch implementation of fast L-BFGS with strong Wolf line search as the optimizer <ref type="bibr" target="#b27">[28]</ref>. We do not use the VPoser pose prior for SMPLify-XMC because we have a strong prior from the presented pose.</p><p>We notice that the presented global orientation is not always mimicked well. For example, in row 4 of <ref type="figure">Fig.</ref> 14 the presented global orientation has a 60 degree rotation, whereas the mimicked image is taken from a frontal view. To better initialize the optimization, we select the best body orientation, ? g , among the seven presented ones based on their re-projection errors; then we compute the camera translation by again minimizing the re-projection error. We set the initial focal length, f x and f y , to 2170, which is the average of available EXIF data. These values, along with mean shape and presented pose are used to initialize the optimization.</p><p>In addition, SMPL and SMPL-X have not been trained to avoid self intersection. Therefore, we identify seven body segments that tend to intersect themselves, e.g. torso and upper arms (see <ref type="figure" target="#fig_4">Fig. 16</ref>). We test each segment for self intersection and thereby filter irrelevant intersections from M I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTP Dataset Details.</head><p>We sample meshes from 3DCP Scan, 3DCP Mocap., and AGORA <ref type="bibr" target="#b35">[35]</ref> to comprise the presented meshes in MTP datatset. In total, we present 1653 different meshes, from which 1498 (90%) are contact poses following Definition 3.1 in the main document. Of the 1653 meshes, 110 meshes are from 3DCP Scan, 1304 meshes are from 3DCP Mocap., and 159 are from AGORA. We collect at least one image for each mesh. From the 3731 collected images, 3421 (92%) images show a person mimicking a contact pose. <ref type="figure" target="#fig_5">Figure 17</ref> shows how many image we collected per subset.</p><p>A.3. Discrete Self-Contact (DSC) Data.</p><p>Image selection. Discrete self-contact annotation may be ambiguous and we find some annotations that we do <ref type="bibr">Figure 19</ref>. RGB images from 3DCP Scan Scan test set. A subject performing a pose with self-contact in a 3D body scanner. not consider to be functional self-contact. For example, in <ref type="figure" target="#fig_6">Fig. 18</ref>, some annotators label the left lower arm and left upper arm to be in contact, because of the slight skin touching at the elbow; we do not treat these as in self-contact. Therefore, we leverage the kinematic tree structure provided by SMPL-X and, in order to train TUCH, ignore the following annotations: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TUCH</head><p>Here we provide details of the SMPLify-XMC and SMPLify-DC methods and how we apply them on MTP and DSC data respectively.</p><p>SMPLify-XMC is explained in Sec. 4.2 of the main paper. It is applied, before the training, to all MTP images to obtain gender-specific pseudo ground-truth SMPL-X fits. To use these fits for TUCH training, two preprocessing steps are necessary. First, they are converted to neutral SMPL fits. Second, we transform the converted SMPL fits to the camera coordinate frame estimated during SMPLify-XMC. This is necessary since SPIN assumes an identity camera rotation matrix. After that, the data is treated as ground truth during training, which means we apply the regressor loss directly on the converted SMPL pose and shape parameters without in-the-loop fitting.</p><p>On the contrary, SMPLify-DC is applied during TUCH training to images with discrete self-contact annotations. We run 10 iterations of SMPLify-DC for each image in a mini batch.</p><p>MTP and the DeepFashion subset of DSC do not have ground-truth 2D keypoints but we find OpenPose detections good enough in both cases. For the 2D re-projection loss, we use ground-truth keypoints (if available) and OpenPose detections weighted by the detection confidence. Each mini batch consists of 50% DSC and 50% MTP data.</p><p>Implementation details: We initialize our regression network with SPIN weights <ref type="bibr" target="#b23">[24]</ref>. We use the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> and a learning rate of 1e ? 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TUCH EX</head><p>One disadvantage of training with fitting in the loop is that it is relatively slow. As an alternative, we also explore Exemplar Fine-Tuning (EFT) <ref type="bibr" target="#b17">[18]</ref>, which is a regression based method for fitting 3D meshes to a single image. The fitted SMPL meshes may then be used as pseudo annotations to train a regressor without in-the-loop optimization. With this approach, the authors train HMR-EFT, with which they achieve good results on 3DPW and MPI-INF-3DHP.</p><p>The idea of using discrete contact annotations is not limited to optimization based approaches. We show that they can also be applied in combination with EFT. Specifically, we extend the regressor loss of EFT with the contact terms from SMPLify-DC. We denote such an "EFT + contact loss" approach as EFT-C. Note that the original EFT loss uses a 2D orientation term to match the lower legs orientation, which we do not use here.</p><p>Each image in DSC is then paired with a pre-computed pseudo ground truth from EFT-C, and we denote the dataset as [DSC] EFT-C . Then, we finetune the HMR-EFT network on MTP, [DSC] EFT-C , as well as other training data from <ref type="bibr" target="#b17">[18]</ref>. This new model is called TUCH with EXemplar Finetuning, TUCH EX . Unlike TUCH that still performs SMPLify-DC in the training loop, TUCH EX is supervised only by pre-computed fits so it can be trained faster.</p><p>Implementation details. We initialize our network with state-of-the-art HMR-EFT weights. We train TUCH EX on [COCO-All] EFT (CAE), H36M, MPI-INF-3DHP (MI), [DSC] EFT-C , and MTP. [COCO-All] EFT denotes the COCO dataset after EFT processing, as described in <ref type="bibr" target="#b17">[18]</ref>. In each batch we use a 10% CAE, 20% H36M, 10% MI, 20% 3DPW, 20% [DSC] EFT-C , and 20% MTP. The remaining details are the same as in the TUCH implementation. For the DSC dataset, we only consider images where the full body is visible. To identify these images, we test whether the OpenPose detection confidence of ankles, hips, shoulders, and knees is ? 0.2. We also ignore discrete contact annotations for connected body parts, as defined in A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>3DCP Scan test images. During the scanning process when creating 3DCP Scan, we also take RGB photos of sub- jects being scanned, as shown in <ref type="figure" target="#fig_7">Figure 19</ref>. These images have high-fidelity ground-truth poses and shapes from the registration process described in Sec. A.1.1, making them a good test set for evaluation purposes. It is worth noting again that TUCH has never seen these images or subjects, but the contact poses were mimicked in creation of MTP, which is used in training TUCH.</p><p>TUCH. In <ref type="figure">Fig. 20</ref> we visualize the improvement of TUCH over SPIN qualitatively. One can see that TUCH reconstructs bodies with better self-contact and less interpenetration (row 1 and row 2). <ref type="figure">Fig. 21</ref>, on the other hand, shows examples where SPIN is better than TUCH. Four of the images in <ref type="figure">Fig. 21</ref> do not show the full body (rows 3, 4, 5, and 8). A possible reason why SPIN is better than TUCH in these cases is that MTP images always show the full body of a person, thus TUCH could be more sensitive to occlusion than SPIN.</p><p>We also evaluate the contribution of MTP data by finetuning SPIN only with it. The results are reported in <ref type="table">Table 7</ref>, where TUCH (MTP+DSC) is the same as reported in <ref type="table">Table 3</ref> of the main paper. This experiment shows that MTP data alone is already sufficient to significantly improve state-of-the-art (SOTA) methods on 3DPW benchmarks. This suggests that the MTP approach is a useful new tool for gathering data to train neural networks.</p><p>TUCH EX .</p><p>For an additional comparison with SOTA EFT <ref type="bibr" target="#b17">[18]</ref>, we evaluate our TUCH EX model on the same datasets (3DPW, MPI-INF-3DHP (MI), and 3DCP Scan) with error measures (MPJPE, PA-MPJPE, and MV2VE) like TUCH, see <ref type="table">Tables  6, 8</ref>, and 9.</p><p>The MPJPE of TUCH EX improves over HMR-EFT when evaluated on 3DPW. PA-MPJPE improves for contact poses and is overall on-par. Also the results on MPI-INF-3DHP improve. For the 3DCP Scan test set, PA-MPJPE improves. This shows that our data can not only be used with optimization based approaches, but also with exemplar finetuning, and that it allows us to improve the latest models in terms of estimating poses with contact. <ref type="figure">Figure 20</ref>. Qualitative results on the self-contact subset of 3DPW. We find all images with an improvement on MPJPE and PA-MPJPE ? 10 mm. From this subset, we select interesting poses. Left column, RGB image for reference. In blue, TUCH result and in violet, the SPIN result. <ref type="figure">Figure 21</ref>. Qualitative results on the self-contact subset of 3DPW. We find all images where SPIN is better than TUCH by at least 10 mm for MPJPE and PA-MPJPE. From this subset, we select interesting poses. Left column, RGB image for reference. In blue, TUCH result and in violet, the SPIN result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 . 1 .</head><label>31</label><figDesc>Given a mesh M with vertices M V , we define two vertices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 )</head><label>4</label><figDesc>. First, using meshes from 3DCP as examples, workers on AMT are asked to mimic the pose as accurately as possible while someone takes their photo showing the full</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Mimic-The-Pose (MTP) dataset. MTP is built via: (1) collecting many 3D meshes that exhibit self-contact. In grey, new 3D scans in self-contact poses, in brown self-contact poses optimized from AMASS mocap data. (2) collecting images in the wild, by asking workers on AMT to mimic poses and contacts. (3) the presented meshes are refined via SMPLify-XMC to match the image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>MTP results. Meshes presented to AMT workers (blue) and the images they submitted with OpenPose keypoints overlaid. In grey, the pseudo ground-truth meshes computed by SMPLify-XMC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>DSC dataset. Image with discrete contact annotation on the left. Right: DSC signature with K = 24 regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Initial wrong contact (left) from the regressor is fixed by SMPLify-DC after 5 (middle) and 10 (right) iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Impact of discrete self-contact labels in human pose estimation. Body parts labeled in contact are shown in the same color. First row shows an initial SPIN estimate, second row the SMPLify fit, third row the SMPLify-DC fit after 20 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Average MPJPE difference (SPIN -TUCH), evaluated on the self-contact subset of 3DPW. The axes show labels for the DSC regions. Green indicates that TUCH has a lower error than SPIN on average across all poses with the corresponding regions in contact. The circle size represents the number of images per region. Regions with small circle sizes are less common.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Sample poses from 20 unique contact signatures, S. We apply S to select interesting self-contact poses in AMASS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Hand on body prior. Dark blue indicates small distances to body on average across all registrations where hands are close to the body. The prior is identical for left and right hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .</head><label>14</label><figDesc>Presentation format and examples of mimicked poses from the MTP data set. On the left side, the presented pose with contact highlighted in blue. Humans mimicking the poses on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 .</head><label>17</label><figDesc>Image count in MTP Dataset per 3DCP subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 .</head><label>18</label><figDesc>Discrete self-contact can be challenging to annotate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .</head><label>15</label><figDesc>Functions to regulate the self-contact pushing and pulling term in SMPLify-XMC. f1 is used in LC , f2 is used in LP . The parameters ensure that inside vertices are pushed out quickly, while vertices in contact are pulled together as long as they are close enough.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>left hand -left lower arm, left lower arm -left elbow, left lower arm -left upper arm, left elbow -left upper arm, left upper arm -torso, left foot -left lower leg, left lower leg -left knee, left lower leg -left upper leg, left knee -left upper leg, right hand -right lower arm, right lower arm -right elbow, right lower arm -right upper arm, right elbow -right upper arm, right upper arm -torso, right foot -right lower leg, right lower leg -right knee, right lower leg-right upper leg, right knee -right upper leg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Evaluation on 3DPW and MPI-INF-3DHP (MI). Bold numbers indicate the best result; units are mm. We report the EFT result denoted in their publication when 3DPW was not part of the training data. Please note that SPIN is trained on MI, but we do not include MI in the fine-tuning set. MI contains mostly indoor lab sequences (100% train, 75% test), while DSC and MTP contain only in-the-wild images. This domain gap likely explains the decreased performance in PA-MPJPE. Evaluation on 3DCP Scan. Numbers are in mm. Note that in contrast to TUCH, this version of SPIN did not see poses in the MTP dataset during training. Please seeTable 5and the corresponding text for an ablation study.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MPJPE</cell><cell cols="2">PA-MPJPE</cell></row><row><cell></cell><cell cols="5">3DPW MI 3DPW MI</cell></row><row><cell cols="6">SPIN [24] 96.9 105.2 59.2 67.5</cell></row><row><cell cols="2">EFT [18]</cell><cell>-</cell><cell>-</cell><cell cols="2">54.2 68.0</cell></row><row><cell cols="2">TUCH</cell><cell cols="4">84.9 101.2 55.5 68.6</cell></row><row><cell></cell><cell cols="5">MPJPE PA-MPJPE MV2VE</cell></row><row><cell cols="3">SPIN [24] 79.7</cell><cell>50.6</cell><cell></cell><cell>95.7</cell></row><row><cell cols="3">EFT [18] 71.4</cell><cell>48.3</cell><cell></cell><cell>83.9</cell></row><row><cell cols="2">TUCH</cell><cell>69.5</cell><cell>42.5</cell><cell></cell><cell>81.5</cell></row><row><cell></cell><cell>MPJPE</cell><cell></cell><cell></cell><cell></cell><cell>PA-MPJPE</cell></row><row><cell cols="6">contact no contact unclear total contact no contact unclear total</cell></row><row><cell>SPIN 100.2</cell><cell>95.5</cell><cell cols="3">96.7 96.9 59.1</cell><cell>61.7</cell><cell>55.7 59.2</cell></row><row><cell>TUCH 85.1</cell><cell>86.6</cell><cell cols="3">81.9 84.9 54.1</cell><cell>58.6</cell><cell>51.2 55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Evaluation of TUCH for contact classes in 3DPW. Num- bers are in mm. See text.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>SPIN</cell><cell>S+</cell><cell>S++</cell><cell>TUCH</cell></row><row><cell>3DPW</cell><cell>96.9/ 59.2</cell><cell>96.1/ 61.4</cell><cell>85.0/ 56.3</cell><cell>84.9/ 55.5</cell></row><row><cell cols="2">3DCP Scan 82.2/ 52.1</cell><cell>86.9/ 52.3</cell><cell>74.8/ 45.7</cell><cell>75.2/ 45.4</cell></row><row><cell>MI</cell><cell cols="4">105.2/ 67.5 105.8/ 69.4 103.1/ 69.0 101.2/ 68.6</cell></row></table><note>. MPJPE/PA-MPJPE (mm) to examine the impact of data and algorithm on 3DPW, 3DCP Scan, and MPI-INF-3DHP (MI).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Finetuning Data MPJPE ? PA-MPJPE ? contact no contact unclear total contact no contact unclear total TUCHEX CAE + H36M + MI + 3DPW + [DSC]EFT-C + MTP 82.8 83.2 80.3 82.3 50.4 54.1 48.7 51.7 Evaluation of TUCHEX for contact classes. CAE = [COCO-All]EFT as denoted in [18]. Bold numbers indicate the better a result. MPJPE ? PA-MPJPE ? MV2VE ? HMR-EFT [18] 71.4 Ablation of MTP data and DSC data. MPJPE ? PA-MPJPE ? 3DPW MI 3DPW MI HMR-EFT [18] 85.3 105.3 51.7 68.4 TUCH EX 82.3 101.5 51.7 66.4 Training with 3DPW. Evaluation on 3DPW and MPI-INF-3DHP (MI). We report MPJPE and PA-MPJPE for different subsets of our data set.</figDesc><table><row><cell>HMR-EFT [18]</cell><cell></cell><cell>-</cell><cell>88.3</cell><cell>84.6</cell><cell>83.6 85.3 52.1</cell><cell>53.3</cell><cell>48.5 51.7</cell></row><row><cell></cell><cell></cell><cell>48.3</cell><cell>83.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TUCH EX</cell><cell>73.4</cell><cell>43.3</cell><cell>82.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 9. Evaluation on 3DCP Scan test images. We report MPJPE,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PA-MPJPE, and MV2VE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MPJPE ? PA-MPJPE ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPIN</cell><cell></cell><cell>96.9</cell><cell>59.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TUCH (MTP)</cell><cell></cell><cell>88.7</cell><cell>57.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TUCH (MTP+DSC)</cell><cell>84.9</cell><cell>55.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We denote loss terms defined in prior work as E while ours as L.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eman</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abd</forename><surname>El Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kseniya</forename><surname>Shabayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rig</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gusev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01462</idno>
		<title level="m">Djamila Aouada, and Bjorn Ottersten. A survey on deep learning advances on different 3d data representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SCAPE: Shape Completion and Animation of PEople</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3D pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3794" to="3801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic FAUST: Registering human bodies in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5573" to="5582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Numerical geometry of non-rigid shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three-Dimensional reconstruction of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7212" to="7221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning complex 3D human self-contact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time human pose tracking from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="738" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context and observation driven latent variable model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trista</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A statistical model of human pose and body shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sunkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (CGF)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resolving 3D human pose ambiguities with 3D scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2282" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coregistration: Simultaneous alignment and modeling of articulated 3D shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Rachlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust inside-outside segmentation using generalized winding numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Kavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shape2Pose: Human-centric shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tracking people interacting with objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face touching: A frequent habit that has implications for hand hygiene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gralton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary-Louise</forename><surname>Mclaws</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Infect Control</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="114" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Determination of 3D human body postures from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating 3D motion and forces of person-object interactions from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongmian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8640" to="8649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pictorial human spaces: How well do humans perceive a 3d articulated pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Fifth International Conference on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">XNect: Real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting hands and recognizing physical contact in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supreeth</forename><surname>Narasimhaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao Paul</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2345" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dyna: A model of dynamic human shape in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>120:1- 120:14</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The caesar project: a 3-d surface anthropometry survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Robinette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="380" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>245:1- 245:17</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Open dynamics engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning invariance through imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Spiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2729" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">and Odest Chadwicke Jenkins. Dynamical simulation priors for human motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="52" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10957" to="10966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monoclothcap: Towards temporally coherent clothing capture from monocular rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene constraints-aided tracking of human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanobu</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsutoshi</forename><surname>Yagishita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A sampling approach to generating closely interacting 3d pose-pairs from 2d annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Edmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2217" to="2227" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
