<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MISSING DATA IMPUTATION FOR SUPERVISED LEARNING ?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-07">7 Aug 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Poulos</surname></persName>
							<email>poulos@berkeley.edu.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Political Science</orgName>
								<address>
									<addrLine>210 Barrows Hall #1950</addrLine>
									<postCode>94720-1950</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MISSING DATA IMPUTATION FOR SUPERVISED LEARNING ?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-07">7 Aug 2018</date>
						</imprint>
					</monogr>
					<note>The code used for this project is available at https://github.com/rafaelvalle/MDI. The online Supplementary Material is available at Isabelle Guyon for the idea for the paper and Joan Bruna and seminar participants at the University of California, Berkeley for comments. Poulos acknowledges support from the National Science Foundation Graduate Research Fellowship [grant number DGE 1106400]. ? Corresponding author mailing address: 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>decision trees</term>
					<term>imputation methods</term>
					<term>missing data</term>
					<term>perturbation</term>
					<term>random forests ?</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Missing data imputation can help improve the performance of prediction models in situations where missing data hide useful information. This paper compares methods for imputing missing categorical data for supervised classification tasks. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different levels of additional missing-data perturbation. We show imputation methods can increase predictive accuracy in the presence of missing-data perturbation, which can actually improve prediction accuracy by regularizing the classifier. We achieve the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning has become an increasingly attractive methodology and proven to be effective in social science applications, such as studies of international and civil conflict <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> and election fraud <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. For supervised classification tasks, the objective is to fit a model on labeled training data in order to categorize new examples. However, the ability of researchers to accurately fit a model and yield unbiased estimates may be compromised by missing data.</p><p>Our objective is to compare the out-of-sample performance of three popular machine learning classifiers -decision trees, random forests, and neural networks -trained on imputed or non-imputed (i.e., one-hot encoded) machine learning benchmark datasets that contain various degrees of missing-data perturbation. Researchers analyzing survey data typically choose decision trees or random forests for classification tasks, largely because these models do not require imputing missing data nor encoding categorical variables, unlike neural networks or other classifiers.</p><p>The primary contribution of this paper is to provide guidance to applied researchers on how to handle missing data for supervised learning tasks. First, we show that imputation methods can increase predictive accuracy in the presence of missing-data perturbation. Second, we show that adding missing-data perturbation prior to imputation can actually improve prediction accuracy by regularizing the classifier. We achieve the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation. Lastly, we show that classifiers trained on one-hot encoded data generally yield higher predictive accuracy when the data are not additionally perturbed. For example, a simple one-hot encoded random forests outperforms the state-of-the-art on the Congressional Voting Records (CVRs) dataset with no missing-data perturbation. This manuscript is organized as follows: Section 2 describes missing data mechanisms and imputation methods; Section 3 describes our experiments on two benchmark datasets and discusses the results; Section 4 concludes and offers possibilities for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Missing data and imputation methods</head><p>In this section, we describe the missing data mechanisms underlying patterns of missing data common to survey-based datasets. We then review popular methods of handling missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Missing data patterns and mechanisms</head><p>It is important to first distinguish between missing data patterns, which describe observed and missing values, and missing data mechanisms, which relate the probability of missingness.</p><p>[12, Chap. 1]. Common missing data patterns in surveys typically include unit nonresponse, where a subset of participants do not complete the survey, and item nonresponse, where missing values are concentrated on particular questions. In opinion polls, nonresponse may reflect either refusal to reveal a preference or lack of a preference <ref type="bibr" target="#b3">[4]</ref>.</p><p>Following the notation of Little and Rubin <ref type="bibr" target="#b11">[12]</ref>, let Y = y ij be a (n ? K) dataset with each row y i = (y i1 , . . . , y iK ) the set of y ij values of feature Y j for example i. Let Y obs define observed values of Y and Y mis define missing values. Define the missing data identity matrix M = m ij , where m ij = 1 if y ij is missing and m ij = 0 if y ij is nonmissing. The missing data mechanism is missing completely at random (MCAR) if the probability of missingness is independent of the data, or</p><formula xml:id="formula_0">f (M |Y, ?) = f (M |?) ? Y, ?,</formula><p>where ? denotes unknown parameters. The missing at random (MAR) assumption is less restrictive than MCAR in that the probability of missingness depends only on the observed data, f (M |Y, ?) = f (M |Y obs , ?) for all Y mis , ?. The missing not at random <ref type="bibr">(MNAR)</ref> assumption is that the probability of missingness may also depend on the unobserved data,</p><formula xml:id="formula_1">f (M |Y, ?) = f (M |Y mis , ?) for all Y mis , ?.</formula><p>Researchers typically assume data is MAR, which mitigates the identifiability problems of MNAR because the probability of missingness depends on data that are observed on all individuals [21, Chap. 6]. Explicit modeling methods assume the data are MAR while implicit modeling methods, which are algorithmic in nature and rely only on implicit assumptions, generally do not assume the underlying missing data mechanism. Implicit methods include random replacement, where an example with missing data is randomly replaced with another complete example randomly sampled, and hot deck imputation, where missing values are replaced by "similar" nonmissing values. Hot deck imputation can be implemented by computing the k-nearest-neighbors (k-NN) of an example with missing data and assigning the mode of the k-neighbors to the missing data. Batista and Monard <ref type="bibr" target="#b0">[1]</ref> use this procedure and find k-NN imputation can outperform summary statistic imputation and internal methods used by decision trees to treat missing data. <ref type="bibr" target="#b2">3</ref> In related work, Silva et al. <ref type="bibr" target="#b18">[19]</ref> empirically compare imputation using neural networks with mean/mode imputation, regression models (logistic regression and multiple linear regression), and hot deck, finding neural networks performs the best on datasets with categorical variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Imputation methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">One-hot encoding</head><p>Another natural strategy in dealing with missing data for supervised learning problems is one-hot encoding. Instead of imputing missing data, one-hot encoding creates a binary feature vector that indicates missing values. For categorical features, one-hot encoding simply treats a missing value symbol (e.g, "?") as a category when the categorical features are binarized.</p><p>For continuous features, missing values are set to a constant value and a missingness indicator is added to the feature space. One-hot encoding for missing data yields biased estimates when the features are correlated, which is often the case with survey data, even when data are MCAR <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we describe our experiment on two machine learning benchmark datasets with missing categorical data, comparing three popular classifiers -neural networks, decision trees, and random forests-trained on either one-hot encoded or imputed data with different degrees of MCAR perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Benchmark datasets</head><p>We experiment on two benchmark datasets from the UCI Machine Learning Repository:</p><p>the Adult dataset and CVRs dataset <ref type="bibr" target="#b10">[11]</ref>. The dataset contains 16 categorical features with three possible values: "yea", "nay", and missing. The prediction task is to classify party affiliation (Republican or Democrat). In contrast to the Adult dataset, in which only a few features are highly correlated, many of the roll call votes in the CVRs dataset exhibit strong correlations ( <ref type="figure" target="#fig_4">Figures SM-1 and SM-2)</ref>.</p><p>The state-of-the-art for the Adult dataset is a Naive Bayes classifier that achieves a 14.05% generalization error after removing examples with missing values <ref type="bibr" target="#b8">[9]</ref>. The CVRs dataset donor claims to achieve a 5-10% error rate using an incremental decision tree algorithm called STAGGER, although it is unknown to the authors what train-test split is used or how missing values are handled <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Patterns of missing data</head><p>Uncovering missing data patterns in the datasets will help to identify possible missing data mechanisms and select appropriate imputation methods. <ref type="figure">Figure</ref>   <ref type="figure">Figure SM-4)</ref>. About a quarter of missing data is in South Africa, which was a controversial amendment to amend the Export Administration Act to bar U.S. exports to South Africa's apartheid regime. Twelve percent of missing data is in the feature Water, which is a water projects authorizations bill, and 7% of missing data rests in the feature Exports, which is a tariff bill. The data are unlikely to be MCAR because 12% of the data are missing in just South Africa and less than 1% of examples are missing across all features. It is most likely in this case that the CVRs data are MNAR because the probability of missing a vote or voting present on one important bill should not theoretically be influenced by observed votes on other important bills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Preprocessing</head><p>After randomly splitting each dataset 2/3 for training and 1/3 for testing, we perturb the training data so that the proportion of missing values in the set of categorical features Y cat follows ? = {0.1, 0.2, 0.3, 0.4} according to the MCAR mechanism</p><formula xml:id="formula_2">Pr(M = 1|Y cat , ?) = ? for all Y cat .</formula><p>We use missing-data perturbation to study the impact of larger amounts of missing data; however, it is also a form of dropout noise that can be used to control overfitting during the training process and improve the generalizability of the model <ref type="bibr" target="#b21">[22]</ref>.</p><p>After one-hot encoding the categorical variables in the training data, we implement each of the following imputation techniques, discussed in Section 2.2: k-NN, prediction model (logistic regression, random forests, or SVMs), mode replacement, and random replacement. We then standardize continuous features by subtracting the mean and dividing by the standard deviation of the feature. The test data is preprocessed in the same manner, with the exception that we do not perturb categorical features in the test data. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model training</head><p>We train three different classifiers on the preprocessed data: decision trees, random forests, and neural networks. The neural networks consist of four layers, each of the two hidden layers having 1024 nodes, and updates with the adaptive learning rate method Adadelta <ref type="bibr" target="#b23">[24]</ref>.</p><p>We explore the hyperparameter space -momentum schedule, dropout regularization, and learning rate -using Bayesian optimization <ref type="bibr" target="#b19">[20]</ref>, which selects optimal models using the mean training error rate as our objective function. <ref type="figure">Figure SM-5</ref> shows the exploration of hyperparameter space during Bayesian optimization for both datasets. Random forests and decision trees are trained with preselected hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Results</head><p>We assess the performance of the classifiers in terms of test set error rate on one-hot encoded or imputed data and for various levels of MCAR perturbation. The results on the Adult dataset and CVRs dataset are plotted in <ref type="figure" target="#fig_4">Figures 1 and 2</ref>, respectively, with error bars representing ?1 standard deviation from the test error rate.</p><p>One-hot-encoded decision trees outperforms the state-of-the-art on the CVRs dataset by over 2% (0.027 ? 0.006). The neural networks classifier trained on data imputed with k-NN yields the lowest generalization error (0.144 ? 0.06) on the Adult dataset with 10% of the categorical feature values perturbed, which is comparable to the state-of-the-art. In comparison, a random forests classifier trained on non-perturbed and one-hot encoded data yields a test error rate of 0.152 ? 0.02. This comparison shows that the classifiers can overfit the data and, in the case of imputed models, perturbation improves prediction accuracy by regularizing the classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper compares methods for imputing missing categorical data for supervised classification tasks. We compare the out-of-sample performance of neural networks against decision tree and random forest classifiers trained on datasets with one-hot encoded or imputed data, across different levels of MCAR-perturbed training data. Our results are comparable to the state-of-the-art on the Adult dataset using a neural networks classifier, k-NN imputation, and MCAR data-perturbation. k-NN imputation likely performs well in this case because it is an implicit modeling method that does not assume the underlying missing data mechanism. This result is in line with Batista and Monard <ref type="bibr" target="#b0">[1]</ref>, who find k-NN imputation can outperform explicit modeling methods for supervised learning tasks.</p><p>We conclude from the results that the performance of the classifiers and imputation strategies generally depend on the nature and proportion of missing data. For the Adult dataset, neural networks trained on imputed data generally outperform other classifiers and imputation methods across different ratios of perturbed data, while classifiers trained on one-hot encoded data perform very poorly on perturbed training data.</p><p>The results of the present study show that perturbation can help increase predictive accuracy for imputed models, but not one-hot encoded models. Future work can identify the conditions under which missing-data perturbation can improve prediction accuracy.</p><p>Interesting extensions of this paper include evaluating the benefits of using missing-data perturbation over more popular regularization techniques such as dropout training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23</ref>].  For neural networks, prediction intervals are obtained from the standard deviation of test set errors of neural networks trained with different convergences <ref type="bibr" target="#b4">[5]</ref>. For random forests and decision trees, prediction intervals follow from the variation created by varying the maximum depth of the decision trees, and for random forests, the number of trees and decision rule for the number of features to consider when looking for the best split.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Complete-case analysis (i.e., discarding examples with missing values) wastes information and biases estimates unless the missing data are MCAR. Since there is no way to distinguish whether the missing data are MCAR or MNAR from the observed data, a natural strategy is to impute missing values and then proceed as if the imputed values are true values. Imputation methods that rely on explicit model assumptions include mean or mode replacement, which substitutes missing values with the mean (for quantitative features) or mode (for qualitative features) of the feature vector, and prediction model imputation, which replaces missing values with the predicted values from a regression of Y mis on Y obs .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The Adult dataset contains N = 48, 842 examples and 14 features (6 continuous and 8 categorical). The prediction task is to determine whether a person makes over $50,000 a year. The CVRs dataset contains N = 435 examples, each the voting record of a member of the U.S. House of Representatives for 16 key roll call votes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SM-3 analyzes patterns of missing data in the Adult dataset, in which 7% of the examples contain missing values. Missing data in the Adult dataset is due to item nonresponse, as missing values are concentrated in three of the categorical features -Work class, Occupation, and Native country-and no examples contain entirely missing data. It is unlikely that the data are MCAR because observations that are missing in Work class are also missing in Occupation (about 6% of examples have missing values in both). Missing values in the CVRs dataset are not simply unknown, but represent values other up-or-down votes, such as voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known. Close to half of the CVRs data contains missing values, which are present in every feature (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Overall, the results show imputation methods can increase predictive accuracy in the presence of missing-data perturbation. For both datasets, one-hot encoded models trained in the absence of perturbation perform as well as imputed models trained on non-perturbed data. In the case of the Adult dataset, imputation clearly improves accuracy in the presence of MCAR-perturbed data. In contrast, each of the three classifiers trained on the one-hot encoded CVRs dataset perform relatively well across different levels of perturbation. The general pattern of results hold when the classifiers are trained on MNAR-perturbed data (Figures SM-6and SM-7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Error rates on the Adult test set with (bottom) and without (top) missing data imputation, for various levels of MCAR-perturbed categorical training features (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Error rates on the CVRs test set with (bottom) and without (top) missing data imputation. See footnotes for Figure 1.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"> Li et al. [10]  propose a hot deck imputation method based on fuzzy k-means.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">When imputing the missing data with mode replacement, we use the training set mode. We also use the training set mean and standard deviation to standardize test set features.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An analysis of four missing data treatment methods for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eapa</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Carolina</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="519" to="533" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving quantitative studies of international conflict: A conjecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Langche</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fraudulent democracy? An analysis of Argentina&apos;s infamous decade using supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Cant?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebasti?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saiegh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="433" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prevention and treatment of item nonresponse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joop</forename><surname>Edith D De Leeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Official Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="176" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Practical confidence and prediction intervals for prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Wiegerinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 9: Proceedingss of the 1996 Conference</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical evaluation of explanations for state repression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary M Jones</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="661" to="687" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indicator and stratification methods for missing explanatory variables in multiple linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">433</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of naive-Bayes classifiers: A decision-tree hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards missing data imputation: a study of fuzzy k-means clustering method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitender</forename><surname>Deogun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Spaulding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Shuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Rough Sets and Current Trends in Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="573" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical Analysis with Missing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning with marginalized corrupted features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="410" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An informed forensics approach to detecting vote irregularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Olivella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crisp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="488" to="505" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing random forest with logistic regression for predicting class-imbalanced civil war onset data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Muchlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Siroky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="103" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Handling &quot;don&apos;t know&quot; survey responses: the case of the Slovenian plebiscite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Donald B Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasja</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vehovar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">431</biblScope>
			<biblScope unit="page" from="822" to="828" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Concept Acquisition Through Representational Adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlimmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Science, University of California, Irvine</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental learning from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granger</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="354" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Missing value imputation on missing completely at random data using multilayer perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther-Lydia</forename><surname>Silva-Ram?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Pino-Mej?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Coello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a-Dolores Cubiles-De-La</forename><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semiparametric Theory and Missing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tsiatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
