<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wollongong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wollongong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discriminative image representations plays a vital role in long-tailed image classification because it can ease the classifier learning in imbalanced cases. Given the promising performance contrastive learning has shown recently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classification accuracy thereon. Specifically, we propose a novel hybrid network structure being composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classifiers, where the learning is progressively transited from feature learning to the classifier learning to embody the idea that better features make better classifiers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsupervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the intensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Extensive experiments on three long-tailed classification datasets demonstrate the advantage of the proposed contrastive learning based hybrid networks in long-tailed classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the real world, the image classes are normally presented in a long-tailed distribution <ref type="bibr" target="#b27">[25]</ref>. While some common classes (head classes) can have sufficient image samples, some uncommon or rare categories (tail classes) can be underrepresented by limited samples. The data imbalance poses great challenge to learning unbiased classifiers.</p><p>Most existing work addresses the data imbalance issue by mitigating the data shortage in tail classes in order to prevent the model from being dominated by the head classes. Typical methods include data re-sampling <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b6">4,</ref><ref type="bibr" target="#b30">28]</ref>, loss re-weighting <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b32">30,</ref><ref type="bibr" target="#b35">33]</ref>, margin modification <ref type="bibr" target="#b5">[3]</ref>, and data augmentation <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b8">6,</ref><ref type="bibr" target="#b31">29]</ref>. Recently a new line of work is proposed which approaches long-tailed image classification by decoupling the representation learning and classifier learning into two stages <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b38">36]</ref>. The shared motivation of such work <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b38">36]</ref> is that image feature learning and classifier learning may favor different data sampling strategies and thus the focus thereon is to identify suitable sampling strategies for these two tasks. Specifically, they find under cross-entropy loss, random data sampling can benefit feature learning more while class-balanced sampling is a better option for classifier learning. Despite promising accuracy achieved, these methods leave the question of whether typical cross-entropy is an ideal loss for learning features from imbalanced data untouched. Intuitively, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the feature distribution learned from typical cross-entropy can be highly skewed, which can lead to biased classifiers <ref type="bibr" target="#b26">[24,</ref><ref type="bibr" target="#b14">12]</ref> that harm long-tailed classification.</p><p>In this work, we explore effective contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost long-tailed image classification. Specifically, we propose a novel hybrid network structure composed of a contrastive loss for learning image representations and a cross-entropy loss to learn classifiers. To embody the idea that better features make better classifiers, we follow a curriculum to progressively transit the learning from feature learning to classifier learning. We realize two variants of supervised contrastive learning strategies, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. By doing this, less skewed features and consequently less biased classifiers are expected to be obtained.</p><p>The first contrastive learning we explore to learn features in imbalanced scenario is the recently proposed supervised contrastive (SC) learning <ref type="bibr" target="#b20">[18]</ref>, which is extended from the state-of-the-art unsupervised contrastive learning <ref type="bibr" target="#b7">[5]</ref> by incorporating different within-class samples as positives for each anchor. Following unsupervised contrastive learning <ref type="bibr" target="#b7">[5,</ref><ref type="bibr" target="#b11">9]</ref> that have two independent stages for feature learning and classifier learning, the original SC learning <ref type="bibr" target="#b20">[18]</ref> learns features using SC loss first and then freezes the features to learn classifiers. We argue in this paper such twostage learning may not be an optimal choice in fully supervised scenario, which can harm the compatibility of the features and classifiers. We propose a hybrid framework to jointly learn features and classifiers, and empirically demonstrate the advantage of our joint learning mode.</p><p>One issue of incorporating within-class positive samples in SC learning is that it leads to extra memory consumption. In SC learning <ref type="bibr" target="#b20">[18]</ref>, the distances to positives from the same class are contrasted with the distances to negatives from other classes, which results in memory consumption linear to the product of the positive size and negative size. Due to this, when under limited memory budget, the negative size needs to be shrunk. This can compromise the quality of the features learned from contrastive loss <ref type="bibr" target="#b7">[5]</ref>, especially when dealing with dataset that has a large number of classes, e.g., iNaturalist <ref type="bibr" target="#b13">[11]</ref>.</p><p>To address the aforementioned memory bottleneck from SC loss, we further propose a prototypical supervised contrastive (PSC) learning strategy, which shares the similar goal with standard SC learning but avoids explicitly sam-pling positives and negatives. In PSC learning, we learn a prototype for each class and force each sample to be pulled towards the prototype of its class and pushed away from prototypes of all the other classes. In this sense, the PSC strategy enables more flexible and efficient data sampling akin to softmax-based cross-entropy. It observes advantages when dealing with large-scale dataset under limited memory budget. In addition, the PSC loss has some other appealing properties that can benefit imbalanced classification, such as less sensitive to data sampling and the potential to capture finer within-class data distribution by using multiple prototypes per class.</p><p>Experiments on three long-tailed image classification datasets demonstrate the proposed contrastive learning based hybrid networks can obviously outperform the cross-entropy based counterparts and establish new state-of-the-art longtailed image classification performance. The contributions of this work can be summarized as follows:</p><p>? We propose a novel hybrid network structure for longtailed image classification. The network is designed to be composed of a contrastive loss for feature learning and a cross-entropy loss for classifier learning. These two learning tasks are performed following a curriculum to embody the idea that better features can ease classifier learning.</p><p>? We explore effective supervised contrastive learning strategies to learn better features to boost long-tailed classification performance. A prototypical supervised contrastive (PSC) learning is proposed to resolve the memory bottleneck resulted from standard supervised contrastive (SC) learning.</p><p>? We unveil supervised contrastive learning can be a better substitute for typical cross-entropy loss for feature learning in long-tailed classification. Benefited from the better features learned, our hybrid network substantially outperforms the cross-entropy based counterparts.</p><p>Our code is publicly available at https://k-han.</p><p>github.io/HybridLT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is closely related to both long-tailed classification and contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Long-tailed image classification</head><p>Long-tailed classification is a long-standing research problem in machine learning, where the key is to overcome the data imbalance issue <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b18">16]</ref>. Given the great success deep neural networks have achieved in balanced classification tasks, increasing attention is being shifted to proposing neural networks based solutions for long-tailed classification. In this work, we mainly focus on the neural networks based approaches, which can be roughly divided into the following categories.</p><p>Data re-sampling Data re-sampling is a commonly used strategy to artificially balance the imbalanced data. Two types of re-sampling techniques are under-sampling <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b10">8]</ref> and over-sampling <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b33">31]</ref>. Under-sampling discards part of the data in head classes and over-sampling repetitively samples data from the tail classes. It is revealed that oversampling can lead to overfitting to the tail classes <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b30">28]</ref>. Under-sampling can potentially lose information about the head classes but it may yield good results if each sample of a head class is close to other samples of the same class <ref type="bibr" target="#b30">[28]</ref>.</p><p>Data augmentation As analyzed above, although oversampling enhances the chance to see more data from the tail classes, it does not generate new information and thus leads to overfitting. One remedy is to use strong data augmentation to enrich the tail classes. Existing work approaches this goal from different angles. The work in <ref type="bibr" target="#b31">[29]</ref> uses generative model to generate new samples for tail classes as convex combination of existing instances. Another line of studies attempt to transfer the information from head classes to tail classes. In <ref type="bibr" target="#b21">[19]</ref>, the authors generate data for tail classes by adding learnable noise to head samples. In another work <ref type="bibr" target="#b8">[6]</ref>, the authors decompose the feature maps of images as classgeneric features and class-specific features and compose new tailed data by combining class-generic features from the head image and class-specific features from a tail image. In <ref type="bibr" target="#b25">[23]</ref>, the intra-class angular variance is transferred from head classes to enlarge the diversity of tail classes.</p><p>Loss re-weighting Apart from the aforementioned databased re-balance strategies, another line of studies propose to mitigate the negative effects of data imbalance by modifying the loss functions. Loss re-weighting is one of the simple but effective ways to tailor the loss function for imbalanced classification, where the basic idea is to upweight the tailed samples and downweight the head samples in the loss function <ref type="bibr" target="#b19">[17]</ref>. The existing solutions differ mainly in how to define the weights for different classes. In classsensitive cross-entropy loss <ref type="bibr" target="#b16">[14]</ref>, the weight assigned to each class is inversely proportional to the number of samples. In class-balanced loss <ref type="bibr" target="#b9">[7]</ref>, the authors decide the re-weighting coefficients based on the real volumes of different classes, named effective numbers. In the work <ref type="bibr" target="#b32">[30]</ref>, the weights to the training examples are optimized to minimize the loss of a held-out evaluation set.</p><p>Margin modification It is revealed that the effect of loss re-weighting can diminish when the datasets are separable <ref type="bibr" target="#b4">[2]</ref>. An intuitive alternative is to shift the separator closer to a dominant class <ref type="bibr" target="#b29">[27]</ref>. In the work <ref type="bibr" target="#b5">[3]</ref>, the authors propose to integrate per-class margin into the cross-entropy loss. The margin is inversely proportional to the prior probability of a class and thus can enforce larger margins between a tail class and other classes. The work <ref type="bibr" target="#b35">[33]</ref> realizes the margin under an alternative motivation, which is to suppress the negative gradients resulted from head samples for each tailed sample.</p><p>Decoupled learning Decoupled learning is a recent line of methods towards imbalanced classification. To identify the specific contributions of different factors to the longtailed recognition capability, the work <ref type="bibr" target="#b17">[15]</ref> decouples longtailed classification into two separate stages: representation learning and classifier learning. They use cross-entropy as loss function for both of these two stages and conclude that feature learning favors random data sampling and classbalanced sampling is a better option for classifier learning. Parallel to this, the work <ref type="bibr" target="#b39">[37]</ref> obtains similar conclusions empirically. In addition, a bilateral-branch network is proposed in <ref type="bibr" target="#b39">[37]</ref>, where one branch uses random sampling to learn head data and the other branch uses revered sampling to emphasize tailed data. One common focus of these two works lies in choosing proper data sampling strategies for different learning tasks underpinning long-tailed classification. But both studies are limited to cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive learning</head><p>Recently, contrastive learning has shown great promise in unsupervised representation learning <ref type="bibr" target="#b7">[5,</ref><ref type="bibr" target="#b11">9]</ref>. The basic idea is to learn a hidden space in which the agreement between differently augmented views of the same image is maximized by contrasting to the agreement between different images. Some key components enable the success of contrastive loss in learning useful representations include proper data augmentations, a learnable nonlinear transformation between the representation and contrastive loss, and large batch size for negative data <ref type="bibr" target="#b7">[5]</ref>. Supervised contrastive (SC) learning <ref type="bibr" target="#b20">[18]</ref> is an extension to contrastive learning by incorporating the label information to compose positive and negative images. Following unsupervised feature learning, SC learning also adopts a two-stage learning fashion, where the first stage learns features by using contrastive loss and the second stage learns classifiers using cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Approach</head><p>In this section, we firstly present the framework for the contrastive learning based hybrid networks proposed for long-tailed classification. Then, we elaborate on the two supervised contrastive learning schemes used as part of the hybrid networks for image representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Hybrid Framework for Long-tailed Classification</head><p>Fig <ref type="figure">. 2</ref> shows the overview of the proposed hybrid framework for long-tailed image classification. The network consists of two branches: one contrastive learning branch for image representation learning and one cross-entropy driven branch for classifier learning. The feature learning branch image representation learning and one cross-entropy driven branch for classifier learning. The feature learning branch aims to learn a feature space which has property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-its s 2 R DC , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss functions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as</p><formula xml:id="formula_0">B SC = {x i , {x + i }, {x i }}.</formula><p>The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_1">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as at the beginning of the training, and then classifier learning gradually dominates the training.</p><formula xml:id="formula_2">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R D E for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R D S which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-network is:</p><formula xml:id="formula_3">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ),</formula><p>where ? is a weighting coefficient inversely proportiona the epoch number, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory sue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extens to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key ference between SC loss and UC loss lies in the com sition of the positive and negative samples of an anc image. In UC loss, the positive image is an alternativ augmented view of the anchor image. In SC loss, ap from the alternatively augmented counterpart, the positi also include some other images from the same class. In t paper, we unify all the positive images of an anchor tween these two learning branches to learn the image representation r 2 R D E for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R D S which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2</ref>.</p><formula xml:id="formula_4">x i {x + i } = {x j |y j = y i , i 6 = j} (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><formula xml:id="formula_5">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-ing branch directly take B CE = {{x i , y i }}. The network is:</p><formula xml:id="formula_6">L hybrid = ? ? L SCL (B</formula><p>where ? is a weighting co the epoch number, as sho</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contra sue</head><p>Supervised contrastiv to unsupervised contrast ference between SC loss sition of the positive an image. In UC loss, the augmented view of the from the alternatively aug also include some other im paper, we unify all the p A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-</p><formula xml:id="formula_7">{x + i } = {x j |y j = y i , i of</formula><formula xml:id="formula_8">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as  <ref type="figure">Figure 2</ref>. Overview of the proposed contrastive learning based hybrid network structure. The network is composed of a supervised contrastive learning (SCL) based feature learning branch and a cross-entropy (CE) loss based classifier learning branch. A backbone is shared between these two branches to extract image representation, after which a non-linear MLP fe(?) combined with`2 normalization is adopted to translate the image representation for contrastive loss, and a single linear layer fc(?) is applied on top of the image representation to predict classification logits. A curriculum is designed to control the weightings of these two branches during network training.</p><formula xml:id="formula_9">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>image representation learning and one cross-entropy driven branch for classifier learning. The feature learning branch aims to learn a feature space which has property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-its s 2 R DC , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss functions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as</p><formula xml:id="formula_10">B SC = {x i , {x + i }, {x i }}.</formula><p>The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_11">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as translate the image representation for contrastive loss, and a single linear layer fc(?) is applied on top of the image representation to predict classification logits. A curriculum is designed to control the weightings of these two branches during network training.</p><formula xml:id="formula_12">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>image representation learning and one cross-entropy driven branch for classifier learning. The feature learning branch aims to learn a feature space which has property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head fe(?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head fe(?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss LSCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer fc(?) to the image representation r to predict the class-wise log-its s 2 R DC , which are used to compute the cross-entropy loss LCE. Due to different natures of the two loss functions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point xi together with positive samples {x + i } = {xj|yi = yj, i 6 = j} from the same class and negative samples {x i } = {xj|yj 6 = yi} from other classes. The input batch of the feature learning branch is denoted as BSC = {xi, {x + i }, {x i }}. The classifier learning branch directly takes image and label pairs as input BCE = {{xi, yi}}. The final loss function for the hybrid network is:</p><formula xml:id="formula_13">Lhybrid = ? ? LSCL(BSC) + (1 ?) ? LCE(BCE), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor xi as aims to learn a feature space which has property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><formula xml:id="formula_14">{x + i } = {xj|yj = yi, i 6 = j} (</formula><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-tions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as B SC = {x i , {x + i }, {x i }}. The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_15">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as class compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><formula xml:id="formula_16">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as B SC = {x i , {x + i }, {x i }}. The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_17">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as fication logits. A curriculum is designed to control the weightings of these two branches during network training. e representation learning and one cross-entropy driven h for classifier learning. The feature learning branch to learn a feature space which has property of intracompactness and inter-class separability. The classifier ing branch is expected to learn less biased classifier on the discriminative features obtained from the sibbranch. To realize the idea that better features ease ifier learning and consequently result in more generble classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust eightings of these two branches during the training . Concretely, the feature learning plays a leading role beginning of the training, and then classifier learning ally dominates the training. backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared ben these two learning branches to learn the image reptation r 2 R DE for each image x. A projection head maps the image representation r into a vector repretion z 2 R DS which is more suitable for contrastive We implement this projection head f e (?) as a nonlinultiple-layer perceptron (MLP) with one hidden layer. projection module is proven important in improving presentation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 noration is applied to z in order that inner product can be as distance measurements. To avoid abuse of notations, s otherwise stated we use z as the normalized represenof x for contrastive loss computation. A supervised astive loss L SCL is applied on top of the normalized sentations for feature learning. The classifier learning h is simpler which applies a single linear layer f c (?) image representation r to predict the class-wise log-its s 2 R DC , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss functions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as</p><formula xml:id="formula_18">{x + i } = {x j |y j = y i , i 6 = j} (</formula><formula xml:id="formula_19">B SC = {x i , {x + i }, {x i }}.</formula><p>The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_20">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes ling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><formula xml:id="formula_21">{x + i } = {x j |y j = y i , i 6 = j} (</formula><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as B SC = {x i , {x + i }, {x i }}. The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_22">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes). The definitions contrastive loss LSCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer fc(?) to the image representation r to predict the class-wise log-also include some other images from the same class. In this paper, we unify all the positive images of an anchor xi as {x + i } = {xj|yj = yi, i 6 = j} (we assume different views of the same image have different indexes tation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes). The definitions contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes sentation z 2 R which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise log-sue Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes). The definitions 4 learning (SCL) based feature learning branch and a cross-entropy (CE) loss based classifier learning branch. A backbone is shared between these two branches to extract image representation, after which a non-linear MLP fe(?) combined with`2 normalization is adopted to translate the image representation for contrastive loss, and a single linear layer fc(?) is applied on top of the image representation to predict classification logits. A curriculum is designed to control the weightings of these two branches during network training.</p><p>image representation learning and one cross-entropy driven branch for classifier learning. The feature learning branch aims to learn a feature space which has property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b38">[36]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (?) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>.`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise logits s 2 R DC , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss func-tions, the feature learning and classifier learning branches have different data sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as</p><formula xml:id="formula_23">B SC = {x i , {x + i }, {x i }}.</formula><p>The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</p><formula xml:id="formula_24">L hybrid = ? ? L SCL (B SC ) + (1 ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i 6 = j} (we assume different views of the same image have different indexes). The definitions for positives and negatives of x i also apply to z i as {z + i } and {z i }. Assuming the minibatch size is N , the SC loss 4 <ref type="figure">Figure 2</ref>. Overview of the proposed contrastive learning based hybrid network structure. The network is composed of a supervised contrastive learning (SCL) based feature learning branch and a cross-entropy (CE) loss based classifier learning branch. A backbone is shared between these two branches to extract image representations, after which a non-linear MLP fe(?) combined with 2-normalization is adopted to translate the image representation for contrastive loss, and a single linear layer fc(?) is applied on top of the image representation to predict classification logits. A curriculum is designed to control the weightings of these two branches, i.e., ? and 1 ? ?, during network training. aims to learn a feature space which has the property of intraclass compactness and inter-class separability. The classifier learning branch is expected to learn less biased classifier based on the discriminative features obtained from the sibling branch. To realize the idea that better features ease classifier learning and consequently result in more generalizable classifiers, we follow a curriculum <ref type="bibr" target="#b39">[37]</ref> to adjust the weightings of these two branches during the training phase. Concretely, the feature learning plays a leading role at the beginning of the training, and then classifier learning gradually dominates the training.</p><p>A backbone network, e.g., ResNet <ref type="bibr" target="#b12">[10]</ref>, is shared between these two learning branches to learn the image representation r ? R D E for each image x. A projection head f e (?) maps the image representation r into a vector representation z ? R D S which is more suitable for contrastive loss. We implement this projection head f e (?) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it <ref type="bibr" target="#b7">[5]</ref>. Then, the 2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. After that, a supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (?) to the image representation r to predict the class-wise logits s ? R D C , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss functions, the feature learning and classifier learning branches have different data sampling strategies. The fea- </p><formula xml:id="formula_25">L hybrid = ? ? L SCL (B SC ) + (1 ? ?) ? L CE (B CE ), (1)</formula><p>where ? is a weighting coefficient inversely proportional to the epoch number, as shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised contrastive loss and its memory issue</head><p>Supervised contrastive (SC) loss <ref type="bibr" target="#b20">[18]</ref> is an extension to unsupervised contrastive (UC) loss <ref type="bibr" target="#b7">[5]</ref>. The key difference between SC loss and UC loss lies in the composition of the positive and negative samples of an anchor image. In UC loss, the positive image is an alternatively augmented view of the anchor image. In SC loss, apart from the alternatively augmented counterpart, the positives also include some other images from the same class. In this paper, we unify all the positive images of an anchor x i as {x + i } = {x j |y j = y i , i = j} (we assume different views of the same image have different indexes). The definitions for positives and negatives of x i also apply to z i as {z + i } and {z ? i }. Assuming the minibatch size is N , the SC loss function is written as:</p><formula xml:id="formula_26">L SCL = N i=1 L SCL (z i ),<label>(2)</label></formula><formula xml:id="formula_27">L SCL (z i ) = ?1 |{z + i }| zj ?{z + i } log exp(z i ? z j /? ) z k ,k =i exp(z i ? z k /? ) ,<label>(3)</label></formula><p>where |{z + i }| denotes the number of positive samples of anchor z i , and ? &gt; 0 is a scalar temperature parameter.</p><p>Comparing to the UC loss <ref type="bibr" target="#b7">[5]</ref>, the SC loss can flexibly incorporate arbitrary number of positives. It optimizes the agreements between such positives by contrasting against negative samples. However, the consequence of using withinclass positives in SC loss is that it results in memory consumption linear to the product of positive size and negative size. For example, when one different within-class image along with an alternative view are used as positives in SC loss, the memory consumption will be doubled comparing to the UC loss with the same size of negatives. This limits the application of SC loss when under limited GPU memory budget. One solution is to shrink the size of negatives. But this can be problematic when dealing with dataset that has large number of classes because small negative size samples small fraction of negative classes, which can compromise the quality of the learned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prototypical supervised contrastive loss</head><p>To simultaneously resolve the memory bottleneck issue and mostly retain the feature learning property of SC loss, we propose a prototypical supervised contrastive (PSC) loss. In PSC loss, we aim to attain similar goal of SC loss by learning a prototype for each class and force differently augmented views of each sample to be close to the prototype of their class and far away from the prototypes of the remaining classes. The benefits of using prototypes are two-fold. Firstly, it enables more flexible data sampling by avoiding explicitly sampling positives and negatives. Thus, we can flexibly adopt the data sampling strategies readily available in long-tailed classification, such as random sampling and class-balanced sampling. Secondly, data sampling efficiency is enhanced in PSC loss. In PSC loss, we contrast each sample against the prototypes of all other classes. If a dataset has C classes, this is essentially equivalent to a negative size of C ? 1. This is practically important when dealing with dataset with large number of classes, e.g., iNaturalist <ref type="bibr" target="#b13">[11]</ref>. The PSC loss function is:</p><formula xml:id="formula_28">L P SC (z i ) = ? log exp(z i ? p yi /? ) C j=1,j =yi exp(z i ? p j /? ) ,<label>(4)</label></formula><p>where p yi is the prototype representation for class y i , which is normalized to the unit hypersphere in R D S and z i is the normalized representation of x i .</p><p>Extension to multiple prototypes per class In the above section, we learn one prototype per class. But PSC loss can be simply extended to multiple prototypes for each class. The rationale behind is that the samples within a class may follow a multimodal distribution, which can be better modeled by using multiple prototypes. The multiple prototype supervised contrastive (MPSC) loss function can be designed as:</p><formula xml:id="formula_29">L M P SC (z i ) = ?1 M M k=1 log w i,k exp(z i ? p k yi /? ) C j=1,j =yi M m=1 exp(z i ? p m j /? ) ,<label>(5)</label></formula><p>where M is the number of prototypes per class, p i j denotes the representation for the i-th prototype of class j, and w i,k (w i,k ? 0, M k=1 w i,k = 1) denotes the affinity value between z i and the k-th prototype of its class, which is used to control the affinity of each sample in finer level. We leave detailed evaluation of MPSC loss as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly introduce the three long-tailed image classification datasets used for our experiments. Then we present some key implementation details of our methods. After that, we compare our proposed hybrid networks with state-of-the-art long-tailed image classification methods. Finally, some ablation studies are given to highlight some important properties of our hybrid networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on three long-tailed image classification datasets. Two of them, long-tailed CIFAR-10 and long-tailed CIFAR-100, are derived artificially from balanced CIFAR <ref type="bibr" target="#b22">[20]</ref> datasets by re-sampling. The third dataset, iNaturalist 2018 <ref type="bibr" target="#b13">[11]</ref>, is a large-scale image dataset, in which the image categories exhibit long-tailed distribution.</p><p>Long-tailed CIFAR-10 and CIFAR-100 The original CIFAR-10 and CIFAR-100 datasets are balanced datasets. They consist of 50,000 training images and 10,000 validations images of size 32 ? 32 in 10 and 100 classes respectively. Following <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b5">3]</ref>, the long-tailed versions are created by reducing the number of training examples per class but with the validation set unchanged. An imbalance ratio ? is used to denote the ratio between sample sizes of the most frequent and least frequent class, i.e., ? = N max /N min . The sample size follows an exponential decay across different classes. Similar to most existing work <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b5">3,</ref><ref type="bibr" target="#b39">37]</ref>, we use imbalance ratios of 10, 50 and 100 in our experiments.</p><p>iNaturalist 2018 The iNaturalist 2018 <ref type="bibr" target="#b13">[11]</ref> is a largescale real-world species classification dataset. It consists of 8,142 species, with 437,513 training and 24,424 validation images. The dataset observes severe imbalance in the sample sizes across different specie categories. We use the official training and validation splits for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>In this section, we present some key implementation details for experiments on long-tailed CIFAR and iNaturalist respectively.</p><p>Implementation details for long-tailed CIFAR For both long-tailed CIFAR-10 and CIFAR-100, we use ResNet-32 <ref type="bibr" target="#b12">[10]</ref> as backbone network to extract image representation. Our hybrid network has two branches, which have independent input data as shown in <ref type="figure">Fig. 2</ref>. The basic set of data augmentation shared by both branches include random cropping with size 32 ? 32, horizontal flip and random grayscale with probability of 0.2. Following SC loss, we also derive different views of an image by using different data augmentations in PSC loss. In our experiments, we simply use with and without color jitter as two different augmentation views. We use batch size of 512 for both SC and PSC based hybrid networks. The classifier learning branch uses class-wise balanced data sampling. We use SGD with a momentum of 0.9 and weight decay of 1?10 ?4 as optimizer to train the hybrid networks. The networks are trained for 200 epochs with the learning rate being decayed by a factor of 10 at the 120 th epoch and 160 th epoch. The initial learning rate is 0.5. For the curriculum coefficient ?, we use a parabolic decay w.r.t the epoch number <ref type="bibr" target="#b39">[37]</ref>, i.e., ? = 1 ? (T /T max ) 2 , where T denotes the current epoch number and T max indicates the maximum epoch number. For SC based hybrid network, the temperature ? in Eq. (3) is fixed to be 0.1. For PSC base hybrid network, ? is set to be 1 for CIFAR-10 and 0.1 for CIFAR-100.</p><p>Implementation details for iNaturalist 2018 For iNaturalist 2018, following most of the existing work, we use ResNet-50 <ref type="bibr" target="#b12">[10]</ref> as backbone network. The data augmentation is similar to that used in long-tailed CIFAR datasets except that random cropping with size 224 ? 224 is used. To fit two NVIDIA 2080Ti GPUs, we use a batch size of 100 for both SC and PSC based hybrid networks. The networks are trained for 100 epochs using SGD with momentum 0.9 and weight decay 1 ? 10 ?4 . The initial learning rate is 0.05, which is decayed by a factor of 10 at epoch 60 and epoch 80. Motivated by the fact that iNaturalist has a large number of classes which can make classifier learning more difficult, we assign higher weighting to the classifier learning branch by using a linearly decayed weighting factor ?, i.e., ? = 1 ? T /T max . The temperature ? is set to be 0.1 for both SC and PSC loss functions. For SC loss function, the number of positive samples for each anchor is fixed to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-art methods</head><p>In this section, we compare the proposed hybrid networks, including both SC and PSC loss based networks, to existing long-tailed classification methods on long-tailed CIFAR and iNaturalist datasets, respectively.</p><p>Experimental results on long-tailed CIFAR The comparison between the proposed hybrid networks and existing methods on long-tailed CIFAR datasets is presented in <ref type="table" target="#tab_13">Table 1</ref>. The compared methods cover various categories of ideas for imbalanced classification, including loss re-weighting <ref type="bibr" target="#b9">[7]</ref>, margin modification <ref type="bibr" target="#b5">[3]</ref>, data augmentation <ref type="bibr" target="#b21">[19]</ref>, decoupling <ref type="bibr" target="#b39">[37]</ref> and some other newly proposed ideas <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b15">13]</ref>. As can be seen from the table, our hybrid networks outperform the compared methods on almost all the settings.</p><p>Among these methods, CE denotes the simplest baseline which directly uses cross-entropy to train the network on the long-tailed datasets. As expected, this baseline method achieves the worst performance, which reveals the limitation of cross-entropy in dealing with imbalanced data. Although the performance can be improved by using advanced loss functions tailored for long-tailed data <ref type="bibr" target="#b5">[3,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b24">22]</ref>, these methods ignore the different properties of feature learning and classifier learning. BBN <ref type="bibr" target="#b39">[37]</ref> takes a step further by decoupling the head data and tailed data modeling. But several factors of BBN compromise the full potential of decoupling learning: 1) It unifies the representation for two data streams with different properties in the penultimate layer; 2) Crossentropy loss is not an ideal loss for imbalanced data in both streams; 3) The final predication in testing phase is calculated as the sum of two prediction functions from two branches with equal weights, which is inconsistent with the training phase. Our methods address such limitations in that: 1) The projection module in our feature learning branch adapts the image representation to a space more suitable for contrastive loss; 2) We use different loss functions to learn the features and classifiers and conclude supervised contrastive loss can be a better substitute for cross-entropy in learning features from imbalanced data; 3) We use a single classifier learning function to predict the class labels for each sample, which avoids the gap between training and testing. Within our methods, SC based hybrid network, a.k.a Hybrid-SC, performs better than the PSC counterpart, a.k.a Hybrid-PSC, but the latter still performs on par with or better than the compared methods.</p><p>Experimental results on iNaturalist 2018 The experimental comparison to some existing work on iNaturalist 2018 is provided in <ref type="table">Table 2</ref>. Again, we compare our hybrid networks to various lines of methods. Among these compared methods, Decoupling <ref type="bibr" target="#b17">[15]</ref> and BBN <ref type="bibr" target="#b39">[37]</ref> are most closely related to our proposal, which are both based on the idea of decoupled learning. The advantage of our methods over BBN has been analyzed above. On iNaturalist, Hybrid-PSC outperforms BBN by 1.8%. Classifier re-training (cRT) is a well-performed method we choose to compare in <ref type="bibr" target="#b17">[15]</ref>. It is a two-stage method, where the first stage learns image features and the second stage freezes the features to learn the classifiers. They use cross-entropy as loss function for both stages but using different data sampling strategies. We argue this method suffers from two limitations: 1) The twostage learning strategy harms the compatibility between the learned features and classifiers; 2) Cross-entropy loss is not an ideal choice for learning image features from imbalanced data. Our hybrid network addresses the first limitation by using a curriculum based learning strategy to smoothly transit from feature learning to classifier learning. The second limitation is also observed in BBN, which can be addressed by our hybrid network as analyzed above. Our Hybrid-PSC network outperforms Decoupling <ref type="bibr" target="#b17">[15]</ref> by nearly 3%.</p><p>Another interesting observation is that Hybrid-PSC performs better than Hybrid-SC. This result is consistent with our expectation. Note that for the two hybrid network versions, we use the same batch size of 100 for contrastive loss. This batch size is too small comparing to the number of classes in the iNaturalist dataset, which fails to provide the SC loss with sufficient negative samples to learn highquality features <ref type="bibr" target="#b7">[5]</ref>. PSC loss avoids this issue because, as analyzed in Sec. 3.3, each sample will contrast with all the negative prototypes regardless of the batch size. Due to this reason, Hybrid-PSC obtains superior classification performance. Generally, we can state that the PSC based hybrid network can observe advantage over the SC loss when dealing with imbalanced dataset with large number of classes under limited GPU memory budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies and discussions</head><p>In this section, we conduct some ablation studies to characterize our hybrid networks. Concretely, we study whether the proposed PSC loss is less sensitive to data sampling, the advantage of using PSC loss in feature learning comparing to cross-entropy loss, and the advantage of our curriculum based joint training comparing to the two-stage learning strategy. <ref type="table">Table 3</ref>. Evaluation of the sensitivity of PSC loss to data sampling. Hybrid-PSC with random PSC and Hybrid-PSC with CB-PSC denote in the PSC based hybrid network, we use random data sampling and class-balanced data sampling for the feature learning branch respectively. Classification accuracy (%) on long-tailed CIFAR-100 is reported.  <ref type="table">Table 4</ref>. Evaluation of the advantage of supervised contrastive losses over cross-entropy loss for feature learning in long-tailed classification. CE-CE denotes both feature learning and classifier learning adopt cross-entropy loss, i.e., our supervised contrastive loss is replaced by cross-entropy loss. Classification accuracy (%) on long-tailed CIFAR-100 is reported. contribute to the insensitivity of the PSC loss on data sampling. Firstly, in PSC loss, the image features and prototypes are both 2 -normalized, which breaks the strong correlations between class frequency and feature norms. Secondly, assuming the affinity score between a sample and its prototype is s yi i = z i ? p yi /? . For a sample x i with label y i ? {1, 2, . . . , C}, the gradient of the PSC loss L P SC (z i ) w.r.t s yi i is constant, and the gradient w.r.t the affinity to a prototype from a negative class c ? {1, 2, . . . , C}\y i , is exp(s c i )/ y?{1,2,??? ,C},y =yi exp(s y i ). The denominator excludes the dominating term of s yi i and thus results in a prominent gradient. The constant gradient for positive class and prominent gradients for negative classes can help to alleviate the overfitting in over-sampling and enhance the inter-class separability of the features.</p><p>Is PSC loss a better substitute for cross-entropy loss for feature learning? In this work, we claim the supervised contrastive losses are expected to learn better features from imbalanced features and consequently lead to better long-tailed classification performance. To verify this, we replace the contrastive loss in our hybrid networks with crossentropy loss. The results are shown in <ref type="table">Table 4</ref>. As can be seen, when using cross-entropy to learn the image features, the performance drops significantly.</p><p>Two-stage learning v.s. curriculum based joint learning In this work, we use a curriculum to smoothly transit the training from feature learning to classifier learning. To justify the advantage of this learning strategy, we firstly choose the original two-stage SC work <ref type="bibr" target="#b20">[18]</ref> as our baseline, which trains the features using SC loss in the first stage and then fixes the features to train classifiers in the second stage. From <ref type="table">Table 5</ref> we can see, this two-stage training scheme <ref type="table">Table 5</ref>. Evaluation of the advantage of the curriculum based joint training over two-stage training. Two-stage SC denotes we train the features and classifiers in separate stages. Hybrid-SC w/o curriculum means we use equal and fixed weighting for the feature and classifier learning during the training process. Classification accuracy (%) on long-tailed CIFAR-100 is reported. results in obviously inferior performance to our curriculum based training, because it harms the compatibility between the features and classifiers. To further highlight the importance of the curriculum, we set the weighting coefficient ? in Eq. (1) to be 0.5. Still, unsatisfactory results are obtained. When the curriculum is used, we allow the supervised contrastive losses to dominate the training first in order to fully exploit their capacity to learn discriminative features, which can benefit the classifier learning in later phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we approached long-tailed image classification by proposing a novel hybrid network, which consists of a supervised contrastive loss to learn image features and a cross-entropy loss to learn classifiers. To embody the idea that better features make better classifiers, a curriculum is followed to smoothly transit the training from feature learning to classifier learning. A new prototypical supervised contrastive loss was proposed to learn features from imbalanced data, which observes advantage under limited GPU memory budget. Experiments on three long-tailed classification datasets showed that our proposal not only significantly outperforms existing methods but also has some other appealing properties that can benefit imbalanced classification. To our knowledge, this is the first work that explores how to maximize the value of supervised contrastive learning in long-tailed image classification. We will continue this direction as our future work, with the deeper exploration of MPSC as the first step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of cross-entropy (upper), standard supervised contrastive (SC) (bottom left), and prototypical supervised contrastive (PSC) (bottom right) loss based feature learning for longtailed image classification. Cross-entropy loss learns skewed features, which can result in biased classifiers. Supervised contrastive learning (bottom two) learns more intra-class compact and interclass separable features, which ease classifier learning. In standard SC learning, an anchor sample together with positive samples from the same class are pulled together and the anchor is pushed away from negatives from other classes. In PSC learning, each sample is pulled towards the prototype (marked by star) of its class and pushed away from prototypes of other classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ture learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i = j} from the same class and negative samples {x ? i } = {x j |y j = y i } from other classes. The input batch of the feature learning branch is denoted as B SC = {x i , {x + i }, {x ? i }}. The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>we assume different views of the same image have different indexes). The definitions 4</figDesc><table><row><cell>357</cell><cell></cell></row><row><cell>358</cell><cell></cell></row><row><cell>359</cell><cell></cell></row><row><cell>360</cell><cell></cell></row><row><cell>361</cell><cell></cell></row><row><cell>362</cell><cell></cell></row><row><cell>363</cell><cell></cell></row><row><cell>364</cell><cell></cell></row><row><cell>365</cell><cell></cell></row><row><cell>366</cell><cell></cell></row><row><cell>367</cell><cell>Embedding</cell></row><row><cell>368</cell><cell></cell></row><row><cell>369</cell><cell></cell></row><row><cell>370</cell><cell></cell></row><row><cell>371</cell><cell></cell></row><row><cell>372</cell><cell></cell></row><row><cell>373</cell><cell></cell></row><row><cell>374</cell><cell></cell></row><row><cell>375</cell><cell></cell></row><row><cell>376</cell><cell></cell></row><row><cell>377</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>we assume different views of the same image have different indexes). The definitions 4</figDesc><table><row><cell>330</cell><cell></cell><cell>384</cell></row><row><cell>331</cell><cell></cell><cell>385</cell></row><row><cell>332</cell><cell></cell><cell>386</cell></row><row><cell>333</cell><cell></cell><cell>387</cell></row><row><cell>334</cell><cell></cell><cell>388</cell></row><row><cell>335</cell><cell></cell><cell>389</cell></row><row><cell>336</cell><cell></cell><cell>390</cell></row><row><cell>337</cell><cell></cell><cell>391</cell></row><row><cell>338</cell><cell></cell><cell>392</cell></row><row><cell>339</cell><cell></cell><cell>393</cell></row><row><cell>340</cell><cell></cell><cell>394</cell></row><row><cell>341</cell><cell></cell><cell>395</cell></row><row><cell>342</cell><cell></cell><cell>396</cell></row><row><cell>343</cell><cell></cell><cell>397</cell></row><row><cell>344</cell><cell></cell><cell>398</cell></row><row><cell>345</cell><cell></cell><cell>399</cell></row><row><cell>346</cell><cell></cell><cell>400</cell></row><row><cell>347</cell><cell></cell><cell>401</cell></row><row><cell>348</cell><cell></cell><cell>402</cell></row><row><cell>349 350</cell><cell>Logits</cell><cell>403 404</cell></row><row><cell>351</cell><cell></cell><cell>405</cell></row><row><cell>352</cell><cell></cell><cell>406</cell></row><row><cell>353</cell><cell></cell><cell>407</cell></row><row><cell>354</cell><cell></cell><cell>408</cell></row><row><cell>355</cell><cell></cell><cell>409</cell></row><row><cell>356</cell><cell></cell><cell>410</cell></row><row><cell>357</cell><cell></cell><cell>411</cell></row><row><cell>358</cell><cell></cell><cell>412</cell></row><row><cell>359</cell><cell></cell><cell>413</cell></row><row><cell>360</cell><cell></cell><cell>414</cell></row><row><cell>361</cell><cell></cell><cell>415</cell></row><row><cell>362</cell><cell></cell><cell>416</cell></row><row><cell>363</cell><cell></cell><cell>417</cell></row><row><cell>364</cell><cell></cell><cell>418</cell></row><row><cell>365</cell><cell></cell><cell>419</cell></row><row><cell>366</cell><cell></cell><cell>420</cell></row><row><cell>367</cell><cell></cell><cell>421</cell></row><row><cell>368</cell><cell></cell><cell>422</cell></row><row><cell>369</cell><cell></cell><cell>423</cell></row><row><cell>370</cell><cell></cell><cell>424</cell></row><row><cell>371</cell><cell></cell><cell>425</cell></row><row><cell>372</cell><cell></cell><cell>426</cell></row><row><cell>373</cell><cell></cell><cell>427</cell></row><row><cell>374</cell><cell></cell><cell>428</cell></row><row><cell>375</cell><cell></cell><cell>429</cell></row><row><cell>376</cell><cell></cell><cell>430</cell></row><row><cell>377</cell><cell></cell><cell>431</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>we assume different views of the same image have different indexes). The definitions 4</figDesc><table><row><cell>343</cell><cell>397</cell></row><row><cell>344</cell><cell>398</cell></row><row><cell>345</cell><cell>399</cell></row><row><cell>346</cell><cell>400</cell></row><row><cell>347</cell><cell>401</cell></row><row><cell>348</cell><cell>402</cell></row><row><cell>349</cell><cell>403</cell></row><row><cell>350</cell><cell>404</cell></row><row><cell>351</cell><cell>405</cell></row><row><cell>352</cell><cell>406</cell></row><row><cell>353</cell><cell>407</cell></row><row><cell>354</cell><cell>408</cell></row><row><cell>355</cell><cell>409</cell></row><row><cell>356</cell><cell>410</cell></row><row><cell>357</cell><cell>411</cell></row><row><cell>358</cell><cell>412</cell></row><row><cell>359</cell><cell>413</cell></row><row><cell>360</cell><cell>414</cell></row><row><cell>361</cell><cell>415</cell></row><row><cell>362</cell><cell>416</cell></row><row><cell>363</cell><cell>417</cell></row><row><cell>364</cell><cell>418</cell></row><row><cell>365</cell><cell>419</cell></row><row><cell>366</cell><cell>420</cell></row><row><cell>367</cell><cell>421</cell></row><row><cell>368</cell><cell>422</cell></row><row><cell>369</cell><cell>423</cell></row><row><cell>370</cell><cell>424</cell></row><row><cell>371</cell><cell>425</cell></row><row><cell>372</cell><cell>426</cell></row><row><cell>373</cell><cell>427</cell></row><row><cell>374</cell><cell>428</cell></row><row><cell>375</cell><cell>429</cell></row><row><cell>376</cell><cell>430</cell></row><row><cell>377</cell><cell>431</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>we assume different views of the same image have different indexes). The definitions 4</figDesc><table><row><cell>349 350</cell><cell></cell><cell>403 404</cell></row><row><cell>351</cell><cell></cell><cell>405</cell></row><row><cell>352</cell><cell></cell><cell>406</cell></row><row><cell>353</cell><cell></cell><cell>407</cell></row><row><cell>354</cell><cell></cell><cell>408</cell></row><row><cell>355</cell><cell></cell><cell>409</cell></row><row><cell>356</cell><cell></cell><cell>410</cell></row><row><cell>357</cell><cell></cell><cell>411</cell></row><row><cell>358</cell><cell></cell><cell>412</cell></row><row><cell>359</cell><cell></cell><cell>413</cell></row><row><cell>360</cell><cell></cell><cell>414</cell></row><row><cell>361</cell><cell></cell><cell>415</cell></row><row><cell>362</cell><cell></cell><cell>416</cell></row><row><cell>363</cell><cell></cell><cell>417</cell></row><row><cell>364</cell><cell></cell><cell>418</cell></row><row><cell>365</cell><cell>Epoch</cell><cell>419</cell></row><row><cell>366</cell><cell></cell><cell>420</cell></row><row><cell>367</cell><cell></cell><cell>421</cell></row><row><cell>368</cell><cell></cell><cell>422</cell></row><row><cell>369</cell><cell></cell><cell>423</cell></row><row><cell>370</cell><cell></cell><cell>424</cell></row><row><cell>371</cell><cell></cell><cell>425</cell></row><row><cell>372</cell><cell></cell><cell>426</cell></row><row><cell>373</cell><cell></cell><cell>427</cell></row><row><cell>374</cell><cell></cell><cell>428</cell></row><row><cell>375</cell><cell></cell><cell>429</cell></row><row><cell>376</cell><cell></cell><cell>430</cell></row><row><cell>377</cell><cell></cell><cell>431</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>we assume different views of the same image have different indexes). The definitions 4</figDesc><table><row><cell>339</cell><cell>374</cell><cell>393</cell><cell>428</cell></row><row><cell>340</cell><cell>375</cell><cell>394</cell><cell>429</cell></row><row><cell>341 342</cell><cell>376 377</cell><cell>395 396</cell><cell>430 431</cell></row><row><cell>343</cell><cell></cell><cell>397</cell><cell></cell></row><row><cell>344</cell><cell></cell><cell>398</cell><cell></cell></row><row><cell>345</cell><cell></cell><cell>399</cell><cell></cell></row><row><cell>346</cell><cell></cell><cell>400</cell><cell></cell></row><row><cell>347</cell><cell></cell><cell>401</cell><cell></cell></row><row><cell>348</cell><cell></cell><cell>402</cell><cell></cell></row><row><cell>349</cell><cell></cell><cell>403</cell><cell></cell></row><row><cell>350</cell><cell></cell><cell>404</cell><cell></cell></row><row><cell>351</cell><cell></cell><cell>405</cell><cell></cell></row><row><cell>352</cell><cell></cell><cell>406</cell><cell></cell></row><row><cell>353</cell><cell></cell><cell>Feature learning 407</cell><cell></cell></row><row><cell>354</cell><cell></cell><cell>408</cell><cell></cell></row><row><cell>355</cell><cell></cell><cell>409</cell><cell></cell></row><row><cell>356</cell><cell></cell><cell>410</cell><cell></cell></row><row><cell>357</cell><cell></cell><cell>411</cell><cell></cell></row><row><cell>358</cell><cell></cell><cell>412</cell><cell></cell></row><row><cell>359</cell><cell></cell><cell>413</cell><cell></cell></row><row><cell>360</cell><cell></cell><cell>414</cell><cell></cell></row><row><cell>361</cell><cell></cell><cell>Classifier learning 415</cell><cell></cell></row><row><cell>362</cell><cell></cell><cell>416</cell><cell></cell></row><row><cell>363</cell><cell></cell><cell>417</cell><cell></cell></row><row><cell>364</cell><cell></cell><cell>418</cell><cell></cell></row><row><cell>365</cell><cell></cell><cell>419</cell><cell></cell></row><row><cell>366</cell><cell></cell><cell>420</cell><cell></cell></row><row><cell>367</cell><cell></cell><cell>421</cell><cell></cell></row><row><cell>368</cell><cell></cell><cell>422</cell><cell></cell></row><row><cell>369</cell><cell></cell><cell>423</cell><cell></cell></row><row><cell>370</cell><cell></cell><cell>424</cell><cell></cell></row><row><cell>371</cell><cell></cell><cell>425</cell><cell></cell></row><row><cell>372</cell><cell></cell><cell>426</cell><cell></cell></row><row><cell>373</cell><cell></cell><cell>427</cell><cell></cell></row><row><cell>374</cell><cell></cell><cell>428</cell><cell></cell></row><row><cell>375</cell><cell></cell><cell>429</cell><cell></cell></row><row><cell>376</cell><cell></cell><cell>430</cell><cell></cell></row><row><cell>377</cell><cell></cell><cell>431</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy (%) on long-tailed CIFAR datasets based on ResNet-32. (Best and second best results are marked in bold.)</figDesc><table><row><cell>Dataset</cell><cell cols="3">Long-tailed CIFAR-10</cell><cell cols="3">Long-tailed CIFAR-100</cell></row><row><cell>Imbalance ratio</cell><cell>100</cell><cell>50</cell><cell>10</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>CE</cell><cell>70.36</cell><cell>74.81</cell><cell>86.39</cell><cell>38.32</cell><cell>43.85</cell><cell>55.71</cell></row><row><cell>Focal loss [22]</cell><cell>70.38</cell><cell>76.72</cell><cell>86.66</cell><cell>38.41</cell><cell>44.32</cell><cell>55.78</cell></row><row><cell>CB-Focal [7]</cell><cell>74.57</cell><cell>79.27</cell><cell>87.10</cell><cell>39.60</cell><cell>45.17</cell><cell>57.99</cell></row><row><cell>CE-DRW [3]</cell><cell>76.34</cell><cell>79.97</cell><cell>87.56</cell><cell>41.51</cell><cell>45.29</cell><cell>58.12</cell></row><row><cell>CE-DRS [3]</cell><cell>75.61</cell><cell>79.81</cell><cell>87.38</cell><cell>41.61</cell><cell>45.48</cell><cell>58.11</cell></row><row><cell>LDAM-DRW [3]</cell><cell>77.03</cell><cell>81.03</cell><cell>88.16</cell><cell>42.04</cell><cell>46.62</cell><cell>58.71</cell></row><row><cell>CB-DA [13]</cell><cell>80.00</cell><cell>82.23</cell><cell>87.40</cell><cell>44.08</cell><cell>49.16</cell><cell>58.00</cell></row><row><cell>M2m [19]</cell><cell>79.10</cell><cell>-</cell><cell>87.50</cell><cell>43.50</cell><cell>-</cell><cell>57.60</cell></row><row><cell>Casual model [34]</cell><cell>80.6</cell><cell>83.60</cell><cell>88.50</cell><cell>44.10</cell><cell>50.30</cell><cell>59.60</cell></row><row><cell>BBN [37]</cell><cell>79.82</cell><cell>81.18</cell><cell>88.32</cell><cell>42.56</cell><cell>47.02</cell><cell>59.12</cell></row><row><cell>Hybrid-SC (ours)</cell><cell>81.40</cell><cell>85.36</cell><cell>91.12</cell><cell>46.72</cell><cell>51.87</cell><cell>63.05</cell></row><row><cell>Hybrid-PSC (ours)</cell><cell>78.82</cell><cell>83.86</cell><cell>90.06</cell><cell>44.97</cell><cell>48.93</cell><cell>62.37</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">For Decoupling [15], the well-performed Classifier Re-training (cRT) is reported as it is closely related to our method. By default, the methods are trained for up to 100 epochs. The number in brackets indicates the accuracy obtained by training for 200 epochs. (Best and second best results are marked in bold</title>
	</analytic>
	<monogr>
		<title level="m">Table 2. Top-1 accuracy (%) on iNaturalist 2018 dataset based on ResNet-50</title>
		<imprint/>
	</monogr>
	<note>Dataset iNaturalist2018</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hybrid-Psc</surname></persName>
		</author>
		<idno>ours) 68.10 (70.35</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">the authors find cross-entropy loss is sensitive to data sampling when it is used to learn features. Concretely, they find random sampling obviously outperforms class-wise balanced sampling for feature learning. For example, in [15], the class-balanced sampling can lead to around 5% accuracy drop comparing to random sampling under cross-entropy loss. As PSC loss in our work has the same data sampling manner as cross-entropy loss, we verify the sensitivity of our PSC loss to data sampling in Table 3. From the table we can see, our Hybrid-PSC network achieves comparable performance by using random sampling and class-balanced sampling, which indicates our PSC can alleviate the overfitting issue resulted from over-sampling</title>
		<imprint/>
	</monogr>
	<note>Sensitivity of PSC loss to data sampling In the decoupled learning work [15, 37. class-balanced sampling belongs to over-sampling). We conjecture that two possible factors References</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What is the effect of importance weighting in deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">C4.5, class imbalance, and cost sensitivity: Why under-sampling beats oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML Workshop on Learning from Imbalanced Datasets</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking classbalanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intelligent Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing classifiers for imbalanced training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigoris</forename><surname>Karakoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Supervised contrastive learning. NeurIPS, 2020. 2, 3, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">M2m: Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Addressing the curse of imbalanced training sets: One-sided selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the statistical consistency of algorithms for binary classification under class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harikrishna</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Survey of resampling techniques for improving classification performance in unbalanced datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>More</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial minority oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shounak</forename><surname>Sankha Subhra Mullick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagatam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Class imbalance, redux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Trikalinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno>arXiv, 2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
