<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Geng</surname></persName>
							<email>zhichenggeng@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
							<email>lulian@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
							<email>tianyuding@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
							<email>zharkov@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Space-time video super-resolution (STVSR) is the task of interpolating videos with both Low Frame Rate (LFR) and Low Resolution (LR) to produce High-Frame-Rate (HFR) and also High-Resolution (HR) counterparts. The existing methods based on Convolutional Neural Network (CNN) succeed in achieving visually satisfied results while suffer from slow inference speed due to their heavy architectures. We propose to resolve this issue by using a spatialtemporal transformer that naturally incorporates the spatial and temporal super resolution modules into a single model. Unlike CNN-based methods, we do not explicitly use separated building blocks for temporal interpolations and spatial super-resolutions; instead, we only use a single end-to-end transformer architecture. Specifically, a reusable dictionary is built by encoders based on the input LFR and LR frames, which is then utilized in the decoder part to synthesize the HFR and HR frames. Compared with the state-of-the-art TMNet [54], our network is 60% smaller (4.5M vs 12.3M parameters) and 80% faster (26.2fps vs 14.3fps on 720 ? 576 frames) without sacrificing much performance. The source code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Space-time video super-resolution (STVSR) refers to the task of simultaneously increasing spatial and temporal resolutions of low-frame-rate (LFR) and low-resolution (LR) videos, which appears in a wide variety of multimedia applications such as video compression <ref type="bibr" target="#b35">[36]</ref>, video streaming <ref type="bibr" target="#b50">[51]</ref>, video conferencing <ref type="bibr" target="#b44">[45]</ref> and so on. In the stage of deployment, many of them have stringent requirements for the computational efficiency, and only LFR and LR frames can be transferred in real-time. STVSR becomes a natural <ref type="bibr">Figure 1</ref>. Performance of RSTT on Vid4 dataset <ref type="bibr" target="#b20">[21]</ref> using small (S), medium (M) and large (L) architectures compared to other baseline models. In the top, we plot FPS versus PSNR. Note that 24 FPS is the standard cinematic frame rate <ref type="bibr" target="#b41">[42]</ref>. In the bottom, we plot the number of parameters (in millions) versus PSNR. We omit the STARnet here since it is significantly larger than others while performs the worst; see <ref type="table" target="#tab_0">Table 1</ref> for details. remedy in this scenario for recovering the high-frame-rate (HFR) and high-resolution (HR) videos. However, the performance of existing STVSR approaches are far from being real-time, and a fast method without sacrificing much visual quality is crucial for practical applications.</p><p>The success of traditional STVSR approaches usually relies on strong illumination consistency assumptions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39]</ref>, which can be easily violated in real world dynamic patterns. Ever since the era of deep neural network (DNN), convolutional neural network (CNN) exhibits promising results in many video restoration tasks, e.g., video denoising <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8]</ref>, video inpainting <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47]</ref>, video superresolution (VSR) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b13">14]</ref> and video frame interpolation (VFI) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11]</ref>. One straightforward way to tackle the STVSR problem is that by treating it as a composite task of VFI and VSR one can sequentially apply VFI, e.g., SepConv <ref type="bibr" target="#b32">[33]</ref>, DAIN <ref type="bibr" target="#b0">[1]</ref>, CDFI <ref type="bibr" target="#b10">[11]</ref>, and VSR, e.g., DUF <ref type="bibr" target="#b16">[17]</ref>, RBPN <ref type="bibr" target="#b13">[14]</ref>, EDVR <ref type="bibr" target="#b48">[49]</ref>, on the input LFR and LR video. Nevertheless, this simple strategy has two major drawbacks: first, it fails to utilize the inner connection between the temporal interpolation and spatial super-resolution <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>; second, both VFI and VSR models need to extract and utilize features from nearby LR frames, which results in duplication of work. Additionally, such two-stage models usually suffer from slow inference speed due to the large amount of parameters, hence prohibits from being deployed on real-time applications.</p><p>To alleviate the above problems, recent learning based methods train a single end-to-end model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, where features are extracted from the input LFR and LR frames only once and then are upsampled in time and space sequentially inside the network. However, researches in this line still consist of two sub-modules: a temporal interpolation network, e.g., Deformable ConvLSTM <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> and Temporal Modulation Block <ref type="bibr" target="#b53">[54]</ref>, and a spatial reconstruction network, e.g., residual blocks used in both Zooming SloMo <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> and TMnet <ref type="bibr" target="#b53">[54]</ref>. One natural question to ask is that whether we can have a holistic design such that it increases the spatial and temporal resolutions simultaneously without separating out the two tasks.</p><p>In this paper, we propose a single spatial temporal transformer that incorporates the temporal interpolation and spatial super resolution modules for the STVSR task. This approach leads to a much smaller network compared with the existing methods, and is able to achieve a real-time inference speed without sacrificing much performance. Specifically, we make the following contributions:</p><p>? We propose a Real-time Spatial Temporal Transformer (RSTT) to increase the spatial and temporal resolutions without explicitly modeling it as two sub-tasks.</p><p>To the best of our knowledge, this is the first time that a transformer is utilized to solve the STVSR problem.</p><p>? Inside RSTT, we design a cascaded UNet-style architecture to effectively incorporate all the spatial and temporal information for synthesizing HFR and HR videos. In particular, the encoder part of RSTT builds dictionaries at multi-resolutions, which are then queried in the decoder part for directly reconstructing HFR and HR frames.</p><p>? We propose three RSTT models with different number of encoder and decoder pairs, resulting in small (S), medium (M) and larger (L) architectures. Experiments show that RSTT is significantly smaller and faster than the stateof-the-art STVSR methods while maintains similar performance: (i) RSTT-L performs similarly as TMNet <ref type="bibr" target="#b53">[54]</ref> with 40% less parameters, RSTT-M outperforms Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> with 50% less parameters and RSTT-S outperforms STARNet <ref type="bibr" target="#b14">[15]</ref> with 96% less parameters; (ii) RSTT-S achieves a frame rate of more than 24 per second (the standard cinematic frame rate) on 720 ? 576 frames. It achieves the performance of Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> with a 75% speedup, and outperforms STARNet <ref type="bibr" target="#b14">[15]</ref> with around 700% speedup (see <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Video frame interpolation (VFI)</head><p>VFI aims at synthesizing an intermediate frame given two consecutive frames in a video sequence, hence the temporal resolution is increased. Conventionally, it is formulated as an image sequence estimation problem, e.g., the path-based <ref type="bibr" target="#b25">[26]</ref> and phase-based approach <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, while it fails in scenes with fast movements or complex image details. CNN-based VFI methods can be roughly categorized into three types: flow-based, kernel-based and deformableconvolution-based. Flow-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> perform VFI by estimating optical flow between frames and then warping input frames with the estimated flow to synthesize the missing ones. Instead of using only pixel-wise information for interpolation, kernel-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> propose to synthesize the image by convolving over local patches around each output pixel, which largely preserves the local textual details. Recently, deformable-convolution-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> combine flow-based and kernel-based methods by taking the advantages of flexible spatial sampling introduced by deformable convolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b63">64]</ref>. This key improvement accommodates to both larger motions and complex textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video super resolution (VSR)</head><p>VSR aims to recover HR video sequences from LR ones, hence the spatial resolution is increased. Most deep learning based VSR methods <ref type="bibr" target="#b21">[22]</ref> adopt the strategy of fusing spatial features from multiple aligned frames (or features), which highly depends on the quality of the alignment. Earlier methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref> utilize optical flow to perform explicit temporal frames alignment. However, the computation of optical flow can be expensive and the estimated flow can be inaccurate. In parallel, TDAN <ref type="bibr" target="#b58">[59]</ref> introduces deformable convolution to implicitly align temporal features and achieves impressive performance, while EDVR <ref type="bibr" target="#b48">[49]</ref> incorporates deformable convolution into a multi-scale module to further improve the feature alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Space-time video super-resolution (STVSR)</head><p>The goal of STVSR is to increase both spatial and temporal resolutions of LFR and LR videos. Dating back two decades, <ref type="bibr" target="#b37">[38]</ref> performs super-resolution simultaneously in time and space by modeling the dynamic scene as 3D representation. However, it requires input sequences of several different space-time resolutions to construct a new one. Due to the recent success of CNN, <ref type="bibr" target="#b14">[15]</ref> proposes an endto-end network STARnet to increase the spatial resolution and frame rate by jointly learning spatial and temporal contexts. Xiang et. al <ref type="bibr" target="#b51">[52]</ref> propose a one-stage framework, named Zooming SlowMo, by firstly interpolating temporal features using deformable convolution and then fusing mutli-frame features through deformable ConvLSTM. Later, Xu et. al <ref type="bibr" target="#b53">[54]</ref> incorporate temporal modulation block into Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref>, named TMNet, so that the model is able to interpolate arbitrary intermediate frames and achieves better visual consistencies in between resulting frames. Nevertheless, these approaches either explicitly or implicitly treat the STVSR problem as a combination of VFI and VSR by designing separate modules for the subtasks, which is computationally expensive. Zhou et al. point out in a more recent work <ref type="bibr" target="#b62">[63]</ref> that VFI and VSR mutually benefit each other. They present a network that cyclically uses the intermediate results generated by one task to improve another and vice versa. While achieving better performance, this idea results in a relatively larger network (about three times larger than Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref>). Our work also makes use of such mutual benefits from VFI and VSR, while avoids the repeated feature computations, and thus ends in a light-weight design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Vision transformer</head><p>Transformer <ref type="bibr" target="#b45">[46]</ref> is a dominant architecture in Natural Language Processing (NLP) and achieves state-of-the-art performance in various tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">55]</ref>. Recently, transformers gain popularity in computer vision field. The pioneering Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> computes attentions between flattened image patches to solve image classification problems and outperforms CNN-based methods to a large extent. Due to transformer's strong ability of learning long dependencies between different image regions, follow-up work using variants of ViT refreshes the stateof-the-art results in many applications, such as segmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b61">62]</ref>, object detection <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b55">56]</ref> and depth estimation <ref type="bibr" target="#b34">[35]</ref>. In the meantime, Liu et al. <ref type="bibr" target="#b22">[23]</ref> propose a novel transformer-based backbone for vision tasks, i.e., Shifted window (Swin) Transformer, to reduce computational complexity by restricting the attention computations inside local and later shifted local windows. Thereafter, <ref type="bibr" target="#b49">[50]</ref> proposes a U-shape network based on Swin Transformer for general image restoration. SwinIR <ref type="bibr" target="#b19">[20]</ref> tackles the image restoration task using Swin Transformer and introduces residual Swin Transformer blocks. In this work, we also use Swin Transformer as basic building blocks to extract local information. However, instead of building dictionaries and queries from identical single frames <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b19">20]</ref>, we compute window and shifted window attentions from multiple input frames, which are then utilized to build reusable dictionaries to synthesize multiple output frames at once. We emphasize that this design is the key that leads to the acceleration of inference time and reduction of model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>In this section, we first give an overview of the proposed approach in Section 3.1. Then we explain the encoder and decoder part of our spatial-temporal transformer in Section 3.2 and Section 3.3, respectively. Finally, the training details are given in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network overview</head><p>Given (n + 1) LFR and LR frames</p><formula xml:id="formula_0">I L = {I L 2t?1 } n+1 t=1 of size H ?W ?3, a STVSR model generates 2n+1 HFR and HR frames I H = {I H t } 2n+1 t=1 of size 4H ? 4W ? 3,</formula><p>where t denotes the time stamp of a frame. Note that only frames with odd time stamp in I H has the LR counterparts in I L .</p><p>We propose a cascaded UNet-style transformer, named Real-time Spatial Temporal Transformer (RSTT), to interpolate the LR sequence I L in both time and space simultaneously without having explicit separations of the model into spatial and temporal interpolation modules. One may shortly observe that this design is a distinct advantage over the existing CNN-based methods since it leads to a real-time inference speed while maintains similar performance.</p><p>We let f denote the underlying function modeled by RSTT, which takes four consecutive LFR and LR frames in I L and outputs seven HFR and HR frames in the sequence:</p><formula xml:id="formula_1">f : (I L 2t?1 , I L 2t+1 , I L 2t+3 , I L 2t+5 ) ? (I H 2t?1 , I H 2t , I H 2t+1 , I H 2t+2 , I H 2t+3 , I H 2t+4 , I H 2t+5 )<label>(1)</label></formula><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, RSTT mainly consists of four encoders E k , k = 0, 1, 2, 3, and corresponding decoders D k , k = 0, 1, 2, 3. In RSTT, a feature extraction block firstly extracts the features of the four input frames, denoted by</p><formula xml:id="formula_2">(F L 2t?1 , F L 2t+1 , F L 2t+3 , F L 2t+5 )</formula><p>; then, a multi-video Swin Transformer encoder block T swin takes the features as input: where T 0 is the embedded feature generated by Swin Transformer. Let ? denote the convolutional block in E 0 , E 1 and E 2 , one can write E 0 = ?(T 0 ). Subsequently, we have</p><formula xml:id="formula_3">T 0 = T swin (F L 2t?1 , F L 2t+1 , F L 2t+3 , F L 2t+5 ),</formula><formula xml:id="formula_4">? ? ? ? ? T k = T swin (E k?1 ), k = 1, 2, 3 E k = ?(T k ), k = 1, 2 E 3 = T 3 (2)</formula><p>Note that each of the encoders E k , k = 0, 1, 2, 3, has four output channels corresponding to the four time stamps of the input LFR and LR frames. To make it clear, we use</p><formula xml:id="formula_5">E k ? (E k,2t?1 , E k,2t+1 , E k,2t+3 , E k,2t+5 )</formula><p>to denote the four output feature maps of each E k . In fact, a reusable dictionary is built in each E k , and the details of the encoder architecture are presented in Section 3.2.</p><p>After computing E 3 , RSTT constructs a query builder that generates features for interpolating HFR and HR frames at finer time stamps. Specifically, we define the query Q as seven-channel feature maps with</p><formula xml:id="formula_6">Q := E 3,2t?1 , 1 2 (E 3,2t?1 + E 3,2t+1 ), E 3,2t+1 , 1 2 (E 3,2t+1 + E 3,2t+3 ), E 3,2t+3 , 1 2 (E 3,2t+3 + E 3,2t+5 ), E 3,2t+5<label>(3)</label></formula><p>As indicated in <ref type="formula" target="#formula_6">(3)</ref>, for odd HFR and HR frames which already have their LFR and LR counterparts, we just adopt the learnt features from the encoder E 3 as the queries; while for even frames that have no LFR and LR counterparts, we use the mean features of their adjacent frames as the queries.</p><p>We are now ready to synthesize the HFR and HR frames by feeding the decoders with the query and the outputs of encoders. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, similar to (2), we have</p><formula xml:id="formula_7">? ? ? ? ? D 3 = ? ?1 (T ?1 swin (T 3 , Q)), D k = ? ?1 (T ?1 swin (T k , D k+1 )), k = 1, 2 D 0 = T ?1 swin (T 0 , D 1 )<label>(4)</label></formula><p>where T ?1 swin is the multi-video Swin Transformer decoder block and ? ?1 denotes the deconvolutional block in D 1 , D 2 and D 3 . The details of the decoder architecture are presented in Section 3.3. For the final synthesis, we learn the residuals instead of the HFR and HR frames themselves. We simply use a trilinear interpolation of the input frames to work as a warming start of the output frames.</p><p>We remark that the key to the architecture of RSTT is the reusable dictionaries built in the encoders E k based on the input LFR and LR frames, which are then utilized in decoders D k combined with queries to synthesize the HFR and HR frames. This design is advantageous over the duplicate feature fusions appearing in many existing methods, <ref type="figure">Figure 3</ref>. The basic Swin Transformer encoder block used in E k , k = 0, 1, 2, 3 of RSTT; see <ref type="figure" target="#fig_0">Figure 2</ref>. It first computes multi-head self attentions in each window partition, and then in each shifted window partition. Here, LN stands for Layer Normalization, W-MSA is Windowed Multi-Head Self-Attention and SW-MSA is Shifted Windowed Multi-Head Self-Attention. e.g., deformable convolutions and ConvLSTM in Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> and TMNet <ref type="bibr" target="#b53">[54]</ref>), and thus accelerates the inference speed to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder</head><p>In this subsection, we explain in details the encoder architecture of our RSTT. Before moving on, for the feature extraction module shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we use a single convolutional layer with kernel size 3 ? 3 to extract C features from four input LFR and LR RGB frames. This shallow feature extractor is significantly smaller than the five residual blocks used in Zooming SlowMo <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> and TMNet <ref type="bibr" target="#b53">[54]</ref>, and thus is computationally efficient.</p><p>Following the light-weight feature extractor, the encoder part of RSTT consists of four stages, denoted by E k , k = 0, 1, 2, 3, each of which is a stack of Swin Transformer <ref type="bibr" target="#b22">[23]</ref> blocks followed by a convolution layer (except E 3 ). Inside E k , Swin Transformer blocks take the approach of shifting non-overlapping windows to reduce the computational cost while keeping the ability of learning long-range dependencies. As demonstrated in <ref type="figure">Figure 3</ref>  <ref type="bibr" target="#b22">[23]</ref> is then applied to introduce the cross-window connections. In this second Swin Transformer block, every module is the same as the previous block except that the input features are shifted by ? M 2 ? ? ? M 2 ? before window partitioning. In this way, Swin Transformer blocks are able to reduce computational costs while capturing long-range dependencies along both the spatial and temporal dimension. Finally, the output of a stack of such Swin Transformer blocks are downsampled by a convolutional layer with stride of two, serving as the input of the next encoder stage and the decoder stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>Same as the encoder part, we use four stages of decoders followed by a deconvolutional layer for feature upsampling. The decoders D k , k = 0, 1, 2, 3 generate peroutput-frame features in each level of details by repeatedly querying the dictionaries (the key-value pairs (K, V ) as shown in <ref type="figure" target="#fig_2">Figure 4</ref>) constructed from the encoders E k 's in the same level. Each decoder consists of several (the same number as its corresponding encoder) Swin Transformer blocks, and each of the blocks takes two inputs: one is the output features from the encoder and the other is a single frame query, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>In RSTT, the first stage query Q = {Q i } 7 i=1 for D 3 is interpolated from the the last encoder E 3 (see (3)) and the later queries are the outputs of the previous decoders. To generate seven HFR and HR output frames, each Swin Transformer decoder block queries the dictionary seven times. As a result, for a decoder contains S such blocks, we need to query 7S times. In practice, the query is performed by Windowed Multi-Head Cross-Attention (W-MCA) <ref type="bibr" target="#b22">[23]</ref> and its shifted version (SW-MCA) <ref type="bibr" target="#b22">[23]</ref> (see <ref type="figure" target="#fig_2">Figure 4</ref>). Note that only the first Swin Transformer decoder block uses Q as query while the rest S ? 1 blocks use the output of the previous block as query. Importantly, dictionaries provided by the encoders are pre-computed for reuse in each block. Suppose we have three Swin Transformer decoder blocks in each D k , the spatial-temporal dictionaries built from the encoders are queried (reused) for 7 ? 3 = 21 times, which is advantageous over the duplication of future fusions in the existing approaches. This design is both computationally efficient and helpful in reducing the model size.</p><p>Final reconstruction module. The output features of the last decoder D 0 can be further processed by an optional reconstruction module to generate the final frames (see <ref type="figure" target="#fig_0">Figure 2)</ref>. We use a module consisting of a 1-to-4 PixelShuffle operation and a single convolutional layer. This design is much more light-weight compared to the practices adopted in Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> and TMNet <ref type="bibr" target="#b53">[54]</ref>, both of which use 40 residual blocks to perform the spatial super resolution. We compare the performance of RSTT with and without such spatial reconstruction module in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training details</head><p>We train the proposed RSTT model using Adam with L 2 and decoupled weight decay <ref type="bibr" target="#b24">[25]</ref> by setting ? 1 = 0.9 and ? 2 = 0.99. The initial learning rate is set to 2 ? 10 ?4 and is gradually decayed following the scheme of Cosine annealing with restart <ref type="bibr" target="#b12">[13]</ref> set to 10 ?7 . The restart performs at every 30,000 iterations. We train our model on two Nvidia Quadro RTX 6000 with batch size set to 7?10, depending on the particular model architecture.</p><p>Objective function. The Charbonnier loss is computed between the estimated frame I H and the ground truth? H :</p><formula xml:id="formula_8">L(? H , I H ) = ?? H ? I H ? 2 + ? 2 ,</formula><p>where ? is set to 10 ?3 in our experiments.</p><p>Training dataset. We train our models on Vimeo-90K <ref type="bibr" target="#b56">[57]</ref>, which contains over 60,000 seven-frame video sequences. Many state-of-the-art methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> also use this dataset for training. For Vimeo-90K, the input LFR and LR frames are four frames of size 112?64, and the output HFR and HR frames are seven frames of size 448 ? 256 (exactly 4? larger in both height and width).</p><p>Evaluation. The models are evaluated on Vid4 <ref type="bibr" target="#b20">[21]</ref> and Vimeo-90K <ref type="bibr" target="#b56">[57]</ref> datasets. Vid4 is a small dataset consists of four video sequences of different scenes with 180 ? 144 input frames and 720 ? 576 output frames. Vimeo-90K validation set is split into fast, medium and slow motion sets as in <ref type="bibr" target="#b51">[52]</ref> that contains 1225, 4972 and 1610 video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We train the proposed RSTT for three versions with small (S), medium (M) and large (L) architectures, corresponding to the number of Swin Transformer blocks used in each stage of encoder E k and decoder D k set to 2, 3 and 4, respectively. We term the three models as RSTT-S, RSTT-M and RSTT-L, and then compare them with other existing methods quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth</head><p>Ground-truth StarNet <ref type="bibr" target="#b14">[15]</ref> Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> TMNet <ref type="bibr" target="#b53">[54]</ref> Overlaid RSTT-S RSTT-M RSTT-L Ground-truth Ground-truth StarNet <ref type="bibr" target="#b14">[15]</ref> Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> TMNet <ref type="bibr" target="#b53">[54]</ref> Overlaid</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RSTT-S RSTT-M RSTT-L</head><p>Ground-truth Ground-truth StarNet <ref type="bibr" target="#b14">[15]</ref> Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> TMNet <ref type="bibr" target="#b53">[54]</ref> Overlaid RSTT-S RSTT-M RSTT-L <ref type="figure">Figure 5</ref>. Visual comparisons on the Vid4 dataset <ref type="bibr" target="#b20">[21]</ref>. RSTT with three different sizes of architectures achieve the state-of-the-art performance in terms of visual qualities on various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative evaluation</head><p>We use Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) as evaluation metrics for quantitative comparison. We also compare the model inference time in Frame Per Second (FPS) and model size in terms of the number of parameters, as shown in <ref type="table" target="#tab_0">Table 1</ref>. We do not list the FPS of methods that sequentially apply separated VFI and VSR models, since they are much slower than the other competitors, as reported in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>We observe that all the RSTT models achieve state-ofthe-art performance in both Vid4 and Vimeo-90K datasets with significantly smaller model size and substantially higher inference speed. Moreover, the performance grows steadily with increasing number of Swin Transformer blocks stacked in the architecture, from RSTT-S, -M to -L. Specifically, in <ref type="table" target="#tab_0">Table 1</ref>, one can see that the smallest model RSTT-S performs similarly as Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref>, while RSTT-M outperforms Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> in Vid4, Vimeo-Medium and Vimeo-Slow with significantly smaller number of parameters and faster inference speed. Our largest model RSTT-L outperforms TMNet <ref type="bibr" target="#b53">[54]</ref> on Vimeo-Medium, which is the largest dataset in <ref type="table" target="#tab_0">Table 1</ref>, with 40% smaller model size. We remark that our RSTT-S achieves a real-time rendering speed (more than 26 FPS) without sacrificing much performance.  <ref type="table">Table 2</ref>. Quantitative comparisons of RSTT with and without the spatial reconstruction block. Top-three numbers of each column are bolded, with the best in red and the second best in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative evaluation</head><p>We visually compare RSTT with other state-of-the art STVSR methods in <ref type="figure">Figure 5</ref>. We choose three different scenarios for the purpose of illustration:</p><p>? The first row shows the video of a still calendar in front of a moving camera. We observe that RSTT-S recovers details around the character. Texture details look more apparent compared with the result of StarNet <ref type="bibr" target="#b14">[15]</ref>.</p><p>? The second row shows the video taken by a still camera in the wild with fast moving vehicles. It is clear that RSTT outperforms StarNet <ref type="bibr" target="#b14">[15]</ref> and Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> with better contours of the moving vehicle.</p><p>? The third row illustrates a difficult case, where both the camera and the foreground objects are moving, especially the fast-flying pigeon. From the overlaid view, one can see that the pigeons in consecutive frames are barely overlapped. Our models give relatively better motion interpolations in this case compared with other state-of-the-arts. In addition, with the increasing sizes of our models, from RSTT-S to RSTT-L, we observe better interpolations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Advantages of RSTT</head><p>We analyze the effectiveness of the Swin Transformer blocks used in encoders and decoders by comparing with using an optional spatial reconstruction block in <ref type="figure" target="#fig_0">Figure 2</ref> with 10 residual blocks (see <ref type="table">Table 2</ref>). This block is similar to but smaller than the 40 residual blocks used in Zooming SlowMo <ref type="bibr" target="#b51">[52]</ref> and TMNet <ref type="bibr" target="#b53">[54]</ref>. We observe that the additional reconstruction block only slightly changes the evaluation results. There are hardly any differences in performance (?0.02db in psnr) on Vid4, Vimeo-Fast and Vimeo-Medium datasets between RSTT models with and without adding reconstruction blocks. However, both the inference time and the network size are largely increased. Furthermore, RSTT-M (6.08M parameters) exhibits non-negligible improvement over RSTT-S-Recon (6.15M parameters) in all of the datasets (?0.2db in PSNR on Vimeo-Fast) with an even smaller model size. This reveals the effectiveness of our design, indicating larger spatial reconstruction block is unnecessary to RSTT. Note that we do not train a model with such additional block on RSTT-L due to the limited time, but we believe a similar pattern holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitations of RSTT</head><p>Long training time. Like other transformer-based methods <ref type="bibr" target="#b11">[12]</ref>, the required training time of RSTT is relatively long. It takes more than twenty-five days for convergence with the usage of two Nvidia Quadro RTX 6000 cards.</p><p>Lack the flexibility to interpolate at arbitrary time stamps. Unlike TMNet <ref type="bibr" target="#b53">[54]</ref>, RSTT lacks the flexibility of interpolating an intermediate frame at arbitrary time stamps since the Query Q defined by <ref type="formula" target="#formula_6">(3)</ref> is fixed. However, we remark that this can be achieved by slightly rephrasing Query Q for Decoder D 3 . Suppose we would like to interpolate n ? 1 frames (at n ? 1 time stamps) between two frames, e.g., E 3,2t?1 and E 3,2t+1 , we just need to make</p><formula xml:id="formula_9">queries on { i n E 3,2t?1 + (1 ? i n )E 3,2t+1 } n?1 i=1 instead of 1 2 E 3,2t?1 + 1 2 E 3,2t+1</formula><p>where n = 2 as a special case in (3). One might need to retrain the model to adopt such modifications, and we leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a real-time spatial-temporal transformer (RSTT) for generating HFR and HR videos from LFR and LR ones. We considered to solve the space-time video super-resolution problem with a unified transformer architecture without having explicit separations of temporal and spatial sub-modules. Specifically, LFR and LR spatialtemporal features extracted from different levels of encoders are used to build dictionaries, which are then queried many times in the decoding stage for interpolating HFR and HR frames simultaneously. We emphasize that the key innovation of the work is the novel holistic formulation of self-attentions in encoders and cross-attentions in decoders. This holistic design leads to a significantly smaller model with much faster (real-time) inference speed compared with the state-of-the-art methods without noticeable difference in model performance.</p><p>Future directions along this line include but are not limited to: fusions of dictionaries built in different levels of encoders to make computations more efficient; controllable temporal super-resolution with the flexibility to interpolate frames at arbitrary time stamps; and sophisticated training loss functions that helps to improve the visual quality. To better understand the mechanism of RSTT, We show windowed 2-Head cross attentions of the last decoder on a sequence in Vimeo-Fast at two different locations (see <ref type="bibr">Figure 6)</ref>. Images bounded within red (yellow) box correspond to the red (yellow) dot highlighted in the outputs. We observe that the learnt attentions generally capture the local image structures in the finest level of detail. of information combinations: concatenation and addition. The performance comparisons are shown in <ref type="table">Table 3</ref> and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Attention visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Effectiveness of Multi-head Cross Attention</head><p>Here, both Windowed-MCA and Shifted Windowed-MCA are replaced by feature concatenation or feature addition by simply using 1d convolutions to match the feature sizes of stacked queries and the encoder outputs before this information fusion. Metrics in both <ref type="table">Table 3</ref> and 4 clearly testify the effectiveness of MCA over feature concatenation and feature addition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed RSTT. The features extracted from four input LFR and LR frames are processed by encoders E k , k = 0, 1, 2, 3 to build dictionaries that will be used as inputs for the decoders D k , k = 0, 1, 2, 3. The query builder generates a vector of queries Q which are then used to synthesize a sequence of seven consecutive HFR and HR frames. The Multi-Swin transformer encoder and decoder blocks contain a set of repeated Swin Transformer Blocks, which are illustrated in more details inFigure 3and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, given a predefined window size M ? M , a Swin Transformer block partitions the input video frames of size N ?H ?W ?C into N ? ? H M ? ? ? W M ? ? C non-overlapping windows, where we choose N = 4, M = 4 and C = 96 in our experiments. After flattening the features in each window to produce feature maps of size N HW M 2 ? M 2 ? C, Layer Normalization (LN) is applied to the features before Window-based Multi-head Self-Attention (W-MSA) [23] computes the local attention inside each window. Next, a Multi-Layer Perception (MLP) following another LN layer are used for further transformation. An additional Swin Transformer block with Shifted Window-based Multi-head Self-Attention (SW-MSA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The basic Swin Transformer decoder block used in D k , k = 0, 1, 2, 3 of RSTT; seeFigure 2. It takes a query Q and the output from the corresponding encoder E k as the input, and outputs HFR and HR features for spatial-temporal interpolation. Here, MCA stands for Multi-Head Cross-Attention, and other notations are similar to those inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Windowed 2-Head attention of the last decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons on various datasets with the state-of-the-art STVSR methods. PSNR and SSIM are computed on Y channel only, as same as<ref type="bibr" target="#b51">[52]</ref>. Top three numbers of each column are bolded, with the best in red and the second best in blue. FPS is computed on Nvidia Quadro RTX 6000 machine and on Vid4 dataset, which has the output frame size of 720 ? 576.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Vid4</cell><cell cols="2">Vimeo-Fast</cell><cell cols="2">Vimeo-Medium</cell><cell cols="2">Vimeo-Slow</cell><cell>FPS</cell><cell>Parameters</cell></row><row><cell>VFI+(V)SR/STVSR</cell><cell cols="2">PSNR? SSIM?</cell><cell cols="2">PSNR? SSIM?</cell><cell cols="3">PSNR? SSIM? PSNR?</cell><cell>SSIM?</cell><cell>?</cell><cell>(Millions) ?</cell></row><row><cell>SuperSloMo [16] + Bicubic</cell><cell>22.84</cell><cell>0.5772</cell><cell>31.88</cell><cell>0.8793</cell><cell>29.94</cell><cell>0.8477</cell><cell>28.37</cell><cell>0.8102</cell><cell>-</cell><cell>19.8</cell></row><row><cell>SuperSloMo [16] + RCAN [61]</cell><cell>23.80</cell><cell>0.6397</cell><cell>34.52</cell><cell>0.9076</cell><cell>32.50</cell><cell>0.8884</cell><cell>30.69</cell><cell>0.8624</cell><cell>-</cell><cell>19.8+16.0</cell></row><row><cell>SuperSloMo [16] + RBPN [14]</cell><cell>23.76</cell><cell>0.6362</cell><cell>34.73</cell><cell>0.9108</cell><cell>32.79</cell><cell>0.8930</cell><cell>30.48</cell><cell>0.8584</cell><cell>-</cell><cell>19.8+12.7</cell></row><row><cell>SuperSloMo [16] + EDVR [49]</cell><cell>24.40</cell><cell>0.6706</cell><cell>35.05</cell><cell>0.9136</cell><cell>33.85</cell><cell>0.8967</cell><cell>30.99</cell><cell>0.8673</cell><cell>-</cell><cell>19.8+20.7</cell></row><row><cell>SepConv [33] + Bicubic</cell><cell>23.51</cell><cell>0.6273</cell><cell>32.27</cell><cell>0.8890</cell><cell>30.61</cell><cell>0.8633</cell><cell>29.04</cell><cell>0.8290</cell><cell>-</cell><cell>21.7</cell></row><row><cell>SepConv [33] + RCAN [61]</cell><cell>24.92</cell><cell>0.7236</cell><cell>34.97</cell><cell>0.9195</cell><cell>33.59</cell><cell>0.9125</cell><cell>32.13</cell><cell>0.8967</cell><cell>-</cell><cell>21.7+16.0</cell></row><row><cell>SepConv [33] + RBPN [14]</cell><cell>26.08</cell><cell>0.7751</cell><cell>35.07</cell><cell>0.9238</cell><cell>34.09</cell><cell>0.9229</cell><cell>32.77</cell><cell>0.9090</cell><cell>-</cell><cell>21.7+12.7</cell></row><row><cell>SepConv [33] + EDVR [49]</cell><cell>25.93</cell><cell>0.7792</cell><cell>35.23</cell><cell>0.9252</cell><cell>34.22</cell><cell>0.9240</cell><cell>32.96</cell><cell>0.9112</cell><cell>-</cell><cell>21.7+20.7</cell></row><row><cell>DAIN [1] + Bicubic</cell><cell>23.55</cell><cell>0.6268</cell><cell>32.41</cell><cell>0.8910</cell><cell>30.67</cell><cell>0.8636</cell><cell>29.06</cell><cell>0.8289</cell><cell>-</cell><cell>24.0</cell></row><row><cell>DAIN [1] + RCAN [61]</cell><cell>25.03</cell><cell>0.7261</cell><cell>35.27</cell><cell>0.9242</cell><cell>33.82</cell><cell>0.9146</cell><cell>32.26</cell><cell>0.8974</cell><cell>-</cell><cell>24.0+16.0</cell></row><row><cell>DAIN [1] + RBPN [14]</cell><cell>25.96</cell><cell>0.7784</cell><cell>35.55</cell><cell>0.9300</cell><cell>34.45</cell><cell>0.9262</cell><cell>32.92</cell><cell>0.9097</cell><cell>-</cell><cell>24.0+12.7</cell></row><row><cell>DAIN [1] + EDVR [49]</cell><cell>26.12</cell><cell>0.7836</cell><cell>35.81</cell><cell>0.9323</cell><cell>34.66</cell><cell>0.9281</cell><cell>33.11</cell><cell>0.9119</cell><cell>-</cell><cell>24.0+20.7</cell></row><row><cell>STARnet [15]</cell><cell>26.06</cell><cell>0.8046</cell><cell>36.19</cell><cell>0.9368</cell><cell>34.86</cell><cell>0.9356</cell><cell>33.10</cell><cell>0.9164</cell><cell>3.85</cell><cell>111.61</cell></row><row><cell>Zooming SlowMo [52]</cell><cell>26.31</cell><cell>0.7976</cell><cell>36.81</cell><cell>0.9415</cell><cell>35.41</cell><cell>0.9361</cell><cell>33.36</cell><cell>0.9138</cell><cell>15.59</cell><cell>11.10</cell></row><row><cell>TMNet [54]</cell><cell>26.43</cell><cell>0.8016</cell><cell>37.04</cell><cell>0.9435</cell><cell>35.60</cell><cell>0.9380</cell><cell>33.51</cell><cell>0.9159</cell><cell>14.33</cell><cell>12.26</cell></row><row><cell>RSTT-L</cell><cell>26.43</cell><cell>0.7994</cell><cell>36.80</cell><cell>0.9403</cell><cell>35.66</cell><cell>0.9381</cell><cell>33.50</cell><cell>0.9147</cell><cell>14.98</cell><cell>7.67</cell></row><row><cell>RSTT-M</cell><cell>26.37</cell><cell>0.7978</cell><cell>36.78</cell><cell>0.9401</cell><cell>35.62</cell><cell>0.9377</cell><cell>33.47</cell><cell>0.9143</cell><cell>19.07</cell><cell>6.08</cell></row><row><cell>RSTT-S</cell><cell>26.29</cell><cell>0.7941</cell><cell>36.58</cell><cell>0.9381</cell><cell>35.43</cell><cell>0.9358</cell><cell>33.30</cell><cell>0.9123</cell><cell>26.19</cell><cell>4.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Quantitative comparisons on Vid4 and Vimeo-Fast datasets between MCA and other information fusion methods. PSNR and SSIM are computed on Y channel only. RSTT employs Multi-head Cross Attention (MCA) to generate features in Decoders based on corresponding Encoders and Queries. To further investigate the effectiveness of this design, we replace MCA in RSTT-S with other ways Quantitative comparisons on Vimeo-Medium and Vimeo-Slow datasets between MCA and other information fusion methods. PSNR and SSIM are computed on Y channel only.</figDesc><table><row><cell>Decoder</cell><cell></cell><cell>Vid4</cell><cell cols="2">Vimeo-Fast</cell></row><row><cell></cell><cell cols="2">PSNR? SSIM?</cell><cell cols="2">PSNR? SSIM?</cell></row><row><cell>MCA</cell><cell>26.29</cell><cell>0.7941</cell><cell>36.58</cell><cell>0.9381</cell></row><row><cell>Concat</cell><cell>26.18</cell><cell>0.7879</cell><cell>36.29</cell><cell>0.9346</cell></row><row><cell>Add</cell><cell>26.13</cell><cell>0.7865</cell><cell>36.25</cell><cell>0.9340</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08070</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10607" to="10614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Videnn: Deep blind video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cdfi: Compression-driven network design for frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8001" to="8011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Space-time-aware multi-resolution video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5792" to="5801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhubo</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanhua</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12928</idno>
		<title level="m">Video super resolution based on deep learning: A comprehensive survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Moving gradients: a path-based method for plausible image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Chung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phasenet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Space-time super-resolution using graph-cut optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uma</forename><surname>Mudenagudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashis</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Kumar Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="995" to="1008" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12622</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learned video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3454" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Increasing space-time resolution in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="753" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Space-time super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Video interpolation via generalized deformable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangdi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10680</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">In the eye of the beholder: The impact of frame rate on human eye blink</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naohisa</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunori</forename><surname>Sugiura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2321" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4472" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards real-time deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1354" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Videoconferencing on the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Turletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Huitema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on networking</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="351" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5232" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning for video super-resolution through hr optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Streaming video over the internet: approaches and directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><forename type="middle">Thomas</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Qin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Peha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="282" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3370" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Zooming slowmo: An efficient one-stage framework for space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal modulation network for controllable space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">Murray</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04588</idno>
		<title level="m">mbert, or bibert? a study on contextualized embeddings for neural machine translation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">End-toend semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09018</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Tdan: Temporally deformable alignment network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu Yapeng Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3360" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Zoom-in-to-check: Boosting video interpolation via instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12183" to="12191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How Video Super-Resolution and Frame Interpolation Mutually Benefit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
