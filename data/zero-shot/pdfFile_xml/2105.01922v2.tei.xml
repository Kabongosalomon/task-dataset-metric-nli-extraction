<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">Amadeus</forename><surname>Varga</surname></persName>
							<email>leon.varga@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">University of Tuebingen</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kiefer</surname></persName>
							<email>benjamin.kiefer@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">University of Tuebingen</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Messmer</surname></persName>
							<email>martin.messmer@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">University of Tuebingen</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
							<email>andreas.zell@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">University of Tuebingen</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and rescue missions in maritime environments due to their flexible and fast operation capabilities. Modern computer vision algorithms are of great interest in aiding such missions. However, they are dependent on large amounts of real-case training data from UAVs, which is only available for traffic scenarios on land. Moreover, current object detection and tracking data sets only provide limited environmental information or none at all, neglecting a valuable source of information. Therefore, this paper introduces a large-scaled visual object detection and tracking benchmark (SeaDronesSee) aiming to bridge the gap from land-based vision systems to sea-based ones. We collect and annotate over 54,000 frames with 400,000 instances captured from various altitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 ? degrees while providing the respective meta information for altitude, viewing angle and other meta data. We evaluate multiple state-of-theart computer vision algorithms on this newly established benchmark serving as baselines. We provide an evaluation server where researchers can upload their prediction and compare their results on a central leaderboard 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unmanned Aerial Vehicles (UAVs) equipped with cameras have grown into an important asset in a wide range of fields, such as agriculture, delivery, surveillance, and search and rescue (SAR) missions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b20">21]</ref>. In particular, UAVs are capable of assisting in SAR missions due to their fast and versatile applicability while providing an overview over the scene <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. Especially in maritime * These authors contributed equally to this work. The order of names is determined by coin flipping <ref type="bibr" target="#b0">1</ref> The leaderboard, the data set and the code to reproduce our results are available at https://seadronessee.cs.uni-tuebingen.de. scenarios, where wide areas need to be quickly overseen and searched, the efficient use of autonomous UAVs is crucial <ref type="bibr" target="#b54">[54]</ref>. Among the most challenging issues in this application scenario is the detection, localization, and tracking of people in open water <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">41]</ref>. The small size of people relative to search radii and the variability in viewing angles and altitudes require robust vision-based systems.</p><p>Currently, these systems are implemented via datadriven methods such as deep neural networks. These methods depend on large-scale data sets portraying real-case scenarios to obtain realistic imagery statistics. However, there is a great lack of large-scale data sets in maritime environ-ments. Most data sets captured from UAVs are land-based, often focusing on traffic environments, such as VisDrone <ref type="bibr" target="#b58">[58]</ref> and UAVDT <ref type="bibr" target="#b15">[16]</ref>. Many of the few data sets that are captured in maritime environments fall in the category of remote sensing, often leveraging satellite-based synthetic aperture radar <ref type="bibr" target="#b11">[12]</ref>. All of these are only valuable for ship detection <ref type="bibr" target="#b10">[11]</ref> as they don't provide the resolution needed for SAR missions. Furthermore, satellite-based imagery is susceptible to clouds and only provides top-down views. Finally, many current approaches in the maritime setting rely on classical machine learning methods, incapable of dealing with the large number of influencing variables and calling for more elaborate models <ref type="bibr" target="#b44">[44]</ref>.</p><p>This work aims to close the gap between large-scale land-based data sets captured from UAVs to maritime-based data sets. We introduce a large-scale data set of people in open water, called SeaDronesSee. We captured videos and images of swimming probands in open water with various UAVs and cameras. As it is especially critical in SAR missions to detect and track objects from a large distance, we captured the RGB footage with 3840?2160 px to 5456?3632 px resolution. We carefully annotated ground-truth bounding box labels for objects of interest including swimmer, floater (swimmer with life jacket), life jacket, swimmer ? (person on boat not wearing a life jacket), floater ? (person on boat wearing a life jacket), and boat.</p><p>Moreover, we note that current data sets captured from UAVs only provide very coarse or no meta information at all. We argue that this is a major impediment in the development of multi-modal systems, which take these additional information into account to improve accuracy or speed. Recently, methods that rely on these meta data were proposed. However, they note the lack of large-scaled publicly available data set in that regime (see e.g. <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b36">36]</ref>). Therefore, we provide precise meta information for every frame and image including altitude, camera angle, speed, time, and others.</p><p>In maritime settings, the use of multi-spectral cameras with Near Infrared channels to detect humans can be advantageous <ref type="bibr" target="#b19">[20]</ref>. For that reason, we also captured multispectral images using a MicaSense RedEdge. This enables the development of detectors taking into account the nonvisible light spectra Near Infrared (842 nm) and Red Edge (717 nm).</p><p>Finally, we provide detailed statistics of the data set and conduct extensive experiments using state-of-the-art models and hereby establish baseline models. These serve as a starting point for our SeaDronesSee benchmark. We release the training and validation sets with complete bounding box ground truth but only the test set's videos/images. The ground truth of the test set is used by the benchmark server to calculate the generalization power of the models. We set up an evaluation web page, where researchers can upload their predictions and opt to publish their results on a central leader board such that transparent comparisons are possible. The benchmark focuses on three tasks: (i) object detection, (ii) single-object tracking and (iii) multi-object tracking, which will be explained in more detail in the subsequent sections. Our main contributions are as follows:</p><p>? To the best of our knowledge, SeaDronesSee is the first large annotated UAV-based data set of swimmers in open water. It can be used to further develop detectors and trackers for SAR missions.</p><p>? We provide full environmental meta information for every frame making SeaDroneSee the first UAV-based data set of that nature.</p><p>? We provide an evaluation server to prevent researches from overfitting and allow for fair comparisons.</p><p>? We perform extensive experiments on state-of-the-art object detectors and trackers on our data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review major labeled data sets in the field of computer vision from UAVs and in maritime scenarios which are usable for supervised learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Labeled Data Sets Captured from UAVs</head><p>Over the last few years, quite a few data sets captured from UAVs have been published. The most prominent are these that depict traffic situations, such as VisDrone <ref type="bibr" target="#b58">[58]</ref> and UAVDT <ref type="bibr" target="#b15">[16]</ref>. Both data sets focus on object detection and object tracking in unconstrained environments. Pei et al. <ref type="bibr" target="#b43">[43]</ref>   <ref type="table">Table 1</ref>. Comparison with the most prominent annotated aerial data sets. 'Altitude' and 'Angle' indicate whether or not there are precise altitude and angle view information available. 'Other meta' refers to time stamps, GPS, and IMU data and in the case of object tracking can also mean attribute information about the sequences. The values with stars have been estimated based on ground truth bounding box sizes and corresponding real world object sizes (for altitude) and qualitative estimation of sample images (for angle). For DOTA and Airbus Ship the range of altitudes is not available because these are satellite-based data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Labeled Data Sets in Maritime Environments</head><p>Many data sets in maritime environments are captured from satellite-based synthetic aperture radar and therefore fall into the remote sensing category. In this category, the airbus ship data set <ref type="bibr" target="#b1">[2]</ref> is prominent, featuring 40k images from synthetic aperture radars with instance segmentation labels. Li et al. <ref type="bibr" target="#b30">[30]</ref> provide a data set of ships with images mainly taken from Google Earth, but also a few UAV-based images. In <ref type="bibr" target="#b52">[52]</ref>, the authors provide satellite-based images from natural scenes, mainly land-based but also harbors. The most similar to our work is <ref type="bibr" target="#b34">[34]</ref>. They also consider the problem of human detection in open water. However, their data mostly contains images close to shores and of swimming pools. Furthermore, it is not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-Modal Data Sets Captured from UAVs</head><p>UAVDT <ref type="bibr" target="#b15">[16]</ref> provides coarse meta data for their object detection and tracking data: every frame is labeled with altitude information (low, medium, high), angle of view (front-view, side-view, bird-view) and light conditions (day, night, foggy). Wu et al. <ref type="bibr" target="#b51">[51]</ref> manually label VisDrone after its release with the same annotation information for the object detection track. Mid-Air <ref type="bibr" target="#b18">[19]</ref> is a synthetic multimodal data set with images in nature containing precise altitude, GPS, time, and velocity data but without annotated objects. Blackbird <ref type="bibr" target="#b6">[7]</ref> is a real-data indoor data set for agile perception also featuring these meta information. In <ref type="bibr" target="#b35">[35]</ref>, street-view images with the same meta data are captured to benchmark appearance-based localization. Bozcan et al.</p><p>[10] release a low-altitude (&lt; 30 m) object detection data set containing images showing a traffic circle and provide meta data such as altitude, GPS, and velocity but exclude the import camera angle information.</p><p>Tracking data sets often provide meta data (or attribute information) for the clips. However, in many cases these do not refer to the environmental state in which the image was captured. Instead, they abstractly describe the way in which a clip was captured: UAV123 <ref type="bibr" target="#b39">[39]</ref> label their clips with information such as aspect ratio change, background clutter, and fast motion, but do not provide frame-by-frame meta data. The same observation can be made for the tracking track of VisDrone <ref type="bibr" target="#b17">[18]</ref>. See <ref type="table">Table 1</ref> for an overview of annotated aerial data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Set Generation</head><p>We gathered the footage on several days to obtain variance in light conditions. Taking into account safety and environmental regulations, we asked over 20 test subjects to be recorded in open water. Boats transported the subjects to the area of interest, where quadcopters were launched at a safe distance from the swimmers. At the same time, the fixed-wing UAV Trinity F90+ was launched from the shore. We used waypoints to ensure a strict flight schedule to maximize data collection efficiency. Care was taken to maintain  <ref type="table">Table 3</ref>. Meta data that comes with every image/frame. a strict vertical separation at all times. Subjects were free to wear life jackets, of which we provided several differently colored pieces (see also <ref type="figure">Figure 2</ref>). To diminish the effect of camera biases within the data set, we used multiple cameras, as listed in <ref type="table">Table 2</ref>, mounted to the following drones: DJI Matrice 100, DJI Matrice 210, DJI Mavic 2 Pro, and a Quantum Systems Trinity F90+. With the video cameras, we captured videos at 30 fps. For the object detection task, we extract at most three frames per second of these videos to avoid having redundant occurrences of frames. See Section 4 for information on the distribution of images with respect to different cameras.</p><p>Lastly, we captured top-down looking multi-spectral imagery at 1 fps. We used a MicaSense RedEdge-MX, which records five wavelengths (475 nm, 560 nm, 668 nm, 717 nm, 842 nm). Therefore, in addition to the RGB channels, the recordings also contain a RedEdge and a Near Infrared channel. The camera was referenced with a white reference before each flight. As the RedEdge-MX captures every band individually, we merge the bands using the development kit provided by MicaSense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Meta Data Collection</head><p>Accompanied with every frame there is a meta stamp, that is logged at 10 hertz. To align the video data (30 fps) and the time stamps, a nearest neighbor method was performed. The data in <ref type="table">Table 3</ref> is logged and provided for every image/frame read from the onboard clock, barometer, IMU and GPS sensor, and the gimbal, respectively.</p><p>Note that ? = 90 ? corresponds to a top-down view, and ? = 0 ? to a horizontally facing camera. The date format is given in the extended form of ISO 8601. Furthermore, note that the UAV roll/pitch/yaw-angles are of minor importance for meta-data-aware vision-based methods as the onboard gimbal filters out movement by the drone such that the camera pitch angle is roughly constant if it is not intentionally changed <ref type="bibr" target="#b24">[25]</ref>. Note that the gimbal yaw angle is not included, as we fix it to coincide with the UAV's yaw angle.</p><p>We need to emphasize that the meta values lie within the error thresholds introduced by the different sensors, but an extended analysis is beyond the scope of this paper (see e.g. <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">29]</ref> for an overview).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Annotation Method</head><p>Using the non-commercial labeling tool DarkLabel <ref type="bibr" target="#b2">[3]</ref>, we manually and carefully annotated all provided images and frames with the categories swimmer (person in water without life jacket), floater (person in water with life jacket), life jacket, swimmer ? (person on boat without life jacket), floater ? (person on boat with life jacket), and boats. We note that it is not sufficient to infer the class floater by the location from swimmer and life jacket as this can be highly ambiguous. Subsequently, all annotations were checked by experts in aerial vision. We choose these classes as they are the hardest and most critical to detect in SAR missions. Furthermore, we annotated regions with other objects as ignored regions, such as boats on land. Moreover, the data set also covers unlabeled objects, which may not be of interest, like driftwood, birds or the coast such that detectors can be robust to distinguish from those objects. Our guidelines for the annotation are described in the appendix. See <ref type="figure">Figure 2</ref> for examples of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Set Split Object Detection</head><p>To ensure that the training, validation, and testing set have similar statistics, we roughly balance them such that the respective subsets have similar distributions with respect to altitude and angle of view, two of the most important factors of appearance changes. Of the individual images, we randomly select 4 /7 and add it to the training set, add 1 /7 to the validation set and another 2 /7 to the testing set. In addition to the individual images, we randomly cut every video into three parts of length 4 /7, 1 /7, and 2 /7 of the original length and add every 10-th frame of the respective parts to the training, validation, and testing set. This is done to avoid having subsequent frames in the training and testing set such that a realistic evaluation is possible. We release the training and validation set with all annotations and the testing set's images, but withhold its annotations. Evalu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Floater Floater</head><p>Swimmer Swimmer Floater ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Floater</head><p>Life jacket Swimmer Swimmer Swimmers ? <ref type="figure">Figure 2</ref>. Examples of objects. Note that these examples are crops from high-resolution images. However, as the objects are small and the images taken from high altitudes, they appear blurry.</p><p>ation will be available via an evaluation server, where the predictions on the test set can be uploaded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Tracking</head><p>Similarly, we take 4 /7 of our recorded clips as the training clips, 1 /7 as the validation clips and 2 /7 as the testing clips. As for the object detection task, we withhold the annotations for the testing set and provide an evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Set Tasks</head><p>There are many works on UAV-based maritime SAR missions, focusing on unified frameworks describing the process of how to search and rescue people <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b21">22]</ref>. These works answer questions corresponding to path planning, autonomous navigation and efficient signal transmission. Most of them rely on RGB sensors and detection and tracking algorithms to actually find people of interest. This commonality motivates us to extract the specific tasks of object detection and tracking, which pose some of the most challenging issues in this application scenario.</p><p>Maritime environments from a UAV's perspective are difficult for a variety of reasons: Reflective regions and shadows resulting from different cardinal points (such as in <ref type="figure" target="#fig_0">Fig. 1</ref>) that could lead to false positives or negatives; people may be hardly visible or occluded by waves or sea foam (see Supplementary material); typically large areas are overseen such that objects are particularly small <ref type="bibr" target="#b38">[38]</ref>. We note that these factors are on top of general UAV-related detection difficulties. Now, we proceed to describe the specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Detection</head><p>There are 5,630 images (training: 2,975; validation: 859; testing: 1,796). See <ref type="figure" target="#fig_1">Figure 3</ref> for the distribution of images/frames with respect to cameras and the class distribu-tion. We recorded most of the images with the L1D-20c and UMC-R10C, having the highest resolution. Having the lowest resolution, we recorded only 432 images with the RedEdge-MX. Note, for the Object Detection Task only the RGB-channels of the multi-spectral images are used to support a uniform data structure.</p><p>Furthermore, the class distribution is slightly skewed towards the class 'boat', since safety precautions require boats to be nearby. We emphasize that this bias can easily be diminished by blackening the respective regions, as is common for areas which are not of interest or undesired (such as boats here; see e.g. <ref type="bibr" target="#b15">[16]</ref>). Right after that, swimmers with life jacket are the most common objects. We argue that this scenario is very often encountered in SAR missions. This type of class often is easier to detect than just swimmer as life jackets mostly are of contrasting color, such as red or orange (see <ref type="figure">Fig. 2</ref> and <ref type="table">Table 4</ref>). However, as it is also a likely scenario to search for swimmers without life jacket, we included a considerable amount. There are also several different manifestations/visual appearances of that class which is why we recorded and annotated swimmers with and without adequate swimwear (such as wet suit). To be able to discriminate between humans in water and humans on boats, we also annotated humans on boats (with and without life jackets). Lastly, we annotated a small amount of life jackets only. However, we note that the discrimination between life jackets and humans in life jackets can become visually ambiguous, especially in higher altitudes. See also <ref type="figure">Fig. 2</ref>. <ref type="figure" target="#fig_2">Figure 4</ref> shows the distribution of images with respect to the altitude and viewing angle they were captured at. Roughly 50% of the images were recorded below 50 m because lower altitudes allow for the whole range of available viewing angles (0 ? 90 ? ). That is, to cover all viewing angles, more images at these altitudes had to be taken. On the other hand, there are many images facing downwards (90 ? ), because images taken at greater altitudes tend to face down-  wards since acute angles yield image areas with tiny pixel density, which is unsuitable for object detection. Nevertheless, every altitude and angle interval is sufficiently represented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single-Object Tracking</head><p>We provide 208 short clips (&gt;4 seconds) with a total of 393,295 frames (counting the duplicates), including all available objects labeled. We randomly split the sequences into 58 training, 70 validation and 80 testing sequences. We do not support long-term tracking. The altitude and angle distributions are similar to these in the object detection section since the origin of the images of the object detection task is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Object Tracking</head><p>We provide 22 clips with a total of 54,105 frames and 403,192 annotated instances, the average consists of 2,460 frames. We differentiate between two use-cases. In the first task, only the persons in water (floaters and swimmers) are tracked, it is called MOT-Swimmer. In the second task, all objects in water are tracked (also the boats, but not people on boats), called MOT-All-Objects-In-Water. In both tasks, all objects are grouped into one class. The data set split is performed as described in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-Spectral Footage</head><p>Along with the data for the three tasks, we provide multispectral images. We supply annotations for all channels of these recordings, but only the RGB-channels are currently part of the Object Detection Task. There are 432 images with 1,901 instances. See <ref type="figure" target="#fig_0">Figure 1</ref> for an example of the individual bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluations</head><p>We evaluate current state-of-the-art object detectors and object trackers on SeaDronesSee. All experiments can be reproduced by using our provided code available on the evaluation server. Furthermore, we refer the reader to the Supplementary Material for the exact form and uploading requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object Detection</head><p>The used detectors can be split into two groups. The first group consists of two-stage detectors, which are mainly built on Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and its improvements. Built for optimal accuracy, these models often lack the inference speed needed for real-time employment, especially on embedded hardware, which can be a vital use-case in UAVbased SAR missions. For that reason, we also evaluate on one-stage detectors. In particular, we perform experiments with the best performing single-model (no ensemble) from the workshop report <ref type="bibr" target="#b60">[60]</ref>: a Faster R-CNN with a ResNeXt-101 64-4d <ref type="bibr" target="#b53">[53]</ref> backbone with P6 removed. For large onestage detectors, we take the recent CenterNet <ref type="bibr" target="#b57">[57]</ref>. To further test an object detector in real-time scenarios, we choose the current best model family on the COCO test-dev according to <ref type="bibr" target="#b3">[4]</ref>, i.e. EfficientDet <ref type="bibr" target="#b49">[49]</ref>, and take the smallest model, D0, which can run in real-time on embedded hardware, such as the Nvidia Xavier <ref type="bibr" target="#b27">[27]</ref>. We refer the reader to the appendix for the exact parameter configurations and training configurations of the individual models.</p><p>Similar to the VisDrone benchmark <ref type="bibr" target="#b58">[58]</ref>, we evaluate detectors according to the COCO json-format <ref type="bibr" target="#b32">[32]</ref>, i.e. average precision at certain intersection-over-unionthresholds. More specifically, we use AP=AP IoU=0.5:0.05:0.95 , AP 50 =AP IoU=0.5 and AP 75 =AP IoU=0.75 . Furthermore, we evaluate the maximum recalls for at most 1 and 10 given detections, respectively, denoted AR 1 =AR max=1 , and AR 10 =AR max=10 . All these metrics are averaged over all categories (except for "ignored region"). We furthermore provide the class-wise average precisions. Moreover, similar to <ref type="bibr" target="#b27">[27]</ref>, we report AP 50 -results on different equidistant levels of altitudes 'low' = 5-56 m (L), 'low-medium' = 55-106 m (LM), 'medium' = 106-157 m (M), 'medium-high' Model AP AP <ref type="bibr" target="#b50">50</ref>   = 157-208 m (MH), and 'high' = 208-259 m (H). To measure the universal cross-domain performance, we report the average over these domains, denoted AP avg 50 . Similarly, we report AP 50 -results for different angles of view: 'acute' = 7-23 ? (A), 'acute-medium' = 23-40 ? (AM), 'medium' = 40-56 ? (M), 'medium-right' = 56-73 ? (MR), and 'right' = 73-90 ? (R). Ultimately, it is the goal to have robust detectors across all domains uniformly, which is better measured by the latter metrics. <ref type="table">Table 4</ref> shows the results for all object detection models. As expected, the large Faster R-CNN with ResNeXt-101 64-4d backbone performs best, closely followed by CenterNet-Hourglass104. Medium-sized networks, such as the ResNet-50-FPN, and fast networks, such as CenterNet-ResNet18 and EfficientDet-D0, expectedly perform worse. However, the latter can run in real-time on an Nvidia Xavier <ref type="bibr" target="#b27">[27]</ref>. Swimmers are detected significantly worse than floaters by most detectors. Notably, life jackets are very hard to detect since from a far distance these are easily confused with swimmers ? (see <ref type="figure">Fig. 2</ref>). Since there is a heavy class imbalance with many fewer life jackets, detectors are biased towards floaters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Single-Object Tracking</head><p>Like VisDrone <ref type="bibr" target="#b59">[59]</ref>, we provide the success and precision curves for single-object tracking and compare models based on a single number, the success score. As comparison trackers, we choose the DiMP family (DiMP50, DiMP18, PrDiMP50, PrDiMP18) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> and Atom <ref type="bibr" target="#b12">[13]</ref> because they were the foundation of many of the submitted trackers to the last VisDrone workshop <ref type="bibr" target="#b17">[18]</ref>.  <ref type="figure">Figure 5</ref>. Success and precision plots for single-object tracking task (best viewed in color). <ref type="figure">Figure 5</ref> shows that the PrDiMP-and DiMP-family expectedly outperform the older Atom tracker in both, success and precision. Surprisingly, PrDiMP50 slightly trails the accuracy of its predecessor DiMP50. Furthermore, all trackers' performances on SeaDronesSee are similar or worse than on UAV123 (e.g. Atom with 65.0 success) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>, for which they were heavily optimized. We argue that in SeaDronesSee there is still room for improvement, especially considering that the clips feature precise meta information that may be helpful for tracking. <ref type="figure">Furthermore</ref> ever, we note that they are not capable of running in realtime on embedded hardware, a use-case especially important for UAV-based SAR missions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multi-Object Tracking</head><p>We use a similar evaluation protocol as the MOT benchmark <ref type="bibr" target="#b37">[37]</ref>. That is, we report results for Multiple Object Tracking Accuracy (MOTA), Identification F1 Score (IDF1), Multiple Object Tracking Precision (MOTP), number of false positives (FP), number of false negatives (FN), recall (R), precision (P), ID switches (ID sw.), fragmentation occurrences (Frag). We refer the reader to <ref type="bibr" target="#b46">[46]</ref> or the appendix for a thorough description of the metrics. We train and evaluate FairMOT <ref type="bibr" target="#b56">[56]</ref>, a popular tracker, which is the base of many trackers submitted to the challenge <ref type="bibr" target="#b16">[17]</ref>. FairMOT-D34 employs a DLA34 <ref type="bibr" target="#b55">[55]</ref> as its backbone while FairMOT-R34 makes use of a ResNet34. Another SOTA tracker is Tracktor++ <ref type="bibr" target="#b7">[8]</ref>, which we also use for our experiments. It performed well on the MOT20 <ref type="bibr" target="#b14">[15]</ref> challenge and is conceptually simple. Surprisingly, Tracktor++ was better than FairMOT in both tasks. One reason for this may be the used detector. Track-tor++ utilizes a Faster-R-CNN with a ResNet50 backbone. In contrast, FairMOT is using a CenterNet with a DLA34 and a ResNet34 backbone, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Meta-Data-Aware Object Detector</head><p>Developing meta-data-aware object detectors is difficult since there are no large-scale data sets to evaluate their performances. However, some works provide promising preliminary results using this metadata <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b27">27]</ref>. We provide an initial baseline from <ref type="bibr" target="#b27">[27]</ref> incorporating the meta data. We evaluate the performances of 5?Altitude@3-and 5?Angle@3-experts, which are constructed on top of a Faster R-CNN with ResNet-50-FPN, respectively. Essentially, these experts make use of meta-data by allowing the features to adapt to their responsible specific environmental domains.</p><p>As <ref type="table">Table 9</ref> shows, meta data can enhance the accuracy of an object detector considerably. For example, 5?Angle@3 outperforms its ResNet-50-FPN baseline by 3.1 AP avg 50 while running at the same inference speed. The improvements are especially significant for underrepresented domains, such as +9.2 and +6.4 AP avg 50 for the acute angle (A) and the medium angle (M), respectively, which are underrepresented as can be seen from <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This work serves as an introductory benchmark in UAVbased computer vision problems in maritime scenarios. We build the first large scaled-data set for detecting and tracking humans in open water. Furthermore, it is the first largescaled benchmark providing full environmental information for every frame, offering great opportunities in the so-far restricted area of multi-modal object detection and tracking. We offer three challenges, object detection, singleobject tracking, and multi-object tracking by providing an evaluation server. We hope that the development of metadata-aware object detectors and trackers can be accelerated by means of this benchmark. Moreover, we provide multispectral imagery for detecting humans in open water. These images are very promising in maritime scenarios, having the ability to capture wavelengths, which set apart objects from the water background.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Typical image examples with varying altitudes and angles of view: 250 m, 90 ? ; 50 m, 30 ? ; 10 m, 0 ? and 20 m, 90 ? (from top left to bottom right). (b) Examples of the Red Edge (717 nm, left) and Near Infrared (842 nm, right) light spectra of an image captured by the MicaSense RedEdge-MX. Note the glowing appearance of the swimmers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Distribution of training images over camera types (left) and distribution of objects over classes (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Distribution of images over altitudes (left) and angles (right), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 4 .</head><label>4</label><figDesc>Average precision results for several baseline models. The right part contains AP50-values for each class individually. All reported FPS numbers are obtained on a single Nvidia RTX 2080 Ti. The abbreviation 'F.' stands for Faster R-CNN. For visualization purposes, the classes are abbreviated as swimmer( ? ) ? S( ? ), floater( ? ) ? F( ? ), boat ? B, life jacket ? LJ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AP 75 AR 1 AR 10</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>S</cell><cell>F</cell><cell>S  ?</cell><cell>F  ?</cell><cell>B</cell><cell>LJ FPS</cell></row><row><cell>F. ResNeXt-101-FPN [53]</cell><cell cols="2">30.4 54.7</cell><cell cols="6">29.7 18.6 42.6 78.1 82.4 25.9 44.3 96.7 0.6</cell><cell>2</cell></row><row><cell>F. ResNet-50-FPN [23]</cell><cell cols="2">14.2 30.1</cell><cell>7.2</cell><cell>6.4</cell><cell cols="3">17.7 24.6 54.1 4.9</cell><cell>7.5 89.2 0.3</cell><cell>14</cell></row><row><cell cols="3">CenterNet-Hourglass104 [57] 25.6 50.3</cell><cell cols="6">22.2 17.7 40.1 65.1 73.6 19.1 48.1 95.8 0.3</cell><cell>6</cell></row><row><cell>CenterNet-ResNet101 [57]</cell><cell cols="2">15.1 36.4</cell><cell>10.8</cell><cell>9.6</cell><cell cols="3">21.4 16.8 39.8 0.8</cell><cell>1.7 74.3</cell><cell>0</cell><cell>22</cell></row><row><cell>CenterNet-ResNet18 [57]</cell><cell>9.9</cell><cell>21.8</cell><cell>9.0</cell><cell>7.2</cell><cell cols="3">19.7 20.9 21.9 2.6</cell><cell>3.3 81.9 0.4</cell><cell>78</cell></row><row><cell>EfficientDet-D0 [49]</cell><cell cols="2">20.8 37.1</cell><cell cols="5">20.6 11.5 29.1 65.3 55.1 3.1</cell><cell>3.3 95.5 0.1</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>and 6 show the performances for different altitudes and angles, respectively. These evaluations help assess the strength and weaknesses of individual models. For example, although ResNeXt-101-FPN performs overall better than Hourglass104 in AP 50 (54.7 vs. 50.3), the latter is better in the domain of medium angles (45.2 vs. 49.7). Furthermore, the great performance discrepancy between CenterNet-ResNet101 and CenterNet-ResNet18 in AP 50 (36.4 vs. 21.8) vanishes when averaged over angle domains (23.8 vs. 23.1 AP avg 50 ) possibly indicating ResNet101's bias towards specific angle domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Results on different angle-domains. For example, ResNeXt's AP50 performance in medium-right (MR) angles (57-73 ? ) is 63.6 AP50.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell>L</cell><cell>LM</cell><cell></cell><cell>M</cell><cell></cell><cell>MH</cell><cell>H</cell><cell>AP avg 50</cell></row><row><cell cols="9">ResNeXt-101-FPN 56.8 54.6 49.2</cell><cell>65</cell><cell>78.3</cell><cell>60.8</cell></row><row><cell></cell><cell cols="3">ResNet-50-FPN</cell><cell cols="6">32.8 29.8 23.5 40.5 48.9</cell><cell>35.1</cell></row><row><cell></cell><cell cols="3">Hourglass104</cell><cell cols="6">50.6 52.0 47.5 64.9 73.2</cell><cell>57.6</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet101</cell><cell cols="6">20.2 30.4 24.1 35.1 38.0</cell><cell>29.6</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet18</cell><cell cols="6">23.8 20.3 19.2 29.3 31.9</cell><cell>24.9</cell></row><row><cell></cell><cell></cell><cell>D0</cell><cell></cell><cell cols="6">39.6 38.0 30.4 42.5 54.5</cell><cell>41.0</cell></row><row><cell cols="10">Table 5. Results on different altitude-domains. E.g. ResNeXt's</cell></row><row><cell cols="10">AP50 performance in low-medium (LM) altitudes is 54.6 AP50.</cell></row><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell>A</cell><cell>AM</cell><cell></cell><cell>M</cell><cell></cell><cell>MR</cell><cell>R</cell><cell>AP avg 50</cell></row><row><cell cols="10">ResNeXt101-FPN 68.3 55.1 45.2 63.6 51.5</cell><cell>56.7</cell></row><row><cell></cell><cell cols="3">ResNet50-FPN</cell><cell cols="6">32.8 35.5 32.7 35.7 27.6</cell><cell>32.9</cell></row><row><cell></cell><cell cols="3">Hourglass104</cell><cell cols="6">66.4 42.1 49.7 58.7 46.9 52.76</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet101</cell><cell cols="6">7.4 35.8 20.5 33.6 21.7</cell><cell>23.8</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet18</cell><cell cols="6">9.6 29.5 26.3 27.9 22.1</cell><cell>23.1</cell></row><row><cell></cell><cell></cell><cell>D0</cell><cell></cell><cell cols="6">26.9 47.0 40.5 40.3 36.8</cell><cell>38.3</cell></row><row><cell cols="2">100</cell><cell></cell><cell>Success plot</cell><cell></cell><cell cols="2">100</cell><cell></cell><cell></cell><cell>Precision plot</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell>Overlap Precision [%]</cell><cell>20 40 60</cell><cell cols="2">DiMP50 [67.3] PrDiMP50 [67.0] PrDiMP18 [65.9] DiMP18 [64.6] Atom [63.8]</cell><cell></cell><cell>Distance Precision [%]</cell><cell>20 40 60</cell><cell></cell><cell></cell><cell>DiMP50 [86.8] PrDiMP50 [84.9] PrDiMP18 [83.5] DiMP18 [82.7] Atom [82.3]</cell></row><row><cell></cell><cell>0.0 0</cell><cell>0.2</cell><cell>0.4 Overlap threshold 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0</cell><cell>0</cell><cell cols="2">10 Location error threshold [pixels] 20 30 40</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>, in our experiments, the faster trackers DiMP18 and Atom run at approximately 27.1 fps on an Nvidia RTX 2080 Ti. How-Multi-Object Tracking evaluation results for the Swimmer task.Table 9. Results on different altitude-and angle-domains.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="6">MOTA IDF1 MOTP MT ML</cell><cell>FP</cell><cell>FN</cell><cell cols="2">Recall Prcn ID Sw. Frag</cell></row><row><cell cols="2">FairMOT-D34 [56]</cell><cell>39.0</cell><cell></cell><cell>44.8</cell><cell>23.6</cell><cell>17</cell><cell cols="3">17 3,604 9,445</cell><cell>57.2</cell><cell>77.8</cell><cell>307</cell><cell>1,687</cell></row><row><cell cols="2">FairMOT-R34 [56]</cell><cell>15.2</cell><cell></cell><cell>27.6</cell><cell>33.7</cell><cell>6</cell><cell cols="3">37 2,502 12,592</cell><cell>30.1</cell><cell>68.4</cell><cell>181</cell><cell>807</cell></row><row><cell cols="2">Tracktor++ [8]</cell><cell>55.0</cell><cell></cell><cell>69.6</cell><cell>25.6</cell><cell>62</cell><cell>4</cell><cell cols="2">7,271 3,550</cell><cell>85.5</cell><cell>74.2</cell><cell>165</cell><cell>347</cell></row><row><cell>Model</cell><cell></cell><cell cols="6">MOTA IDF1 MOTP MT ML</cell><cell>FP</cell><cell>FN</cell><cell cols="2">Recall Prcn ID Sw. Frag</cell></row><row><cell cols="2">FairMOT-D34 [56]</cell><cell>36.5</cell><cell></cell><cell>43.8</cell><cell>20.9</cell><cell>28</cell><cell cols="3">49 3,788 20,867</cell><cell>47.2</cell><cell>83.1</cell><cell>447</cell><cell>1,599</cell></row><row><cell cols="2">FairMOT-R34 [56]</cell><cell>30.5</cell><cell></cell><cell>40.8</cell><cell>27.3</cell><cell cols="4">29 127 4,401 28,999</cell><cell>40.2</cell><cell>81.6</cell><cell>285</cell><cell>1,588</cell></row><row><cell cols="2">Tracktor++ [8]</cell><cell>71.9</cell><cell></cell><cell>80.5</cell><cell>20.1</cell><cell>123</cell><cell>5</cell><cell cols="2">7,741 5,496</cell><cell>88.5</cell><cell>84.5</cell><cell>192</cell><cell>438</cell></row><row><cell></cell><cell cols="11">Table 8. Multi-Object Tracking evaluation results for the All-Objects-In-Water task.</cell></row><row><cell>Model</cell><cell>L</cell><cell>LM</cell><cell>M</cell><cell>MH</cell><cell>H</cell><cell>AP avg 50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">F. ResNet-50-FPN 32.8 29.8 23.5 40.5 48.9</cell><cell>35.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">5?Altitude@3[27] 32.8 29.9 26.2 41.5 48.9</cell><cell>35.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>A</cell><cell>AM</cell><cell>M</cell><cell>MR</cell><cell>R</cell><cell>AP avg 50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">F. ResNet-50-FPN 32.8 35.5 32.7 35.7 27.6</cell><cell>32.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5?Angle@3[27]</cell><cell cols="5">42.0 35.5 39.3 35.7 27.7</cell><cell>36.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Sebastian Koch, Hannes Leier and Aydeniz Soezbilir, without whose contribution this work would not have been possible. This work has been supported by the German Ministry for Economic Affairs and Energy, Project Avalon, FKZ: 03SX481B.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Aerial data accuracy -an experiment comparing 4 drone approaches</title>
		<ptr target="https://www.sitemark.com/blog/accuracy" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://www.kaggle.com/c/airbus-ship-detection.Ac-cessed" />
	</analytic>
	<monogr>
		<title level="j">Airbus Ship Detection Challenge</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Darklabel video/image labeling and annotation tool</title>
		<ptr target="https://github.com/darkpgmr/DarkLabel" />
		<imprint>
			<biblScope unit="page" from="2020" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Object Detection on COCO test-dev</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hyperspectral imaging: A review on uav-based sensors, data processing and applications for agriculture and forestry. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Ad?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon??</forename><surname>Hru?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><surname>P?dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Bessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Peres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquim Joao</forename><surname>Sousa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sardo: An automated search-and-rescue dronebased solution for victims localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Albanese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Sciancalepore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Costa-P?rez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05819</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The blackbird dataset: A large-scale dataset for uav perception in aggressive flight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amado</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winter</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sayre-Mccord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="130" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Au-air: A multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Bozcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erdal</forename><surname>Kayacan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8504" to="8510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Laurent Demagistri, and Michel Petit. A complete processing chain for ship detection using optical satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Corbane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Pecoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="5837" to="5854" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The state-of-the-art in ship detection in synthetic aperture radar imagery. defence science and technology organization (dsto)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dj Crisp</surname></persName>
		</author>
		<idno>No. DSTO-RR-0272</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
	<note>Information Science Laboratory</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">MOT20: A benchmark for multi object tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taix?taix?</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The unmanned aerial vehicle benchmark: Object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visdrone-mot2020: The vision meets drone multiple object tracking challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="713" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visdrone-sot2020: The vision meets drone single object tracking challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="728" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mid-air: A multi-modal dataset for extremely low altitude drone flights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection of bodies in maritime rescue operations using unmanned aerial vehicles with multispectral cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio-Javier</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="782" to="796" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uav-based situational awareness system using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Geraldes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Villerabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Salta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="122583" to="122594" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Determining position of target subjects in maritime search and rescue (msar) operations using rotary wing unmanned aerial vehicles (uavs)</title>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Information and Communication Technology (ICICTM)</title>
		<editor>Siti Nur Alidda Mohd Ghazali, Hardy Azmir Anuar, Syed Nasir Alsagoff Syed Zakaria, and Zaharin Yusoff</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dronebased object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4145" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The prototype of gyro-stabilized uav gimbal for day-night surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Jedrasiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Nawrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced technologies for intelligent systems of national border security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="107" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Karaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgur</forename><surname>Tatli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aynur</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Pasli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muhammed Fatih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suleyman</forename><surname>Beser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turedi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The potential use of unmanned aircraft systems (drones) in mountain search and rescue operations. The American journal of emergency medicine</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Leveraging domain labels for object detection from uavs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kloeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2118" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Accuracy assessment on drone measured heights at different height levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David L Kulhavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanli</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hsf-net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7147" to="7161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new method to combine detection and tracking algorithms for fast and accurate human localization in uav-based sar operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleftherios</forename><surname>Lvsouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Gasteratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Unmanned Aircraft Systems (ICUAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1688" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised human detection with an embedded vision system on a fully autonomous uav for search and rescue operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleftherios</forename><surname>Lygouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Santavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anastasios Taitzoglou, Konstantinos Tarchanidis, Athanasios Mitropoulos, and Antonios Gasteratos</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3542</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The zurich urban micro aerial vehicle dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andr?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Majdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Till</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="273" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gaining scale invariance in uav bird&apos;s eye view object detection by adaptive resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12694</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pratik Narang, and Vipul Mishra. Drone-surveillance for search and rescue in natural disaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balmukund</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communications</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wesam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shipwrecked victims localization and tracking using uavs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meriem</forename><surname>Chekir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Besbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Wireless Communications &amp; Mobile Computing Conference (IWCMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1344" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining human computing and machine learning to make sense of big (aerial) data for disaster response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Briant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Millet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Parkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human trajectory prediction in crowded scene using social-affinity long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Hong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Are object detection assessment criteria ready for maritime computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huixu</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chai</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5295" to="5304" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Autosos: Towards multi-uav systems supporting maritime search and rescue with lightweight ai and edge computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Pe?a Queralta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenni</forename><surname>Raitoharju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Nguyen Gia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomi</forename><surname>Westerlund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unmanned vehicle collaboration research environment for maritime search and rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Griendling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mavris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Congress of the International Council of the Aeronautical Sciences. International Council of the Aeronautical Sciences (ICAS)</title>
		<meeting><address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Uav delivery monitoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Khin Thida San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong</forename><surname>Ju Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon Seok</forename><surname>Hun Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MATEC Web of Conferences</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">4011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Nature conservation drones for automatic localization and counting of animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jan C Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Camiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Verschoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kitso</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Epema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Lian Pin Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Delving into robust object detection from unmanned aerial vehicles: A deep nuisance disentanglement approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A review on marine search and rescue operations using unmanned aerial vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sp Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Marine and Environmental Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="396" to="399" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairmot</surname></persName>
		</author>
		<title level="m">On the fairness of detection and re-identification in multiple object tracking. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ahenb&quot;uhl. Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07437</idno>
		<title level="m">Vision meets drones: A challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06303</idno>
		<title level="m">Vision meets drones: Past, present and future</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visdrone-det2018: The vision meets drone object detection in image challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinqin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Precise positioning of uavs-dealing with challenging rtk-gps measurement conditions during automated uav flights. IS-PRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Klingbeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
