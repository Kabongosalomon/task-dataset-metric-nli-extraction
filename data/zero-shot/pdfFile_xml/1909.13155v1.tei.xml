<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Energy-Based Learning for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
							<email>leipeng@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<email>sinisa@oregonstate.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Amazon.com Services, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Energy-Based Learning for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is about labeling video frames with action classes under weak supervision in training, where we have access to a temporal ordering of actions, but their start and end frames in training videos are unknown. Following prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU) for frame labeling. Our key contribution is a new constrained discriminative forward loss (CDFL) that we use for training the HMM and GRU under weak supervision. While prior work typically estimates the loss on a single, inferred video segmentation, our CDFL discriminates between the energy of all valid and invalid frame labelings of a training video. A valid frame labeling satisfies the ground-truth temporal ordering of actions, whereas an invalid one violates the ground truth. We specify an efficient recursive algorithm for computing the CDFL in terms of the logadd function of the segmentation energy. Our evaluation on action segmentation and alignment gives superior results to those of the state of the art on the benchmark Breakfast Action, Hollywood Extended, and 50Salads datasets. ?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents an approach to weakly supervised action segmentation by labeling video frames with action classes. Weak supervision means that in training our approach has access only to the temporal ordering of actions, but their ground-truth start and end frames are not provided. This is an important problem with a wide range of applications, since the more common fully supervised action segmentation typically requires expensive manual annotations of action occurrences in every video frame.</p><p>Our fundamental challenge is that the set of all possible segmentations of a training video may consist of multiple distinct valid segmentations that satisfy the provided ground-truth ordering of actions, along with invalid segmentations that violate the ground truth. It is not clear how * The work was done at the Oregon State University before Peng Lei joined Amazon. ? The code is available at https://github.com/JunLi-Galios/CDFL. to estimate loss (and subsequently train the segmenter) over multiple valid segmentations.</p><p>Motivation: Prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> typically uses a temporal model (e.g., deep neural network, or HMM) to infer a single, valid, optimal video segmentation, and takes this inference result as a pseudo ground truth for estimating the incurred loss. However, a particular training video may exhibit a significant variation (not yet captured by the model along the course of training), which may negatively affect estimation of the pseudo ground truth, such that the inferred action segmentation is significantly different from the true one. In turn, the loss estimated on the incorrect pseudo ground truth may corrupt training by reducing, instead of maximizing, the discriminative margin between the ground truth and other valid segmentations. In this paper, we seek to alleviate these issues.</p><p>Contributions: Prior work shows that a statistical language model is useful for weakly supervised learning and modeling of video sequences <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3]</ref>. Following <ref type="bibr" target="#b21">[22]</ref>, we also adopt a Hidden Markov Model (HMM) grounded on a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref> for labeling video frames. The major difference is that we do not generate a unique pseudo ground truth for training. Instead, we efficiently account for all candidate segmentations of a training video when estimating the loss. To this end, we formulate a new Constrained Discriminative Forward Loss (CDFL) as a difference between the energy of valid and invalid candidate video segmentations. In comparison with prior work, the CDFL improves robustness of our training, because minimizing the CDFL amounts to maximizing the discrimination margin between candidate segmentations that satisfy and violate ground truth, whereas prior work solely optimizes a score of the inferred single valid segmentation. Robustness of training is further improved when the CDFL takes into account only hard invalid segmentations whose edge energy is lower than that of valid ones. Along with the new CDFL formulation, our key contribution is a new recursive algorithm for efficiently estimating the CDFL in terms of the logadd function of the segmentation energy.</p><p>Our Approach: <ref type="figure">Fig. 1</ref> shows an overview of our weakly supervised training of the HMM with GRU that consists of <ref type="figure">Figure 1</ref>. Our weakly supervised training: For a training video, we first estimate candidate segmentation cuts using a Hidden Markov Model (HMM) grounded on a Gated Recurrent Unit (GRU), and then build a fully connected segmentation graph whose paths represent candidate action segmentations (colors mark different action classes along the paths). Then, we efficiently compute the Constrained Discriminative Forward Loss (CDFL) in terms of accumulated energy of all valid and invalid paths in the graph for our end-to-end training. (best seen in color) two steps. In the first step, we run a constrained Viterbi algorithm for HMM inference on a given training video so the resulting segmentation is valid. This initial video segmentation is used for efficiently building a fully connected segmentation graph aimed at representing alternative candidate segmentations. In this graph, nodes represent segmentation cuts of the initially inferred segmentation -i.e., video frames where one action ends and a subsequent one starts -and edges represent video segments between every two temporally ordered cuts. For improving action boundary detection, we further augment the initial set of nodes with video frames that are in a vicinity of every cut, as well as the initial set of edges with corresponding temporal links between the added nodes. Directed paths of such a fully connected graph explicitly represent many candidate action segmentations, beyond the initial HMM's inference.</p><p>The second step of our training efficiently computes a total energy score of frame labeling along all paths in the segmentation graph. Efficiency comes from our novel recursive estimation of the segmentation energy, where we exploit the accumulative property of the logadd function. A difference of the accumulated energy of action labeling along the valid and invalid paths is used to compute the CDFL. In this paper, we also consider several other loss formulations expressed in terms of the energy of valid and invalid paths. The loss is then used for training HMM parameters and back-propagated to the GRU for end-to-end training.</p><p>For inference on a test video, as in the first step of our training, we use a constrained Viterbi algorithm to perform the HMM inference which will satisfy at least one action sequence seen in training. Then, we use this initial video segmentation as an anchor for building the segmentation graph that comprises paths with finer action boundaries. Our output is the MAP path in the graph.</p><p>For evaluation, we consider the tasks of action segmentation and action alignment, where the latter provides additional information on the temporal ordering of actions in the test video. For both tasks on the Breakfast Action dataset <ref type="bibr" target="#b9">[10]</ref>, Hollywood Extended dataset <ref type="bibr" target="#b0">[1]</ref>, and 50-Salads dataset <ref type="bibr" target="#b23">[24]</ref>, we outperform the state of the art.</p><p>In the following, Sec. 2 reviews related work, Sec. 3 formulates our HMM and Constrained Viterbi for action segmentation, Sec. 4 describes how we construct the segmentation graph, Sec. 5 specifies our CDFL and related loss functions, and Sec. 6 presents our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews closely related work on weakly supervised action segmentation and Graph Transformer Networks. While a review of fully supervised action segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref> is beyond our scope, it is worth mentioning that our approach uses the same recurrent deep models for frame labeling as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref>. Also, our approach is motivated by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> which integrate HMMs and modeling of action length priors within a deep learning architecture.</p><p>Weakly supervised action segmentation has recently made much progress <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>. For example, Extended Connectionist Temporal Classification (ECTC) addresses action alignment under the constraint of being consistent with frame-to-frame visual similarity <ref type="bibr" target="#b7">[8]</ref>. Also, action segmentation has been addressed with a convex relaxation of discriminative clustering, and efficiently solved with the conditional gradient (Frank-Wolfe) algorithm <ref type="bibr" target="#b0">[1]</ref>. Other approaches use a local action model and a global temporal alignment model that are alternatively trained <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. Some methods initially predict a video segmentation with a temporal convolutional network, and then iteratively refine the action boundaries <ref type="bibr" target="#b6">[7]</ref>. Other approaches first generate pseudo-ground-truth labels for all video frames, e.g., with the Viterbi algorithm <ref type="bibr" target="#b21">[22]</ref>, and then train a classifier on these frame labels by minimizing the standard cross entropy loss. Finally, <ref type="bibr" target="#b20">[21]</ref> addresses a different weakly supervised setting from ours when the ground truth provides only a set of actions present without their temporal ordering .</p><p>All these approaches base their learning and prediction on estimating a penalty or probability of labeling individ-ual frames. In contrast, we use an energy-based framework with the following differences. First, in training, we minimize the total energy of valid paths in the segmentation graph rather than optimize labeling probabilities of each frame. Second, instead of considering a single optimal valid path in the segmentation graph, we specify a loss function in terms of all valid paths. Hence, the Viterbi-initialized training on pseudo-labels of frames <ref type="bibr" target="#b21">[22]</ref> represents a special case of our training done only for one valid path. In addition, our loss enforces discriminative training by accounting for invalid paths in the segmentation graph. Unlike <ref type="bibr" target="#b2">[3]</ref> that randomly selects invalid paths, we efficiently account for all hard invalid paths in training. Finally, our training is not iterative as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>, and does not require iterative refinement of action boundaries as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our CDFL extends the loss used for training of the Graph Transformer Network (GTN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>. To the best of our knowledge, the GTN has been used only for text parsing, and never for action segmentation. In comparison with the GTN training, we significantly reduce complexity by building the video's segmentation graph. Also, while the loss used for training the GTN accounts for both valid and invalid text parses, it cannot handle the special case when valid parses have lower scores than invalid ones. In contrast, our CDFL effectively accounts for the energy of valid and invalid paths, even when valid paths have significantly lower energy than invalid paths in the segmentation graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Model for Action Segmentation</head><p>Problem Setup: For each training video of length T , we are given unsupervised frame-level features, x 1:T = [x 1 , x 2 , ..., x T ], and the ground-truth ordering of action classes a 1:N = [a 1 , a 2 , ..., a N ], also referred to as the transcript. N is the length of the annotation sequence, and a n is nth action class in a 1:N that belongs to the set of K action classes, a n ? A = {1, 2, ..., K}. Note that T and N may vary across the training set, and that there may be more than one occurrences of the same action class spread out in a 1:N (but of course a n = a n+1 ).</p><p>In inference, given frame features x 1:T of a video, our goal is to find an optimal segmentation (? 1:N ,l 1:N ), wher? N is the predicted length of the action sequence, and l 1:N = [l 1 ,l 2 , ? ? ? ,lN ] includes the predicted number of video framesl n occupied by the predicted action? n .</p><p>The Model: We use an HMM to model the posterior distribution of a video segmentation (a 1:N , l 1:N ) given </p><p>In <ref type="formula" target="#formula_0">(1)</ref>, the likelihood p(x t |a) is estimated as</p><formula xml:id="formula_1">p(x t |a) ? p(a|x t ) p(a) ,<label>(2)</label></formula><p>where p(a|x t ) is the GRU's softmax score for action a ? A at frame t, and the prior distribution of action classes p(a) is an normalized frame frequency of action occurrences in the training dataset. The likelihood of action length is modeled as a class-dependent Poisson distribution</p><formula xml:id="formula_2">p(l|a) = ? l a l! e ??a ,<label>(3)</label></formula><p>where ? a is the mean length for class a ? A. Finally, the joint prior p(a 1:N ) is a constant if the transcript a 1:N exists in the training set; otherwise, p(a 1:N ) = 0. The same modeling formulation was well-motivated and used in state of the art <ref type="bibr" target="#b21">[22]</ref>. Constrained Viterbi Algorithm: Given a training video, we first find an optimal valid action segmentation (? 1:N ,l 1:N ) by maximizing (1) with a constrained Viterbi algorithm, which ensures that? 1:N is equal to the annotated transcript,? 1:N = a 1:N . Similarly, for inference on a test video, we first perform the constrained Viterbi algorithm against all transcripts {a 1:N } seen in training, i.e., ensure that the predicted? 1:N has been seen at least once in training. Thus, the initial step of our inference on a training or test video is the same as in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Our key difference from <ref type="bibr" target="#b21">[22]</ref>, is that we use the initial (? 1:N ,l 1:N ) to efficiently build a fully connected segmentation graph of the video, as explained in Sec. 4. Importantly, in training, the segmentation graph is not constructed to find a more optimal video segmentation that improves upon the initial prediction. Instead, the graph is used to efficiently account for all valid and invalid segmentations.</p><p>Given a video x 1:T and a transcript a 1:N , the constrained Viterbi algorithm recursively maximizes the posterior in <ref type="bibr" target="#b0">(1)</ref> such that the first n action labels of the transcript a 1:n = [a 1 , ..., a n ] ? a 1:N are respected at time t: p(a 1:n ,l 1:n |x 1:t ) = max <ref type="figure">Figure 2</ref>. Building the segmentation graph G (best seen in color). The initial nodes of G represent segmentation cuts bn obtained in the Constrained Viterbi (the predicted action classes are marked with different colors). Each bn generates additional vertices bn = {vns} representing neighboring video frames within a window centered at bn (the black rectangles), and corresponding new edges (vns, v n s ) (the dashed lines) between all temporally ordered pairs of vertices in G. For clarity, we show only a few edges. G has exponential many paths, each representing a candidate action segmentation.</p><formula xml:id="formula_3">N + 1 cuts, b 1:N +1 = [b 1 , . . . , b N +1 ],</formula><p>i.e., video frames where previous action ends and the next one starts including the very first frame b 1 and last frame b N +1 at time T .</p><p>We use these cuts to anchor our construction of the fully connected segmentation graph, G = (V, E, W), where V = {b 1:N +1 } is the set of nodes, E is the set of directed edges linking every two temporally ordered nodes, and W are the corresponding edge weights.</p><p>Some of the estimated cuts in b 1:N +1 may be false positives or may not exactly coincide with the true cuts. To improve action boundary detection, we augment the initial V with nodes representing neighboring video frames of each cut b n within a temporal window of length ? centered at b n , as illustrated in <ref type="figure">Fig. 2</ref>. For the first and last frames, we set ? = 1. Thus, each b n can be viewed as a hyper-node comprising additional vertices in G, V = {b n = {v n1 , ? ? ? , v ni , ? ? ? , v n? } : n = 1, . . . , N +1}, and accordingly additional edges E = {(v ni , v n i ) : n ? n , i &lt; i }. In the following, we simplify notation for vertices v ni ? v i ? V, and edges (v ni , v n i ) ? e ii = (v i , v i ).</p><p>Each edge e ii is assigned a weight vector w ii = [w ii (a)], where w ii (a) is defined as the energy of labeling the video segment (v i , v i ) with action class a ? A:</p><formula xml:id="formula_4">w ii (a) = t?(vi,v i ) ? log p(a|x t ),<label>(5)</label></formula><p>where p(a|x t ) is the GRU's softmax score for action a at frame t. G comprises exponentially many directed paths P = {?}, where each ? represents a particular video segmentation. In each ?, every edge e ii gets assigned only one action class a ? ii ? A. Thus, the very same edge with K different class assignments belongs to K distinct paths in P. We compute the energy of a path as</p><formula xml:id="formula_5">E ? = e ii ?? w ii (a ? ii ).<label>(6)</label></formula><p>A subset of valid paths P V ? P satisfies the given transcript. The other paths are invalid, P I = P \ P V .</p><p>In the next section, we explain how to efficiently compute a total energy score of the exponentially many paths in P for estimating our loss in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Constrained Discriminative Forward Loss</head><p>In this paper, we study three distinct loss functions, defined in terms of a total energy score of paths in G. As there are exponentially many paths in G, our key contribution is the algorithm for efficiently estimating their total energy. Below, we specify our three loss functions ordered by their complexity. As we will show in Sec. 6, we obtain the best performance when using the CDFL in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Forward Loss</head><p>We define a forward loss, L F , in terms of a total energy of all valid paths using the standard logadd function as</p><formula xml:id="formula_6">L F = ? log( ??P V exp(?E ? )),<label>(7)</label></formula><p>where energy of a path E ? is given by <ref type="bibr" target="#b5">(6)</ref>. As there are exponentially many paths in P V , we cannot directly compute L F as specified in <ref type="bibr" target="#b6">(7)</ref>. Therefore, we derive a novel recursive algorithm for accumulating the energy scores of edges along multiple paths, as specified below. We begin by defining the logadd function as logadd(a, b) = ? log(exp(?a) + exp(?b)).</p><p>Note that the logadd function is commutative and associative, so it can be defined on a set S in a recursive manner:</p><formula xml:id="formula_8">logadd(S) = logadd(S\{x}, x),<label>(9)</label></formula><p>where x is an element in S. Therefore, the forward loss given by <ref type="bibr" target="#b6">(7)</ref> can be expressed as</p><formula xml:id="formula_9">L F = logadd({E ? : ? ? P V }).<label>(10)</label></formula><p>Below, we simplify notation as L F = logadd(P V ).</p><p>We recursively compute the energy score i (a 1:n ) of a path that ends at node i and covers first n labels of the ground truth a 1:n = [a 1 , ..., a n ] ? a 1:N in terms of the logadd scores i (a 1:n?1 ) of all valid paths that end at node i, i &lt; i , and cover first n ? 1 labels as i (a 1:n ) = logadd({ i (a 1:n?1 ) + w ii (a n ) : i &lt; i }).</p><p>(11) To prove <ref type="bibr" target="#b10">(11)</ref>, suppose that i (a 1:n?1 ) = logadd({E ?i :</p><formula xml:id="formula_10">? i ? P V }) = ? log( ?i?P V exp(?E ?i )),<label>(12)</label></formula><p>Input: G, b 1:N +1 , a 1:N Output: Forward loss L F = T (a) 1 Initialization: 0 (?) = 0 ; 2 for n = 1 to N do <ref type="bibr" target="#b2">3</ref> for i in the neighborhood of b n do 4 i (a 1:n ) = ?; <ref type="bibr" target="#b4">5</ref> for i in the neighborhood of b n?1 do 6 temp = i (a 1:n?1 ) + w ii (a n ); 7 i (a 1:n ) = logadd( i (a 1:n ), temp); where ? i is a path that ends at i with a transcript of a 1:n?1 . Then, we have i (a 1:n ) = logadd({ i (a 1:n?1 ) + w ii (a n ) :</p><formula xml:id="formula_11">i &lt; i }). = ? log( i&lt;i ? i ?P V exp(?E ?i ? w ii (a n ))) = ? log( ? i ?P V exp(?E ? i )) = logadd({E ? i : ? i ? P V }).<label>(13)</label></formula><p>where ? i is a path that ends at i with a transcript of a 1:n . For a training video with length T and ground-truth constraint sequence a 1:N , we define</p><formula xml:id="formula_12">L F = T (a 1:N ).<label>(14)</label></formula><p>The recursive algorithm for computing L F is presented in Alg. 1. It is worth noting that in a special case of Alg. 1, when we take only the initial segmentation cuts b 1:N +1 as nodes of G (i.e., the window size ? = 0), the forward loss is equal to the training loss used in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discriminative Forward Loss</head><p>We also consider the Discriminative Forward Loss, L DF , which extends L F by additionally accounting for invalid paths in G:</p><formula xml:id="formula_13">L DF = logadd(P V ) ? ? logadd(P),<label>(15)</label></formula><p>where logadd(P) aggregates a total energy of all paths in G, and ? &gt; 0 is a regularization factor that controls the relative importance of the valid and invalid paths for L DF . Alg. 2 summarizes our recursive algorithm for computing logadd(P) in <ref type="formula" target="#formula_0">(15)</ref>, whereas Alg. 1 shows how to compute logadd(P V ) in <ref type="bibr" target="#b14">(15)</ref>. One advantage of L DF over L F is that minimizing L DF amounts to maximizing the decision margin between the valid and invalid paths. However, a potential shortcoming of L DF is that valid paths might have little effect in <ref type="bibr" target="#b14">(15)</ref>. In the case, when the energy of valid paths dominates the total energy of all paths, the former gets effectively subtracted in <ref type="bibr" target="#b14">(15)</ref>, and hence has very little effect on learning.</p><p>Moreover, we observe that in some cases the backpropagation of L DF is dominated by the invalid paths. This can be clearly seen from the following derivation. We compute the gradient ?L DF as</p><formula xml:id="formula_14">?L DF = ?logadd(P V ) ? ? ?logadd(P), = c 1 ??P V exp(?E ? )?E ? ?c 2 ??P I exp(?E ? )?E ? ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_15">c 1 = (1??) ??P V exp(?E?)+ ??P I exp(?E?) ( ??P V exp(?E?))( ??P exp(?E?)) , c 2 = ? ??P exp(?E?) .</formula><p>(17) From (16)-(17), we note that in the case of ? ? 1, the backpropagation will be dominated by the invalid paths, whereas there would be no effect for invalid paths in training if ? = 0. Sec. 6 presents how different choices of ? affect our performance.</p><p>In the next section, we define the constrained discriminative forward loss to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Constrained Discriminative Forward Loss</head><p>We define the CDFL as</p><formula xml:id="formula_16">L CDF = logadd(P V ) ? logadd(P Ic ),<label>(18)</label></formula><p>where P Ic consists of a subset of invalid paths in G, where each edge e ii gets assigned an action class a such that its weight w ii (a) &lt; w ii (a n ), where a n = a is the pseudo ground truth class for e ii . This constraint effectively addresses the aforementioned issue when the valid paths have significantly lower energy than the invalid paths. Alg. 3 summarizes our recursive algorithm for computing logadd(P Ic ) in <ref type="formula" target="#formula_0">(18)</ref>, whereas Alg. 1 shows how to compute logadd(P V ) in <ref type="bibr" target="#b17">(18)</ref>. As L DF accounts for the invalid paths, L CDF further accounts for the hard invalid paths. Therefore, the model robustness is further improved by minimizing L CDF which amounts to maximizing the decision margin between the valid and hard invalid paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Our Computational Efficiency</head><p>As summarized in Alg. 1-3, our training first runs the constrained Viterbi algorithm (see Sec. 3) to get the initial segmentation cuts with complexity O(T 2 N ) for a video of length T and the ground-truth action sequence of length N . Then, CDFL efficiently accumulates the energy of both valid and invalid paths in G with complexity O(? 2 KN ) for the neighborhood window size ? and the class set size K. Therefore, our total complexity of training is</p><formula xml:id="formula_17">O(T 2 N + ? 2 KN ).</formula><p>Note that prior work <ref type="bibr" target="#b21">[22]</ref> also runs the Constrained Viterbi with complexity O(T 2 N ), so relative to theirs our complexity is increased by O(? 2 KN ). This additional complexity is significantly smaller than O(T 2 N ) as ? 2 K T 2 . In our experimental evaluation, we get the best results for ? ? 20 frames, whereas video length T can go to several minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Both action segmentation and alignment are evaluated on the Breakfast Actions <ref type="bibr" target="#b9">[10]</ref>, Hollywood Extended <ref type="bibr" target="#b0">[1]</ref>, and 50Salads <ref type="bibr" target="#b23">[24]</ref> datasets. We perform the same cross-validation strategy as the state of the art, and report our average results. We call our approach CDFL, trained with loss given by <ref type="bibr" target="#b17">(18)</ref>.</p><p>Datasets. For all datasets, we use as input the preprocessed, public, unsupervised frame-level features. The same frame features are used by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. The features are dense trajectories represented by PCA-projected Fisher vectors <ref type="bibr" target="#b10">[11]</ref>. Breakfast <ref type="bibr" target="#b9">[10]</ref> consists of 1, 712 videos of people making breakfast with 10 cooking activities. The cooking activities are comprised of 48 action classes. On average, every video has 6.9 action instances, and the video length ranges from a few seconds to several minutes. Hollywood Extended <ref type="bibr" target="#b0">[1]</ref> contains 937 video clips from different Hollywood movies, showing 16 action classes. Each clip contains 2.5 actions on average. 50Salads <ref type="bibr" target="#b23">[24]</ref> has 50 very long videos showing 17 classes of human manipulative gestures. On average, each video has 20 action instances. There are 600, 000 annotated frames.</p><p>Evaluation Metrics. We use the following four standard metrics, as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. The mean-over-frames (Mof) is the average percentage of correctly labeled frames. To overcome the potential drawback that frames are dominated by background class, we compute mean-over-frames without background(Mof-bg) as the average percentage of correctly labeled video frames with background frames removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breakfast</head><p>Mof Mof-bg IoU IoD OCDC <ref type="bibr" target="#b0">[1]</ref> 8.9 ---CTC <ref type="bibr" target="#b7">[8]</ref> 21.8 ---HTK <ref type="bibr" target="#b10">[11]</ref> 25.9 -9.8 -ECTC <ref type="bibr" target="#b7">[8]</ref> 27.7 ---HMM/RNN <ref type="bibr" target="#b19">[20]</ref>    The intersection over union (IoU) and the intersection over detection (IoD) are computed as IoU = |GT ?D|/|GT ?D|, and IoD = |GT ?D|/|D| , where |GT | denotes the extent of the ground truth segment and |D| is the extent of a correctly detected action segment.</p><p>Training. We train a single-layer GRU with 64 hidden units in 10 5 iterations, where for each iteration one training video is randomly selected. The initial learning rate of 0.01 is decreased to 0.001 at the 60, 000th iteration. The mean action lengths ? a in (3), and the action priors p(a) in (2) are estimated from the history of pseudo ground truths. Unlike <ref type="bibr" target="#b21">[22]</ref>, we do not use the history of pseudo ground truths for computing loss in the current iteration. Consequently, our training time per iteration is less than that of <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Action Segmentation</head><p>Tab. 1 compares CDFL with the state of the art. From the table, CDFL achieves the best performance in terms of all the four metrics. <ref type="figure" target="#fig_3">Fig. 3</ref> qualitatively compares the ground truth and CDFL's output on an example test video in Breakfast dataset. As can be seen, CDFL typically misses the true start or end of actions by only a few frames. In general, CDFL successfully detects most action occurrences.</p><p>Ablation Study for Action Segmentation. Tab. 2 compares our action segmentation performance on Breakfast when using different sizes of the neighborhood window placed around the initial segmentation cuts (as explained in Sec. 4) and different loss functions (as specified in Sec. 5). From the table, training by accounting for invalid paths in L DF and L CDF gives better performance than only accounting for valid paths in L F . In addition, considering neighboring frames for action boundary refinement within a window around the initial segmentation cuts gives better perfor-   mance than taking into account only a single optimal path in the segmentation graph when the window size is 0. The best test performance is achieved using L CDF with window size of 20 in training. <ref type="figure" target="#fig_5">Fig. 5</ref> illustrates the CDFL's action segmentations on a sample test video from the Breakfast Action dataset using different window sizes and L CDF . As can be seen, considering neighboring frames around the anchor segmentation improves performance.</p><p>Tab. 3 shows how different regularization factors ? in L DF affect our action segmentation on the Breakfast Action dataset, for different neighbor-window sizes. As expected, using small ? in training tends to give better performance. The best accuracy is achieved with ? = 0.1 and window size of 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Action Alignment</head><p>Tab. 4 shows that CDFL outperforms the state-of-theart approaches in action alignment on the three benchmark</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breakfast</head><p>Mof Mof-bg IoU IoD ECTC <ref type="bibr" target="#b7">[8]</ref> 35.0 --45.0 HTK <ref type="bibr" target="#b10">[11]</ref> 43.  datasets. <ref type="figure" target="#fig_6">Fig. 6</ref> illustrates that CDFL is good at action alignment on a sample test video from Hollywood Extended. Ablation Study for Action Alignment. Tab. 5 presents our alignment results using different loss functions as specified in Sec. 5, and different neighbor-window sizes on Hollywood Ext. From the table, training with L DF and L CDF that account for invalid paths, outperforms our approach trained with L F . In addition, taking into account neighboring frames around segmentation cuts of the initial segmentation (i.e., window size is greater than 0) improves performance relative to the case when window size is 0. The best performance is achieved using L CDF with the window sizes of 6 in training. <ref type="figure" target="#fig_7">Fig. 7</ref> illustrates that CDFL gives good action alignment results on the sample test video from Hollywood Ext, using L CDF and window size of 6 in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have extended the existing work on weakly supervised action segmentation that uses an HMM and GRU for Window size L F L DF L CDF 8 48.7 49.8 51. <ref type="bibr">6 6</ref> 49.3 50.5 52. <ref type="bibr">9 4</ref> 49.0 50.0 52.0 2 48.5 49.5 50.7 0 48.7 49.3 49.8 <ref type="table">Table 5</ref>. IoD evaluations of our approach in action alignment on Hollywood Extended using different loss functions and different neighbor-window sizes in training. Using CDFL with neighborwindow size of 6 shows the best result. labeling video frames by formulating a new energy-based learning on a video's segmentation graph. The graph is constructed so as to facilitate computation of loss, expressed in terms of the energy of valid and invalid paths representing candidate action segmentations. Our key contribution is the new recursive algorithm for efficiently computing the accumulated energy of exponentially many paths in the segmentation graph. Among the three loss functions that we have defined, and evaluated, the CDFL -specified to maximize the discrimination margin between valid and high-scoring invalid paths -gives the best performance. A comparison with the state of the art on both action segmentation and action alignment tasks, for the Breakfast Action, Hollywood Extended and 50Salads datasets, supports our novelty claim that using our CDFL in training gives superior results than a loss function estimated on a single inferred segmentation, as done by prior work. Our results on both action segmentation and action alignment tasks also demonstrate advantages of considering many candidate segmentations in neighborwindows around the initial video segmentation, and maximizing the margin between all valid and hard invalid segmentations. Our small increase in complexity relative to that of related work seems justified considering our significant performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x 1:T as p(a 1:N , l 1:N |x 1:T ) ? p(x 1:T |a 1:N , l 1:N )p(l 1:N |a 1:N )p(a 1:N ), = T t=1p(x t |a n(t) ) N n=1 p(l n |a n ) p(a 1:N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 : 6 for a ? A do 7 i 2 :</head><label>1672</label><figDesc>Computing the Forward loss L F . Input: G, b 1:N +1 Output: logadd(P) = T 1 Initialization: 0 = 0 ; 2 for n = 1 to N do 3 for i in the neighborhood of b n do 4 i = ?; 5for i in the neighborhood of b n?1 do = logadd( i , i + w ii (a)); Computing the logadd score of all paths in P, for the discriminative forward loss L DF .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Input: G, b 1 : 3 for i in the neighborhood of b n do 4 i = ?; 5 for i in the neighborhood of b n?1 do 6 for a ? A do 7 temp = i ; 8 if 11 i 3 :</head><label>1345678113</label><figDesc>N +1 , a 1:N Output: logadd(P I c ) = T 1 Initialization: 0 = 0 ; 2 for n = 1 to N do w ii (a) &lt; w ii (a n ) then 9 temp = i + w ii (a) 10 end = logadd( i , temp); Computing the logadd score of a subset of invalid paths P Ic , for estimating the constrained discriminative forward loss L CDF .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Ground truth action sequence (take cup, spoon powder, pour milk, stir milk) (top) and our CDFL's action segmentation (bottom) on the sample test video P03 stereo01 P03 milk from Breakfast dataset. The background frames are marked in white. CDFL may miss the true start and end of some actions, but successfully detects the actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Top-down, the rows correspond to ground truth sequence of actions (pour oil, crack egg, fry egg, put egg2plate) and our action segmentations with neighbor-window size of 20 on the sample video P03 cam01 P03 friedegg from Breakfast dataset using LCDF, LDF and LF, respectively. The background frames are marked in white. The result for LCDF is the best. window size ? = 0 ? = 0.1 ? = 0.2 ? = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Ground truth action sequence (pour oil, crack egg, fry egg, take plate, put egg2plate) (top) and CDFL's action segmentations using different neighbor-window sizes on the sample test video P04 webcam02 P04 friedegg from Breakfast. The background frames are marked in white. The window size of 20 gives the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Ground truth action sequence (StandUp,SitDown, Drive-Car, OpenDoor, OpenDoor, HugPerson) (top) and our action alignments (bottom) on the sample video 0261 from Hollywood Extend. The background frames are marked in white. CDFL typically achieves a good action alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Ground truth action sequence (OpenDoor, OpenDoor, OpenCarDoor) (top) and CDFL's action alignments on the sample test video 0361 from Hollywood Extended, when trained using varying window sizes. The background frames are marked in white. Using CDFL and neighbor-window size of 6 gives the best results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mof and IoD evaluations on Breakfast for different neighborhood window sizes and different losses. CDFL with neighborwindow size of 20 shows the best result.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Mof evaluations on Breakfast using LDF in training with different regularization factors and neighbor-window sizes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Action alignment evaluations on Breakfast, Hollywood Ext and 50Salads. The dash indicates that no results reported by prior work.</figDesc><table><row><cell></cell><cell>9</cell><cell>-</cell><cell>26.6 42.6</cell></row><row><cell>OCDC [1]</cell><cell>-</cell><cell>-</cell><cell>-23.4</cell></row><row><cell cols="2">HMM/RNN [20] -</cell><cell>-</cell><cell>-47.3</cell></row><row><cell cols="4">TCFPN [7] 53.5 51.7 35.3 52.3</cell></row><row><cell>D3TW [3]</cell><cell>57.0</cell><cell>-</cell><cell>-56.3</cell></row><row><cell>Our CDFL</cell><cell cols="3">63.0 61.4 45.8 63.9</cell></row><row><cell cols="4">Hollywood Ext Mof Mof-bg IoU IoD</cell></row><row><cell>ECTC[8]</cell><cell>-</cell><cell>-</cell><cell>-41.0</cell></row><row><cell>HTK [11]</cell><cell>49.4</cell><cell>-</cell><cell>29.1 46.9</cell></row><row><cell>OCDC [1]</cell><cell>-</cell><cell>-</cell><cell>-43.9</cell></row><row><cell cols="2">HMM/RNN [20] -</cell><cell>-</cell><cell>-46.3</cell></row><row><cell cols="4">TCFPN [7] 57.4 36.1 22.3 39.6</cell></row><row><cell cols="2">NN-Viterbi [22] -</cell><cell>-</cell><cell>-48.7</cell></row><row><cell>D3TW [3]</cell><cell>59.4</cell><cell>-</cell><cell>-50.9</cell></row><row><cell>Our CDFL</cell><cell cols="3">64.3 70.8 40.5 52.9</cell></row><row><cell>50Salads</cell><cell cols="3">Mof Mof-bg IoU IoD</cell></row><row><cell>Our CDFL</cell><cell cols="3">68.0 65.3 45.5 58.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , t &lt;t p(a 1:n?1 ,l 1:n?1 |x 1:t ) ? t s=t p(x s |a n(s) ) ? p(l n |a n ) ? p(a 1:n ) , (4) where l n = t ? t . We set p(?|x 1:0 ) = 1, and p(a 1:n ) = ?, where ? &gt; 0 is a constant.4. Constructing the Segmentation GraphGiven a video x 1:T , we first run the constrained Viterbi algorithm to obtain an initial video segmentation (? 1:N ,l 1:N ). For simplicity, in the following, we ignore the symbol?. This initial segmentation is characterized by</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by DARPA XAI Award N66001-17-2-4029 and AFRL STTR AF18B-T002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph transformer networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bulletin of the 55th Biennial Session of the International Statistical Institute (ISI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3546" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07818</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4297" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of actions from transcripts. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading checks with multilayer graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="154" />
		</imprint>
	</monogr>
	<note type="report_type">ICASSP-97</note>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ctc network with statistical language modeling for action sequence recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakamasa</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">Lea</forename><surname>Michael D Flynn Ren?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidal Austin Reiter</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neuralnetwork-Viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
