<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Quality in Linear Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
						</author>
						<title level="a" type="main">Transformer Quality in Linear Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH 3 , matches the perplexity of improved Transformers over both short <ref type="formula">(512)</ref> and long (8K) context lengths, achieving training speedups of up to 4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref> and vision <ref type="bibr" target="#b8">(Dosovitskiy et al., 2020)</ref>. Although they have been growing in model size, most Transformers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications.</p><p>Many techniques have been proposed to speedup Transformers over extended context via more efficient attention mechanisms <ref type="bibr" target="#b3">(Child et al., 2019;</ref><ref type="bibr" target="#b5">Dai et al., 2019;</ref><ref type="bibr" target="#b25">Rae et al., 2019;</ref><ref type="bibr" target="#b4">Choromanski et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020;</ref><ref type="bibr" target="#b18">Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b36">Zaheer et al., 2020;</ref><ref type="bibr" target="#b19">Kitaev et al., 2020;</ref><ref type="bibr" target="#b29">Roy et al., 2021;</ref><ref type="bibr" target="#b16">Jaegle et al., 2021)</ref>. Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in   state-of-the-art systems. Here we examine this issue from a practical perspective, and find existing efficient attention methods suffer from at least one of the following drawbacks:</p><p>? Inferior Quality. Our studies reveal that vanilla Transformers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in <ref type="figure" target="#fig_1">Figure 1</ref>). Existing efficient attention methods often incur significant quality drop compared to augmented Transformers, and this drop outweighs their efficiency benefits.</p><p>? Overhead in Practice. As efficient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs.</p><p>? Inefficient Auto-regressive Training. Most attention linearization techniques enjoy fast decoding during inference, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training.  <ref type="bibr">(x, v, s=128)</ref>: z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum( bns,bms?bnm , q, k) a = relu(qk + rel_pos_bias(q, k)) * * 2 return tf.einsum <ref type="bibr">( bnm,bme?bne , a, v)</ref> def gated_attn_unit(x, d=768, e=1536):</p><p>shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u * attn(x, v) return dense(x, d) + shortcut <ref type="figure">Figure 2</ref>: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity.</p><p>We address the above issues by developing a new model family that, for the first time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys linear scalability over the context size on modern accelerators. Unlike existing efficient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which naturally enables higher-quality approximation. Specifically, our model, named FLASH, is developed in two steps:</p><p>First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit (GAU) in <ref type="figure">Figure 2</ref>. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of attention. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss.</p><p>We then propose an efficient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to first group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. We further describe how an accelerator-efficient implementation can be naturally derived from this formulation, achieving linear scalability in practice with only a few lines of code change. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gated Attention Unit</head><p>Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers:</p><p>Vanilla MLP. Let X ? R T ?d be the representations over T tokens. The output for Transformer's MLP can be formu-</p><formula xml:id="formula_0">lated as O = ?(XW u )W o where W u ? R d?e , W o ? R e?d .</formula><p>Here d denotes the model size, e denotes the expanded intermediate size, and ? is an element-wise activation function.</p><p>Gated Linear Unit (GLU). This is an improved MLP augmented with gating . GLU has been proven effective in many cases <ref type="bibr" target="#b30">(Shazeer, 2020;</ref><ref type="bibr" target="#b22">Narang et al., 2021)</ref> and is used in state-of-the-art Transformer language models <ref type="bibr" target="#b9">(Du et al., 2021;</ref><ref type="bibr" target="#b33">Thoppilan et al., 2022)</ref>.</p><formula xml:id="formula_1">U = ? u (XW u ), V = ? v (XW v ) ? R T ?e (1) O = (U V )W o ? R T ?d<label>(2)</label></formula><p>where stands for element-wise multiplication. In GLU, each representation u i is gated by another representation v i associated with the same token. Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512).</p><p>Gated Attention Unit (GAU). The key idea is to formulate attention and GLU as a unified layer and to share their computation as much as possible ( <ref type="figure">Figure 2</ref>). This not only results in higher param/compute efficiency, but also naturally enables a powerful attentive gating mechanism. Specifically, GAU generalizes Eq.</p><p>(2) in GLU as follows:</p><formula xml:id="formula_2">O = (U V )W o whereV = AV<label>(3)</label></formula><p>where A ? R T ?T contains token-token attention weights. Unlike GLU which always uses v i to gate u i (both associated with the same token), our GAU replaces v i with a potentially more relevant representationv i = j a ij v j "retrieved" from all available tokens using attention. The above will reduce to GLU when A is an identity matrix.</p><p>Consistent with the findings in <ref type="bibr" target="#b21">Liu et al. (2021)</ref>, the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: where Z is a shared representation (s d) 4 , Q and K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay-erNorms), and b is the relative position bias. We also find the softmax in MHSA can be simplified as a regular activation function in the case of GAU 5 . The GAU layer and its 4 Unless otherwise specified, we set s =128 in this work. <ref type="bibr">5</ref> We use squared ReLU    pseudocode are illustrated in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_3">Z = ? z (XW z ) ? R T ?s (4) A = relu 2 Q(Z)K(Z) + b ? R T ?T<label>(</label></formula><p>Unlike Transformer's MHSA which comes with 4d 2 parameters, GAU's attention introduces only a single small dense matrix W z with ds parameters on top of GLU (scalars and offsets in Q and K are negligible). By setting e = 2d for GAU, this compact design allows us to replace each Transformer block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed.</p><p>GAU vs. Transformers. <ref type="figure">Figure 3</ref> shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention.</p><p>Layer Ablations. In <ref type="table">Table 1</ref> &amp; 2 we show that both GAUs and Transformers are locally optimal on their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fast Linear Attention with GAU</head><p>There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences:</p><p>? First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention without quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.</p><p>? In addition, the number of attention modules is naturally doubled with GAU -recall MLP+MHSA?2?GAU in terms of cost (Section 2). Since approximate attention usually requires more layers to capture full dependency <ref type="bibr" target="#b5">(Dai et al., 2019;</ref><ref type="bibr" target="#b3">Child et al., 2019)</ref>, this property also makes GAU more appealing in handling long sequences.</p><p>With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformerlevel quality in linear time on long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Existing Linear-Complexity Variants</head><p>Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/sparse patterns, including local window <ref type="bibr" target="#b5">(Dai et al., 2019;</ref><ref type="bibr" target="#b25">Rae et al., 2019)</ref>, local+sparse <ref type="bibr" target="#b3">(Child et al., 2019;</ref><ref type="bibr" target="#b20">Li et al., 2019;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b36">Zaheer et al., 2020)</ref>, axial <ref type="bibr" target="#b14">(Ho et al., 2019;</ref><ref type="bibr" target="#b15">Huang et al., 2019)</ref>, learnable patterns through hashing <ref type="bibr" target="#b19">(Kitaev et al., 2020)</ref> or clustering <ref type="bibr" target="#b29">(Roy et al., 2021)</ref>.</p><p>Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical benefits (speed and RAM efficiency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model.</p><p>Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decomposing the attention matrix and then re-arranging the order of matrix multiplications <ref type="bibr" target="#b4">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020;</ref><ref type="bibr" target="#b18">Katharopoulos et al., 2020;</ref><ref type="bibr">Peng et al., 2021)</ref>. Schematically, the linear attention can be expressed a? Another desirable property of linear attention is its constant 6 computation and memory for each auto-regressive decoding step at inference time. To see that, define M t = K :t V :t and notice that the computation of M t can be fully incremental:</p><formula xml:id="formula_4">V lin = Q K V R d?d approx ????V quad = Softmax QK R T ?T V where Q, K, V ? R T</formula><formula xml:id="formula_5">M t = M t?1 + K t V t<label>(6)</label></formula><p>6 Constant is with respective to the sequence length T . This means we only need to maintain a cache with constant O(d 2 ) memory and whenever a new input arrives at time stamp t, only constant O(d 2 ) computation is required to accumulate K t V t into M t?1 and get M t . On the contrary, full quadratic attention requires linear O(T d) computation and memory for each decoding step, as each new input has to attend to all the previous steps.</p><p>However, on the other hand, re-arranging the computation in linear attention leads to a severe inefficiency during autoregressive training. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref> (mid), due to the causal constraint for auto-regressive training, the query vector at each time step Q t corresponds to a different cache value M t = K :t V :t . This requires the model to compute</p><formula xml:id="formula_6">and cache T different values {M t } T t=1 instead of only one value K V in the non-autoregressive case. In theory, the sequence {M t } T t=1 can be obtained in O(T d 2 ) by first com- puting {K t V t } T t=1</formula><p>and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependency of T steps, where an O(d 2 ) state needs to be processed each step. The sequential dependency not only limits the degree of parallelism, but more importantly requires T memory access in the loop, which usually costs much more time than computing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoretical complexity and actual running time. In practice, we find that directly computing the full quadratic attention matrix is even faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Method: Mixed Chunk Attention</head><p>Based on the strengths and weaknesses of existing linearcomplexity attentions, we propose mixed chunk attention, which merges the benefits from both partial attention and linear attention. The high-level idea is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. Below we reformulate GAU to incorporate this idea.</p><formula xml:id="formula_7">Preparation. The input sequence is first chunked into G non-overlapping chunks of size C, i.e. [T ] ? [T /C ? C]. Then, U g ? R C?e , V g ? R C?e</formula><p>and Z g ? R C?s are produced for each chunk g following the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Q quad g , K quad g , Q lin g , K lin g are produced from Z g by applying per-dim scaling and offset (this is very cheap).</p><p>We will describe how GAU's attention can be efficiently approximated using a local attention plus a global attention. Note all the major tensors U g , V g and Z g are shared between the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Q lin g and K lin g (4?s parameters).</p><p>Local Attention per Chunk. First, a local quadratic attention is independently applied to each chunk of length C to produce part of the pre-gating state:</p><formula xml:id="formula_8">V quad g = relu 2 Q quad g K quad g + b V g . The complexity of this part is O(G ? C 2 ? d) = O(T Cd),</formula><p>which is linear in T given that C remains constant.</p><p>Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture longrange interaction across chunks</p><formula xml:id="formula_9">Non-Causal:V lin g = Q lin g G h=1 K lin h V h ,<label>(7)</label></formula><p>Causal</p><formula xml:id="formula_10">:V lin g = Q lin g g?1 h=1 K lin h V h .<label>(8)</label></formula><p>Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in tokenlevel linear attention by a factor of C (a typical C is 256 in our experiments), leading to a significant training speedup.</p><p>Finally,V quad g andV lin g are added together, followed by gating and a post-attention projection analogous to Eq. <ref type="formula" target="#formula_2">(3)</ref>: def attn <ref type="bibr">(x, v, causal, s=128)</ref>:</p><formula xml:id="formula_11">O g = U g V quad g +V lin g W o .</formula><formula xml:id="formula_12"># x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>causal) return v_quad + v_lin</head><p>Code 1: Pseudocode for mixed chunk attention.</p><p>The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">DISCUSSIONS</head><p>Fast Auto-regressive Training. Importantly, as depicted in <ref type="figure" target="#fig_3">Fig. 4 (bottom)</ref>, thanks to chunking, the sequential dependency in the auto-regressive case reduces from T steps in the standard linear attention to G = T /C steps in the chunked version in Eq. (8). Therefore, we observe the autoregressive training becomes dramatically faster with the chunk size is in {128, 256, 512}. With the inefficiency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and computation of O(Cd 2 ), where the additional constant C comes from the local quadratic attention.</p><p>On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, instead of using the non-overlapping local attention, any partial attention variant could be used as a substitute while keeping the chunked linear attention fixed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Beltagy et al., 2020) and BigBird <ref type="bibr" target="#b36">(Zaheer et al., 2020)</ref>. While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-benefit trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal partial attention variant is task-specific, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention. Connections to Combiner. Similar to our method, Combiner <ref type="bibr" target="#b28">(Ren et al., 2021)</ref> also splits the sequence into nonoverlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the longrange information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We focus on two of our models that have different complexities with respect to the context length. The quadraticcomplexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH consists of both GAUs and the proposed mixed chunk attention.</p><p>To demonstrate their efficacy and general applicability, we evaluate them on both bidirectional and auto-regressive sequence modeling tasks over multiple large-scale datasets.</p><p>Baselines. First of all, the vanilla Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> with GELU activation function <ref type="bibr" target="#b13">(Hendrycks &amp; Gimpel, 2016)</ref> is included as a standard baseline for calibration. Despite of being a popular baseline in the literature, we find that RoPE <ref type="bibr" target="#b32">(Su et al., 2021)</ref> and GLU <ref type="bibr" target="#b30">(Shazeer, 2020)</ref> can lead to significant performance boosts. We there-fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity.</p><p>To demonstrate the advantages of our models on long sequences, we further compare our models with two notable linear-complexity Transformer variants-Performer (Choromanski et al., 2020) and Combiner <ref type="bibr" target="#b28">(Ren et al., 2021)</ref>, where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-benefit trade-off over many other approaches <ref type="bibr" target="#b28">(Ren et al., 2021)</ref>. To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner-Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE.</p><p>For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Profiler. See Appendix B for detailed settings and model specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bidirectional Language Modeling</head><p>In BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on the C4 dataset <ref type="bibr" target="#b26">(Raffel et al., 2020)</ref>. We consistently train each model with 2 18 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. <ref type="figure" target="#fig_4">Figure 5</ref>(a) shows the latency of each training step for all models at different context lengths. Results for Trans-former+ are omitted for brevity as it lies in between Transformer and Transformer++. Across all the six models, latencies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating linear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2? as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in <ref type="figure" target="#fig_4">Figures 5(b)</ref>-5(f), for all sequence lengths ranging from 512 to 8192, our models always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++'s final perplexity at step 125K, FLASH-Quad and FLASH can reduce the train-ing cost by 1.1?-2.5? and 1.0?-4.8?, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Auto-regressive Language Modeling</head><p>For auto-regressive language modeling, we focus on the Wiki-40B <ref type="bibr" target="#b12">(Guo et al., 2020)</ref> and PG-19 <ref type="bibr" target="#b25">(Rae et al., 2019)</ref> datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model performance over long context lengths. We train and evaluate all models with 2 18 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19.</p><p>Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasing  <ref type="table">Table 10</ref>) is used for all models in comparison. The results are summarized in <ref type="table" target="#tab_8">Table 3</ref>. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and training time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the final perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23? and 12.12? of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the longrange nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplexity than all of the full-attention Transformer variants while being significantly faster, demonstrating the effectiveness of our efficient attention design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-tuning</head><p>To demonstrate the effectiveness of FLASH over downstream tasks, we fine-tune our pre-trained models on the TriviaQA dataset <ref type="bibr" target="#b17">(Joshi et al., 2017)</ref>. Passages in Trivi-aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For fine-tuning, we sweep over three different learn-ing rates, including 1e ?4 , 7e ?5 , and 5e ?5 , and report the best validation-set F1 score across these runs. We observe that the fine-tuning results of the FLASH family can benefit from several minor changes in the model configuration. As shown in <ref type="table" target="#tab_9">Table 4</ref>, increasing the head size of FLASH-Quad from 128 to 512 leads to a significant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant specifically, including using a small chunk size (128), disabling gradient clipping during finetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the first token in each chunk to attend to the entire sequence using softmax. With those changes, FLASH s=512 achieves comparable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8? and 2.7? as fast as Transformer+ in pretraining and fine-tuning, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Significance of quadratic &amp; linear components. To better understand the efficacy of FLASH, we first study how much the local quadratic attention and the global linear attention contribute to the performance individually. To this end, we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob-alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In <ref type="figure" target="#fig_6">Figure 7</ref> we see a significant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other -both are critical to the quality of the proposed mixed chunk attention.</p><p>Significance of GAU. Here we study the importance of using GAU in FLASH. To achieve this, we apply the same idea of mixed chunk attention to Transformer++. We refer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC-TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. <ref type="figure" target="#fig_6">Figure 7</ref> shows that FLASH outperforms MC-TFM++ by a large margin (more than 2? speedup when the sequence length is greater than 2048), confirming the importance of GAU in our design. We further look into the perplexity increase due to our approximation method in <ref type="table" target="#tab_10">Table 5</ref>, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than going from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneficial to weaker/approximate attention mechanisms.</p><p>Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efficient Transformer variants. This is achieved by designing a performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Connections to Combiner</head><p>To capture long-term information, Combiner <ref type="bibr" target="#b28">(Ren et al., 2021)</ref> additionally summarizes each chunk into summary key and value vectors K sum , V sum ? R T /C?d and concatenate them into the local quadratic attention, i.e.</p><formula xml:id="formula_13">V g = Softmax Q[K g ; K sum ] [V g ; V sum ].</formula><p>Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix K lin h V h of size O(sd) which is s times larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners.</p><p>Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra [T /C ? T /C] attention matrix to combine the chunk summaries, e.g.</p><formula xml:id="formula_14">A lin = relu 2 Q sum K sum + b sum , V lin g = Q lin g T /C h=1 a lin gh K lin h V h .</formula><p>We indeed briefly experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C + T /C)d 2 ) which is length-dependent and no longer constant. Hence, we do not include this feature in our default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Hyperparameters</head><p>Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in <ref type="table" target="#tab_11">Table 6</ref>. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in <ref type="table" target="#tab_12">Table 7</ref>. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Model Specifications</head><p>Detailed specifications of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish <ref type="bibr" target="#b10">(Elfwing et al., 2018;</ref><ref type="bibr" target="#b13">Hendrycks &amp; Gimpel, 2016;</ref><ref type="bibr" target="#b27">Ramachandran et al., 2017)</ref> is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU <ref type="bibr" target="#b13">(Hendrycks &amp; Gimpel, 2016)</ref> in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model.  We observe that the inefficiency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in <ref type="table" target="#tab_14">Table 11</ref>, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Tabular MLM and LM Results</head><p>We summarize the experimental results of MLM on C4 and LM on Wiki-40B in <ref type="table" target="#tab_6">Tables 12 and 13</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Ablation Study of Chunk Size</head><p>The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefficient auto-regressive training. <ref type="figure" target="#fig_7">Figure 8</ref> shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pseudocode For FLASH-Quad and FLASH</head><p>We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.  def segment_ids_to_mask(segment_ids, causal=False):</p><p>"""Generate the segment mask from the segment ids.</p><p>The segment mask is used to remove the attention between tokens in different documents. """ min_ids, max_ids = tf.reduce_min(segment_ids, axis=?1), tf.reduce_max(segment_ids, axis=?1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki-40B -All models are comparable in size at around 110M and trained for 125K steps with 2 18 tokens per batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>?d are the query, key and value representations, respectively. Re-arranging the computation reduces the complexity w.r.t T from quadratic to linear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (C is always greater than or equal to 128 in our experiments). Our method significantly reduces the compute in quadratic attention (red links), while requiring substantially less RNN-style steps (green squares) in conventional linear attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Masked language modeling validation-set results on the C4 dataset -All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 2 18 tokens per batch. The quality is measured in negative log perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Auto-regressive language modeling validation-set results on the Wiki-40B dataset -All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 2 18 tokens per batch. The quality is measured in negative log perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study of the proposed FLASH architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): """Create sinusoidal position embedding with a scaling factor.""" hidden_size = int(embeddings.shape[?1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 * * ?freq_seq sinusoid = tf.einsum( s,d?sd , pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis=?1) scalar = tf.get_variable( scaledsin_scalar , shape=(), initializer=tf.constant_initializer(1 / hidden_size * * 0.5)) scaledsin * = scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): """RoPE position embedding.""" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len * = i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[?1] + 1, len(shape) ? 1, 1): position = tf.expand_dims(position, axis=?1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Impact of various modifications on MHSA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum( bgcs,bgce?bgse , k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum( bgcs,bgse?bgce , q, kv) else: kv = tf.einsum( bgcs,bgce?bse , k, v) return tf.einsum( bgcs,bse?bgce , q, kv)</figDesc><table><row><cell>def _local_quadratic_attn(q, k, v, causal):</cell></row><row><cell>qk = tf.einsum( bgns,bgms?bgnm , q, k)</cell></row><row><cell>a = relu(qk + rel_pos_bias(q, k))  *  *  2</cell></row><row><cell>a = causal_mask(a) if causal else a</cell></row><row><cell>return tf.einsum( bgnm,bgme?bgne , a, v)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Auto-regressive language models on the PG-19 dataset -Latency (Lat.) is measured with 64 TPU-v4 cores. Indicates that the specific model fails to achieve the same perplexity as Transformer+.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Context Length</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>1024</cell><cell></cell><cell>2048</cell><cell></cell><cell>4096</cell><cell></cell><cell></cell><cell>8192</cell><cell></cell></row><row><cell></cell><cell cols="9">PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup*</cell></row><row><cell>Transformer+</cell><cell>44.45 282</cell><cell>1.00?</cell><cell>43.14 433</cell><cell>1.00?</cell><cell>42.80 698</cell><cell>1.00?</cell><cell cols="2">43.27 1292</cell><cell>1.00?</cell></row><row><cell cols="2">Transformer++ 44.47 292</cell><cell>-</cell><cell>43.18 441</cell><cell>-</cell><cell>43.13 712</cell><cell>-</cell><cell cols="2">43.26 1272</cell><cell>1.21?</cell></row><row><cell>Combiner</cell><cell>46.04 386</cell><cell>-</cell><cell>44.68 376</cell><cell>-</cell><cell>43.99 374</cell><cell>-</cell><cell>44.12</cell><cell>407</cell><cell>-</cell></row><row><cell>FLASH-Quad</cell><cell>43.40 231</cell><cell>2.18?</cell><cell>42.01 273</cell><cell>3.29?</cell><cell>41.46 371</cell><cell>3.59?</cell><cell>41.68</cell><cell>560</cell><cell>5.23?</cell></row><row><cell>FLASH</cell><cell>44.06 234</cell><cell>1.66?</cell><cell>42.17 237</cell><cell>3.85?</cell><cell>40.72 234</cell><cell>6.75?</cell><cell>41.07</cell><cell>250</cell><cell>12.12?</cell></row><row><cell cols="4">context lengths in Figures 6(b)-6(f). Similar to the findings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">on MLM tasks, our models dominate all other models in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">terms of quality-training speed for all sequence lengths.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Specifically, FLASH-Quad reduces the training time of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Transformer++ by 1.2? to 2.5? and FLASH cuts the com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">pute cost by 1.2? to 4.9? while reaching a similar perplex-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ity as Transformer++. Between our own models, FLASH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">closely tracks the perplexity of FLASH-Quad and starts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to achieve a better perplexity-cost trade-off when the con-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">text length goes beyond 2048. Detailed quality and speed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">comparisons for all models are included in Appendix C.2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Measured based on time taken to match Transformer+'s final quality (at step 125K) on TPU.-For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results on TrivialQA with context length 4096 -</figDesc><table><row><cell>to-all"</cell></row></table><note>"PT" stands for pre-training and "FT" stands for fine-tuning. All models are comparable in size at around 110M. s stands for the head size of the single-head attention. For FLASH, "first-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">: Perplexity increases when mixed chunk attention is</cell></row><row><cell cols="6">applied to GAU (? FLASH) or to TFM++ (? MC-TFM++) -</cell></row><row><cell cols="6">Results are reported for MLM and LM with increasing context</cell></row><row><cell>lengths from 512 to 8192.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM on C4</cell><cell>512</cell><cell cols="4">1024 2048 4096 8192</cell></row><row><cell>FLASH-Quad ? FLASH</cell><cell>0.0</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell><cell>0.07</cell></row><row><cell>TFM++ ? MC-TFM++</cell><cell>0.36</cell><cell>0.37</cell><cell>0.49</cell><cell>0.48</cell><cell>0.43</cell></row><row><cell>LM on Wiki-40B</cell><cell>512</cell><cell cols="4">1024 2048 4096 8192</cell></row><row><cell cols="3">FLASH-Quad ? FLASH -0.05 0.06</cell><cell>0.22</cell><cell>0.30</cell><cell>0.11</cell></row><row><cell>TFM++ ? MC-TFM++</cell><cell>0.54</cell><cell>0.75</cell><cell>0.86</cell><cell>0.90</cell><cell>0.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for MLM pretraining on C4.</figDesc><table><row><cell></cell><cell>MLM Results (Figure 5)</cell></row><row><cell>Data</cell><cell>C4</cell></row><row><cell>Sequence length</cell><cell>512 -8192</cell></row><row><cell>Tokens per batch</cell><cell>2 18</cell></row><row><cell>Batch size</cell><cell>2 18 / Sequence length</cell></row><row><cell>Number of steps</cell><cell>125K</cell></row><row><cell>Warmup steps</cell><cell>10K</cell></row><row><cell>Peak learning rate</cell><cell>7e-4</cell></row><row><cell>Learning rate decay</cell><cell>Linear</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row><row><cell>Adam (?1, ?2)</cell><cell>(0.9, 0.999)</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row><row><cell>Local gradient clipping*</cell><cell>0.1</cell></row><row><cell>Chunk size</cell><cell>256</cell></row><row><cell>Hidden dropout</cell><cell>0</cell></row><row><cell>GELU dropout</cell><cell>0</cell></row><row><cell>Attention dropout (if applicable)</cell><cell>0</cell></row></table><note>* Applied to all models except the vanilla Transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for LM pretraining on Wiki-40B and PG-19.</figDesc><table><row><cell></cell><cell cols="2">LM Results (Figure 6) LM Results (Table 3)</cell></row><row><cell>Data</cell><cell>Wiki-40B</cell><cell>PG-19</cell></row><row><cell>Sequence length</cell><cell>512 -8192</cell><cell>1024 -8192</cell></row><row><cell>Tokens per batch</cell><cell>2 18</cell><cell></cell></row><row><cell>Batch size</cell><cell cols="2">2 18 / Sequence length</cell></row><row><cell>Number of steps</cell><cell>125K</cell><cell></cell></row><row><cell>Warmup steps</cell><cell>10K</cell><cell></cell></row><row><cell>Peak learning rate</cell><cell>7e-4</cell><cell></cell></row><row><cell>Learning rate decay</cell><cell>Linear</cell><cell></cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell></cell></row><row><cell>Adam</cell><cell>1e-6</cell><cell></cell></row><row><cell>Adam (?1, ?2)</cell><cell>(0.9, 0.999)</cell><cell></cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell></cell></row><row><cell>Local gradient clipping*</cell><cell>0.1</cell><cell></cell></row><row><cell>Hidden dropout</cell><cell>0</cell><cell></cell></row><row><cell>GELU dropout</cell><cell>0</cell><cell></cell></row><row><cell>Attention dropout (if applicable)</cell><cell>0</cell><cell></cell></row><row><cell>Chunk size</cell><cell>256</cell><cell>512</cell></row></table><note>* Applied to all models except the vanilla Transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Model configurations for MLM experiments on the C4 dataset in Section 4.</figDesc><table><row><cell>FLASH-Quad</cell><cell>FLASH</cell><cell>Transformer Transformer+ Transformer++</cell><cell>Combiner</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU -Latency is reported in millisecond. OOM stands for the CUDA out of memory error.</figDesc><table><row><cell>Performer-Matmul</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Context Length</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>512</cell><cell></cell><cell>1024</cell><cell></cell><cell>2048</cell><cell></cell><cell cols="2">4096</cell><cell>8192</cell><cell></cell></row><row><cell></cell><cell cols="8">PPLX Latency PPLX Latency PPLX Latency PPLX Latency</cell><cell>PPLX</cell><cell>Latency</cell></row><row><cell>Transformer</cell><cell>4.517</cell><cell>47.7</cell><cell>4.436</cell><cell>63.9</cell><cell>4.196</cell><cell>90.9</cell><cell>4.602</cell><cell>142.5</cell><cell>4.8766</cell><cell>252.7</cell></row><row><cell>Transformer+</cell><cell>4.283</cell><cell>48.8</cell><cell>4.151</cell><cell>64.4</cell><cell>4.032</cell><cell>91.5</cell><cell>3.989</cell><cell>142.9</cell><cell>3.986</cell><cell>252.9</cell></row><row><cell cols="2">Transformer++ 4.205</cell><cell>47.6</cell><cell>4.058</cell><cell>64.6</cell><cell>3.920</cell><cell>91.6</cell><cell>3.876</cell><cell>143.4</cell><cell>3.933</cell><cell>252.1</cell></row><row><cell>Performer</cell><cell>5.897</cell><cell>37.2</cell><cell>6.324</cell><cell>37.6</cell><cell>8.032</cell><cell>39.1</cell><cell>12.622</cell><cell>36.9</cell><cell>102.980</cell><cell>40.9</cell></row><row><cell>Combiner</cell><cell>4.449</cell><cell>67.2</cell><cell>4.317</cell><cell>66.4</cell><cell>4.238</cell><cell>66.4</cell><cell>4.195</cell><cell>68.3</cell><cell>4.225</cell><cell>77.3</cell></row><row><cell>FLASH-Quad</cell><cell>4.176</cell><cell>43.7</cell><cell>3.964</cell><cell>50.1</cell><cell>3.864</cell><cell>61.7</cell><cell>3.828</cell><cell>84.9</cell><cell>3.830</cell><cell>132.1</cell></row><row><cell>FLASH</cell><cell>4.172</cell><cell>51.2</cell><cell>4.015</cell><cell>50.1</cell><cell>3.928</cell><cell>51.4</cell><cell>3.902</cell><cell>50.7</cell><cell>3.897</cell><cell>59.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Context Length</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>512</cell><cell></cell><cell>1024</cell><cell></cell><cell>2048</cell><cell></cell><cell cols="2">4096</cell><cell>8192</cell><cell></cell></row><row><cell></cell><cell cols="10">PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency</cell></row><row><cell>Transformer</cell><cell>17.341</cell><cell>54.0</cell><cell>19.808</cell><cell>70.9</cell><cell>18.154</cell><cell>96.3</cell><cell>17.731</cell><cell>149.1</cell><cell>18.254</cell><cell>260.7</cell></row><row><cell>Transformer+</cell><cell>16.907</cell><cell>55.6</cell><cell>15.999</cell><cell>70.3</cell><cell>15.653</cell><cell>96.1</cell><cell>15.515</cell><cell>149.3</cell><cell>15.478</cell><cell>261.9</cell></row><row><cell cols="2">Transformer++ 16.835</cell><cell>54.7</cell><cell>15.943</cell><cell>70.9</cell><cell>15.489</cell><cell>96.6</cell><cell>15.282</cell><cell>149.2</cell><cell>15.254</cell><cell>261.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Pseudocode for relative position bias. .math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto?regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower=?1, num_upper=0) kernel * = causal_mask x = u * tf.einsum( bnm,bme?bne , kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer= zeros ) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad).</figDesc><table><row><cell>WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02)</cell></row><row><cell>WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type= layer_norm , expansion_factor=2):</cell></row><row><cell>def rel_pos_bias(n): """GAU block.</cell></row><row><cell>"""Relative position bias.""" Input shape: batch size x sequence length x model size if n &lt; 512: """ # Construct Toeplitz matrix directly when the sequence length is less than 512. seq_len = tf.shape(x)[1] w = tf.get_variable( d = int(x.shape[?1]) weight , shape=[2  *  n ? 1], e = int(d  *  expansion_factor)</cell></row><row><cell>dtype=tf.float32, initializer=WEIGHT_INITIALIZER) shortcut, x = x, norm(x, begin_axis=?1, norm_type=norm_type)</cell></row><row><cell>t = tf.pad(w, [[0, n]]) s = 128 t = tf.tile(t, [n]) uv = tf.layers.dense(x, 2  *  e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer= zeros ) t = t[..., :?n] t = tf.reshape(t, [n, 3  *  n ? 2]) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis=?1)</cell></row><row><cell>r = (2  *  n ? 1) // 2 # Generate Query (q) and Key (k) from base. t = t[..., r:?r] gamma = tf.get_variable( gamma , shape=[2, s], initializer=WEIGHT_INITIALIZER) else: beta = tf.get_variable( beta , shape=[2, s], initializer=tf.initializers.zeros()) # Construct Toeplitz matrix using RoPE when the sequence length is over 512. base = tf.einsum( ...r,hr?...hr , base, gamma) + beta a = tf.get_variable( base = rope(base, axis=1) a , shape=[128], q, k = tf.unstack(base, axis=?2)</cell></row><row><cell>dtype=dtype, # Calculate the quadratic attention. initializer=WEIGHT_INITIALIZER) qk = tf.einsum( bnd,bmd?bnm , q, k) b = tf.get_variable( bias = rel_pos_bias(seq_len) b , shape=[128], kernel = tf</cell></row><row><cell>dtype=dtype,</cell></row><row><cell>initializer=WEIGHT_INITIALIZER)</cell></row><row><cell>a = rope(tf.tile(a[None, :], [n, 1]), axis=0)</cell></row><row><cell>b = rope(tf.tile(b[None, :], [n, 1]), axis=0)</cell></row><row><cell>t = tf.einsum( mk,nk?mn , a, b)</cell></row><row><cell>return t</cell></row><row><cell>Code 4: def norm(x, begin_axis=?1, eps=1e?5, norm_type= layer_norm ):</cell></row><row><cell>"""Normalization layer."""</cell></row><row><cell>shape = x.shape.as_list()</cell></row><row><cell>axes = list(range(len(shape)))[begin_axis:]</cell></row><row><cell>if norm_type == layer_norm :</cell></row><row><cell>mean, var = tf.nn.moments(x, axes, keepdims=True)</cell></row><row><cell>x = (x ? mean)  *  tf.rsqrt(var + eps)</cell></row><row><cell>gamma = tf.get_variable(</cell></row><row><cell>gamma , shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones())</cell></row><row><cell>beta = tf.get_variable(</cell></row><row><cell>beta , shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros())</cell></row><row><cell>return gamma  *  x + beta</cell></row><row><cell>elif norm_type == scale_norm :</cell></row><row><cell>mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) half_size = shape[?1] // 2 x = x  *  tf.rsqrt(mean_square + eps) freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) scalar = tf.get_variable( scalar , shape=(), initializer=tf.constant_initializer(1.0)) inv_freq = 10000  *  *  ?freq_seq sinusoid = tf.einsum( ...,d?...d , position, inv_freq) return scale  *  x</cell></row><row><cell>sin = tf.sin(sinusoid)</cell></row><row><cell>cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis=?1) Code 5: Pseudocode for LayerNorm and ScaleNorm.</cell></row><row><cell>return tf.concat([x1  *  cos ? x2  *  sin, x2  *  cos + x1  *  sin], axis=?1)</cell></row><row><cell>Code 3: Pseudocode for RoPE.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>.math.divide_no_nan(mask, tf.reduce_sum(mask, axis=?1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask. , base = tf.split(tf.nn.silu(uv), [e, e, s], axis=?1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable( gamma , shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable( beta , shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum( ...r,hr?...hr , base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis=?2) if causal: # Linear attention part. lin_kv = tf.einsum( bgnk,bgne?bgke , lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum( bhke,bgh?bgke , lin_kv, mask) linear = tf.einsum( bgnk,bgke?bgne , lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum( bgnk,bgmk?bgnm , quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto?regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower=?1, num_upper=0) quadratic = tf.einsum( bgnm,bgme?bgne , kernel * causal_mask, v)</figDesc><table><row><cell>logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 ? tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper=?1) mask  * = causal_mask def FLASH(x, causal, segment_ids, norm_type= layer_norm , expansion_factor=2): """FLASH block. Input shape: batch size x num chunks x chunk length x model size """ _, g, n, d = x.shape.as_list() e = int(d  *  expansion_factor) shortcut, x = x, norm(x, begin_axis=?1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2  *  e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer= zeros ) u, velse: # Linear attention part lin_kv = tf.einsum( bgnk,bgne?bgke , lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum( bhke,bgh?bgke , lin_kv, mask) linear = tf.einsum( bgnk,bgke?bgne , lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum( bgnk,bgmk?bgnm , quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum( bgnm,bgme?bgne , kernel, v) x = u  *  (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer= zeros ) return x + shortcut mask = tfWEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) Code 8: Pseudocode for FLASH.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b10">Elfwing et al. (2018)</ref><p>; <ref type="bibr" target="#b13">Hendrycks &amp; Gimpel (2016)</ref>; <ref type="bibr" target="#b27">Ramachandran et al. (2017)</ref>. 3 ScaleNorm and LayerNorm are proposed by <ref type="bibr" target="#b23">Nguyen &amp; Salazar (2019)</ref> and <ref type="bibr" target="#b0">Ba et al. (2016)</ref>, respectively. 4 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. <ref type="bibr">5</ref> The learnable position embedding is proposed by <ref type="bibr" target="#b11">Gehring et al. (2017)</ref>. <ref type="bibr">6</ref> The model is consist of 12 attention layers and 12 FFN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head><p>Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in <ref type="table">Table 11</ref>) and the ablation study of chunk size for FLASH (in <ref type="figure">Figure 8</ref>).  <ref type="formula">(2016)</ref>; <ref type="bibr" target="#b27">Ramachandran et al. (2017)</ref>. 3 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. <ref type="bibr">4</ref> The learnable position embedding is proposed by <ref type="bibr" target="#b11">Gehring et al. (2017)</ref>. <ref type="bibr">5</ref> The model is consist of 12 attention layers and 12 FFN layers.  <ref type="bibr" target="#b27">Ramachandran et al. (2017)</ref>. 3 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. <ref type="bibr">4</ref> The model is consist of 36 attention layers and 36 FFN layers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06905</idno>
		<title level="m">Efficient scaling of language models with mixtureof-experts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wiki-40b: Multilingual language model dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perceiver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trivi-aQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="1147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5243" to="5253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pay attention to mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11972</idno>
		<title level="m">Do transformer modifications transfer across implementations and applications? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.05895" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Searching for activation functions. CoRR, abs/1710.05941</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.05941" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combiner: Full attention transformer with sparse computation cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MQQeeDiO5vv" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W.</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">GLU variants improve transformer. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>abs/2002.05202</idno>
		<ptr target="https://arxiv.org/abs/2002.05202" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Primer: Searching for efficient transformers for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma?ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
