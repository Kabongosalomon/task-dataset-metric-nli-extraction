<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTISPECTRAL FUSION FOR OBJECT DETECTION WITH CYCLIC FUSE-AND-REFINE BLOCKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univ Rennes</orgName>
								<address>
									<country>IRISA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ATERMES company</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univ Rennes</orgName>
								<address>
									<country>IRISA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">IUF</orgName>
								<address>
									<region>Inria</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sbastien</forename><surname>Lefevre</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Bretagne Sud</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">ATERMES company</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MULTISPECTRAL FUSION FOR OBJECT DETECTION WITH CYCLIC FUSE-AND-REFINE BLOCKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multispectral object detection</term>
					<term>Multi- spectral feature fusion</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multispectral images (e.g. visible and infrared) may be particularly useful when detecting objects with the same model in different environments (e.g. day/night outdoor scenes). To effectively use the different spectra, the main technical problem resides in the information fusion process. In this paper, we propose a new halfway feature fusion method for neural networks that leverages the complementary/consistency balance existing in multispectral features by adding to the network architecture, a particular module that cyclically fuses and refines each spectral feature. We evaluate the effectiveness of our fusion method on two challenging multispectral datasets for object detection. Our results show that implementing our Cyclic Fuse-and-Refine module in any network improves the performance on both datasets compared to other state-of-theart multispectral object detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Visible and thermal image channels are expected to be complementary when used for object detection in the same outdoor scenes. In particular, visible images tend to provide color and texture details while thermal images are sensitive to objects' temperature, which may be very helpful at night time. However, because they provide a very different view of the same scene, the features extracted from different image spectra may be inconsistent and lead to a difficult, uncertain and error-prone fusion <ref type="figure">(Fig. 1</ref>). In this figure, we use a Convolutional Neural Network (CNN, detailed later) to predict two segmentation masks based on the two (aligned) mono-spectral extracted features from the same image and then fuse the features to detect pedestrians in the dataset. During the training phase, the object detection and the semantic segmentation losses are jointly optimised (the segmentation ground truths are generated according to pedestrian bounding box annotations). We can observe that most pedestrians are visible either on the RGB or on the infrared segmentation masks which illustrates the complementary of the channels. However, even though the visible-thermal image pairs are well aligned, the similarity between the two predicted segmentation masks is visible images visible masks thermal masks thermal images <ref type="figure">Fig. 1</ref>. Examples of thermal and RGB images of the same aligned scenes taken from KAIST multispectral pedestrian detection dataset <ref type="bibr" target="#b0">[1]</ref> with detected bounding boxes. The segmentation masks (2nd and 4th columns) are predicted based on the (mono-)spectral features before any fusion process. small, i.e., the multispectral features may be inconsistent.</p><p>In order to augment the consistency between features of different spectra, we design a novel feature fusion approach for convolutional neural networks based on Cyclic Fuse-and-Refine modules. Our main idea is to refine the mono-spectral features with the fused multispectral features multiple times consecutively in the network. Such a fusion scheme has two advantages: 1) since the fused features are generally more discriminative than the spectral ones, the refined spectral features should also be more discriminative than the original spectral features and the fuse-and-refine loop gradually improves the overall feature quality; 2) since the mono-spectral features keep being refined with the same features, their consistency progressively increases, along with the decrease of their complementary, and the consistency/complementary balance is achieved by controlling the number of loops.</p><p>We review the related works on multispectral feature fusion with CNN in Section 2. We detail our novel network module named Cyclic Fuse-and-Refine, which loops on the fuse-and-refine operations to adjust the multispectral features' complementary/consistency balance in Section 3. In Section 4, we show experiments on the well known KAIST multispectral pedestrian detection dataset <ref type="bibr">[</ref> state-of-the-art results, and on the less known FLIR ADAS dataset <ref type="bibr" target="#b1">[2]</ref> on which we set a first strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Existing approaches mainly differ on the strategies ("when" and "how") used to fuse the multispectral features. When to fuse. The first study on CNN-based multispectral pedestrian detection is made by <ref type="bibr" target="#b2">[3]</ref>, and they evaluate two fusion strategies: early and late fusions. Then <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> explore this further and show that a fusion of features halfway in the network, achieves better results than the early or the late fusion. Since then, the halfway fusion has become the default strategy in deep learning-based multispectral (and multimodal) works ( <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). We also choose to locate our fuse-and-refine fusion module halfway in the network. How to fuse. Features extracted from each spectral channel have different physical properties and choosing how to fuse these complementary information is another central research topic. Basic fusion methods include element-wise addition/average, element-wise maximum and concatenation sometimes in addition to a 1 ? 1 convolution to compress the number of channels as done e.g. in <ref type="bibr" target="#b9">[10]</ref>. Building on this, more advanced methods such as <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref> use illumination information to guide the multispectral feature fusion. <ref type="bibr" target="#b10">[11]</ref> apply Gated Fusion Units (GFU) <ref type="bibr" target="#b11">[12]</ref> to combine two SSD networks <ref type="bibr" target="#b12">[13]</ref> on color and thermal inputs. <ref type="bibr" target="#b7">[8]</ref> propose a cross-modality interactive attention network to dynamically weight the fusion of thermal/visible features. Our strategy is different: we suggest a cyclic fusion scheme to progressively improve the quality of the spectral features and automatically adjust the complementary/consistence balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APPROACH</head><p>Overview. The fusion and refinement operations are the main ones of our proposed approach. They are repeated (through a cycle) multiple times to increase the consistency of the multispectral features and to decrease the complementarity of the features. An illustration of our Cyclic Fuse-and-Refine module with 3 loops in the cycle is presented in <ref type="figure" target="#fig_0">Fig. 2</ref>. Fuse-and-Refine. In each loop i, for the fused ( f ), visible ( v ) and thermal ( t ) features, the multispectral feature fusion can be formalized as</p><formula xml:id="formula_0">f i f = F(?(f i?1 t , f i?1 v ))</formula><p>, where ? is a feature concatenation operation, and F is a 3 ? 3 convolution followed by a batch normalization operation. For simplicity and to avoid over-fitting, the operation F in all loops shares weights. The fused features are then assigned as residuals of the spectral features for refinement:</p><formula xml:id="formula_1">f i t = H(f i?1 t +f i f ), f i v = H(f i?1 v + f i f ).</formula><p>H is the activation function (e.g. ReLU). Semantic supervision. In order to prevent the vanishing gradient problem when learning the parameters of the network and to better guide the multispectral feature fusion, an auxiliary semantic segmentation task is used to bring separate supervision information for each refined spectral features. Concretely, after being refined with the fused features, the thermal and visible features go through a 1 ? 1 convolution (aiming at replacing a fully-connected layer so to ensure a fully-convolutional network) to predict two pedestrian segmentation masks, one for each channel. These predicted masks are also used to tune (or at least visualize) the number of loops in the cyclic module according to the complementary/consistency variations in the features. Final fusion. Following <ref type="bibr" target="#b13">[14]</ref>, since the optimal cycling number is unknown and could be different for different image pairs, we aggregate all the refined spectral features to generate the final fused features that will be used for the object detection part of the network. The aggregation is a simple element-wise average function. Let I be the number of loops, the final computation is: 1 2I (</p><formula xml:id="formula_2">I 1 f i t + I 1 f i v ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate the proposed Cyclic Fuse-and-Refine Module on KAIST Multispectral Pedestrian Detection <ref type="bibr" target="#b0">[1]</ref> and FLIR ADAS dataset <ref type="bibr" target="#b1">[2]</ref>, and compare our results with the state-ofthe-art multispectral methods. Examples of image pairs with their ground truth bounding boxes are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>KAIST. We use the processed version of this multispectral pedestrian detection dataset which contains 7,601 colorthermal image pairs for training and 2,252 pairs for testing. We kept the bounding boxes annotated as "person", "person?" or "people" as positive pedestrian examples. <ref type="bibr" target="#b6">[7]</ref> proposed a "sanitized" version of the training annotations which eliminated some of the annotation errors from the original training annotations. According to <ref type="bibr" target="#b3">[4]</ref>, inaccurate annotations in the test set leads to unfair comparisons, so we only use their "sanitized" testing annotations for our evaluation, with the usual "Miss Rate" performance metric under reasonable setting, i.e., a test subset containing not/partially occluded pedestrians which are larger than 55 pixels. FLIR. This recently released multispectral (multi-)object detection dataset contains around 10k manually-annotated thermal images with their corresponding reference visible images, collected during daytime and nighttime. We only kept the 3 more frequent classes which are "bicycle", "car" and "person". We manually removed the misaligned visible-thermal image pairs and ended with 4,129 well-aligned image pairs for training and 1,013 image pairs for test <ref type="bibr" target="#b0">1</ref> . Some examples of the well-aligned and misaligned visible-thermal image pairs are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>Network architecture. We implemented our Cyclic Fuseand-Refine module on the single stage object detector FSSD <ref type="bibr" target="#b14">[15]</ref>, which is an improved version of the well known SSD object detector <ref type="bibr" target="#b12">[13]</ref>. Note that our proposed module is independent from the chosen network architecture. Following <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref>, the mono-spectral features are extracted independently through a VGG16 <ref type="bibr" target="#b15">[16]</ref> network, and fused after the conv4 3 layer (halfway through the network). Our baseline architecture uses the element-wise average for the multispectral feature fusion and we integrate and evaluate the proposed module with different number of loops. Data augmentation. As implemented in SSD <ref type="bibr" target="#b12">[13]</ref> and FSSD <ref type="bibr" target="#b14">[15]</ref>, a few data augmentation methods are applied, such as image random cropping, padding, flipping and distorting for both visible and thermal images.</p><p>Anchor designing. Following <ref type="bibr" target="#b16">[17]</ref>, the anchor designing strategy is adapted for the pedestrian detection for KAIST dataset: we fix the aspect ratio of each anchor box to 0.41 and we only keep three detection layers with scales 32 and 32 ? 2, 64 and 64 ? 2, 128 and 128 ? 2 from fine to coarse respectively. For FLIR, we use the same scale settings but we augment the aspect ratio setting to {1, 2, 1 2 }. Loss functions. To improve object detection, SDS RCNN <ref type="bibr" target="#b17">[18]</ref> and MSDS RCNN <ref type="bibr" target="#b6">[7]</ref> use an additional task, semantic segmentation, and jointly optimize the loss for the segmentation and detection tasks while training the network. To fairly compare our work to these competitors, we also use this auxiliary loss to supervise the training of the proposed module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods</head><p>On KAIST. We compare the experimental results of our approach with state-of-the-art methods in <ref type="table">Table 1</ref>. For these experiments, we make 3 loops in the Fuse-and-Refine cycle. Depending on what was done in the literature and to allow a fair comparison, we report our detection accuracy with sanitized and original training annotations respectively. All the deep learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> use the same input image resolution (640 ? 512) and the same backbone network (VGG16). The results show that our proposed method allows us to obtain better detection results than all its competitors for both the sanitized and original training annotations. Note that the computational overhead from CFR is quite small. During inference, each cycle only add ?0.4ms of inference time. On FLIR. Because of the misalignment problems in the dataset, there is, to our knowledge, no paper which uses the FLIR dataset <ref type="bibr" target="#b1">[2]</ref> for multispectral object detection. We use our sanitized version of the dataset and compare the mAP percentage of two different models: a baseline model which uses the traditional halfway fusion architecture (with the VGG backbone) and the same model with our proposed module. Again, we can see in <ref type="table" target="#tab_0">Table 2</ref> that our method provides important mAP gains for all the considered object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>We study in details (on the KAIST dataset with the sanitized training annotations and the reasonable test set) the ef-input images first refine third refine second refine fectiveness of the proposed fusion module and the relationship between the number of loops in the fuse-and-refine cycle and the multispectral feature complementary/consistency balance. The experimental results are summarised in <ref type="table">Table  3</ref>. We provide the Miss Rate and DICE scores <ref type="bibr" target="#b19">[20]</ref> between the pedestrian masks predicted by each version of the refined thermal/visible features. These DICE scores are used as an indicator of similarity between the spectral features. From the table we observe successive accuracy gains from the baseline (no loop) to 3 loops, and a decrease after 4 loops; meanwhile the value of DICE scores continue to increase along with the number of loops. We then visualize, on two sample image pairs, the pedestrian masks predicted by visible/thermal features after each refinement in <ref type="figure" target="#fig_2">Figure 4</ref>. The first column corresponds to input images marked with the detected pedestrians; The second, third and fourth columns correspond to segmentation masks predicted after 1 to 3 loops. The first and third lines (resp. second and fourth) are for visible (res. thermal) images and their corresponding segmentation masks. It can be observed that the quality and similarity of the masks gradually increase with the number of loops. With the increase of similarity between the spectral features, their consistency increases and their complementarity decreases. As mentioned in Section 1, the lack of consistency between the multispectral features is harmful; on the contrary, too much consistency leads to sharp emerge/plunge in the feature values, and makes the fusion meaningless. That explains why the Miss Rate starts to decrease after 4 loops. In practice the number of loops should be tuned for any dataset but we believe that very few values should be tried (between 2 and 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Miss  <ref type="table">Table 3</ref>. Miss rates versus DICE scores w.r.t. different numbers of Fuse-and-Refine loops. Each experiment is repeated five times and we report the average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper proposes a novel cycle fuse-and-refine module to improve the multispectral feature fusion while taking into account the complementary/consistency balance of the features. Experiments on KAIST <ref type="bibr" target="#b0">[1]</ref> and FLIR <ref type="bibr" target="#b1">[2]</ref> datasets show that integrating the proposed fusion module to a "vanilla" multispectral pedestrian detector leads to substantial accuracy improvements. Several visible/thermal image pairs have a misalignment problem in FLIR dataset. This problem could be more serious in real world applications due to calibration errors or temporal shifts. A Region Feature Alignment (RFA) module <ref type="bibr" target="#b20">[21]</ref> tackled such a cross-modality disparity problem in a supervised manner and in a two-stage object detection setting. In the future, we would like to explore a more general solution to this problem with a similar cyclic-align scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>1] on which we obtain new arXiv:2009.12664v1 [cs.CV] 26 Sep 2020 Illustration (folded on the left part and unfolded on the right) of the proposed Cyclic Fuse-and-Refine Module with 3 loops. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of visible/thermal image pairs with their ground truth from the KAIST dataset (in the first line) and from the FLIR dataset in the second and third lines (the ground truth annotations are given according to the thermal images). The third line gives an example of misaligned pairs in the FLIR dataset. Better viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of pedestrian segmentation masks predicted on 2 visible/thermal image pairs (one taken at day time, one taken at night) of the KAIST dataset after a different number of loops (1-3) in the fuse-and-refine cycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>mAP results for two CNN object detection architectures which use (or not) our Cyclic Fuse-and-Refine (CFR) blocks on FLIR dataset<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Rate (lower, better)</cell></row><row><cell></cell><cell></cell><cell>R-All</cell><cell>R-Day</cell><cell>R-Night</cell></row><row><cell cols="4">Training with sanitized annotations:</cell></row><row><cell cols="2">MSDS-RCNN [7]</cell><cell>7.49%</cell><cell>8.09%</cell><cell>5.92%</cell></row><row><cell>CFR 3</cell><cell></cell><cell>6.13%</cell><cell>7.68%</cell><cell>3.19%</cell></row><row><cell cols="4">Training with original annotations:</cell></row><row><cell cols="2">ACF+T+THOG [1]</cell><cell cols="3">47.24% 42.44% 56.17%</cell></row><row><cell cols="2">Halfway Fusion [4]</cell><cell cols="3">26.15% 24.85% 27.59%</cell></row><row><cell cols="5">Fusion RPN+BF [19] 16.53% 16.39% 18.16%</cell></row><row><cell cols="2">IAF R-CNN [5]</cell><cell cols="3">16.22% 13.94% 18.28%</cell></row><row><cell cols="2">IATDNN+IASS [6]</cell><cell cols="3">15.78% 15.08% 17.22%</cell></row><row><cell cols="2">MSDS-RCNN [7]</cell><cell cols="3">11.63% 10.60% 13.73%</cell></row><row><cell>CFR 3</cell><cell></cell><cell cols="2">10.05% 9.72%</cell><cell>10.80%</cell></row><row><cell cols="5">Table 1. Detection accuracy comparisons in terms of Miss</cell></row><row><cell cols="5">Rate percentage on KAIST Dataset [1]. Our competitors' re-</cell></row><row><cell cols="3">sults are taken from [5] and [7].</cell><cell></cell></row><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">Bicycle Car</cell><cell>Person</cell></row><row><cell cols="5">Baseline 71.17% 56.39% 83.90% 73.28%</cell></row><row><cell>CFR 3</cell><cell cols="4">72.39% 57.77% 84.91% 74.49%</cell></row><row><cell cols="4">Methods Miss Rate DICE Scores</cell></row><row><cell cols="2">Baseline 7.68%</cell><cell>-</cell><cell></cell></row><row><cell>CFR 1</cell><cell>6.90%</cell><cell>{64.53%}</cell><cell></cell></row><row><cell>CFR 2</cell><cell>6.40%</cell><cell cols="2">{78.89%, 89.70%}</cell></row><row><cell>CFR 3</cell><cell>6.13%</cell><cell cols="3">{74.60%, 90.60%, 94.17%}</cell></row><row><cell>CFR 4</cell><cell>7.09%</cell><cell cols="3">{58.25%, 85.91%, 92.9%, 96.11%}</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This new aligned dataset can be downloaded here: http:// shorturl.at/ahAY4</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonmin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukyung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Free flir thermal dataset for algorithm training</title>
		<ptr target="https://www.flir.com/oem/adas/adas-dataset-form/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection using deep fusion convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th European Symposium on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multispectral deep neural networks for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Illumination-aware faster R-CNN for robust multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection via simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">225</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crossmodality interactive attention network for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly aligned crossmodal learning for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GFD-SSD: gated fusion double SSD for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izzat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahrzad</forename><surname>Izzat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziaee</surname></persName>
		</author>
		<idno>abs/1903.06999</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>John Edison Arevalo Ovalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Montes Y Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzlez</surname></persName>
		</author>
		<idno>abs/1702.01992</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>Octo- ber 11-14</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Residual-guide network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huafeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">17511759</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FSSD: feature fusion single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1712.00960</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07032</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional region proposal networks for multispectral person detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jarvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Teutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The cross-modality disparity problem in multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
