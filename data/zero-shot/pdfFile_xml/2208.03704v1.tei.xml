<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
							<email>lutzs@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mechanical, Manufacturing and Biomedical Engineering</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Blythman</surname></persName>
							<email>blythmar@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Accenture Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustav</forename><surname>Ghosal</surname></persName>
							<email>ghosalk@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Moynihan</surname></persName>
							<email>mamoynih@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mechanical, Manufacturing and Biomedical Engineering</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciaran</forename><surname>Simms</surname></persName>
							<email>csimms@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
							<email>smolica@tcd.ie</email>
						</author>
						<title level="a" type="main">Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D human pose estimation technologies have the potential to greatly increase the availability of human movement data. The best-performing models for single-image 2D-3D lifting use graph convolutional networks (GCNs) that typically require some manual input to define the relationships between different body joints. We propose a novel transformer-based approach that uses the more generalised self-attention mechanism to learn these relationships within a sequence of tokens representing joints. We find that the use of intermediate supervision, as well as residual connections between the stacked encoders benefits performance. We also suggest that using error prediction as part of a multi-task learning framework improves performance by allowing the network to compensate for its confidence level. We perform extensive ablation studies to show that each of our contributions increases performance. Furthermore, we show that our approach outperforms the recent state of the art for singleframe 3D human pose estimation by a large margin. Our code and trained models are made publicly available on Github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Movement analysis is vitally important for applications including mixed reality, human-computer interaction, sports biomechanics and physiotherapy. However, human motion is highly complex and subjective, and the high dimensionality and the variation between subjects means that much is still not understood. While motion capture systems have been used to quantify movement since the 1980s <ref type="bibr" target="#b0">[1]</ref>, the equipment is expensive, the datasets are largely constrained to laboratory settings and relatively few are publicly-available. At the same time, the internet has collected vast amounts of in-the-wild (unlabelled and unconstrained) images and videos of moving humans. The maturation of monocular 3D human pose estimation (HPE) technologies has the potential to create a stepincrease in data available and scenarios that can be assessed, which can ultimately be used to improve our understanding of human movement.</p><p>Monocular 3D human pose estimation involves the prediction of 3D joint positions from a single viewpoint. While video techniques can leverage temporal information to improve * denotes equal contribution accuracy, single-image estimators remain useful in their own right. For example, useful information about human movement can be learned from online image datasets or video datasets with low frame-rate. Furthermore, some video approaches opt to use a combination of single-frame spatial models along with a multi-frame temporal model <ref type="bibr" target="#b1">[2]</ref>. Thus, strong single-image 3D pose lifters can also improve the performance on video data.</p><p>Direct estimation techniques aim to estimate 3D human pose directly from images <ref type="bibr" target="#b2">[3]</ref>. However, diverse image datasets with 3D pose labels are sparse, and it is convenient to leverage the high accuracy of off-the-shelf 2D pose estimators <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> that are well-suited for the pixel-processing task. Lifting approaches predict the 3D pose from 2D joint predictions provided from such estimators. The types of neural network architectures used to learn this mapping have progressively evolved. The original simple baseline <ref type="bibr" target="#b5">[6]</ref> for pose lifting used a multi-layer perceptron to achieve surprising accuracy, even without information from image features. More recent works have highlighted that treating pose as a vector ignores the spatial relationships between joints, and that graph structures may be better-suited <ref type="bibr" target="#b6">[7]</ref>. However, existing works on graph convolutions require a hand-crafted adjacency matrix to define the relationship between nodes. Also, standard graph convolutions model the relationships between neighboring joints with a shared weight matrix, which is sub-optimal for modelling articulated bodies since the relations between different body joints may be different <ref type="bibr" target="#b7">[8]</ref>. Furthermore, stacking GCN layers may result in over-smoothing <ref type="bibr" target="#b8">[9]</ref>. In contrast, the self-attention operator of the transformer model generalises the feed-forward layer to be dynamic to the input, and the relationship between joints can be learned rather than manually encoded.</p><p>Transformers first unseated recurrent neural networks as the predominant models in natural language processing (NLP) <ref type="bibr" target="#b9">[10]</ref>, and have recently gained success in replacing convolutional neural networks in vision tasks such as image classification <ref type="bibr" target="#b10">[11]</ref>, object detection <ref type="bibr" target="#b11">[12]</ref>, and action recognition <ref type="bibr" target="#b12">[13]</ref>. However, very few studies have applied transformers to the task of 3D human pose estimation to date. Existing works have either used a direct estimation approach <ref type="bibr" target="#b13">[14]</ref> or focused their studies on video-based 2D-3D lifting <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>. To our knowledge, ours is the first method to adopt a transformer for single-image 2D-3D pose lifting. The input and output tokens of the sequence represent joints, and thus we refer to our method as the Jointformer.</p><p>Accordingly, our contributions are as follows:</p><p>? We present a novel single-frame 2D-3D lifting Joint Transformer for human pose estimation. ? We suggest that predicting the error associated with each of the joints improves accuracy, by enabling the network to compensate for its own uncertainty. ? We propose a Refinement Transformer to refine the 3D pose predictions of the Joint Transformer, based on the 3D prediction themselves, the input 2D joints, and the predicted errors for each joint. ? We introduce intermediate supervision on the 3D joint and error predictions within the stack of transformer encoders, facilitated by linear layers. ? We show that this architecture achieves state-of-the-art results on the Human3.6M and MPI-INF-3DHP datasets for single-frame 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Human Pose Estimation: Due to its applicability in diverse areas such as action recognition, augmented and mixed reality etc., HPE has emerged as a very active problem in computer vision. We present the literature relevant to direct estimation and 2D-3D lifting methods. Direct estimation refers to estimating the 3D pose directly from raw images. Pavlakos et al. <ref type="bibr" target="#b15">[16]</ref> use the ordinal depths of human joints to provide a weaker supervision signal. Sun et al. <ref type="bibr" target="#b16">[17]</ref> exploit the joint connection structure and define a compositional loss function that encodes long range interactions of the pose. Martinez et al. <ref type="bibr" target="#b5">[6]</ref> explored decoupling of the problem into 2D human pose estimation and 2D-3D lifting and used a vanilla neural network to learn the mapping. Zhao et al. <ref type="bibr" target="#b6">[7]</ref> exploit the spatial relations of nodes (both local and global) in a semantic graph convolutional network (SemGCN) to improve performance. Ci et al. <ref type="bibr" target="#b17">[18]</ref> introduced locally-connected networks to overcome the limited representation power of GCNs for estimating 3D pose. Xu et al. <ref type="bibr" target="#b18">[19]</ref> use a graph stacked hourglass network to process graph-structured features across three different scales of human skeletal representations. Tekin et al. <ref type="bibr" target="#b19">[20]</ref> design a network to combine the strengths and weaknesses of direct estimation and lifting. Yang et al. <ref type="bibr" target="#b20">[21]</ref> propose a method for in-the-wild HPE using adversarial learning aided by a geometry-aware discriminator. Fang et al. <ref type="bibr" target="#b21">[22]</ref> present a lifting framework based on bidirectional recurrent networks which explicitly models auxiliary information such as kinematics, symmetry and motor coordination. Sharma et al. <ref type="bibr" target="#b22">[23]</ref> use variational autoencoder for predicting a set of candidate 3D poses and ranks them using an ordinal score or oracle. Liu et al. <ref type="bibr" target="#b7">[8]</ref> -in a comprehensive review of HPE -study different weight sharing strategies for graph convolution based lifting methods. Zhou et al. <ref type="bibr" target="#b23">[24]</ref> propose a two-stage method in which the 2D points are mapped to an intermediate latent space, followed by a volumetric regression to the 3D space.</p><p>Transformers for Vision: Convolutional neural networks (CNNs) <ref type="bibr" target="#b24">[25]</ref> have remained the dominant model in computer vision for three decades. Since the convolution operates within small local neighbourhoods, deeply stacking convolutions becomes necessary to form large receptive fields and capture long-distance dependencies. Inspired by the self-attention mechanism in NLP <ref type="bibr" target="#b9">[10]</ref>, the non-local neural network <ref type="bibr" target="#b25">[26]</ref> used a small number of non-local blocks between various stages of a CNN to better capture long-distance relationships for action recognition. Ramachandran et al. <ref type="bibr" target="#b26">[27]</ref> explored using stand-alone self-attention within small windows as a basic building block (rather than an augmentation on top of convolutions) for image classification and object detection. Similarly, Zhao et al. <ref type="bibr" target="#b27">[28]</ref> used a self-attention network with pairwise and patchwise self-attention for image recognition.</p><p>Rather than basing the structure of building blocks on ResNet-style architectures, the vision transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> directly applied a standard transformer (with as few modifications as possible) to classify images. Images were input as sequences of flattened patches, with patches treated similar to tokens (words) in NLP applications. Similar approaches have since been applied to video classification <ref type="bibr" target="#b28">[29]</ref> and action recognition <ref type="bibr" target="#b12">[13]</ref>.</p><p>Despite showing significant promise in other vision tasks, few studies have applied transformers to the task of 3D human pose estimation. Liu et al. were the first to use an attention mechanism to adaptively identify significant frames in temporal windows of human motion videos. Lin et al. <ref type="bibr" target="#b13">[14]</ref> used a transformer for direct estimation of 3D human pose and shape for a single input frame. The input tokens consisted of a feature vector extracted with a CNN that is concatenated with either randomly-initialised 3D joints or mesh vertices. The input input token dimensions were progressively reduced after each encoder. Zheng et al. <ref type="bibr" target="#b1">[2]</ref> addressed the task of video-based 3D human pose estimation and incorporated a spatial transformer as a component within a larger temporal transformer, where the output tokens are encoded features rather than 3D joint positions. Thus, it cannot be applied to single-frame 2D-3D lifting and its performance was not evaluated on images. Li et al. <ref type="bibr" target="#b14">[15]</ref> used a lifting transformer that replaced feed-forward layers in the encoder with strided temporal convolutions. It operated on a temporal sequence, where each input token was a whole skeleton. This means that useful information cannot be passed between joints in the skeleton.</p><p>Error Prediction, Refinement and Intermediate Supervision: The predictions from human pose estimation models inevitably contain errors. Ronchi et al. <ref type="bibr" target="#b29">[30]</ref> performed a rigorous analysis of the prediction errors of 2D HPE estimators on the COCO dataset <ref type="bibr" target="#b30">[31]</ref>. The relative frequency and impact to performance of different categories of errors (jitter, inversion, swap, and miss) was investigated, and it was found that the errors showed similar distributions independent of the type of 2D pose estimator used. Moon et al. <ref type="bibr" target="#b31">[32]</ref> used these error statistics to generate synthetic poses, which were input to a refinement network. Similarly, Chang et al. <ref type="bibr" target="#b32">[33]</ref> used specific error distributions to synthesise 2D poses to be used for 2D-3D pose lifting. Fieraru et al. <ref type="bibr" target="#b33">[34]</ref> introduced a pose refinement network that takes as input both the image and a given pose estimate. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> constructed a pose graph to consider the relationship between different keypoints during refinement. Wang et al. <ref type="bibr" target="#b35">[36]</ref> used a graph pose refinement (GPR) module to refine the visual features based on the relationship between keypoints.</p><p>Intermediate supervision is the practice of using additional loss terms at various stages of deep neural networks to improve performance. In 2D human pose estimation, Newell et al. <ref type="bibr" target="#b36">[37]</ref> used repeated bottom-up/top-down processing with intermediate supervision with their stacked hourglass network. DETR <ref type="bibr" target="#b11">[12]</ref> used intermediate supervision between the stacked transformer decoder blocks for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we describe our proposed network architecture for 3D human pose estimation. Following previous 2D-3D pose lifting approaches <ref type="bibr" target="#b5">[6]</ref>, we use an off-the-shelf model to generate 2D poses from images, and use these predictions to estimate the corresponding 3D poses. The 3D poses are estimated in camera coordinates and centered on the pelvis joint. While most of the previous state of the art use multilayer perceptrons (MLPs) <ref type="bibr" target="#b5">[6]</ref> or GCNs <ref type="bibr" target="#b6">[7]</ref>, we use transformers to predict and refine the 3D pose in two parts. First, our Joint Transformer estimates the 3D pose and the prediction error from the 2D pose of a single frame. Afterwards, our Refinement Transformer further improves this prediction by using the intermediate prediction and prediction error. This novel combination of networks allows us achieve state-of-theart performance in single frame 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Joint Transformer</head><p>Our Joint Transformer is designed to lift the input 2D pose to 3D for a single frame. Given a set of J 2D joint coordinates x, we consider each joint a token and first embed each into a higher dimension of size c: f embed (x) : R J?2 ? R J?c . Traditionally in transformers, the embedding is followed by a positional encoding. In our case however, the order of joints is not changed during both training and testing, implicitly encoding the position in the input itself (i.e. the first joint in the input is always the hip joint). This makes the addition of an explicit positional encoding redundant, and in fact hurts our prediction performance as we show in Section IV-C. Following the embedding, we feed the joint tokens into a stack of transformer encoders where self-attention is applied to share information across each joint. We use the original implementation for the transformer encoder <ref type="bibr" target="#b9">[10]</ref> and do not modify the hidden dimension further. Given L transformer encoders, the output of each encoder is z L ? R J?c . Finally, we regress the output of the last encoder z to predict the 3D pose using an MLP block consisting of layer normalization, dropout and a single linear layer f pred (z) : R J?c ? R J?3 . 1) Intermediate supervision: By design, the transformer encoder layers share information between tokens and learns which tokens are important for prediction. This gives even the first transformer encoder in our stack the ability (though not necessarily the capacity) to learn how joint tokens interact and to predict the 3D pose. We leverage this by training our network using intermediate supervision. Compared to previous methods, we do not compute the loss at the end of the network alone, but introduce a loss term after each individual transformer encoder in the stack. This allows the network to learn initial estimates that get further refined by each transformer encoder in the stack. Subsequent transformer encoders get passed down highly discriminative features from previous encoders and can focus on increasingly fine-grained features to refine previous estimates. To further help with this, we add residual connections around every encoder. In our implementation, each encoder is followed by an MLP predicting the 3D pose, to which a loss can be applied. We add an additional linear layer to the predicted 3D pose to embed the prediction back to the hidden dimension of the transformer and add the embedding to the original transformer output, as can be seen in the upper right of <ref type="figure">Figure 1</ref>.</p><p>2) Pose embedding: The 2D input pose coordinates need to be expanded to the hidden dimension of our network. Previous methods used linear layers or graph convolutions for this task. In our method however, we use 1D convolutions with a kernel size of 1 across all joints. This is equivalent to a linear layer that expands the 2D coordinates to the hidden dimension individually for each joint with weight sharing between each joint. Since the embedding weights are shared for each joint, the layer can focus on finding the best way to expand the 2D coordinates regardless of the location of the joint. Traditionally, transformers also require a positional encoding of the embedded tokens. Since the input order of joints never changes, the position is implicitly already encoded and additional encodings are redundant. We show in <ref type="table" target="#tab_2">Table III</ref> that our method works better without any explicit positional encoding and our convolutional embedding outperforms a linear layer.</p><p>3) Error prediction: Giving an estimate of the confidence in its prediction gives the network the ability to compensate. Since classification heads generate one prediction per class before a softmax layer, their prediction confidence can be directly inferred. In contrast, regression heads lack this ability out of the box. We add a second MLP mirroring the 3D pose prediction to the output of the transformer encoder, and force the network to implicitly learn its own uncertainty. Therefore, the error prediction will predict its own error per joint and per coordinate f error (z) : R J?c ? R J?3 . We observe that the addition of the error prediction stabilizes the training and leads to better overall results (Section IV-C). It can also serve as important additional information when visualizing the pose or for practical application of the technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss function:</head><p>The loss function which we use to train our network consists of two terms: the 3D prediction loss L 3D and the error loss L error . For the 3D prediction loss, we use  <ref type="figure">Fig. 1</ref>. Our proposed Jointformer for single-frame 2D-3D human pose lifting.</p><p>the mean-squared error between the predicted 3D poses y and the ground-truth poses?. For the error loss, we first define the true error? as the absolute difference between the predicted and the ground-truth pose? = |y ??|. This allows us to use the mean-squared error between the predicted error e and the true error? as the error loss. Due to our intermediate supervision, we apply both of these loss terms after every transformer encoder in the stack. This leads to the combined loss:</p><formula xml:id="formula_0">L combined = 1 L L i=1 L i 3D + L i error 2 ,<label>(1)</label></formula><p>where L is the number of transformer encoders in the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Refinement Transformer</head><p>In order to make further use of the error prediction and boost the accuracy of the pose prediction, we design an additional Refinement Transformer. This network is similar to our Joint Transformer with the following difference: the input is the original 2D pose concatenated to the predicted 3D pose and error prediction. We only stack two transformer encoders with a hidden dimension of 256, and we do not use intermediate supervision. As with the Joint Transformer, the Refinement Transformer tokenizes each joint and uses a 1D convolution to embed the 8-channel input to the hidden dimension. Following the stacked transformer encoders, we use a linear layer to predict the final 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>We implement our approach in PyTorch <ref type="bibr" target="#b37">[38]</ref> and use a single NVIDIA GeForce RTX 2080 TI for training. We train the Joint Transformer for 30 epochs with a batch size of 256 using the AdamW <ref type="bibr" target="#b38">[39]</ref> optimizer, an initial learning rate of 0.001 and cosine annealing learning rate decay <ref type="bibr" target="#b39">[40]</ref>. All of our prediction layers also include dropout of 0.2 during training. Afterwards, we fix the weights of the Joint Transformer and train the Refinement Transformer using the 3D prediction loss and employing the same hyperparameters. During evaluation we use test-time data augmentation by horizontal flipping, following <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce the datasets used to evaluate our proposed approach. The results are then compared to the state of the art, and an ablation study is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation</head><p>We consider two popular benchmarks for 3D HPE following recent works <ref type="bibr" target="#b18">[19]</ref>.</p><p>Human3.6M <ref type="bibr" target="#b41">[42]</ref>: This is the most widely-used dataset for 3D human pose estimation. It uses a motion capture system and 4 video cameras to capture the 3D pose information and accompanying images, respectively. The camera calibration parameters are used to project the 3D joint positions to the 2D image plane of each camera. The dataset contains 3.6 million images of 7 professional actors performing 15 generic movements such as walking, eating, sitting, making a phone call and engaging in a discussion. We follow the standard protocol, using subjects 1, 5, 6, 7 and 8 for training, and subjects 9 and 11 for evaluation.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b42">[43]</ref>: This dataset contains images of three different scenarios: (i) studio with a green screen (GS), (ii) studio without green screen (noGS) and (iii) outdoor scene (Outdoor). We follow the common skeleton representation for both of these datasets, which contains 17 joints.</p><p>Following common practice, for Human3.6M we evaluate by using the mean per-joint pixel error (MPJPE) in mm between the ground truth and our predictions across all joints and cameras, after alignment of the root joint (Protocol #1). It is also common to align the prediction with the ground truth using a rigid transformation <ref type="bibr" target="#b43">[44]</ref> (Protocol #2). For the MPI-INF-3DHP test set we use 3D-PCK (Percentage of Correct Keypoints) and AUC (Area Under the Curve) as evaluation metrics following previous works <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b17">[18]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Art Methods</head><p>We compare the performance of our proposed model with recent state-of-the-art methods for single-frame 3D human pose estimation across two popular datasets: Human3.6M and MPI-INF-3DHP. We include both direct estimation and 2D-3D lifting methods in our analysis. Existing lifting methods use either multilayer perceptron or graph convolutional models to map from 2D to 3D. In contrast, the Joint Transformer uses a transformer architecture with intermediate supervision, error prediction and refinement. To highlight the significance of our improvements, we also compare to a "Base Transformer" architecture that consists of a linear embedding, spatial positional encoding <ref type="bibr" target="#b1">[2]</ref> and transformer encoder blocks.</p><p>We first investigate the in-the-wild performance of our network where it takes the 2D predictions of a 2D pose detector as input. Table I(a) shows the results on the Human3.6M dataset on 2D predictions of the cascaded pyramid network (CPN). Our Joint Transformer outperforms the state-of-theart approaches by more than 1.4 mm. The addition of the Refinement Transformer decreases our error a further 0.4 mm for a total improvement of 1.8 mm. Our Joint Transformer also outperforms the Baseline Transformer without intermediate supervision, error prediction and with linear pose embedding by more than 3 mm.</p><p>Since the above results are dependent on the quality of the output of the 2D pose detector, it is useful to remove the effect of the 2D backbone to characterise the theoretical performance of our network. Table I(b) presents results on the Human3.6M dataset with ground truth 2D joint positions passed as input. Here, we find that our Joint Transformer surpasses the state of the art by 1.8 mm. In this case, we find that our Refinement Transformer does not further decrease the prediction error. In fact, the performance of the Refinement Transformer seems correlated to the magnitude of the error prediction of the Joint Transformer. In the case of the groundtruth 2D inputs the predicted error is quite small, whilst the larger magnitude of the error prediction for the CPN keypoints can be effectively used by the Refinement Transformer to achieve a better prediction. In comparison to the state of the art, our method only uses 1.48M parameters, compared to 3.70M and 4.38M parameters for the second- <ref type="bibr" target="#b18">[19]</ref> and third- <ref type="bibr" target="#b7">[8]</ref> placed methods respectively.</p><p>To show the generalisation ability of our method, we evaluate our Joint Transformer trained on Human3.6M directly on the MPI-INF-3DHP test dataset without any additional training. As shown in <ref type="table" target="#tab_2">Table II</ref>, we outperform the previous state of the art, even though some previous methods use additional training data from MPI-INF-3DHP. We also present the performance of our Joint Transformer followed by the Refinement Transformer. As can be seen, the Refinement Transformer further enhances our results by an additional small margin. This shows the ability of our method to generalise to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation</head><p>In this section, we evaluate the effectiveness of our modifications to the standard transformer architecture on the Hu-man3.6M dataset <ref type="bibr" target="#b41">[42]</ref>. The results are reported in <ref type="table" target="#tab_2">Table III</ref>. To remove the influence of the 2D pose detector, the results are reported using 17 2D ground truth joints as input. The purpose of the ablation study is to investigate the effects of the number of transformer layers (T layers ), size of the encoder (D h ), type of pose embedding used (Embed P ose ), intermediate supervision (Int Sup.), error prediction (Err Pred.) and positional encoding (Pos Enc.). All reported errors are the average of 3 training runs. In the upper part of the   <ref type="bibr" target="#b41">[42]</ref>. AS <ref type="bibr">INPUT</ref> the network in a multi-task framework further improves the results. We also observe that explicitly adding positional encoding to the transformer tokens reduces the performance.</p><p>Since the order of the joints never changes, their positions are already implicitly encoded and further addition of an explicit encoding seems to confuse the network somewhat. This is the case for both the frequency based positional encoding, as well as the spatial encoding <ref type="bibr" target="#b1">[2]</ref>. In the lower part of the table, we show the effect on a different number of transformer encoders and size of the hidden dimension. We show that the highest performance and most stable training can be achieved by using 4 layers and a hidden dimension of 64. While a larger number in either of these parameters increases the capacity of the network, the larger capacity seems not to be needed for best results. Finally, we can also observe that using a 1D convolution to embed the 2D inputs to the hidden dimension instead of a linear layer not only saves some network parameters, but also leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LIMITATIONS</head><p>While the proposed system is robust to GT keypoints and robust detectors, we expect that the use of a noisy keypoint detector will contribute highly to poor performance. The Refinement Transformer improves results for the single-frame scenario. However, it is not trained on temporal noise and as such temporal inconsistencies will propagate to the estimated 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we have presented a novel architecture for 3D pose estimation to analyse the previously-unexplored performance of transformers on single images. We demonstrate that implicitly encoding kinematic joint structure allows for a robust learning framework that outperforms the state of the art on established benchmarks. While the current approach addresses only single-frame input, it may be used as a modular component of temporal or multi-frame architectures in subsequent research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENTS</head><p>This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under the Grant Number 15/RP/2776.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, GT KEYPOINTS ARE USED. WE START WITH A BASIC JOINTFORMER WITH T layers = 4, D h = 64 AND Embed P ose = Conv1D (FIRST ROW) AND NEXT ADD INTERMEDIATE SUPERVISION (SECOND ROW), ERROR PREDICTION (THIRD ROW) AND POSITIONAL ENCODING (FOURTH AND FIFTH ROW). WE NOTICE IMPROVEMENTS WITH INT SUP. AND ERROR PRED. BUT POS ENC. NEGATIVELY IMPACTS RESULTS. NEXT, WITH INT SUP. AND ERROR PRED. ADDED TO THE BASIC JOINTFORMER, WE EXPERIMENT WITH T layers (FIFTH AND SIXTH ROWS), D h (SEVENTH AND EIGHTH ROWS) AND Embed P ose = Linear (NINTH ROW). WE OBTAIN THE BEST RESULTS AT T layers = 4, D h = 64 , Embed P ose = Conv1D, WITHOUT POSITION ENCODING AND WITH INT SUP. AND ERR PRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(a) CPN keypoints Direct Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>Martinez et al. [6] ICCV'17</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Tekin et al. [20] ICCV'17</cell><cell>54.2</cell><cell>61.4</cell><cell>60.2</cell><cell>61.2</cell><cell>79.4</cell><cell>78.3</cell><cell>63.1</cell><cell>81.6</cell><cell>70.1</cell><cell>107.3</cell><cell>69.3</cell><cell>70.3</cell><cell>74.3</cell><cell>51.8</cell><cell>63.2</cell><cell>69.7</cell></row><row><cell>Sun et al. [17] ICCV'17 (+)</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>67.2</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5</cell><cell>53.4</cell><cell>61.6</cell><cell>47.1</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Yang et al. [21] CVPR'18 (+)</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4</cell><cell>49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Fang et al. [22] AAAI'18</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Pavlakos et al. [16] CVPR'18 (+)</cell><cell>48.5</cell><cell>54.4</cell><cell>54.5</cell><cell>52.0</cell><cell>59.4</cell><cell>65.3</cell><cell>49.9</cell><cell>52.9</cell><cell>65.8</cell><cell>71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>Zhao et al. [7] CVPR'19</cell><cell>48.2</cell><cell>60.8</cell><cell>51.8</cell><cell>64.0</cell><cell>64.6</cell><cell>53.6</cell><cell>51.1</cell><cell>67.4</cell><cell>88.7</cell><cell>57.7</cell><cell>73.2</cell><cell>65.6</cell><cell>48.9</cell><cell>64.8</cell><cell>51.9</cell><cell>60.8</cell></row><row><cell>Sharma et al. [23] ICCV'19</cell><cell>48.6</cell><cell>54.5</cell><cell>54.2</cell><cell>55.7</cell><cell>62.2</cell><cell>72.0</cell><cell>50.5</cell><cell>54.3</cell><cell>70.0</cell><cell>78.3</cell><cell>58.1</cell><cell>55.4</cell><cell>61.4</cell><cell>45.2</cell><cell>49.7</cell><cell>58.0</cell></row><row><cell>Ci et al. [18] ICCV'19 (+)</cell><cell>46.8</cell><cell>52.3</cell><cell>44.7</cell><cell>50.4</cell><cell>52.9</cell><cell>68.9</cell><cell>49.6</cell><cell>46.4</cell><cell>60.2</cell><cell>78.9</cell><cell>51.2</cell><cell>50.0</cell><cell>54.8</cell><cell>40.4</cell><cell>43.3</cell><cell>52.7</cell></row><row><cell>Liu et al. [8] ECCV'20</cell><cell>46.3</cell><cell>52.2</cell><cell>47.3</cell><cell>50.7</cell><cell>55.5</cell><cell>67.1</cell><cell>49.2</cell><cell>46.0</cell><cell>60.4</cell><cell>71.1</cell><cell>51.5</cell><cell>50.1</cell><cell>54.5</cell><cell>40.3</cell><cell>43.7</cell><cell>52.4</cell></row><row><cell>Lin et al. [14] CVPR'21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.0</cell></row><row><cell>Xu et al. [19] CVPR'21</cell><cell>45.2</cell><cell>49.9</cell><cell>47.5</cell><cell>50.9</cell><cell>54.9</cell><cell>66.1</cell><cell>48.5</cell><cell>46.3</cell><cell>59.7</cell><cell>71.5</cell><cell>51.4</cell><cell>48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1</cell><cell>51.9</cell></row><row><cell>Base Transformer</cell><cell>50.3</cell><cell>53.7</cell><cell>49.7</cell><cell>53.0</cell><cell>56.7</cell><cell>62.8</cell><cell>52.2</cell><cell>49.0</cell><cell>61.8</cell><cell>69.0</cell><cell>54.5</cell><cell>51.0</cell><cell>56.1</cell><cell>42.1</cell><cell>44.7</cell><cell>53.8</cell></row><row><cell>Ours</cell><cell>45.0</cell><cell>48.8</cell><cell>46.6</cell><cell>49.4</cell><cell>53.2</cell><cell>60.1</cell><cell>47.0</cell><cell>46.7</cell><cell>59.6</cell><cell>67.1</cell><cell>51.2</cell><cell>47.1</cell><cell>53.8</cell><cell>39.4</cell><cell>42.4</cell><cell>50.5</cell></row><row><cell>Ours + Refinement</cell><cell>45.6</cell><cell>49.7</cell><cell>46.0</cell><cell>49.3</cell><cell>52.2</cell><cell>58.8</cell><cell>47.5</cell><cell>46.1</cell><cell>58.2</cell><cell>66.1</cell><cell>50.7</cell><cell>47.5</cell><cell>52.6</cell><cell>39.2</cell><cell>41.6</cell><cell>50.1</cell></row><row><cell>(b) GT keypoints</cell><cell cols="3">Direct Discuss Eating</cell><cell cols="13">Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Martinez et al. [6] ICCV'17</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell>57.3</cell><cell>65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Zhou et al. [24] ICCV'19 (+)</cell><cell>34.4</cell><cell>42.4</cell><cell>36.6</cell><cell>42.1</cell><cell>38.2</cell><cell>39.8</cell><cell>34.7</cell><cell>40.2</cell><cell>45.6</cell><cell>60.8</cell><cell>39.0</cell><cell>42.6</cell><cell>42.0</cell><cell>29.8</cell><cell>31.7</cell><cell>39.9</cell></row><row><cell>Ci et al. [18] ICCV'19 (+)</cell><cell>36.3</cell><cell>38.8</cell><cell>29.7</cell><cell>37.8</cell><cell>34.6</cell><cell>42.5</cell><cell>39.8</cell><cell>32.5</cell><cell>36.2</cell><cell>39.5</cell><cell>34.4</cell><cell>38.4</cell><cell>38.2</cell><cell>31.3</cell><cell>34.2</cell><cell>36.3</cell></row><row><cell>Zhao et al. [7] CVPR'19</cell><cell>37.8</cell><cell>49.4</cell><cell>37.6</cell><cell>40.9</cell><cell>45.1</cell><cell>41.4</cell><cell>40.1</cell><cell>48.3</cell><cell>50.1</cell><cell>42.2</cell><cell>53.5</cell><cell>44.3</cell><cell>40.5</cell><cell>47.3</cell><cell>39.0</cell><cell>43.8</cell></row><row><cell>Liu et al. [8] ECCV'20</cell><cell>36.8</cell><cell>40.3</cell><cell>33.0</cell><cell>36.3</cell><cell>37.5</cell><cell>45.0</cell><cell>39.7</cell><cell>34.9</cell><cell>40.3</cell><cell>47.7</cell><cell>37.4</cell><cell>38.5</cell><cell>38.6</cell><cell>29.6</cell><cell>32.0</cell><cell>37.8</cell></row><row><cell>Xu et al. [19] CVPR'21</cell><cell>35.8</cell><cell>38.1</cell><cell>31.0</cell><cell>35.3</cell><cell>35.8</cell><cell>43.2</cell><cell>37.3</cell><cell>31.7</cell><cell>38.4</cell><cell>45.5</cell><cell>35.4</cell><cell>36.7</cell><cell>36.8</cell><cell>27.9</cell><cell>30.7</cell><cell>35.8</cell></row><row><cell>Ours</cell><cell>31.0</cell><cell>36.6</cell><cell>30.2</cell><cell>33.4</cell><cell>33.5</cell><cell>39.0</cell><cell>37.1</cell><cell>31.3</cell><cell>37.1</cell><cell>40.1</cell><cell>33.8</cell><cell>33.5</cell><cell>35.0</cell><cell>28.7</cell><cell>29.1</cell><cell>34.0</cell></row><row><cell>Ours + Refinement</cell><cell>32.2</cell><cell>37.4</cell><cell>29.7</cell><cell>33.4</cell><cell>33.4</cell><cell>38.1</cell><cell>38.1</cell><cell>32.1</cell><cell>37.1</cell><cell>40.1</cell><cell>33.9</cell><cell>34.1</cell><cell>35.2</cell><cell>28.6</cell><cell>28.8</cell><cell>34.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>RESULTS ON THE HUMAN3.6M [42] DATASET FOR SINGLE-FRAME 3D HPE WITH (A) CPN [46] PREDICTIONS AND (B) GT KEYPOINTS USED AS INPUT. THE EVALUATION IS PERFORMED USING MPJPE IN mm UNDER Protocol #1. METHODS DENOTED BY (+) USE EXTRA DATA FROM MPII [47]. BEST RESULTS ARE SHOWN IN BOLD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>table we observe the impact of the main contributions of this paper. Just by adding the intermediate supervision, we drastically improve the performance. Adding the error prediction and trainingTABLE II QUANTITATIVE RESULTS FOR PCK AND AUC ON THE MPI-INF-3DHP [43] TEST DATASET WITH GT KEYPOINTS USED AS INPUT. BEST RESULTS ARE SHOWN IN BOLD. THE JOINTFORMER IS NOT TRAINED ON THIS DATASET, SHOWING THE GENERALISATION ABILITY OF THE PROPOSED METHOD TO UNSEEN DATA.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training data</cell><cell cols="4">GS (PCK) noGS (PCK) Outdoor (PCK)</cell><cell>All (PCK) All (AUC)</cell></row><row><cell cols="4">Martinez et al. [6] ICCV'17</cell><cell>H3.6M</cell><cell cols="2">49.8</cell><cell>42.5</cell><cell>31.2</cell><cell>42.5</cell><cell>17.0</cell></row><row><cell></cell><cell cols="3">Mehta et al. [43] 3DV'17</cell><cell>H3.6M</cell><cell cols="2">70.8</cell><cell>62.3</cell><cell>58.8</cell><cell>64.7</cell><cell>31.7</cell></row><row><cell cols="4">Yang et al. [21] CVPR'18</cell><cell>H3.6M+MPII</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>32.0</cell></row><row><cell></cell><cell cols="3">Zhou et al. [48] ICCV'17</cell><cell>H3.6M+MPII</cell><cell cols="2">71.1</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell></row><row><cell cols="4">Luo et al. [45] BMVC'18</cell><cell>H3.6M</cell><cell cols="2">71.3</cell><cell>59.4</cell><cell>65.7</cell><cell>65.6</cell><cell>33.2</cell></row><row><cell></cell><cell cols="3">Ci et al. [18] ICCV'19</cell><cell>H3.6M</cell><cell cols="2">74.8</cell><cell>70.8</cell><cell>77.3</cell><cell>74.0</cell><cell>36.7</cell></row><row><cell></cell><cell cols="3">Zhou et al. [24] ICCV'19</cell><cell>H3.6M+MPII</cell><cell cols="2">75.6</cell><cell>71.3</cell><cell>80.3</cell><cell>75.3</cell><cell>38.0</cell></row><row><cell></cell><cell cols="3">Xu et al. [19] CVPR'21</cell><cell>H3.6M</cell><cell cols="2">81.5</cell><cell>81.7</cell><cell>75.2</cell><cell>80.1</cell><cell>45.8</cell></row><row><cell></cell><cell cols="3">Base Transformer</cell><cell>H3.6M</cell><cell cols="2">83.3</cell><cell>81.6</cell><cell>81.3</cell><cell>82.1</cell><cell>49.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>H3.6M</cell><cell cols="2">83.8</cell><cell>83.8</cell><cell>81.4</cell><cell>83.0</cell><cell>51.2</cell></row><row><cell></cell><cell cols="3">Ours + Refinement</cell><cell>H3.6M</cell><cell cols="2">84.2</cell><cell>85.4</cell><cell>81.4</cell><cell>83.7</cell><cell>52.4</cell></row><row><cell>4</cell><cell>T layers 5</cell><cell>6</cell><cell>D h 64 128 256</cell><cell cols="2">Embed P ose Conv1D Linear</cell><cell>Int Sup.</cell><cell>Err Pred.</cell><cell cols="2">Pos Enc. Spatial Frequency</cell><cell>Scores MPJPE P-MPJPE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.2</cell><cell>33.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.5</cell><cell>26.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.0</cell><cell>26.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.5</cell><cell>27.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.2</cell><cell>27.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.6</cell><cell>26.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.8</cell><cell>26.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.4</cell><cell>26.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.3</cell><cell>26.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.1</cell><cell>28.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY WITH QUANTITATIVE RESULTS IN mm ON THE HUMAN3.6M DATASET</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A gait analysis data collection and reduction technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ounpuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tyburski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human movement science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="575" to="587" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10455</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part X, ser. Lecture Notes in Computer Science</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J. Frahm</editor>
		<meeting>Part X, ser. Lecture Notes in Computer Science<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12355</biblScope>
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2020 -16th European Conference</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06972</idno>
		<title level="m">Towards deeper graph neural networks with differentiable group normalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lifting transformer for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14304</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Takano</surname></persName>
		</author>
		<idno>abs/2103.16385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3961" to="3970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>S. A. McIlraith and K. Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6821" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">85</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruggero</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Poselifter: Absolute 3d human pose lifting network from a single noisy 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12029</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph-PCNN: Two stage human pose estimation with graph pose refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="492" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. B. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
