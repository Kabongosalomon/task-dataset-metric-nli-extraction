<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DS-PASS: Detail-Sensitive Panoramic Annular Semantic Segmentation through SwaftNet for Surrounding Sensing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">DS-PASS: Detail-Sensitive Panoramic Annular Semantic Segmentation through SwaftNet for Surrounding Sensing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantically interpreting the traffic scene is crucial for autonomous transportation and robotics systems. However, state-of-the-art semantic segmentation pipelines are dominantly designed to work with pinhole cameras and train with narrow Field-of-View (FoV) images. In this sense, the perception capacity is severely limited to offer higher-level confidence for upstream navigation tasks. In this paper, we propose a network adaptation framework to achieve Panoramic Annular Semantic Segmentation (PASS), which allows to re-use conventional pinhole-view image datasets, enabling modern segmentation networks to comfortably adapt to panoramic images. Specifically, we adapt our proposed SwaftNet to enhance the sensitivity to details by implementing attention-based lateral connections between the detail-critical encoder layers and the context-critical decoder layers. We benchmark the performance of efficient segmenters on panoramic segmentation with our extended PASS dataset, demonstrating that the proposed realtime SwaftNet outperforms state-of-the-art efficient networks. Furthermore, we assess real-world performance when deploying the Detail-Sensitive PASS (DS-PASS) system on a mobile robot and an instrumented vehicle, as well as the benefit of panoramic semantics for visual odometry, showing the robustness and potential to support diverse navigational applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantically interpreting the traffic scene is one of the crucial perception components for system-critical autonomous transportation and robotics applications. Currently, semantic image segmentation, which unifies detection and classification at the pixel level <ref type="bibr" target="#b0">[1]</ref>, has demonstrated accurate parsing capability in standard conditions. However, modern segmentation algorithms pertain to work with narrow Field-of-View (FoV) pinhole cameras, although panoramic camera is gradually becoming more attractive to be integrated on Intelligent Vehicles (IV) systems. One of the essential reasons lies in that most publicly available segmentation datasets <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref> contain merely pinhole images whose FoV is not as wide as panoramas. When taking a pre-trained segmentation network from pinhole imagery to the omnidirectional imagery, the performance drops significantly and often catastrophically due to the divergence of global semantic and structural information across the image domains <ref type="bibr" target="#b4">[4]</ref>. In addition, for panoramic images, semantic segmentation is required to perform with megapixel resolution to cover This work has been partially funded through the project "Research on Vision Sensor Technology Fusing Multidimensional Parameters" (111303-I21805) by Hangzhou SurImage Technology Co., Ltd., as well as supported by Hangzhou HuanJun Technology Co., Ltd. and Hangzhou KrVision Technology Co., Ltd. (krvision.cn). <ref type="bibr" target="#b0">1</ref> Authors are with Institute for Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany. kailun.yang@kit.edu <ref type="bibr" target="#b1">2</ref> Authors are with National Optical Instrumentation Engineering Technology Research Center, Zhejiang University, China <ref type="figure">Fig. 1</ref>. Panoramic annular semantic segmentation. On the left: raw annular image; First row on the right: unfolded panorama; Second row: panoramic segmentation of the baseline method <ref type="bibr" target="#b4">[4]</ref>, the classification heatmap of pedestrian is blurry; Third row: detail-sensitive panoramic segmentation of the proposed method, the heatmap and semantic map are detail-preserved.</p><p>the FoV as wide as 360 ? , and perceive tiny scene elements such as pedestrians in the distance (see <ref type="figure">Fig. 1</ref>). Nevertheless, encoder-decoder networks such as <ref type="bibr" target="#b5">[5]</ref> <ref type="bibr" target="#b6">[6]</ref> largely sacrifice the detail-sensitivity in the down-sampling process with slim upsampling path to maintain efficiency. It is even more serious for large panoramic images, as the resolution difference is more significant between the feature map at the semanticallyrich deep layer and the original panorama.</p><p>To address these gaps, we propose a network adaptation framework, which allows to re-use knowledge learned from conventional pinhole image datasets and avoid blind spots at the panorama borders, such that contemporary semantic segmenters can adapt seamlessly to panoramic images. Specifically, we put forward a real-time semantic segmentation network named SwaftNet by using attention-based lateral connections, further enhancing the capacity to exploit lowlevel detail-sensitive features for high-level context-critical surrounding sensing.</p><p>To verify the effectiveness of our network adaptation strategy and the detail-sensitivity of attention-augmented Swaft-Net, we extend the PASS dataset <ref type="bibr" target="#b4">[4]</ref> <ref type="bibr" target="#b7">[7]</ref> by finely annotating more detail-critical classes such as sidewalks and pedestrians. With this extended panoramic dataset, we benchmark the performance of real-time scene parsing networks on the challenging task: panoramic annular semantic segmentation. The comprehensive set of experiments demonstrates that the network adaptation strategy is consistently positive for all state-of-the-art efficient segmenters. Moreover, the proposed attention-connected SwaftNet reaches the best performance while maintaining a fast inference speed far above the realtime constraint.</p><p>Furthermore, the panoramic segmentation pipeline is deployed on a mobile robot and an instrumented vehicle, fully validating the robustness against real-world unseen domains and the potential to support diverse autonomous transporta-tion applications. The framework is additionally combined with a modern panoramic visual odometry <ref type="bibr" target="#b8">[8]</ref>, throwing hints on how navigation tasks benefit from the surrounding scene parsing results. The extended PASS dataset and panoramic image sequences collected by diverse systems, as well as the codes of our framework, are made publicly available at 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation Networks</head><p>Semantic segmentation has experienced rapid progress since Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b9">[9]</ref>, along with U-Net <ref type="bibr" target="#b10">[10]</ref> and SegNet <ref type="bibr" target="#b11">[11]</ref> which were recognized as the milestone works. Subsequently proposed networks like PSPNet <ref type="bibr" target="#b12">[12]</ref> and ACNet <ref type="bibr" target="#b13">[13]</ref> have reached higher accuracy on diverse semantic segmentation benchmarks. However, these networks rely on heavy backbones and sophisticated structures, which significantly burden the function complexity, leaving them irrelevant with real-time applications. Following SegNet <ref type="bibr" target="#b11">[11]</ref> that has demonstrated near real-time speed ENet <ref type="bibr" target="#b5">[5]</ref> was presented as the first work to implement real-time semantic segmentation with many inspiring practical techniques. Unfortunately, ENet largely sacrifices the accuracy, unable to yield finely-grained segmentation maps.</p><p>To address this problem, ERFNet/ERF-PSPNet [1] <ref type="bibr" target="#b6">[6]</ref>, LinkNet <ref type="bibr" target="#b15">[14]</ref> and SwiftNet <ref type="bibr" target="#b16">[15]</ref> <ref type="bibr" target="#b17">[16]</ref> were proposed to strike a better balance between efficiency and accuracy. Most of these networks achieve dense predictions based on an encoderdecoder architecture starting from gradual down-sampling of the input image, then the contextually-rich feature maps are restored to the original resolution through the corresponding up-sampling process. Lateral skip connections were naturally used to merge the features from the deep layers and the spatially-rich shallow layers to be aware of detailed variance within local regions <ref type="bibr" target="#b10">[10]</ref>[14] <ref type="bibr" target="#b16">[15]</ref>. For panoramic images, semantic segmentation is required to perform with very large resolution to cover the wide FoV up to 360 ? , and perceive tiny scene elements such as pedestrians at distances of over hundreds of meters in order to support safe autonomous transportation applications. Nevertheless, the resolution difference is even more significant between the large panorama and the feature map at the semantically-rich deep layer of a typical segmentation network. In this sense, with the ultimate goal of eliminating blind spots for surrounding sensing, the detail-sensitivity of a segmentation network should be further enhanced to better exploit spatial variance while maintaining the high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Panoramic Segmentation Approaches</head><p>To realize larger Field of View (FoV) of semantic perception, research efforts have been made on fisheye image segmentation <ref type="bibr" target="#b18">[17]</ref>, surround-view segmentation <ref type="bibr" target="#b19">[18]</ref>[19] <ref type="bibr" target="#b21">[20]</ref>, panoramic semantic change detection <ref type="bibr" target="#b22">[21]</ref>, cross-view semantic segmentation <ref type="bibr" target="#b23">[22]</ref>, spherical semantic segmentation <ref type="bibr" target="#b24">[23]</ref> and panoramic lane marking segmentation <ref type="bibr" target="#b25">[24]</ref>. However, most of them need several cameras to form the 360 ? [18] <ref type="bibr" target="#b20">[19]</ref>[22] <ref type="bibr" target="#b25">[24]</ref>. For fisheye images, the significant distortion is clearly not beneficial for upper-level tasks <ref type="bibr" target="#b18">[17]</ref>, which needs to be rectified for accurate perception with respect to spatial configurations of the surroundings. For spherical input, it is required to be converted to an unfolded mesh to provide a holistic labeling of the scene <ref type="bibr" target="#b24">[23]</ref>. More importantly, deploying several cameras incurs large processing latency and a cluster of hard tasks such as calibration, synchronization and sensor fusion, which again are not advantageous for autonomous navigation applications.</p><p>For this reason, our previous work <ref type="bibr" target="#b4">[4]</ref> introduced the task of Panoramic Annular Semantic Segmentation (PASS) to address surrounding sensing in a unified way, which only involves a single camera to capture distortion-controlled annular images, running with a single forward pass of ERF-PSPNet that was structured using a combination of ERFNet <ref type="bibr" target="#b6">[6]</ref>[25] and PSPNet <ref type="bibr" target="#b12">[12]</ref>. Nevertheless, the chosen network architecture is not ideally suitable for panoramic images. In spite of the decent generalizability, the parsing results are too smooth/blurry, leaving many scene details not perceived. In this work, we aim to adapt in an optimal way, making panoramic semantic segmentation not only robust but also detail-sensitive. generally pursue the encoder-decoder fashion <ref type="bibr" target="#b11">[11]</ref>. In this work, for a given semantic segmentation network, it is methodologically re-separated into a feature model and a fusion model, to facilitate the adaptation from pinhole source domain to panoramic target domain. The feature model could be equivalent to the encoder (which encodes semanticallymeaningful features), while the fusion model corresponds to the decoder (which fuses the features), or the re-separation position could vary with the target to realize continuous panoramic segmentation kept in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Adaptation Framework</head><p>The panoramic annular image is firstly unfolded using the interface offered by the toolbox for omnidirectional camera calibration <ref type="bibr" target="#b27">[26]</ref>. After that, the unfolded panorama is partitioned into several segments (4 segments illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>). Each panorama segment will be fed into the feature model of a semantic segmentation network to obtain semantically-meaningful dense feature maps. This adaptation is due to that although a large-scale panoramic segmentation dataset is not available for training, there is a correspondence between the features inferred from each panorama segment and the features learned from pinhole image dataset. In this regard, robust ultra-wide FoV panoramic segmentation is reachable by leveraging conventional narrow-FoV training data that has already demonstrated diversity in terms of geographical location and observational viewpoint <ref type="bibr" target="#b2">[3]</ref>. Finally, the feature maps of different segments will be concatenated and max-pooled (along the unfolding direction) to be fed into the fusion model to yield the final panoramic pixel-wise classification results. This is due to that the fusion model with lean convolutional layers is mainly responsible for the classification when the semantically-meaningful feature map has already been predicted and aggregated. The fusion model has to be no leaner to enable continuous segmentation as it could take a wide context around the borders of panorama segments into consideration.</p><p>In addition, to prevent blind spots around the borders which is critical in surrounding scene perception, each feature model interacts with the neighboring one (see <ref type="figure" target="#fig_0">Fig. 2</ref>) to enable real 360 ? processing. More precisely, we extend ringpadding <ref type="bibr" target="#b28">[27]</ref> that is also termed as circular convolution <ref type="bibr" target="#b29">[28]</ref>, by replacing traditional zero-padding around the panorama border with the specialized padding by copying features from the opposite side of the unfolded panorama, which theoretically eliminates the inherent image border. Our key extension lies in the specialized padding near the border of panorama segments where padding is taken from the neighboring feature map, without creating blind spots in the interchanges of the FoVs (segments), such that surrounding environments can be seamlessly interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SwaftNet: Proposed Network with Attention Connections</head><p>To enhance the detail-sensitivity of semantic segmentation which is especially critical for panoramic images, we spotlight the low-level spatially-rich features exploited at the down-sampling stages by using lateral attentional connections as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The proposed architecture follows the single-scale model of SwiftNet <ref type="bibr" target="#b16">[15]</ref> that transforms the input RGB image into dense pixel-level semantic predictions, throughout a down-sampling encoder and up-sampling decoder, which is based on an U-shape structure like U-Net <ref type="bibr" target="#b10">[10]</ref> and LinkNet <ref type="bibr" target="#b15">[14]</ref>. Specifically, the encoder and the decoder are asymmetric, as the dimensionality of encoder features increases along the down-sampling path, while the dimensionality of the decoder features keeps constant. This is because recognition (encoder) requires more capacity when rough semantic content is known. We therefore opt for a simple decoder composed of minimalistic up-sampling modules. Accordingly, it is necessary to adjust the dimensionality to blend features from both paths, e.g., by using 1?1 convolutions in the lateral connections <ref type="bibr" target="#b16">[15]</ref>.</p><p>On the other hand, we argue that the informative spatiallydetailed features could be further spotlighted. With the purpose of increasing sensitivity to useful components and suppressing less informative features, we replace the 1?1 convolutions with attention operations <ref type="bibr" target="#b30">[29]</ref>, by squeezing spatial information into a channel descriptor, and elementwisely multiplying the feature map taken from the corresponding down-sampling stage and the re-shaped descriptor. The re-weighted feature map has the same dimensionality as the feature map at the corresponding up-sampling stage. The feature maps from both paths will be added, resulting an attention-augmented lateral connection. The key to enhance detail-sensitivity lies in that the conventional 1?1 convolution-based lateral connections are adapted to attention operations as the skip-connections in our architecture. The proposed SwaftNet is further adapted by leveraging our network adaptation framework in a general way. In addition, we insert a channel-wise attention module after the Spatial Pyramid Pooling (SPP) module <ref type="bibr" target="#b16">[15]</ref> that is a simplified version of pyramidal pooling module from PSPNet <ref type="bibr" target="#b12">[12]</ref>. In our architecture, SPP features must pass through the entire decoder (up-sampling path) before getting classified. Hence, it acts as an instrument to enlarge the receptive field. The channel attention (squeeze-excite operation) is light-weight <ref type="bibr" target="#b30">[29]</ref>, which enables adaptive feature refinement, hence the re-calibrated detail-sensitive feature responses have been further highlighted.  <ref type="figure">Fig. 4</ref>. Examples from the extended PASS dataset with annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Setups</head><p>For evaluation purposes, the Mapillary Vistas validation dataset (2000 samples) <ref type="bibr" target="#b2">[3]</ref> and the Panoramic Annular Semantic Segmentation (PASS) dataset (400 images) <ref type="bibr" target="#b4">[4]</ref> are used. The original PASS dataset contains 4 classes, precisely Road, Car, Crosswalk and Curb which are important for large-scale road scene understanding. We further manually annotate the 400 images on two classes: Sidewalk and Person, whose segmentation are more detail sensitivity-desired, yet exceptionally safety-critical. The panoramic images have a 2048?692 (1.4 megapixel) resolution. Example panoramas for testing with pixel-fine annotations are shown in <ref type="figure">Fig. 4</ref>.</p><p>Mapillary Vistas training set (18000 images) is chosen to train our panoramic scene parsers, leveraging its high diversity in focal length, viewpoint, as well as geographical location which are both important for robust segmentation. We take the critical 27 classes as listed in <ref type="table" target="#tab_0">Table I</ref> to adapt to our campus scenarios. We apply a heterogeneous set of data augmentation operations <ref type="bibr" target="#b4">[4]</ref>, separating in geometry, texture, distortion and style transformations, which are of special relevance to the generalizability, when a trained semantic segmentation model is required to work reliably in new, unseen domains. Our architecture is trained under Adam optimization, starting with a learning rate of 1.0?10 ?4 that decreases exponentially until reaching convergence with a batch size of 10 and an input resolution of 1024?512.</p><p>In this way, the proposed SwaftNet achieves a mean Intersection over Union (mIoU) of 59.4% on the Vistas validation set (2000 images) as shown in <ref type="table" target="#tab_0">Table I</ref>, which demonstrates the learning capacity against large dataset. For the classes corresponding to the evaluation classes of PASS dataset, including Road (88.6%), Car (90.3%), Side- Segmentation accuracy in mIoU across Mapillary VISTAS validation and PASS testing datasets. Red points: accuracies of networks without adaptation; Green points: with adaptation, which is consistently effective for state-of-the-art efficient networks. walk (68.8%), Crosswalk (62.3%), Curb (55.5%) and Person (69.9%), accuracies are readily qualified, such that we step further to study on the robustness aspect by assessing the performance in the panoramic imagery domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarking Panoramic Semantic Segmentation</head><p>With the extended PASS dataset, we benchmark the performance of state-of-the-art networks especially those designed for efficient semantic segmentation as real-time setup is crucial for autonomous transportation applications, including SegNet <ref type="bibr" target="#b11">[11]</ref>, ENet <ref type="bibr" target="#b5">[5]</ref>, LinkNet <ref type="bibr" target="#b15">[14]</ref>, EDANet <ref type="bibr" target="#b31">[30]</ref>, BiSeNet <ref type="bibr" target="#b32">[31]</ref>, CGNet <ref type="bibr" target="#b33">[32]</ref>, ERFNet <ref type="bibr" target="#b6">[6]</ref> and PSPNet18 <ref type="bibr" target="#b12">[12]</ref>. We take the proposed training hyper-parameters for them in their respective publications, but all with same data augmentation techniques to facilitate fair comparison. We also contrast with the baseline method ERF-PSPNet that was preliminarily used for the task Panoramic Annular Semantic Segmentation <ref type="bibr" target="#b4">[4]</ref>, where the pyramid pooling module has been similarly employed with channel attention operations to form as a variant named ERF-APSPNet <ref type="bibr" target="#b7">[7]</ref>. The per-class results on PASS dataset are shown in <ref type="table" target="#tab_0">Table II</ref>, where <ref type="figure" target="#fig_3">Fig. 5</ref> provides a visualization of the comparison.</p><p>It turns out the proposed network SwaftNet outperforms the known efficient networks measured in mIoU on Mapillary Vistas validation set and the extended PASS dataset. Compared with ERF-PSPNet and its variant ERF-APSPNet which achieved state-of-the-art results on the PASS dataset <ref type="bibr" target="#b4">[4]</ref>, the proposed model outperforms them by large margins, precisely 6.7% and 4.8% respectively. The proposed usage of attention lateral connections enables to exploit more informative low-level features, such that it leads to mIoU In addition to the new state-of-the-art scores yielded by our SwaftNet, we find that the proposed adaptation strategy is consistently and significantly positive for all the trained networks, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. This is verified by comparing the performance of the networks in two settings: deploying with or without the adaptation framework, where encoder is used as feature model, and fusion model corresponds to decoder. We take the conclusion in <ref type="bibr" target="#b7">[7]</ref> which finds that when a single model is tested with a FoV of 90 ? , it achieves the best performance. Accordingly, for an adapted network in 360 ? imagery, it is deployed with 4 features models (just as <ref type="figure" target="#fig_0">Fig. 2 shows)</ref>, each of which interacts with the neighboring one. In the other setting, each network is tested in an endto-end way by viewing the whole panorama as a single segment without any 360 ? adaptation. As shown in <ref type="table" target="#tab_0">Table II</ref>, for ENet, ERFNet and SwiftNet, they have even attained IoU boosts of over 25.0%, demonstrating the importance of network adaptation when taken to the panoramic imagery. The results also show that the adaptation is not strictly tied with a specific architecture, but can be effective and deployed with different segmentation models. Even more, the proposed network achieves 29.9% of mIoU boost. When the segment number is appropriate, details exploited by the feature model are also more adapted to leverage the detail-aware capability featured by our proposed network, such that the benefit of the network adaptation is more remarkable than being deployed with other architectures.</p><p>Moreover, <ref type="figure" target="#fig_4">Fig. 6</ref> displays the segmentation maps of example panoramas from the PASS dataset, which were parsed by using the baseline method ERF-PSPNet and our proposed SwaftNet. It can be easily seen that our approach enables finer segmentation, accurately preserving the details of many small classes such as the pedestrians and traffic signs, as well as the tiny traffic scene elements like the thin poles, traffic lights and the cars in the distance. In these senses, panoramic annular semantic segmentation has been endowed with detail-sensitivity evidenced by both numerical and qualitative parsing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Real-World Performance with Views to Real Deployment</head><p>Stepping further than validating on the PASS dataset which was originally captured by a wearable personal navigation assistance prototype <ref type="bibr" target="#b4">[4]</ref>, the proposed framework has been deployed on a mobile robot (see <ref type="figure">Fig. 7a</ref> and <ref type="figure" target="#fig_5">Fig. 8d</ref>) and an instrumented vehicle (see <ref type="figure">Fig. 9</ref>). With a view to real deployment in autonomous transportation applications like these two systems, real-time setup and real-world robustness are two of the most important factors that should be taken into consideration.</p><p>To measure the inference speed, we test the adapted models running through the 400 panoramas, where each feature model is fed with a 1024?512 segment, as shown in <ref type="table" target="#tab_0">Table III</ref>. We also include the mIoU results when training on Cityscapes dataset <ref type="bibr" target="#b1">[2]</ref> and evaluating on its validation set using 19 classes. We use the implementation of <ref type="bibr" target="#b17">[16]</ref> to facilitate the comparison on Cityscapes. The proposed network reaches 88.9FPS on a Titan RTX GPU processor, which is slightly slower than the basic SwiftNet (96.5FPS), but the proposed usage of attention skip-connection helps to attain huge accuracy boosts on diverse datasets, especially on VISTAS which contains more small and detail-critical classes than Cityscapes. Compared with the state-of-the-art method ERF-PSPNet <ref type="bibr" target="#b4">[4]</ref> (40.2FPS) for the PASS task and its variant ERF-APSPNet (38.0FPS), SwaftNet is significantly faster. Overall, our proposed SwaftNet holds an excellent tradeoff between accuracy and efficiency, while for panoramic annular semantic segmentation, it is not only more efficient but also more precise than the baseline work. We use a NVIDIA Jetson Nano that is very portable to be deployed on the mobile robot, where a single forward pass of our network achieves near real-time computation (22.15FPS at the resolution of 640?480). <ref type="figure" target="#fig_5">Fig. 8</ref> shows representative segmentation maps for the captured panoramas when the mobile robot is navigating through the campus. Since the panoramic annular lens system is installed at a lower position (see <ref type="figure">Fig. 7a</ref>), the perspective is significantly different to that of the training samples, as a large part of images in Mapillary Vistas are captured from vehicles. In spite of the significantly lower viewpoint, panoramic segmentation based on our SwaftNet maintains highly qualified and robust as shown in <ref type="figure" target="#fig_5">Fig. 8d</ref>, meanwhile more details are well preserved than the baseline ERF-PSPNet <ref type="figure" target="#fig_5">(Fig. 8c</ref>).</p><p>For the instrumented vehicle, we use a newly designed panoramic annular lens system, which images a FoV of 360 ? ?70 ? . When installing the camera on top of the instrumented electric vehicle as shown in <ref type="figure">Fig. 9a</ref>, it has a FoV of 40 ? above the horizontal plane, and a surrounding view with 30 ? vertical FoV below, which allows to perceive more information about the roadways than the mobile robot. As the FoV is very matched with the instrumented vehicle, the observed roadways are beneficial for sensing the street structure, which leads to clear and well-defined semantic maps as displayed in <ref type="figure">Fig. 9d</ref>, making our DS-PASS system ideally suitable for IV applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Semantic Panoramic Visual Odometry</head><p>In order to investigate the benefit of the predicted panoramic semantics for upper-level navigational tasks, we leverage a state-of-the-art visual odometry algorithm PALVO <ref type="bibr" target="#b8">[8]</ref> that has achieved excellent robustness against rapid motions and dynamic scene elements. Based on PALVO, we build a semantic panoramic visual odometry system. First, the SwaftNet-predicted 360 ? semantics for the unfolded panoramas <ref type="figure" target="#fig_5">(Fig. 8d</ref>) are projected/folded back to the raw annular image as shown in <ref type="figure" target="#fig_5">Fig. 8e</ref>. Second, the semantic maps are fed to the odometry jointly with the raw panoramas <ref type="figure" target="#fig_5">(Fig. 8a)</ref>.</p><p>More specifically, the semantic information are used to determine the key points. Only the matched points with consistent semantic labels across frames are leveraged to compute the 6-DoF pose, otherwise they will be regarded as outliers which will not be involved for pose estimation. As shown in <ref type="figure">Fig. 7b</ref>, finally the trajectory (more than 50m) produced by the semantic PALVO system is smoother than that of the original PALVO. As the mobile robot moves at a steady speed, the velocity would not change drastically. This is supported by the velocity curves visualized in <ref type="figure" target="#fig_6">Fig. 10</ref>, as the standard deviations of velocity estimated by semantic  Velocity curves produced by the panoramic visual odometry with/without using semantics predicted by our DS-PASS system. Note that the odometry based on PALVO <ref type="bibr" target="#b8">[8]</ref> is scale-invariant.</p><p>PALVO are smaller in both directions.</p><p>Overall, the higher-level robustness of the semantic panoramic visual odometry system is attributed to two factors: the large FoV to robustify against swift motion and dynamic real-world scenarios; and the panoramic semantics to reduce the drift and smooth the trajectory. Moreover, we find that the semantic PALVO system working with the baseline network frequently leads to failure, which is due to the less accurate and detail-lost semantic maps, such that the inlier points are often too sparse to accurately estimate the pose and successfully track across frames. In this sense, the superiority of the proposed DS-PASS system with SwaftNet is further validated by deploying with the state-of-the-art PALVO algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a Detail-Sensitive Panoramic Annular Semantic Segmentation (DS-PASS) framework to perceive the surroundings. DS-PASS is presented with a general network adaptation method that enables state-of-theart semantic segmentation network trained on perspective imaging datasets to work comfortably in panoramic imagery domain. Specifically, we put forward a real-time semantic segmentation network named SwaftNet, by blending semantically-meaningful deep layers with spatially-detailed shallow layers using lateral attentional skip-connections, to perform segmentation both swiftly and accurately.</p><p>The network adaptation method is demonstrated to be consistently effective, allowing state-of-the-art networks to re-use the knowledge learned from pinhole images, and parse annular images in a suitable and comfortable way. This unlocks the usage of panoramic cameras for countless semantic sensing-desired systems, bypassing the laborious and time-consuming training label acquisition. Our proposed SwaftNet yields the new state-of-the-art scores on the extended challenging PASS dataset with more annotated classes to reflect detail-sensitivity. Furthermore, the proposed DS-PASS framework has been deployed on a mobile robot and an instrumented vehicle, demonstrating the real-time speed and the real-world robustness.</p><p>With the mobile robot, we show that when the vertical perspective is not ideal, the segmentation maintains highly qualified, and the predicted panoramic semantics are beneficial for upstream visual odometry algorithms like the state-of-the-art PALVO. With the instrumented fully electric vehicle, we show that the proposed DS-PASS system is ideally tailored for Intelligent Vehicles (IV) applications, which only needs a single camera to address surrounding sensing in a single pass, leaving rich opportunities to be fused with LiDAR point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed framework for panoramic annular semantic segmentation. Each feature model (corresponding to the single feature model like encoder in conventional architectures) is responsible for predicting the semantically-meaningful high-level feature map of a panorama segment while interacting with the neighboring ones through cross-segment padding (indicated by the dotted arrows). Fusion model incorporates the feature maps and completes the panoramic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>depicts the diagram of the proposed network adaptation framework. Efficient semantic segmentation networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed architecture with attention-based lateral connections to blend semantically-rich deep layers with spatially-detailed shallow layers. The down-sampling path with the SPP module (encoder) corresponds to the feature model inFig. 2, while the up-sampling path (decoder) corresponds to the fusion model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Segmentation accuracy in mIoU across Mapillary VISTAS validation and PASS testing datasets. Red points: accuracies of networks without adaptation; Green points: with adaptation, which is consistently effective for state-of-the-art efficient networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative examples of panoramic annular semantic segmentation: (a) Raw annular images from the PASS dataset, (b) Unfolded panoramas, (c) Segmentation maps yielded by ERF-PSPNet (coarse, detail lost) and (d) SwaftNet (fine, detail preserved). (a) Mobile robot (b) Reconstructed trajectories Fig. 7. (a) The mobile robot installed with Panoramic Annular Lens and NVIDIA Jetson Nano; (b) Reconstructed trajectories by a state-the-of-art visual odometry PALVO with/without predicted panoramic semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 .</head><label>8</label><figDesc>(a) Raw annular images captured by Panoramic Annular Lens installed on the mobile robot, (b) Unfolded panoramas, (c) Segmentation maps yielded by ERF-PSPNet and (d) SwaftNet, (e) Folded-back PASS maps. Raw annular images and folded-back semantic maps are fed to PALVO [8]. (a) Instrumented vehicle (b) Raw (c) Unfolded panorama (d) DS-PASS Fig. 9. (a) The instrumented electric vehicle installed with Panoramic Annular Lens and LiDAR sensors, (b) Raw annular images, (c) Unfolded panoramas, (d) Panoramic segmentation maps produced by our DS-PASS system with suitable FoVs for intelligent vehicles applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Velocity curves produced by the panoramic visual odometry with/without using semantics predicted by our DS-PASS system. Note that the odometry based on PALVO [8] is scale-invariant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SEGMENTATION</head><label>I</label><figDesc>ACCURACY OF THE PROPOSED SWAFTNET ON MAPILLARY VISTAS DATASET.</figDesc><table><row><cell>Pole</cell><cell>Street Light</cell><cell>Billboard</cell><cell>Traffic Light</cell><cell>Car</cell><cell>Truck</cell><cell>Bicycle</cell><cell>Motorcycle</cell><cell>Bus</cell><cell>Sign Front</cell><cell>Sign Back</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Curb Cut</cell></row><row><cell>47.5%</cell><cell>35.8%</cell><cell>43.4%</cell><cell>62.8%</cell><cell>90.3%</cell><cell>70.4%</cell><cell>55.9%</cell><cell>59.1%</cell><cell>75.1%</cell><cell>69.5%</cell><cell>38.7%</cell><cell>88.6%</cell><cell>68.8%</cell><cell>14.7%</cell></row><row><cell>Plain</cell><cell>Bike Lane</cell><cell>Curb</cell><cell>Fence</cell><cell>Wall</cell><cell>Building</cell><cell>Person</cell><cell>Rider</cell><cell>Sky</cell><cell>Vegetation</cell><cell>Terrain</cell><cell>Marking</cell><cell>Crosswalk</cell><cell>mIoU</cell></row><row><cell>17.4%</cell><cell>37.3%</cell><cell>55.5%</cell><cell>55.0%</cell><cell>46.7%</cell><cell>86.6%</cell><cell>69.9%</cell><cell>47.3%</cell><cell>98.2%</cell><cell>89.7%</cell><cell>63.7%</cell><cell>53.5%</cell><cell>62.3%</cell><cell>59.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF STATE-OF-THE-ART EFFICIENT NETWORKS ON MAPILLARY VISTAS VALIDATION DATASET AND PASS TESTING DATASET.</figDesc><table><row><cell>Network</cell><cell>On VISTAS mIoU</cell><cell>Car</cell><cell>Road</cell><cell>Sidewalk</cell><cell cols="2">On PASS Crosswalk Curb</cell><cell>Person</cell><cell>mIoU</cell><cell>mIoU boost</cell></row><row><cell>SegNet [11]</cell><cell>48.4%</cell><cell>82.3%</cell><cell>72.0%</cell><cell>31.0%</cell><cell>32.7%</cell><cell>18.1%</cell><cell>48.0%</cell><cell>47.4%</cell><cell>18.0%</cell></row><row><cell>ENet [5]</cell><cell>47.0%</cell><cell>89.2%</cell><cell>76.2%</cell><cell>43.4%</cell><cell>53.5%</cell><cell>29.0%</cell><cell>60.2%</cell><cell>58.6%</cell><cell>27.8%</cell></row><row><cell>LinkNet [14]</cell><cell>47.7%</cell><cell>87.4%</cell><cell>77.4%</cell><cell>42.9%</cell><cell>47.4%</cell><cell>22.1%</cell><cell>47.5%</cell><cell>54.1%</cell><cell>23.9%</cell></row><row><cell>EDANet [30]</cell><cell>49.6%</cell><cell>85.0%</cell><cell>78.3%</cell><cell>40.6%</cell><cell>51.5%</cell><cell>21.7%</cell><cell>49.0%</cell><cell>54.4%</cell><cell>24.1%</cell></row><row><cell>BiSeNet [31]</cell><cell>49.5%</cell><cell>83.1%</cell><cell>65.0%</cell><cell>41.9%</cell><cell>18.1%</cell><cell>13.5%</cell><cell>35.0%</cell><cell>42.8%</cell><cell>15.3%</cell></row><row><cell>CGNet [32]</cell><cell>52.8%</cell><cell>72.8%</cell><cell>73.6%</cell><cell>29.5%</cell><cell>37.9%</cell><cell>15.1%</cell><cell>18.9%</cell><cell>41.3%</cell><cell>11.2%</cell></row><row><cell>ERFNet [6]</cell><cell>52.7%</cell><cell>91.0%</cell><cell>76.1%</cell><cell>49.0%</cell><cell>56.4%</cell><cell>27.9%</cell><cell>59.6%</cell><cell>60.0%</cell><cell>25.8%</cell></row><row><cell>PSPNet18 [12]</cell><cell>50.4%</cell><cell>89.0%</cell><cell>77.1%</cell><cell>47.2%</cell><cell>46.9%</cell><cell>26.9%</cell><cell>42.9%</cell><cell>55.0%</cell><cell>20.4%</cell></row><row><cell>ERF-PSPNet [1][4]</cell><cell>52.1%</cell><cell>91.4%</cell><cell>77.6%</cell><cell>43.5%</cell><cell>52.9%</cell><cell>33.4%</cell><cell>67.2%</cell><cell>61.0%</cell><cell>22.0%</cell></row><row><cell>ERF-APSPNet</cell><cell>54.8%</cell><cell>91.0%</cell><cell>78.6%</cell><cell>46.8%</cell><cell>57.2%</cell><cell>30.8%</cell><cell>72.8%</cell><cell>62.9%</cell><cell>27.6%</cell></row><row><cell>SwiftNet [15]</cell><cell>52.4%</cell><cell>91.3%</cell><cell>79.7%</cell><cell>43.6%</cell><cell>55.9%</cell><cell>34.6%</cell><cell>72.2%</cell><cell>62.9%</cell><cell>25.7%</cell></row><row><cell>SwaftNet</cell><cell>59.4%</cell><cell>93.6%</cell><cell>77.6%</cell><cell>53.7%</cell><cell>62.1%</cell><cell>38.3%</cell><cell>80.7%</cell><cell>67.7%</cell><cell>29.9%</cell></row><row><cell cols="5">increases of 7.0% and 4.8% over the basic SwiftNet on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">respective dataset. Our network slightly falls behind on Road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">which is usually close to the dark textures near the panorama</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">outer borders, so it becomes more cautious to classify those</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">areas as safe roadways. More importantly, on other safety-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">critical yet detail-sensitive classes, remarkable improvements</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">have been observed: Sidewalk (10.1%), Crosswalk (6.2%),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Curb (3.7%) and Person (8.5%).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ACCURACY</head><label>III</label><figDesc>(IN MIOU) AND SPEED (IN FPS) ANALYSIS.</figDesc><table><row><cell>Network</cell><cell>Cityscapes</cell><cell>VISTAS</cell><cell>PASS (FPS)</cell></row><row><cell>ERFNet [6]</cell><cell>65.8%</cell><cell>52.7%</cell><cell>60.0% (34.7)</cell></row><row><cell>ERF-PSPNet [1][4]</cell><cell>64.1%</cell><cell>52.1%</cell><cell>61.0% (40.2)</cell></row><row><cell>SwiftNet [15][16]</cell><cell>69.8%</cell><cell>52.4%</cell><cell>62.9% (96.5)</cell></row><row><cell>Our SwaftNet</cell><cell>72.1%</cell><cell>59.4%</cell><cell>67.7% (88.9)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Datasets and codes of DS-PASS: https://github.com/ elnino9ykl/DS-PASS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5000" to="5009" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can we pass beyond the field of view? panoramic annular semantic segmentation for real-world surrounding perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bridging the day and night domain gap for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1312" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pass: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Palvo: visual odometry based on panoramic annular lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="24" to="481" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1440" to="1444" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">In defense of pretrained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">616</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A comparative study of high-recall real-time semantic segmentation based on swift factorized network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11394</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cnn based semantic segmentation for urban traffic scenes using fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Restricted deformable convolution-based road scene semantic segmentation using surround view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fisheyemodnet: Moving object detection on surround-view cameras for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mariotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11789</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-complexity scene understanding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iordache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="245" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Weakly supervised silhouette-based semantic change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11985</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Crossview semantic segmentation for sensing surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03560</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Orientationaware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dfnet: Semantic segmentation on panoramic images with dynamic loss weights and residual fusion block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5887" to="5892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A toolbox for easily calibrating omnidirectional cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5695" to="5701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eliminating the blind spot: Adapting 3d object detection and monocular depth estimation to 360 panoramic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Payen De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="789" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Circular convolutional neural networks for panoramic images and laser data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>P?schmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="653" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient dense modules of asymmetric convolution for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06323</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cgnet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08201</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
