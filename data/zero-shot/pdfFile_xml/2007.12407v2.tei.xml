<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Compositional Learning for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computer Vision and Pattern Recognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<email>xj.peng@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computer Vision and Pattern Recognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computer Vision and Pattern Recognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dachengtao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Compositional Learning for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human-Object Interaction, Compositional Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object interaction (HOI) detection aims to localize and infer relationships between human and objects in an image. It is challenging because an enormous number of possible combinations of objects and verbs types forms a long-tail distribution. We devise a deep Visual Compositional Learning (VCL) framework, which is a simple yet efficient framework to effectively address this problem. VCL first decomposes an HOI representation into object and verb specific features, and then composes new interaction samples in the feature space via stitching the decomposed features. The integration of decomposition and composition enables VCL to share object and verb features among different HOI samples and images, and to generate new interaction samples and new types of HOI, and thus largely alleviates the long-tail distribution problem and benefits low-shot or zero-shot HOI detection. Extensive experiments demonstrate that the proposed VCL can effectively improve the generalization of HOI detection on HICO-DET and V-COCO and outperforms the recent state-of-the-art methods on HICO-DET. Code is available at https://github.com/zhihou7/VCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human-Object interaction (HOI) detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> aims to localize and infer relationships (verb-object pairs) between human and objects in images. The main challenges of HOI come from the complexity of interaction and the longtail distribution of possible verb-object combinations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. In practice, a few types of interactions dominate the HOI samples, while a large number of interactions are rare which are always difficult to obtain sufficient training samples.</p><p>The visual scenes are composed of basic elements, such as objects, parts, and other semantic regions. It is well-acknowledged that humans perceive world in  a compositional way in which visual scenes are treated as a layout of distinct semantic objects <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>. We can understand HOIs by decomposing them into objects and human interaction (verb) types. This decomposition helps to solve the rare Human-Object Interactions with long-tailed distribution. For example, in HICO-DET dataset <ref type="bibr" target="#b6">[7]</ref>, hug, suitcase is a rare case with only one example, while we have more than 1000 HOI samples including object "suitcase", and 500 samples including verb "hug". Obviously, object representations can be shared among different HOIs. And samples with the same verb usually exhibit similar human pose or action characteristics <ref type="bibr" target="#b35">[36]</ref>. By compositing the concepts of "suitcase" and "hug" learned from these large number samples, one can handle the rare case hug, suitcase . This inspires to reduce the complexity of HOI detection and handle unseen/rare categories via learning compositional components, i.e. human verb and objects from visual scenes. Note this idea is near but different from disentangling representation learning <ref type="bibr" target="#b4">[5]</ref> (e.g. factors method <ref type="bibr" target="#b28">[29]</ref> in HOI detection) which aims to separate the distinct and informative factors from input examples. Similar to disentangling, compositional learning of HOI also includes the decomposing step. Unlike disentangling, compositional learning further composes novel HOI examples with decomposed factors, which is helpful to address low-shot and zero-shot HOI detection.</p><p>Inspired by the above analysis, this paper proposes a deep Visual Compositional Learning (VCL) frame work for Human-Object Interaction Detection, which performs compositional learning on visual verb and object representations. VCL simultaneously encourages shared verb and object representation across images and HOI types. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, with the semantic features of 'horse' and 'ride' in the images of f eed, horse and ride, bicycle , one may compose a new interaction feature ride, horse using off-the-shelf features.</p><p>To perform compositional learning for HOI detection, VCL faces three challenges. Firstly, verb features are usually highly correlated with human and object features. It is non-trivial to decouple the verb representations from those of hu-man and objects in scenes. Unlike prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, which extract the verb representations from human boxes, we build verb representation from the union box of human and object. Meanwhile, we share weights between verb and human stream in the mutli-branches structure to emphasize the verb representation on human box region. Our verb representation yields more discriminative cues for the final detection task. Secondly, the number of verbs and objects within a single image is limited for composition. We present a novel feature compositional learning approach by composing HOI samples with verb and object features from different images and different HOI types. In this way, our VCL encourages the model to learn the shared and distinctive verb and object representations that are insensitive to variations (i.e. the specific images and interactions). Thirdly, HOIs always exhibit long-tail distribution where a large number of categories have very few even zero training samples. VCL can compose new interactions and novel types of HOIs in the feature space (i.e. verb + object), e.g., the rare HOI wash, cat can be drawn from wash, dog and f eed, cat .</p><p>Overall, our main contributions can be summarized as follows,</p><p>-We creatively devise a deep Visual Compositional Learning framework to compose novel HOI samples from decomposed verbs and objects to relieve low-shot and zero-shot issues in HOI detection. Specifically, we propose to extract verb representations from union box of human and object and compose new HOI samples and new types of HOIs from pairwise images. -Our VCL outperforms considerably previous state-of-the-art methods on the largest HOI Interaction detection dataset HICO-DET <ref type="bibr" target="#b6">[7]</ref>, particularly for rare categories and zero-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Human-Object Interaction Detection</head><p>Human-Object Interaction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> is essential for deeper scene understanding. Different from Visual Relationship Detection <ref type="bibr" target="#b23">[24]</ref>, HOI is a kind of humancentric relation detection. Several large-scale datasets (V-COCO <ref type="bibr" target="#b10">[11]</ref> and HICO-DET <ref type="bibr" target="#b6">[7]</ref>) were released for the exploration of HOI detection. Chao et al. <ref type="bibr" target="#b6">[7]</ref> introduced a multi-stream model combining visual features and spatial location features to help tackle this problem. Following the multi-stream structure in <ref type="bibr" target="#b6">[7]</ref>, Gao et al. <ref type="bibr" target="#b8">[9]</ref> further exploited an instance centric attention module and Li et al. <ref type="bibr" target="#b19">[20]</ref> utilized interactiveness to explicitly discriminate non-interactive pairs to improve HOI detection. Recently, Wang et al. <ref type="bibr" target="#b31">[32]</ref> proposed a contextual attention framework for Human-Object Interaction detection. GPNN <ref type="bibr" target="#b26">[27]</ref> and RPNN <ref type="bibr" target="#b37">[38]</ref> were introduced to model the relationships with graph neural network among parts or/and objects. Pose-aware Multi-level Feature Network <ref type="bibr" target="#b30">[31]</ref> aimed to generate robust predictions on fine-grained human object interaction. Different from the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> who mainly focus on learning better HOI features, we address the long-tailed and zero-shot issues in HOI detection via Visual Compositional Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low-shot and Zero-shot Learning</head><p>Our work also ties with low-shot learning <ref type="bibr" target="#b32">[33]</ref> within long-tailed distribution <ref type="bibr" target="#b21">[22]</ref> and zero-shot learning recognition <ref type="bibr" target="#b34">[35]</ref>. Shen et al. <ref type="bibr" target="#b28">[29]</ref> introduced a factorized model for HOI detection that disentangles reasoning on verbs and objects to tackle the challenge of scaling HOI recognition to the long tail of categories, which is similar to our work. But we design a compositional learning approach to compose HOI examples. Visual-language joint embedding models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> enforce the representation of the same verb to be similar among different HOIs by the intrinsic semantic regularities. <ref type="bibr" target="#b25">[26]</ref> further transfered HOIs embeddings from seen HOIs to unseen HOIs using analogies for zero-shot HOI detection. However, different from <ref type="bibr" target="#b25">[26]</ref> who aims to zero-shot learning, VCL targets at Generalized Zero-Shot Learning <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a generic object detector was incorporated to generalize to interactions involving previously unseen objects. Also, Yang et al. <ref type="bibr" target="#b36">[37]</ref> proposed to alleviate the predicate bias to objects for zero-shot visual relationship detection. Similar to previous approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, we also equally treat the same verb from different HOIs. However, all those works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref> largely ignore the composition of verbs and objects. In contrast, we propose the Visual Compositional Learning framework to relieve the long-tail and zero-shot issues of HOI detection jointly, and we demonstrate the efficiency by massive experiments, especially for rare/unseen data in HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Disentangling and Composing</head><p>Disentangled representation learning has attracted increasing attention in various kinds of visual task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> and the importance of Compositional Learning to build intelligent machines is acknowledged <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. Higgins et al. <ref type="bibr" target="#b14">[15]</ref> proposed Symbol-Concept Association Network (SCAN) to learn hierarchical visual concepts. Recently, <ref type="bibr" target="#b5">[6]</ref> proposed Multi-Object network (MONet) to decompose scenes by training a Variational Autoencoder together with a recurrent attention network. However, both SCAN <ref type="bibr" target="#b14">[15]</ref> and MONet <ref type="bibr" target="#b5">[6]</ref> only validate their methods on the virtual datasets or simple scenes. Besides, Compositional GAN <ref type="bibr" target="#b1">[2]</ref> was introduced to generate new images from a pair of objects. Recently, Label-Set Operations network (LaSO) <ref type="bibr" target="#b0">[1]</ref> combined features of image pairs to synthesize feature vectors of new label sets according to certain set operations on the label sets of image pairs for multi-label fewshot learning. Both Compositional GAN <ref type="bibr" target="#b1">[2]</ref> and LaSO <ref type="bibr" target="#b0">[1]</ref>, however, compose the features from two whole images and depend on generative network or reconstruct loss. In addition, Kato et al. <ref type="bibr" target="#b17">[18]</ref> introduced a compositional learning method for HOI classification <ref type="bibr" target="#b7">[8]</ref> that utilizes the visual-language joint embedding model to the feature of the whole of image. But <ref type="bibr" target="#b17">[18]</ref> did not involve multiple objects detection in the scene. Our visual compositional learning framework differs from them in following aspects: i) it composes interaction features from regions of images, ii) it simultaneously encourages discriminative and shared verb and object representations. Given two images, we first detect human and objects with Faster-RCNN <ref type="bibr" target="#b27">[28]</ref>. Next, with ROI-Pooling and Residual CNN blocks, we extract human features, verb features (i.e. the union box of human and object), and object features. Then, these features are fed into the following branches: individual spatial-human branch, verb-object branch and composited branch. Finally, HOI representations from verb-object branch and composited branch are classified by a shared FC-Classifier, while HOI representations from spatial-human branch are classified by an individual FC-Classifier. Note that all the parameters are shared across images and the newly composited HOI instances can be from a single image if the image includes multiple HOIs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Compositional Learning</head><p>In this section, we present our Visual Compositional Learning (VCL) framework for HOI detection. We first provide an overview of VCL and then detail the HOI branches of VCL. Last, we describe how we compose new HOIs and apply VCL to zero-shot detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>To address the long-tail and zero-shot issues of HOI detection, we propose the Visual Compositional Learning (VCL) framework to learn shared object and verb features and compose novel HOI samples with these features. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, to perform compositional learning, our VCL takes as input a randomly selected image pair. Then we employ a Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> with backbone ResNet-50 <ref type="bibr" target="#b12">[13]</ref> to detect human and objects in images. Subsequently, we use ROI-Pooling and Residual CNN blocks to obtain features of human, verbs, and objects individually. Then, to obtain HOI detection, these features together with a human-object spatial map are fed into a spatial-human branch and a verb-object branch. Particularly, composited features are fed into the composited branch for compositional learning. It is worth noting that all the parameters are shared across images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-branch Network</head><p>Multi-branch architectures are usually used in previous HOI detection works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> where each branch processes a kind of input information. Similarly, VCL includes a spatial-human branch and a verb-object branch. But unlike previous works, VCL has a composition branch which helps the training of HOI detection.</p><p>Spatial-Human branch. The spatial-human branch processes human feature and the spatial map of human and object. Following <ref type="bibr" target="#b8">[9]</ref>, the input of spatial feature is a two-channel 64x64 tensor consisting of a person map and an object map. For the person map, the value of a position will be 1 if it is in in the person box otherwise 0. The object map is similar. We concatenate the spatial map with the human feature.</p><p>Verb-Object branch. The verb-object branch in each image includes a verb stream and an object stream. Unlike prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> which view the human features as verb representations, our newly introduced verb branch extracts a verb representation from the union box of a human box and an object box. Meanwhile, similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, we share the weights between human stream in Spatial-Human branch and verb stream from the union box in Verb-Object branch. Our verb representation is more discriminative which contains more useful contextual information within the union box of human and object.</p><p>Composition branch. We compose new verb-object interaction samples in the feature space from the verbs and objects between and within images and then these synthesized samples are trained jointly with the interactions annotated in the dataset. It in turn improves the generalization of HOI detection, particularly for rare and unseen HOIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Composing Interactions</head><p>The key idea of our proposed VCL framework is to compose new interaction samples within and between images. This composition process encourages the network to learn shared object and verb features across HOIs by composing massive diverse HOI samples. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, the composition process mainly contains two stages: generating all possible interactions (i.e. verb-object pairs) and removing infeasible interactions. Given two images I 1 and I 2 , we compose new interaction samples within single image and between images by first considering all possible verb-object pairs and then removing infeasible interactions in the HOI label space.</p><p>Existing HOI labels mainly contain one object and at least one verb, which set the HOI detection as a multi-label problem. To avoid frequently checking verb-object pairs, we design an efficient composing and removing strategy. First, we decouple the HOI label space into a verb-HOI matrix A v ? R Nv?C and an object-HOI matrix A o ? R No?C , where N v , N o , and C denote the number of verbs, objects and HOI categories respectively. A v (A o ) can be viewed as the cooccurence matrix between verbs (objects) and HOIs. Then, given binary HOI label vectors y ? R N ?C , where N , C denote the number of interactions and HOI categories respectively. we can obtain the object label vector and verb label vector as follows,</p><formula xml:id="formula_0">l o = yA T o , l v = yA T v ,<label>(1)</label></formula><p>where l o ? R N ?No is usually one-hot vectors meaning one object of a HOI example, and l v ? R N ?Nv is possiblely multi-hot vectors meaning multiple verbs. e.g. {hold, sip}, cup ). Similarly, we can generate new interactions from arbitrary l o and l v as follows,?</p><formula xml:id="formula_1">= (l o A o )&amp;(l v A v ),<label>(2)</label></formula><p>where &amp; denotes the "and" logical operation. The infeasible HOI labels that do not exist in the given label space are all-zero vectors after the logical operation. And then, we can filter out those inefasible HOIs. In implementation, we obtain verbs and objects from two images by ROI pooling and treat them within and between images as same. Therefore, we do not treat two levels of composition differently during composing HOIs.</p><p>Zero-Shot Detection. The composition process makes VCL handling zeroshot detection naturally. Specifically, with the above-mentioned operation, we can generate HOI samples for zero-shot (in the given HOI label space) between and within images which may not be annotated in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>Training. We train the proposed VCL in an end-to-end manner with Cross Entropy (CE) losses from multiple branches: L sp from the spatial-human branch for original HOI instances, L verb obj from verb-object branch for original HOI instances, and L comp from verb-object branch for composited HOI instances. Formally, the total training loss is defined as follows,</p><formula xml:id="formula_2">L = L sp + ? 1 L verb obj + ? 2 L comp ,<label>(3)</label></formula><p>where ? 1 and ? 2 are two hyper-parameters. We employ the composition process (i.e. composing new HOI instances) in each minibatch at training stage.</p><p>Inference. At test stage, we remove the composition operation and use the spatial-human branch and the verb-object branch to recognize interaction (i.e. human-object pair) of an input image. We predict HOI scores in a similar manner to <ref type="bibr" target="#b8">[9]</ref>. For each human-object bounding box pair (b h , b o ), we predict the score </p><formula xml:id="formula_3">S c h,o = s h ? s o ? s c verb obj ? s c sp<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we first introduce datasets and metrics and then provide the details of the implementation of our method. Next, we report our experimental results compared with state-of-the-art approaches and zero-shot results. Finally, we conduct ablation studies to validate the components in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Datasets. we adopt two HOI datasets HICO-DET <ref type="bibr" target="#b6">[7]</ref> and V-COCO <ref type="bibr" target="#b10">[11]</ref>. HICO-DET <ref type="bibr" target="#b6">[7]</ref> contains 47,776 images <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO <ref type="bibr" target="#b10">[11]</ref> provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects. Metrics. We follow the settings in <ref type="bibr" target="#b8">[9]</ref>, i.e. a prediction is a true positive only when the human and object bounding boxes both have IoUs larger than 0.5 with reference to ground truth and the HOI classification result is accurate. We use the role mean average precision to measure the performance on V-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For HICO-DET dataset, we utilize the provided interaction labels to decompose and compose interactions labels. For V-COCO dataset, which only provides verb (i.e. action) annotations for COCO images <ref type="bibr" target="#b20">[21]</ref>, we obtain object labels (only 69 classes in V-COCO images) from the COCO annotations. Therefore, we have 69 classes of objects and 29 classes of actions in V-COCO that construct 238 classes of Human-Object pairs to facilitate the detection of 29 classes of actions with VCL. In addition, following the released code of <ref type="bibr" target="#b19">[20]</ref>, we also apply the same reweighting strategy in HICO-DET and V-COCO, and we keep it for the composited interactions. Besides, to prevent composited interactions from dominating the training of the model, we randomly select composited interactions in each minibatch to maintain the same number of composited interactions as we do of non-composited interactions.</p><p>For a fair comparison, we adopt the object detection results and pre-trained weights provided by authors of <ref type="bibr" target="#b8">[9]</ref>. We apply two 1024-d fully-connected layers to classify the interaction feature concatenated by verb and object. We train our network for 1000k iterations on the HICO-DET dataset and 500k iterations on V-COCO dataset with an initial learning rate of 0.01, a weight decay of 0.0005, and a momentum of 0.9. We set ? 1 of 2 and ? 2 of 0.5 for HICO-DET dataset, while for V-COCO dataset, ? 1 is 0.5 and ? 2 is 0.1. In order to compose enough new interactions between images for training, we increase the number of interactions in each minibatch while reducing the number of augmentations for each interaction to keep the batch size unchanged. All experiments are conducted on a single Nvidia GeForce RTX 2080Ti GPU with Tensorflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Comparisons</head><p>We compare our proposed Visual Compositional Learning with those state-ofthe-art HOI detection approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> on HICO-DET. The HOI detection result is evaluated with mean average precision (mAP) (%). We use the settings: Full (600 HOIs), Rare (138 HOIs), Non-Rare (462 HOIs) in Default mode and Known Object mode on HICO-DET.</p><p>Comparisons with state-of-the-art. From <ref type="table">Table 1</ref>, we can find that we respectively improve the performance of Default and Know modes with Full setting to 19.43% and 22.00% without external knowledge. In comparison with <ref type="bibr" target="#b30">[31]</ref> who achieves the best mAP in Rare category on HICO-DET among the previous works, we improve the mAP by 1.97% in Full category and by 0.9% in Rare category. Particularly, we do not use pose information like <ref type="bibr" target="#b30">[31]</ref>. We improve dramatically by over 3% in Rare category compared to other visual methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref>. Besides, we achieve 1.92% better result than <ref type="bibr" target="#b25">[26]</ref> in the rare category although we only use the visual information.</p><p>Particularly, <ref type="bibr" target="#b2">[3]</ref> incorporates ResNet101 as their backbone and finetune the object detector on HICO-DET. When we the same backbone and finetune the object detector on HICO-DET, we can largely improve the performance to 23.63%. This also illustrate the great effect of object detector on current two-stage HOI detection method. in the two-stage HOI detection, we should not only focus on HOI recognition such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>, but also improve the object detector performance.</p><p>Visualization. In <ref type="figure" target="#fig_5">Figure 4</ref>, we qualitatively show that our proposed Visual Compositional Learning framework can detect those rare interactions correctly while the baseline model without VCL misclassifies on HICO-DET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalized Zero-shot HOI Detection</head><p>We also evaluate our method in HOI zero-shot detection since our method can naturally be applied to zero-shot detection. Following <ref type="bibr" target="#b28">[29]</ref>, we select 120 HOIs from HOI label set and make sure that the remaining contains all objects and <ref type="table">Table 1</ref>. Comparisons with the state-of-the-art approaches on HICO-DET dataset <ref type="bibr" target="#b6">[7]</ref>. Xu et al. <ref type="bibr" target="#b35">[36]</ref>, Peyre et al. <ref type="bibr" target="#b25">[26]</ref> and Bansal et al. <ref type="bibr" target="#b2">[3]</ref> utilize language knowledge. We include the results of <ref type="bibr" target="#b2">[3]</ref> with the same COCO detector as ours. * means we use the res101 backbone and finetune the object detector on HICO-DET dataset like <ref type="bibr">[</ref> verbs. The annotations on HICO-DET, however, are long-tail distributed. The result of selecting preferentially rare labels is different from that of selecting non-rare labels and we do not know the specific partition of zero shot in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. For a fair comparison, we split the HOI zero-shot experiment into two groups: rare first selection and non-rare first selection. Rare first selection means we pick out as many rare labels as possible for unseen classes according to the number of instances for zero-shot (remain 92,705 training instances), while non-rare first selection is that we select as many non-rare labels as possible for unseen classes (remain 25,729 training instances). The unseen HOI detection result is evaluated with mean average precision (mAP) (%). We report our result in the settings: Unseen (120 HOIs), Seen (480 HOIs), Full (600 HOIs) in Default mode and Known Object mode on HICO-DET. We can find in <ref type="table" target="#tab_1">Table 2</ref>, both selection strategies witness a consistent increase in all categories compared to the corresponding baseline. Noticeably, both kinds of selecting strategies witness a surge of performance by over 4% than baseline in unseen category with VCL. Particularly, our baseline without VCL still achieves low results (3.30% and 5.06%) because we predict 600 classes (including 120 unseen categories) in baseline. The model could learn the verb and object of unseen HOIs individually from seen HOIs. Meanwhile, non-rare first selection has more verbs and objects of unseen HOIs individually from seen HOIs than that of rare first selection. Therefore, non-rare first selection has better baseline than rare first selection. Besides, we improve <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> largely nearly among all categories. In detail, for a fair comparison to <ref type="bibr" target="#b2">[3]</ref>, we also use Resnet-101 and finetune the object detector on HICO-DET dataset, which can largely improve the per- formance. This demonstrates that our VCL effectively improves the detection of unseen interactions and maintains excellent HOI detection performance for seen interactions at the same time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analysis</head><p>To evaluate the design of our VCL, we first conduct ablation studies about VCL and verb representation on HICO-DET dataset and V-COCO dataset. For V-COCO, we evaluate AP role (24 actions with roles) following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. Besides, we evaluate two branches and composing interactions between images and/or within images on HICO-DET. See supplementary material for more analysis. Visual Compositional Learning is our core method. With VCL, we can respectively see an obvious increase by 1.00% in the Full category from <ref type="table">Table 3</ref> (row 6 vs row 7). Particularly, the improvement (2.41%) in Rare category is <ref type="table">Table 3</ref>. Ablation study of the proposed Visual Compositional Learning framework on HICO-DET and V-COCO test set. VCL means Visual Compositional Learning and Union Verb means we learn verb feature from union box of human and object. Sharing W means sharing weights between human stream and verb stream for verb representation. Re-weighting means we use re-weighting strategy in the code of <ref type="bibr" target="#b19">[20]</ref> VCL considerably better than that in Non-Rare category, which means the proposed VCL is more beneficial for rare data. Noticeably, the improvement of VCL with verb representation learning is better than that without verb representation, which implies that the more discriminative the verb representation is, the better improvement VCL obtains in HOI detection. We can see a similar trend of VCL on V-COCO in <ref type="table">Table 3</ref> where the performance decreases by 0.8% without VCL. Noticeably, V-COCO aims to recognize the actions (verbs) rather than the human-object pairs, which means the proposed VCL is also helpful to learn the discriminative action (verbs) representation. Besides, from row 1 and row 2 in <ref type="table">Table 3</ref>, we can find the proposed VCL is pluggable to re-weighting strategy <ref type="bibr" target="#b16">[17]</ref> (See details in supplementary materials).</p><p>Verb Representation, which we learn from the union box of human and object, is considerably helpful for the performance. We evaluate its efficiency by comparing human box and union box of human and object. From row 5 and row 7 in <ref type="table">Table 3</ref>, we can find that with the proposed verb representation we improve the performance by 0.86% on HICO-DET and 0.6% in V-COCO respectively within VCL, which means learning verb representation from the union box of human and object is more discriminative and advantageous for HOI detection. Particularly, we share the weights of resnet block between human and verb stream after ROI pooling in the proposed verb representation method. <ref type="table">Table 3</ref> (row 3 vs row 7) shows sharing weights helps the model improve the performance largely from 18.93% to 19.43%. This may be explained by that the union region contains more noise and the model would emphasize the region of the human in the union box by sharing weights to obtain better verb representation.</p><p>Branches. There are two branches in our method and we evaluate their contributions in <ref type="table" target="#tab_3">Table 4</ref>. Noticeably, we apply VCL to Verb-Object branch during training, while we do not apply VCL to Spatial-Human. By keeping one branch each time on HICO-DET dataset during inference, we can find the verbobject branch makes the larger contribution, particularly for rare category (3%). This efficiently illustrates the advantage of VCL for rare categories. But we can improve the performance dramatically from 16.89% to 19.43% with Spatial-Human Branch. Meanwhile, we can find the proposed VCL is orthogonal to spatial-human branch from the last two rows in <ref type="table" target="#tab_3">Table 4</ref>. Noticeably, by comparing verb-object branch only during inference and verb-object branch only from training, we can find the spatial-human branch can facilitate the optimization of verb-object branch (improving the mAP from 15.77% to 16.89%). See supplementary materials for more analysis of two branches in zero-shot detection where the performance of verb-object branch with VCL is over 3.5% better than spatial-human branch in unseen data. Composing interactions within and/or between images. In <ref type="table" target="#tab_4">Table 5</ref>, we can find composing interaction samples between images is beneficial for HOI detection, whose performance in the Full category increases to 19.06% mAP, while composing interaction samples within images has similar results to baseline. It might be because the number of images including multiple interactions is few on HICO-DET dataset. Remarkably, composing interaction samples within and between images notably improves the performance up to 19.43% mAP in Full and 16.55% mAP in Rare respectively. Those results mean composing interactions within images and between images is more beneficial for HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization of features</head><p>We also illustrate the verb and object features by t-SNE visualization <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates that VCL overall improves the the discrimination of verb and object features. There are many noisy points (see black circle region) in <ref type="figure" target="#fig_6">Figure 5</ref> without VCL and verb presentation. Meanwhile, we can find the proposed verb representation learning is helpful for verb feature learning by comparing verb t-SNE graph between the left and the middle. Besides, the object features are more discriminative than verb. We think it is because the verb feature is more abstract and complex and the verb representation requires further exploration.  <ref type="bibr" target="#b24">[25]</ref>. Left is the visual illustration of baseline, middle includes verb representation and the right uses both VCL and verb representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a simple yet efficient deep Visual Compositional Learning framework, which composes the interactions from the shared verb and object latent representations between images and within images, to relieve the long-tail distribution issue and zero-shot learning in human-object interaction detection. Meanwhile, we extract a more discriminative verb representation from the union box of human and object rather than human box. Lastly, we evaluate the efficiency of our model on two HOI detection benchmarks, particularly for low-shot and zero-shot detection.</p><p>Acknowledge This work is partially supported by Science and Technology Service Network Initiative of Chinese Academy of Sciences (KFJ-STS-QYZX-092), Guangdong Special Support Program (2016TX03X276), National Natural Science Foundation of China (U1813218, U1713208), Shenzhen Basic Research Program (JCYJ20170818164704758, CXB201104220032A), the Joint Lab of CAS-HK, Australian Research Council Projects (FL-170100117). We evaluate the contribution of the two branches in zero-shot HOI detection. From <ref type="table" target="#tab_5">Table 6</ref>, we can find the performance of verb-object branch in Seen category and Full category is similar to that of spatial-human branch, while verb-object branch is 3.52% and 4.90% better than spatial branch in selecting rare first and selecting non-rare first respectively in the Unseen category. Particularly, after we fuse the result of the two branches, the Unseen category witnesses a considerable decrease in the two selecting strategies. This illustrates that the additional spatial-human branch contributes to the full performance while the verb-object branch with VCL efficiently benefits the zero-shot recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The effect of the number of interactions in minibatch</head><p>In order to compose enough interactions for Visual Compositional Learning, we increase the number of interactions in each minibatch while reducing the number of augmentations for each interaction and the number of negative interactions. Therefore, we can still optimize the network in a single GPU. From <ref type="table" target="#tab_6">Table 7</ref>, we can find the baseline model of different iteractions has similar results with 18.43 mAP and 18.47 mAP respectively. However, we witness a better improvement (1.0 mAP vs 0.44 mAP) if we increase the interaction classes in the minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-Parameters and Baseline</head><p>In our proposed framework, there are two hyper-parameters ? 1 and ? 2 . We evaluate the performance when we set different values for the two hyper-parameters in <ref type="table" target="#tab_7">Table 8</ref>.</p><p>Like <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, we first detect the objects in the image and then use the object detection results to infer the HOI categories during test. We use the same score threshold (0.8 for human and 0.3 for object ) same as <ref type="bibr" target="#b19">[20]</ref> in resnet50 coco detector. We use 0.3 for human and 0.1 for object in resnet101 detector that is finetuned on HICO-DET dataset since the finetuned object detection result is largely better. We conduct experiments based on the code of <ref type="bibr" target="#b19">[20]</ref> who released the code in their final version. We find there are two simple but very useful strategies for improvement: reweighting and postprocess for detection. Reweighting is that they allocate different weights for the cross entropy loss according to the number of classes. Postprocess for detection is that we decrease the detection threshold for those images the the detector can't detect any objects and humans. See <ref type="table" target="#tab_8">Table 9</ref> for comparison. See our code for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Unseen labels</head><p>See our released code for the labels of the two selection strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>corresponding author arXiv:2007.12407v2 [cs.CV] 4 Oct 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>An illustration of Visual Compositional Learning (VCL). VCL constructs the new concept of ride, horse from f eed, horse and ride, bicycle via visual compositional learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed Visual Compositional Learning (VCL) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the process of composing new interactions. Given two images I1 and I2, we compose new interaction samples within single image and between them by first considering all possible verb-object pairs and then removing infeasible interactions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>S c h,o for each category c ? 1, ..., C, where C denotes the total number of possible HOI categories. The score S c h,o depends on the confidence for the individual object detection scores (s h and s o ) and the interaction prediction based on verbobject branch s c verb obj and spatial-human branch s c sp . Specifically, Our final HOI score S c h,o for the human-object bounding box pair (b h , b o ) is :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Some rare HOI detections (Top 1 result) detected by the proposed Compositional Learning and the model without Compositional Learning. The first row is the results of baseline model without VCL. The second row is the results of VCL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Visual illustration of object features (80 classes) (up) and verb features (117 classes) (bottom) on HICO-DET dataset (20000 samples) via t-SNE visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Zero Shot Detection results of our proposed Visual Compositional Learning framework. * means we uses the res101 backbone and finetune object detector on HICO-DET. Full means all categories including Seen and Unseen</figDesc><table><row><cell>Method</cell><cell cols="2">Default Unseen Seen</cell><cell cols="4">Known Object Full Unseen Seen Full</cell></row><row><cell>Shen et al. [29]</cell><cell>5.62</cell><cell>-</cell><cell>6.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bansal* et al. [3]</cell><cell>10.93</cell><cell cols="2">12.60 12.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>w/o VCL (rare first)</cell><cell>3.30</cell><cell cols="2">18.63 15.56</cell><cell>5.53</cell><cell cols="2">21.30 18.15</cell></row><row><cell>w/o VCL (non-rare first)</cell><cell>5.06</cell><cell cols="2">12.77 11.23</cell><cell>8.81</cell><cell cols="2">15.37 14.06</cell></row><row><cell>VCL (rare first)</cell><cell cols="6">7.55 18.84 16.58 10.66 21.56 19.38</cell></row><row><cell>VCL (non-rare first)</cell><cell cols="6">9.13 13.67 12.76 12.97 16.31 15.64</cell></row><row><cell>VCL* (rare first)</cell><cell cols="6">10.06 24.28 21.43 12.12 26.71 23.79</cell></row><row><cell>VCL* (non-rare first)</cell><cell cols="6">16.22 18.52 18.06 20.93 21.02 20.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The branches ablation study of the model on HICO-DET test set. Verb-object branch only means we train the model without spatial-human branch.</figDesc><table><row><cell>Method</cell><cell cols="2">Full Rare NonRare</cell></row><row><cell>Two branches</cell><cell>19.43 16.55</cell><cell>20.29</cell></row><row><cell>Verb-Object branch only during inference</cell><cell>16.89 15.35</cell><cell>17.35</cell></row><row><cell cols="2">Spatial-Human branch only during inference 16.13 12.39</cell><cell>17.24</cell></row><row><cell>Verb-Object branch only</cell><cell>15.77 13.35</cell><cell>16.49</cell></row><row><cell>Verb-Object branch only (w/o VCL)</cell><cell>15.33 10.85</cell><cell>16.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Composing Strategies study of VCL on HICO-DET test set</figDesc><table><row><cell>Method</cell><cell cols="3">Full (mAP %) Rare (mAP %) NonRare (mAP %)</cell></row><row><cell>Baseline (w/o VCL)</cell><cell>18.43</cell><cell>14.14</cell><cell>19.71</cell></row><row><cell>Within images</cell><cell>18.48</cell><cell>14.46</cell><cell>19.69</cell></row><row><cell>Between images</cell><cell>19.06</cell><cell>14.33</cell><cell>20.47</cell></row><row><cell>Between and within images</cell><cell>19.43</cell><cell>16.55</cell><cell>20.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Two branches ablation study of the proposed Visual Compositional Learning framework in zero-shot HOI detection on HICO-DET test set during inference.</figDesc><table><row><cell>Method</cell><cell>Unseen Seen Full</cell></row><row><cell>Verb-Object branch (rare first)</cell><cell>7.85 15.48 13.95</cell></row><row><cell>Spatial-Human branch (rare first)</cell><cell>4.33 15.92 13.60</cell></row><row><cell>Two branches (rare first)</cell><cell>7.55 18.84 16.58</cell></row><row><cell>Verb-Object branch (non-rare first)</cell><cell>10.61 10.95 10.88</cell></row><row><cell cols="2">Spatial-Human branch (non-rare first) 5.71 11.82 10.60</cell></row><row><cell>Two branches (non-rare first)</cell><cell>9.13 13.67 12.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The results of the number of interactions in minibatch in HICO-DET.</figDesc><table><row><cell cols="2">the number of interactions VCL Full Rare NonRare</cell></row><row><cell>1</cell><cell>-18.41 14.17 19.68</cell></row><row><cell>1</cell><cell>18.85 14.98 20.01</cell></row><row><cell>5</cell><cell>-18.43 14.14 19.71</cell></row><row><cell>5</cell><cell>19.43 16.55 20.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The results of different values for ?1 when ?2 is 0.5 and ?2 when ?1 is 2.0 in HICO-DET . Full 18.96 18.95 19.43 19.29 19.34 ?2 0.05 0.1 0.5 1.0 1.5 Full 19.18 19.30 19.43 19.10 18.90</figDesc><table><row><cell>?1</cell><cell>1.0 1.5 2.0 2.5</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Comparison of strategies for baseline</figDesc><table><row><cell>Strategy</cell><cell cols="3">Full (mAP %) Rare (mAP %) NonRare (mAP %)</cell></row><row><cell>w/o reweighting</cell><cell>16.87</cell><cell>10.07</cell><cell>18.90</cell></row><row><cell>w/o postprocess</cell><cell>17.14</cell><cell>12.92</cell><cell>18.40</cell></row><row><cell>our baseline</cell><cell>18.03</cell><cell>13.62</cell><cell>19.35</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A The two branches study in zero-shot HOI detection</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laso: Label-set operations networks for multi-label few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alfassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6548" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07560</idno>
		<title level="m">Compositional gan: Learning conditional image composition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Detecting humanobject interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Monet: Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
	<note>ieee winter conference on applications of computer vision (wacv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ican: Instance-centric attention network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reconciling deep learning with symbolic artificial intelligence: representing objects and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05967</idno>
		<title level="m">No-frills human-object interaction detection: Factorization, appearance and layout encodings, and training techniques</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03389</idno>
		<title level="m">Scan: Learning hierarchical compositional visual concepts</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Richards</surname></persName>
		</author>
		<title level="m">Parts of recognition</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="449" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08264</idno>
		<title level="m">Transferable interactiveness prior for human-object interaction detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359</idno>
		<title level="m">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Principles of object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Spelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9469" to="9478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep contextual attention for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07721</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to detect humanobject interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shuffle-then-assemble: Learning object-agnostic visual relationship features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
