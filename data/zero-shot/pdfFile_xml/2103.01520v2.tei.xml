<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Huang</surname></persName>
							<email>zzhuang19@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<email>jpzhang@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Science and Technology for Brain-inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Key</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To minimize the effects of age variation in face recognition, previous work either extracts identity-related discriminative features by minimizing the correlation between identity-and age-related features, called age-invariant face recognition (AIFR), or removes age variation by transforming the faces of different age groups into the same age group, called face age synthesis (FAS); however, the former lacks visual results for model interpretation while the latter suffers from artifacts compromising downstream recognition. Therefore, this paper proposes a unified, multi-task framework to jointly handle these two tasks, termed MTL-Face, which can learn age-invariant identity-related representation while achieving pleasing face synthesis. Specifically, we first decompose the mixed face feature into two uncorrelated components-identity-and age-related featurethrough an attention mechanism, and then decorrelate these two components using multi-task training and continuous domain adaption. In contrast to the conventional one-hot encoding that achieves group-level FAS, we propose a novel identity conditional module to achieve identity-level FAS, with a weight-sharing strategy to improve the age smoothness of synthesized faces. In addition, we collect and release a large cross-age face dataset with age and gender annotations to advance the development of the AIFR and FAS. Extensive experiments on five benchmark cross-age datasets demonstrate the superior performance of our proposed MTLFace over existing state-of-the-art methods for AIFR and FAS. We further validate MTLFace on two popular general face recognition datasets, showing competitive performance for face recognition in the wild. The source code and dataset are available at https://github. com/Hzzone/MTLFace. 11-20 21-30 31-40 51-60 60+ 10-41-50 Age Progression Age Regression 8.0 13.0 76.0 55.0 43.0 31.0 27.0 Figure 1: Sample results by our MTLFace. First row: the real faces of the same person at different ages with estimated age labels underneath. Remaining rows: the synthesized faces when given input faces in the red boxes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has been a hot research topic in computer vision for many years. Recently, deep-learning-based methods achieve excellent performance, even surpassing humans in several scenarios, by empowering the face recognition models with deep neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b42">42]</ref>. The traditional wisdom is to utilize the margin-based metrics to increase the intra-class compactness and train the models with a massive amount of data to improve face recognition performance <ref type="bibr" target="#b53">[53]</ref>.</p><p>Despite the remarkable success of general face recognition (GFR), how to minimize the effects of age variation is a lingering challenge for current face recognition systems to correctly identify faces in many practical applications such as finding lost children. It is therefore of great significance to achieve face recognition without age variation, i.e., ageinvariant face recognition or AIFR. However, AIFR remains extremely challenging in the following three aspects. First, when the age gap becomes large in cross-age face recognition, age variation can largely affect the facial appearance, compromising the face recognition performance. Second, face age synthesis (FAS) is a complex process involving face aging/rejuvenation (a.k.a age progression/regression) since the facial appearance drastically changes over a long time and differs from person to person. Last, it is infeasible to obtain a large paired face dataset to train a model in rendering faces with natural effects while preserving identities.</p><p>To overcome these issues, current methods for AIFR can be roughly divided into two categories: generative and discriminative models. Give a face image, the generative models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b34">34]</ref> aim to transform the faces of different ages into the same age group in order to assist the face recognition. Recently, generative adversarial networks (GANs) <ref type="bibr" target="#b11">[12]</ref> have been successfully used to enhance the image quality of synthesized faces <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>; they typically use the one-hot encoding to specify the target age group. However, the one-hot encoding represents the age group-level face transformation, ignoring the identitylevel personalized patterns and leading to unexpected artifacts. As a result, the performance of AIFR cannot be significantly improved due to the unpleasing synthesized faces and unexpected changes in identity. On the other hand, the discriminative models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">49]</ref> focus on extracting age-invariant features by disentangling the identity-related information from the mixed information so that only the identity-related information is expected for the face recognition systems. Although achieving promising performance in AIFR, they cannot provide users, for example policemen, with visual results as the generative methods to further verify the identities, which can compromise the model interpretability in the decision-making processes of many practical applications.</p><p>To further improve the image quality for generative models and provide the model interpretability for discriminative models, we propose a unified, multi-task learning framework to simultaneously achieve AIFR and FAS, termed MTLFace, which can enjoy the best of both worlds; i.e. learning age-invariant identity-related representation while achieving pleasing face synthesis. More specifically, we first decompose the mixed high-level features into two uncorrelated components-identity-and age-related features-through an attention mechanism. We then decorrelate these two components in a multi-task learning framework, in which an age estimation task is to extract agerelated features while a face recognition task is to extract identity-related feature; in addition, a continuous crossage discriminator with a gradient reversal layer <ref type="bibr" target="#b7">[8]</ref> further encourages the identity-related age-invariant features.</p><p>Moreover, we propose an identity conditional module to achieve identity-level transformation patterns for FAS, with a weight-sharing strategy to improve the age smoothness of synthesized faces; i.e., the faces are aged smoothly. Extensive experiments demonstrate superior performance over existing state-of-the-art methods for AIFR and FAS, and competitive performance for face recognition in the wild. <ref type="figure">Fig. 1</ref> presents an example of age progression/regression of the same person from our MTLFace, showing that our framework can synthesize photorealistic faces while preserving identity.</p><p>Our contributions are summarized as follows. First, we propose a unified, multi-task learning framework to jointly handle AIFR and FAS, which can learn age-invariant identity-related representation while achieving pleasing face synthesis. Second, we propose an attention-based feature decomposition to separate the age-and identity-related feature on high-level feature maps, which can constrain the decomposition process in contrast to the previous unconstrained decomposition on feature vectors. Age estimation and face recognition tasks are incorporated to supervise the decomposition process in conjunction with a continuous domain adaption. Third, compared to previous one-hot encoding achieving age group-level face transformation, we propose a novel identity conditional module to achieve identity-level face transformation, with a weightsharing strategy to improve the age smoothness of synthesized faces. Fourth, extensive experiments demonstrate the effectiveness of the proposed framework for AIFR and FAS on five benchmark datasets, and competitive performance on two popular GFR datasets. Last, we collect and release a large cross-age dataset of millions of faces with age and gender annotations, which can advance the development of the AIFR and FAS. In addition, it is expected to be useful for other face-related research tasks; e.g., pretraining for face age estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Age-invariant face recognition (AIFR). Prior studies usually tackle age variation by disentangling age-invariant features from mixed features. For example, <ref type="bibr" target="#b9">[10]</ref> adopted the hidden factor analysis (HFA) to factorize the mixed features and then reduce the age variation in identity-related features. <ref type="bibr" target="#b52">[52]</ref> extended HFA <ref type="bibr" target="#b9">[10]</ref> into a deep learning framework with the latent factor guided convolutional neural network (LF-CNN). At the same time, <ref type="bibr" target="#b60">[60]</ref> introduced an age estimation task to guide the AIFR. Most recently, CNNsbased discriminative methods have achieved promising results for AIFR. OE-CNN [49] adapted a modified softmax loss <ref type="bibr" target="#b26">[26]</ref> for AIFR by decomposing the facial embeddings into two orthogonal components such that the identity-and age-related features are represented as the angular and radial direction, respectively. Similarly, DAL <ref type="bibr" target="#b45">[45]</ref> achieved the feature decomposition in an adversarial manner under the assumption that the two components are uncorrelated. The work related to ours is <ref type="bibr" target="#b59">[59]</ref>, in which a cGANsbased model, with cross-age domain adversarial training extracting age-invariant representations, is adopted to achieve the two tasks simultaneously. However, it generates oversmoothed faces with subtle changes. Different from <ref type="bibr" target="#b59">[59]</ref>, our framework has following advantages: 1) our feature decomposition is done on feature maps through an attention mechanism; 2) a continuous domain adaption with gradient reversal layer is used to learn age-invariant identityrelated representation; and 3) identity conditional module can achieve identity-level face synthesis and improve the age smoothness of synthesized faces. Face age synthesis (FAS). Existing methods for FAS can be roughly divided into physical model-, and prototypeand deep generative model-based methods. Physical modelbased methods <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b43">43]</ref> mechanically model the changes of appearance over time, but they are computationally expensive and require massive paired images of the same person with a long time. Prototype-based methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b40">40]</ref> achieve face aging/rejuvenation using the average of faces in each age group, hence the identity cannot be well preserved. The deep generative model-based methods <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b48">48]</ref> exploit the deep neural network for this task. For example, recurrent face aging (RFA) <ref type="bibr" target="#b48">[48]</ref> used a recurrent neural network to model the intermediate transition states of age progression/regression, traversing on which a smooth face aging process can be achieved. Inspired by the powerful capability of generative adversarial networks (GANs) <ref type="bibr" target="#b11">[12]</ref>, especially conditional GANs (cGANs) <ref type="bibr" target="#b29">[29]</ref>, in generating high-quality images, many recent studies <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56]</ref> resort to them to improve the visual quality of synthesized faces and train models with unpaired age data. For example, <ref type="bibr" target="#b58">[58]</ref> used a conditional adversarial autoencoder (CAAE) to achieve both age progression/regression by traversing on a low-dimensional face manifold. <ref type="bibr" target="#b51">[51]</ref> introduced the perceptual loss to preserve the identities during face aging/rejuvenation. <ref type="bibr" target="#b56">[56]</ref> designed a discriminator with the pyramid architecture to enhance the aging details.</p><p>However, these methods mainly aim at improving the visual quality of generated faces, and hardly improve the performance of AIFR due to the artifacts resulting from group-level face transformation, and the unexpected change in identity. Our method differs in the following aspects: 1) the proposed MTLFace achieves AIFR and FAS simultaneously to enhance the visual quality with identity-related information from AIFR; 2) the proposed identity conditional module (ICM) achieves an identity-level face age synthesis in contrast to the previous group-level face age synthesis; and 3) a weight-sharing strategy in ICM can improve the age smoothness of synthesized faces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention-based Feature Decomposition</head><p>As the faces change a lot over time, the critical problem of AIFR is that the age variation usually introduces the increasing intra-class distances. As a result, it is chal-lenging to correctly recognize two faces of the same person with a large gap, since the mixed facial representations are severely entangled with unrelated information such as facial shape and texture changes. Recently, Wang et al. design a linear factorization module to decompose the feature vectors into these two unrelated components <ref type="bibr" target="#b45">[45]</ref>. Formally, given the feature vector x ? R d extracted from an input image I ? R 3?H?W , their linear factorization module is defined as <ref type="bibr" target="#b45">[45]</ref>:</p><formula xml:id="formula_0">x = x age + x id ,<label>(1)</label></formula><p>where x age and x id denote the age-and identity-related components, respectively. This decomposition is implemented through a residual mapping. However, it has the following drawbacks: 1) this decomposition performs on onedimensional feature vector, the resultant identity-related component lacks spatial information of face, not suitable for FAS; and 2) this decomposition is unconstrained, which may lead to unstable training.</p><p>To address these drawbacks, we instead propose to decompose the mixed feature-maps at a high-level semantic space through an attention mechanism, termed attentionbased feature decomposition or AFD. The main reason is that manipulating on the feature vectors is more complicated than on the feature maps since the aging/rejuvenation effects, such as beards and wrinkles, appear in the semantic feature space but lose in the one-dimensional features. Formally, we use a ResNet-like backbone as encoder E to extract mixed feature maps X ? R C?H ?W from an input image I, i.e. X = E(I), the AFD can be defined as follows:</p><formula xml:id="formula_1">X = X ? ?(X) Xage + X ? 1 ? ?(X) X id ,<label>(2)</label></formula><p>where ? denotes element-wise multiplication and ? represents an attention module. In doing so, the age-related information in the feature maps can be separated through the attention module supervised by an age estimation task, and the residual part, regarded as the identity-related information, can be supervised by a face recognition task. As a result, the attention mechanism constrains the decomposition module, better at detecting the age-related features in semantic feature maps. We note that X is assumed to only contain the age and identity information as driven by the two corresponding tasks, the remaining information such as background is important for FAS, which is preserved by skip connections from encoder to decoder. <ref type="figure" target="#fig_0">Fig. 2</ref>(b) details the proposed AFD.</p><p>In this paper, we adopt the sum average of channel attention (CA) <ref type="bibr" target="#b15">[15]</ref> and spatial attention (SA) <ref type="bibr" target="#b54">[54]</ref> to highlight age-related information at both channel and spatial levels. Note that the outputs of these two attentions have different sizes, we first stretch each of them to the original input size and then average them. Different attention modules such as CA, SA, and CBAM <ref type="bibr" target="#b54">[54]</ref> are also investigated in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Identity Conditional Module</head><formula xml:id="formula_2">0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 1 , 1 , 1 , 1 1 , 1 , 1 , 1 1 , 1 , 1 , 1 1 , 1 , 1 , 1 a) One-hot encoding 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 0 , 0 , 0 , 0 b) Identity Conditional Block S e l e c t C o n v o l u t i o n B l o c k C o n v o l u t i o n B l o c k Identity Age Condition Synthesized Feature T a r g e t G r o u p</formula><p>Input Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity-related Feature</head><p>A g e P r o g r e s s io The mainstream face aging studies <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref> usually split the ages into several non-overlapping age groups, since the changes over time are minor with a small age gap. These methods typically use one-hot encoding to specify the target age group to control the aging/rejuvenation process <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b58">58]</ref> as illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Consequently, a group-level aging/rejuvenation pattern, such as people having a beard when they are 30 years old, is learned for each age group due to the use of one-hot age condition. Its drawbacks are twofold: 1) one-hot encoding represents the age group-level aging/rejuvenation pattern, ignoring identity-level personalized pattern, particularly for different genders and races; and 2) one-hot encoding may not ensure the age smoothness of synthesized faces.</p><formula xml:id="formula_3">n 1 1 - 2 0 2 1 - 3 0 6 1 + 5 1 - 6 0 1 0 - [ 0 , 1 , 0 , . . . , 0 , 0 , 0 ]</formula><p>To address these issues raised by one-hot encoding, we propose an identity conditional block (ICB) to achieve identity-level aging/rejuvenation pattern, with a weightsharing strategy to improve the age smoothness of synthesized faces. Specifically, the proposed ICB takes the identity-related feature from AFD as input to learn an identity-level aging/rejuvenation pattern. Next, we propose a weights-sharing strategy to improve the age smoothness of synthesized faces so that some convolutional filters are shared across adjacent age groups as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. The rationale behind this idea is that faces are gradually changed over time, where the shared filters can learn some common aging/rejuvenation patterns between adjacent age groups. Note that X id is reduced from 512 to 128 using 1 ? 1 convolutions to reduce the computational cost. In this paper, a hyper-parameter s to control how many filters are shared for two adjacent age groups, which is empirically set to 1/8; i.e., the adjacent two age groups share 16 filters. We stack ICBs to form an identity conditional module (ICM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task Learning Framework</head><p>This section presents our MTLFace including AIFR and FAS. Age-invariant face recognition (AIFR) task. To encourage AFD to robustly decompose features, we use an age estimation task and a face recognition task to supervise the feature decomposition. Specifically, X age draws the age variations by an age estimation task while X id encodes the identity-related information. To be clear, we include an age estimation network A with two linear layers of 512 and 101 neurons to achieve age regression similar to deep expectation (DEX) <ref type="bibr" target="#b39">[39]</ref> that learns the age distribution by computing a softmax expected value. Another linear layer W ? R 101?N is appended at the last for age classification, regularizing the learned distribution; here, N is the number of age groups. The loss function to optimize age estimation can be defined as:</p><formula xml:id="formula_4">AE (X age ) = E I MSE (DEX(A(X age )), y age ) + CE (A(X age )W , c age ) ,<label>(3)</label></formula><p>where y age , c age , MSE , and CE are the ground truth age, ground truth age group, and mean squared error (MSE) for age regression, cross-entropy (CE) loss for age group classification, respectively. Next, we leverage one linear layer L of 512 neurons to extract the feature vectors, and use the CosFace loss to supervise the learning of X id for identity classification. We also introduce a cross-age domain adversarial learning that encourages X id to be age-invariant through a continuous domain adaption <ref type="bibr" target="#b46">[46]</ref> with a gradient reversal layer (GRL) <ref type="bibr" target="#b7">[8]</ref>. The final loss for AIFR is formulated as:</p><formula xml:id="formula_5">L AIFR = COSFACE (L(X id ), y id ) (4) + ? AIFR age L AE (X age ) + ? AIFR id L AE (GRL(X id )),</formula><p>where the first term is the CosFace loss, the second term is the age estimation loss, and the last term is the domain adaption loss, y id is the identity label, and ? * controls the balance of different loss terms. Note that the second and third terms use the same network structure, but have different inputs and are trained independently. The activation functions and batch normalizations are ignored for simplicity, and our face recognition model is designed strictly following the setting in <ref type="bibr" target="#b5">[6]</ref> except the AFD. Face age synthesis (FAS) task. <ref type="figure" target="#fig_0">Fig. 2(f)</ref> demonstrates the FAS process of our proposed method. In detail, the identitylevel age condition is derived from the discriminative facial representations X id by applying an identity conditional module (ICM) with a series of ICBs. Then, the decoder D reconstructs the progressed/regressed faces from the multilevel high-resolution features extracted from the encoder E, under the control of the learned identity-level age condition.</p><p>Formally, the process of rendering input face I to I t with target age group t can be written as:</p><formula xml:id="formula_6">I t = D {E l (I)} 3 l=1 , ICM(X id , t) ,<label>(5)</label></formula><p>where l denotes the index of different levels of highresolution features extracted from different layers of the encoder E.</p><p>To facilitate the visual quality of generated faces, the FAS task is trained using GANs framework. In this paper, we adopt the PatchDiscriminator from <ref type="bibr" target="#b18">[18]</ref> as our discriminator D img to emphasize the local-patch of generated and real images. Furthermore, the least-squares GANs <ref type="bibr" target="#b28">[28]</ref> are employed to optimize the GANs framework for an improved quality of generated images and stable training process, which can be formulated as follows:</p><formula xml:id="formula_7">L FAS adv = 1 2 E I [D img ([I t ; C t ]) ? 1] 2 ,<label>(6)</label></formula><p>where C t is the one-hot encoding used in traditional cGANs framework for aligning the age condition, and [; ] denotes the matrix concatenation along channel dimension.</p><p>To preserve the identities of input faces and improve the age accuracy, we leverage the encoder E and AFD to supervise the FAS task. Consequently, we can achieve both face aging and rejuvenation in a holistic, end-to-end manner, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. This process can be formulated as follows:</p><formula xml:id="formula_8">X t age , X t id = AFD E(I t )<label>(7)</label></formula><p>L FAS age = CE X t age , t ,</p><formula xml:id="formula_9">L FAS id = E Xs X t id ? X id 2 F ,<label>(8)</label></formula><p>where ? F represents the Frobenius norm. The final loss to optimize this task can be written as:</p><formula xml:id="formula_11">L FAS = ? FAS adv L FAS adv + ? FAS id L FAS id + ? FAS age L FAS age ,<label>(10)</label></formula><p>where ? FAS * controls the importance of different loss terms of FAS task. The loss function to optimize the discriminator D img in the context of least-squares GANs is defined as:</p><formula xml:id="formula_12">L FAS Dimg = 1 2 E I D img [I; C] ? 1 2 + 1 2 E It D img [I t ; C t ] 2 .<label>(11)</label></formula><p>At the testing stage, the only difference from existing FAS methods is that our method needs to specify the corresponding group of filters. Consequently, our method enjoys the advantages similar to <ref type="bibr" target="#b14">[14]</ref> that the computational cost can be significantly reduced by only encoding input faces once, instead of N times in previous works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>, where N is the number of age groups. Optimization and inference. In our MTLFace, the AIFR learns the discriminative facial representations and age estimation while the FAS produces the visual results which can boost the model interpretability for AIFR. Therefore, both two tasks can be jointly accomplished by optimizing these two tasks in a GAN-like manner; they mutually leverage each other to boost themselves. In other words, the AIFR encourages FAS to render faces to preserve its identity while FAS can facilitate the extraction of the identity-related feature and boost the model interpretability for AIFR. Consequently, we alternately train these two tasks in a unified, multi-task, end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Data collection. Current research on AIFR lacks a largescale face dataset of millions of face images with a large age gap. To advance the development of AIFR and FAS, we create and release a new large cross-age face dataset (LCAF) with 1.7M faces from cross-age celebrities. Specifically, we first use the public Azure Facial API <ref type="bibr" target="#b1">[2]</ref> to estimate the ages and genders of faces from the clean MS-Celeb-1M dataset provided by <ref type="bibr" target="#b5">[6]</ref>. Then, we randomly select faces from a total of 5M faces to check whether the faces are correctly labeled, and try our best to manually correct them if any apparent mistakes; we mainly focus on the young ages under 20 that are often mislabeled by the API <ref type="bibr" target="#b1">[2]</ref>. Finally, a large-scale balanced age dataset is constructed by balancing both age and gender. We further build a subset of cross-age face dataset (SCAF) containing about 0.5M images from 12K individuals following <ref type="bibr" target="#b45">[45,</ref><ref type="bibr">49]</ref> for fair comparisons. We note that the training (LCAF) and testing data may have very little, or even no identities overlapping as <ref type="bibr" target="#b5">[6]</ref> already removed 500+ identities from their clean MS-Celeb-1M dataset by checking the similarity of faces between training and testing data. Following the mainstream literature <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b56">56]</ref> with the time span of 10 years for each age group, the ages in this paper are divided into seven non-overlapping groups; i.e., <ref type="bibr">10-, 11-20, 21-30, 31-40, 41-50, 51-60</ref>, and 61+. Note that it is a much more challenging problem to perform FAS on seven groups than on four groups in previous works. <ref type="figure" target="#fig_3">Fig. 4</ref> presents example images and dataset statistics of SCAF. Training details. We adopted ResNet-50 similar to <ref type="bibr" target="#b5">[6]</ref> as the encoder E. In the decoder D, the identity age condition is bilinearly upsampled and processed with multilevel high-resolution features extracted from E by two Res-Blocks <ref type="bibr" target="#b12">[13]</ref>, each of which follows the instance normalization <ref type="bibr" target="#b44">[44]</ref> and ReLU activation, the synthesized faces of size 112 ? 112 were produced by one 1 ? 1 convolutional layer. There are four ICBs in ICM. In the discriminator D img , six convolutional layers with strides of 2, 2, 2, 2, 1, 1 follow the spectral normalization <ref type="bibr" target="#b30">[30]</ref> and leaky ReLU activation except the last one, outputting 7 ? 7 confidence map. The AIFR is optimized by SGD with an initial learning rate of 0.1 and momentum of 0.9 while the ICM, decoder D, and D img are trained by Adam with a fixed learning rate of 10 ?4 , ? 1 of 0.9 and ? 2 of 0.99 for the face age synthesis. We trained all models with a batch size of 512 on 8 NVIDIA GTX 2080Ti GPUs, 110K iterations for LCAF and 36K iterations for SCAF. The learning rate of AIFR was warmed up linearly from 0 to 0.1, reduced by a factor of 0.1, at iterations 5K, 70K, and 90K on LCAF and 1K, 20K, 23K on SCAF, respectively. The hyper-parameters in the loss functions were empirically set as follows: ? AIFR age was 0.001, ? AIFR id was 0.002, ? FAS adv was 75, ? FAS id was 0.002, and ? FAS age was 10. The multiplicative margin and scale factor of CosFace loss [47] were set to 0.35 and 64, respectively. All images were aligned to 112 ? 112, with five facial landmarks detected by MTCNN <ref type="bibr" target="#b57">[57]</ref>, and linearly normalized to [?1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on AIFR</head><p>Next, we evaluate the MTLFace on several benchmark cross-age datasets, including CACD-VS <ref type="bibr" target="#b3">[4]</ref>, CALFW <ref type="bibr" target="#b61">[61]</ref>, AgeDB <ref type="bibr" target="#b31">[31]</ref>, and FG-NET <ref type="bibr" target="#b0">[1]</ref>, to compare with the stateof-the-art methods. Note that MORPH is excluded since the version in <ref type="bibr" target="#b45">[45,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b59">59]</ref> is prepared for commercial use only. Result on AgeDB. AgeDB <ref type="bibr" target="#b31">[31]</ref> contains 16,488 face images of 568 distinct subjects with manually annotated age labels. It provides four protocols for age-invariant face verification protocols under the different age gaps of face pair; i.e. 5, 10, 20, and 30 years. Similar to LFW <ref type="bibr" target="#b16">[16]</ref>, this dataset is split into 10 folds for each protocol, where each fold consists of 300 intra-class and 300 inter-class pairs. We strictly follow the protocol of 30 years to perform the 10-fold crossvalidation since the protocol of 30 years is the most challenging. We use the models trained on SCAF to evaluate the performance on AgeDB for fair comparisons. <ref type="table" target="#tab_1">Table 1a</ref> shows the verification accuracy of our models compared against the other state-of-the-art AIFR methods, demonstrating the superior performance of the proposed method. Result on CALFW. Cross-age labeled faces in the wild (CALFW) dataset <ref type="bibr" target="#b61">[61]</ref> is designed for unconstrained Method Acc (%) RJIVE <ref type="bibr" target="#b41">[41]</ref> 55.20 VGG Face <ref type="bibr" target="#b35">[35]</ref> 89.89 Center Loss <ref type="bibr" target="#b53">[53]</ref> 93.72 SphereFace <ref type="bibr" target="#b26">[26]</ref> 91.70 CosFace <ref type="bibr">[47]</ref> 94.56 ArcFace <ref type="bibr" target="#b5">[6]</ref> 95.15 DAAE <ref type="bibr" target="#b23">[23]</ref> 95.30</p><p>MTLFace (ours) 96.23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) AgeDB-30</head><p>Method Acc (%) HUMAN-Individual 82.32 HUMAN-Fusion 86.50</p><p>Center Loss <ref type="bibr" target="#b53">[53]</ref> 85.48 SphereFace <ref type="bibr" target="#b26">[26]</ref> 90.30 VGGFace2 <ref type="bibr" target="#b2">[3]</ref> 90.57 ArcFace <ref type="bibr" target="#b5">[6]</ref> 95.45</p><p>MTLFace (ours) 95.62</p><formula xml:id="formula_13">(b) CALFW Method Acc (%)</formula><p>HFA <ref type="bibr" target="#b9">[10]</ref> 84.40 CARC <ref type="bibr" target="#b3">[4]</ref> 87.60 VGGFace <ref type="bibr" target="#b35">[35]</ref> 96.00 Center Loss <ref type="bibr" target="#b53">[53]</ref> 97.48 LF-CNN <ref type="bibr" target="#b52">[52]</ref> 98.50 Marginal Loss <ref type="bibr" target="#b6">[7]</ref> 98.95 OE-CNN <ref type="bibr">[49]</ref> 99.20 AIM <ref type="bibr" target="#b59">[59]</ref> 99.38 DAL <ref type="bibr" target="#b45">[45]</ref> 99.40</p><p>MTLFace (ours) 99.55</p><formula xml:id="formula_14">(c) CACD-VS Method Rank-1 (%)</formula><p>Park et al. <ref type="bibr" target="#b34">[34]</ref> 37.40 Li et al. <ref type="bibr" target="#b25">[25]</ref> 47.50 HFA <ref type="bibr" target="#b9">[10]</ref> 69.00 MEFA <ref type="bibr" target="#b10">[11]</ref> 76.20 CAN <ref type="bibr" target="#b55">[55]</ref> 86.50 LF-CNN <ref type="bibr" target="#b52">[52]</ref> 88.10 AIM <ref type="bibr" target="#b59">[59]</ref> 93.20 DAL <ref type="bibr" target="#b45">[45]</ref> 94.50</p><p>MTLFace (ours) 94.78</p><formula xml:id="formula_15">(d) FG-NET (leave-one-out) Method Rank-1 (%)</formula><p>FUDAN-CS SDS <ref type="bibr" target="#b50">[50]</ref> 25.56 SphereFace <ref type="bibr" target="#b26">[26]</ref> 47.55 TNVP <ref type="bibr" target="#b33">[33]</ref> 47  face verification with large age gaps, which contains 12,176 face images of 4,025 individuals collected using the same identities in LFW. Similarly, we follow the same protocol as the LFW, where each fold consists of 600 positive and negative pairs. We train the model on LCAF to evaluate our method on this dataset, and the results are shown in <ref type="table" target="#tab_1">Table 1b</ref>. Particularly, our method outperforms the recent state-of-the-art AIFR methods by a large margin, establishing a new state-of-the-art on the CALFW.</p><p>Result on CACD-VS. As a public age dataset for AIFR, cross-age celebrity dataset (CACD) contains 163,446 face images of 2,000 celebrities in the wild, with significant variations in age, illumination, pose, and so on. Since collected by search engine, CACD is noisy with mislabeled and duplicate images. Therefore, a carefully annotated version, CACD verification sub-set or CACD-VS, is constructed for fair comparisons, which also follows the protocol of LFW. <ref type="table" target="#tab_1">Table 1c</ref> presents the comparison of the proposed method with other state-of-the-arts on CACD-VS <ref type="bibr" target="#b3">[4]</ref>. Our MTL-Face surpasses other state-of-the-arts by a large margin, introducing an improvement of 0.15 against the recent one.</p><p>Result on FG-NET. FG-NET <ref type="bibr" target="#b0">[1]</ref> is the most popular and challenging age dataset for AIFR, which consists of 1,002 face images from 82 subjects collected from the wild with huge age variations ranging from child to elder. We strictly follow the evaluation pipeline in <ref type="bibr" target="#b45">[45,</ref><ref type="bibr">49]</ref>. Specifically, the model is trained on SCAF and tested under the protocols of leave-one-out and MegaFace challenge 1 (MF1). In the leave-one-out protocol, faces are used to match the rest faces, repeating 1,002 times. <ref type="table" target="#tab_1">Table 1d</ref> reports the rank-1 recognition rate. Our method outperforms prior work by a large margin. On the other hand, the MF1 contains additional 1M images as the distractors in the gallery set from 690K different individuals, where models are evaluated under the large and small training set protocols. The small protocol requires the training set less than 0.5M images. The small protocol is strictly followed to evaluate our trained model on FG-NET, and the experimental results are reported in <ref type="table" target="#tab_1">Table 1e</ref>. Our method achieves competitive performance against other methods since the distractors in MF1 contains a large number of mislabeled probe and gallery face images.</p><p>Ablation study. To investigate the efficacy of different modules in MTLFace, we perform ablation studies based on four benchmark datasets for AIFR by considering the following variants of our method: 1) Baseline: we remove all extra components but only the CosFace loss to train the face recognition model. 2) +Age: this variant is jointly trained under the supervision of both CosFace and age estimation loss, similar to <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b60">60]</ref>. 3) +AFD (CA), +AFD (SA), +AFD (CBAM), +AFD: these four variants utilize the proposed attention-based feature decomposition to highlight the age-related information at different level, by the different attention modules including CA <ref type="bibr" target="#b15">[15]</ref>, SA <ref type="bibr" target="#b54">[54]</ref>, CBAM <ref type="bibr" target="#b54">[54]</ref>, and the proposed one. 4) Ours: our proposed MTLFace is trained simultaneously by the AFD and cross-age domain adaption loss. The experimental results are reported in <ref type="table" target="#tab_1">Table 1f</ref>. We note that the verification rate  of the baseline model on AgeDB-30 is higher than those of ArcFace and DAAE since the training data (i.e., SCAF) is age-balanced, which is an important feature of our collected dataset. Even though the age estimation task is performed in the face recognition model, it cannot introduce any improvement of AIFR compared to the baseline model. On the other hand, AFD achieves remarkable performance improvement on all cross-age datasets. Nevertheless, as the AFD highlights the age-related information at both channel and spatial levels in parallel, our method achieves consistent performance improvements, demonstrating its effectiveness compared to the single level (CA and SA) or sequential level (CBAM). Furthermore, the use of cross-age domain adversarial training leads to an additional performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAAE AIM Ours</head><p>Aging Aging Aging Rejuvenation Rejuvenation Rejuvenation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on GFR</head><p>To validate the generalization ability of our MTLFace for the general face recognition (GFR), we further conduct experiments on the LFW <ref type="bibr" target="#b16">[16]</ref> and MegaFace Challenge 1 Facescrub (MF1-Facescrub) <ref type="bibr" target="#b19">[19]</ref> datasets. LFW <ref type="bibr" target="#b16">[16]</ref> is the most popular public benchmark dataset for GFR, which contains 13,233 face images from 5,749 subjects. The MF1-Facescrub <ref type="bibr" target="#b19">[19]</ref> uses the Facescrub dataset <ref type="bibr" target="#b32">[32]</ref> of 106,863 face images from 530 celebrities as a probe set. The most challenging problem of MF1 is that it uses an additional 1M face images in the gallery set to distract the face matching. That is, the results on MF1 are not as reliable as LFW due to the extremely noisy distractors in MF1. We strictly follow the same procedure as <ref type="bibr" target="#b45">[45,</ref><ref type="bibr">49]</ref>, where the training dataset contains 0.5M images (SCAF). <ref type="table" target="#tab_1">Table 1g</ref> reports the verification rate on LFW and the rank-1 identification rate in MF1-Facescrub against the state-of-the-art GFR methods. Our method achieves competitive performance on both datasets, demonstrating the strong generalization ability of our MTLFace. We highlight that our MTLFace can provide photo-realistic synthesized faces to improve model interpretability, which is absent in other methods <ref type="bibr" target="#b45">[45,</ref><ref type="bibr">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on FAS</head><p>We further evaluate the model trained on SCAF for FAS. <ref type="figure" target="#fig_4">Fig. 5</ref> presents some sample results on the external datasets including LCAF, MORPH, and FG-NET. Our method is able to simulate the face age synthesis process between age groups with high visual fidelity. Although there exist variations in terms of race, gender, expression, and occlusion, the synthesized faces are still photo-realistic, with natural details in the skin, muscles, and wrinkles while consistently preserving identities, confirming the generalization ability of the proposed method.</p><p>We conduct qualitative comparisons with prior work in-cluding CAAE <ref type="bibr" target="#b58">[58]</ref> and AIM <ref type="bibr" target="#b59">[59]</ref> on MORPH and FG-NET. <ref type="figure" target="#fig_5">Fig. 6</ref> shows that both CAAE and AIM produce oversmoothed faces due to their image reconstruction while our MTLFace uses the identity age condition to synthesize faces based on multi-level features extracted from the encoder. Note that the results of competitors are directly referred from their own papers for a fair comparison, which is widely adopted in the FAS literature such as <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b56">56]</ref> to avoid any bias or error caused by self-implementation. See Appendix for quantitative comparisons with CAAE <ref type="bibr" target="#b58">[58]</ref>, IPCGAN <ref type="bibr" target="#b51">[51]</ref>, and an ablation study of identity conditional module in terms of two evaluation criteria-age accuracy and identity preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a multi-task learning framework, termed MTLFace, to achieve AIFR and FAS simultaneously. We proposed two novel modules: AFD to decompose the features into age-and identity-related features, and ICM to achieve identity-level face age synthesis. Extensive experiments on both cross-age and general benchmark datasets for face recognition demonstrate the superiority of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed MTLFace including two tasks. AIFR: The encoder E first extracts the mixed feature maps from input faces, which are then decomposed into two disjoint identity-and age-related feature maps by the multitask training and continuous domain adaption. FAS: The decoder D produces synthesized faces through identity conditional module based on multi-level features; the PatchDiscriminator D img penalizes the framework for better visual quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>presents the architecture of the proposed MTL-Face, which will be detailed in the following subsections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between one-hot encoding and ICB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Sample faces (a) and dataset statistics (b and c) on SCAF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results by applying our MTLFace trained on SCAF dataset to three external datasets : a) LCAF excluding identities in SCAF; b) MORPH; and c) FG-NET. Red boxes indicate input faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparisons with prior work on FG-NET (top 3 rows) and MORPH (bottom 3 rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on several benchmark AIFR and GFR datasets with the best results in bold. We reported the verification rate (%) for AgeDB, CALFW, CACD-VS, and LFW, and the rank-1 identification rate (%) for FG-NET and MF1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. Quantitative comparison on face age synthesis</head><p>We trained all models on the SCAF dataset for fair comparisons, which are then directly applied to three external crossage datasets; i.e., MORPH <ref type="bibr" target="#b38">[38]</ref>, FG-NET <ref type="bibr" target="#b0">[1]</ref> and CACD <ref type="bibr" target="#b3">[4]</ref>. We further evaluate them in terms of two widely-used metrics, i.e. age accuracy and identity preservation, since the synthesized faces should be in the target age groups while consistently preserving the identities.</p><p>These two metrics are detailed as follows:</p><p>1. Age accuracy: We randomly select 80% of LCAF to train a ResNet-100 model using AE as the loss function to predict the ages of all synthesized faces and test the trained model on the remaining faces. Then, we compute the proportion of the predicted ages falling into the target age groups as the age accuracy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://yanweifu.github.io/FG_NET_data" />
		<title level="m">Fg-net aging database</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Microsoft azure cognitive services facial recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Azure</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/en-us/services/cognitive-services/face/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Bor-Chun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debayan</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07538</idno>
		<title level="m">Finding missing children: Aging deep face features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden factor analysis for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A maximum entropy feature descriptor for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5289" to="5297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">S2gan: Share aging factors across ages and share aging trends among individuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PFA-GAN: Progressive face aging with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIFS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3334" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ran He, and Zhenan Sun. Hierarchical face aging through disentangled latent characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peipei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Age progression and regression with spatial attention modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A discriminative model for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unsang</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1028" to="1037" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attribute-aware face aging with wavelet-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn Represent</title>
		<meeting>Int. Conf. Learn Represent</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kha</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unsang</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf</title>
		<meeting>British Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling age progression in young faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling shape and textural variations in aging faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MORPH: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>Int. Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Manipulating facial appearance through shape and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recovering joint and individual components in facial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2668" to="2681" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A concatenational graph evolution aging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinli</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2083" to="2096" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decorrelated adversarial learning for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3527" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01807</idno>
		<title level="m">Continuously indexed domain adaptation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recurrent face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Orthogonal deep features decomposition for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="738" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task deep neural network for joint face recognition and facial attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keke</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2017 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face aging with identity-preserved conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Latent factor guided convolutional neural networks for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Age invariant face recognition and retrieval by coupled auto-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning face age progression: A pyramid architecture of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Look across elapse: Disentangled representation learning and photorealistic cross-age face synthesis for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Age estimation guided convolutional neural network for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Identity preservation: An external well-trained face recognition model, ResNet-100 network pre-trained on MS-Celeb-1M</title>
		<imprint/>
	</monogr>
	<note>dataset provided by [6], is used for fair comparisons to compute the cosine similarity between the input and synthesized faces</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">IPC-GAN [51], our proposed MTLFace and its variant (w/o ICM). It can be observed that MTLFace outperforms CAAE and IPCGAN by a clear margin; this is a direct results of the AIFR task and ICM. On the other hand, without ICM, MTLFace reduces to a common cGANs-based method that uses one-hot encoding to control face aging/rejuvenation at the group level. Remarkably, the MTLFace without ICM still outperforms these two baseline methods, implying that our multi-learning framework with attention</title>
	</analytic>
	<monogr>
		<title level="m">Table A.1 presents the quantitative results of different face aging/rejuvenation methods</title>
		<imprint/>
	</monogr>
	<note>including CAAE [58. based feature decomposition is effective in improving the age accuracy and identity preservation</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">1: Quantitative comparisons of our MTLFace with the state-of-the-art face aging/rejuvenation methods. We reported the mean value of age accuracy (Age Acc.) and cosine similarity (Cos. Sim.) computed over all age mappings. For each dataset, the best and second best are in red and blue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
		<idno>respectively. MORPH FG-NET CACD</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Age Acc. (%) Cos. Sim. Age Acc (%) Cos. Sim. Age Acc</title>
		<imprint>
			<publisher>%) Cos Sim</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
