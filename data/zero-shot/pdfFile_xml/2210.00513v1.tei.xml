<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient Gating for Deep Multi-Rate Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Konstantin</forename><surname>Rusch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ? ? Trusch@ethz</forename><surname>Ch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>??</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
						</author>
						<title level="a" type="main">Gradient Gating for Deep Multi-Rate Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Gradient Gating (G 2 ), a novel framework for improving the performance of Graph Neural Networks (GNNs). Our framework is based on gating the output of GNN layers with a mechanism for multi-rate flow of message passing information across nodes of the underlying graph. Local gradients are harnessed to further modulate message passing updates. Our framework flexibly allows one to use any basic GNN layer as a wrapper around which the multi-rate gradient gating mechanism is built. We rigorously prove that G 2 alleviates the oversmoothing problem and allows the design of deep GNNs. Empirical results are presented to demonstrate that the proposed framework achieves state-of-the-art performance on a variety of graph learning tasks, including on large-scale heterophilic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning tasks involving graph structured data arise in a wide variety of problems in science and engineering. Graph Neural Networks (GNNs) <ref type="bibr" target="#b49">(Sperduti, 1994;</ref><ref type="bibr" target="#b22">Goller &amp; Kuchler, 1996;</ref><ref type="bibr" target="#b50">Sperduti &amp; Starita, 1997;</ref><ref type="bibr" target="#b19">Frasconi et al., 1998;</ref><ref type="bibr" target="#b24">Gori et al., 2005;</ref><ref type="bibr" target="#b47">Scarselli et al., 2008;</ref><ref type="bibr" target="#b7">Bruna et al., 2014;</ref><ref type="bibr" target="#b14">Defferrard et al., 2016;</ref><ref type="bibr" target="#b28">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b37">Monti et al., 2017;</ref><ref type="bibr" target="#b21">Gilmer et al., 2017)</ref> are a popular deep learning architecture for graph-structured and relational data. GNNs have been successfully applied in domains including computer vision and graphics <ref type="bibr" target="#b37">(Monti et al., 2017)</ref>, recommender systems <ref type="bibr" target="#b56">(Ying et al., 2018)</ref>, transportation <ref type="bibr" target="#b15">(Derrow-Pinion et al., 2021)</ref>, computational chemistry <ref type="bibr" target="#b21">(Gilmer et al., 2017)</ref>, drug discovery <ref type="bibr">(Gaudelet et al., 2021)</ref>, particle physics <ref type="bibr" target="#b48">(Shlomi et al., 2020)</ref> and social networks. See <ref type="bibr" target="#b58">Zhou et al. (2019)</ref>;  for extensive reviews.</p><p>Despite the widespread success of GNNs and a plethora of different architectures, several fundamental problems still impede their efficiency on realistic learning tasks. These include the bottleneck <ref type="bibr">(Alon &amp; Yahav, 2021)</ref>, oversquashing <ref type="bibr" target="#b51">(Topping et al., 2021)</ref>, and oversmoothing <ref type="bibr" target="#b38">(Nt &amp; Maehara, 2019;</ref><ref type="bibr" target="#b39">Oono &amp; Suzuki, 2020)</ref> phenomena. Oversmoothing refers to the observation that all node features in a deep (multi-layer) GNN converge to the same constant value as the number of layers is increased. Thus, and in contrast to standard machine learning frameworks, oversmoothing inhibits the use of very deep GNNs for learning tasks. These phenomena are likely responsible for the unsatisfactory empirical performance of traditional GNN architectures in heterophilic datasets, where the features or labels of a node tend to be different from those of its neighbors <ref type="bibr" target="#b59">(Zhu et al., 2020)</ref>.</p><p>Given this context, our main goal is to present a novel framework that alleviates the oversmoothing problem and allows one to implement very deep multi-layer GNNs that can significantly improve performance in the setting of heterophilic graphs. Our starting point is the observation that in standard Message-Passing GNN architectures (MPNNs), such as <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> or GAT <ref type="bibr" target="#b52">(Velickovic et al., 2018)</ref>, each node gets updated at exactly the same rate within every hidden layer. Yet, realistic learning tasks might benefit from having different rates of propagation (flow) of information on the underlying graph. This insight leads to a novel multi-rate message passing scheme capable of learning these underlying rates. Moreover, we also propose a novel procedure that harnesses graph gradients to ameliorate the oversmoothing problem. Combining these elements leads to a new architecture described in this paper, which we term Gradient Gating (G 2 ).</p><p>Main Contributions. We will demonstrate the following advantages of the proposed approach:</p><p>? G 2 is a flexible framework wherein any standard message-passing layer (such as GAT, <ref type="bibr">GCN, GIN, or GraphSAGE)</ref> can be used as the coupling function. Thus, it should be thought of as a framework into which one can plug existing GNN components. The use of multiple rates and gradient gating facilitates the implementation of deep GNNs and generally improves performance.</p><p>? G 2 can be interpreted as a discretization of a dynamical system governed by nonlinear differential equations. By investigating the stability of zero-Dirichlet energy steady states of this system, we rigorously prove that our gradient gating mechanism prevents oversmoothing. To complement this, we also prove a partial converse, that the lack of gradient gating can lead to oversmoothing.</p><p>? We provide extensive empirical evidence demonstrating that G 2 achieves state-of-the-art performance on a variety of graph learning tasks, including on large heterophilic graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gradient Gating</head><p>Let G = (V, E ? V ? V) be an undirected graph with |V| = v nodes and |E| = e edges (unordered pairs of nodes {i, j} denoted i ? j). The 1-neighborhood of a node i is denoted N i = {j ? V : i ? j}. Furthermore, each node i is endowed with an m-dimensional feature vector X i ; the node features are arranged into a v ? m matrix X = (X ik ) with i = 1, . . . , v and k = 1, . . . , m.</p><p>A typical residual Message-Passing GNN (MPNN) updates the node features by performing several iterations of the form,</p><formula xml:id="formula_0">X n = X n?1 + ?(F ? (X n?1 , G)),<label>(1)</label></formula><p>where F ? is a learnable function with parameters ?, and ? is an element-wise non-linear activation function. Here n ? 1 denotes the n-th hidden layer with n = 0 being the input. One can interpret (1) as a discrete dynamical system in which F plays the role of a coupling function determining the interaction between different nodes of the graph. In particular, we consider local (1-neighborhood) coupling of the form Y i = (F(X, G)) i = F(X i , {{X j?Ni }}) operating on the multiset of 1-neighbors of each node. Examples of such functions used in the graph machine learning literature  are graph convolutions Y i = j?Ni c ij X j (GCN, <ref type="bibr" target="#b28">(Kipf &amp; Welling, 2017)</ref>) and graph attention Y i = j?Ni a(X i , X j )X j (GAT, <ref type="bibr" target="#b52">(Velickovic et al., 2018)</ref>).</p><p>We observe that in (1), at each hidden layer, every node and every feature channel gets updated with exactly the same rate. However, it is reasonable to expect that in realistic graph learning tasks one can encounter multiple rates for the flow of information (node updates) on the graph. Based on this observation, we propose a multi-rate (MR) generalization of (1), allowing updates to each node of the graph and feature channel with different rates,</p><formula xml:id="formula_1">X n = (1 ? ? n ) X n?1 + ? n ?(F ? (X n?1 , G)),<label>(2)</label></formula><p>where ? denotes a v ? m matrix of rates with elements ? ik ? [0, 1]. Rather than fixing ? prior to training, we aim to learn the different update rates based on the node data X and the local structure of the underlying graph G, as follows</p><formula xml:id="formula_2">? n (X n?1 , G) =?(F?(X n?1 , G)),<label>(3)</label></formula><p>whereF? is another learnable 1-neighborhood coupling function, and? is a sigmoidal logistic activation function to constrain the rates to lie within [0, 1]. Since the multi-rate message-passing scheme (2) using (3) does not necessarily prevent oversmoothing (for any choice of the coupling function), we need to further constrain the rate matrix ? n . To this end, we note that the graph gradient of scalar node features y on the underlying graph G is defined as (?y) ij = y j ? y i at the edge i ? j <ref type="bibr" target="#b35">(Lim, 2015)</ref>. Next, we will use graph gradients to obtain the proposed Gradient Gating (G 2 ) framework given b?</p><formula xml:id="formula_3">? n = ?(F ? (X n?1 , G)), ? n ik = tanh ? ? j?Ni |? n jk ?? n ik | p ? ? , X n = (1 ? ? n ) X n?1 + ? n ?(F ? (X n?1 , G)),<label>(4)</label></formula><p>where? n jk ?? n ik = (?? n * k ) ij denotes the graph-gradient and? n * k is the k-th column of the rate matrix? n and p ? 0. Since j?Ni |? n jk ?? n ik | p ? 0 for all i ? V, it follows that ? n ? [0, 1] v?m for all n, retaining its interpretation as a matrix of rates. The sum over the neighborhood N i in (4) can be replaced by any permutation-invariant aggregation function (e.g., mean or max). Moreover, any standard message-passing procedure can be used to define the coupling functions F andF (and, in particular, one can setF = F). As an illustration, <ref type="figure">Fig. 1</ref> shows a schematic diagram of the layer-wise update of the proposed G 2 architecture.</p><p>X n?1 X n <ref type="figure">Figure 1</ref>: Schematic diagram of G 2 (4) showing the layerwise update of the latent node features X (at layer n). The norm of the graph-gradient (i.e., sum in second equation in (4)) is denoted as ? p p .</p><p>The intuitive idea behind gradient gating in (4) is the following: If for any node i ? V local oversmoothing occurs, i.e., lim n?? j?Ni X n i ? X n j = 0, then G 2 ensures that the corresponding rate ? n i goes to zero (at a faster rate), such that the underlying hidden node feature X i is no longer updated. This prevents oversmoothing by early-stopping of the message passing procedure.</p><p>3 Properties of G 2 -GNN G 2 is a flexible framework. An important aspect of G 2 (4) is that it can be considered as a "wrapper" around any specific MPNN architecture. In particular, the hidden layer update for any form of message passing (e.g., <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b52">(Velickovic et al., 2018)</ref>, GIN <ref type="bibr" target="#b54">(Xu et al., 2018)</ref> or GraphSAGE <ref type="bibr" target="#b25">(Hamilton et al., 2017)</ref>) can be used as the coupling functions F,F in (4). By setting ? ? I, (4) reduces to</p><formula xml:id="formula_4">X n = ? F ? (X n?1 , G) ,<label>(5)</label></formula><p>a standard (non-residual) MPNN. As we will show in the following, the use of a non-trivial gradient-gated learnable rate matrix ? allows implementing very deep architectures that avoid oversmoothing.</p><p>Maximum Principle for node features. Node features produced by G 2 satisfy the following Maximum Principle.</p><p>Proposition 3.1. Let X n be the node feature matrix generated by iteration formula (4). Then, the features are bounded as follows:</p><formula xml:id="formula_5">min (?1, ?) ? X n ik ? max (1, ?) , ?1 ? n,<label>(6)</label></formula><p>where the scalar activation function is bounded by ? ? ?(z) ? ? for all z ? R.</p><p>The proof follows readily from writing (4) component-wise and using the fact that 0 ? ? n ik ? 1, for all 1 ? i ? v, 1 ? k ? m and 1 ? n. that interesting properties of GNNs (with residual connections) can be understood by taking the continuous (infinite-depth) limit and analyzing the resulting differential equations.</p><p>In this context, we can derive a continuous version of (4) by introducing a small-scale 0 &lt; ?t &lt; 1 and rescaling the rate matrix ? n to ?t? n leading to</p><formula xml:id="formula_6">X n = (1 ? ?t? n ) X n?1 + ?t? n ? F ? (X n?1 , G) .<label>(7)</label></formula><p>Rearranging the terms in <ref type="formula" target="#formula_6">(7)</ref>, we obtain</p><formula xml:id="formula_7">X n ? X n?1 ?t = ? n ? F ? (X n?1 , G) ? X n?1 .<label>(8)</label></formula><p>Interpreting X n ? X(n?t) = X(t n ), i.e., marching in time, corresponds to increasing the number of hidden layers. Letting ?t ? 0, one obtains the following system of graph-coupled ordinary differential equations (ODEs):</p><formula xml:id="formula_8">dX(t) dt = ? (t) (? (F ? (X(t), G)) ? X(t)) , ?t ? 0, ? ik (t) = tanh ? ? j?Ni |? ik (t) ?? jk (t)| p ? ? , ? (t) =?(F?(X n?1 , G)).<label>(9)</label></formula><p>We observe that the iteration formula (4) acts as a forward Euler discretization of the ODE system (9). Hence, one can follow <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref> and design more general (e.g., higher-order, adaptive, or implicit) discretizations of the ODE system (9). All these can be considered as design extensions of (4).</p><p>Oversmoothing. Using the interpretation of (4) as a discretization of the ODE system (9), we can adapt the mathematical framework recently proposed in <ref type="bibr" target="#b45">Rusch et al. (2022a)</ref> to study the oversmoothing problem. In order to formally define oversmoothing, we introduce the Dirichlet energy defined on the node features X of an undirected graph G as</p><formula xml:id="formula_9">E(X) = 1 v i?V j?Ni X i ? X j 2 .<label>(10)</label></formula><p>Following <ref type="bibr" target="#b45">Rusch et al. (2022a)</ref>, we say that the scheme (9) oversmoothes if the Dirichlet energy decays exponentially fast, E(X(t)) ? C 1 e ?C2t , ?t &gt; 0,</p><p>for some C 1,2 &gt; 0. In particular, the discrete version of (11) implies that oversmoothing happens when the Dirichlet energy, decays exponentially fast as the number of hidden layers increases ((Rusch et al., 2022a) Definition 3.2). Next, one can prove the following proposition further characterizing oversmoothing with the standard terminology of dynamical systems <ref type="bibr" target="#b53">(Wiggins, 2003)</ref>.</p><p>Proposition 3.2. The oversmoothing problem occurs for the ODEs (9) iff the hidden states X * i = c, for all i ? V are exponentially stable steady states (fixed points) of the ODE (9), for some c ? R m .</p><p>In other words, for the oversmoothing problem to occur for this system, all the trajectories of the ODE (9) that start within the corresponding basin of attraction have to converge exponentially fast in time (according to (11)) to the corresponding steady state c. Note that the basins of attraction will be different for different values of c. The proof of this Proposition is a straightforward adaptation of the proof of Proposition 3.3 of <ref type="bibr" target="#b45">Rusch et al. (2022a)</ref>.</p><p>Given this precise formulation of oversmoothing, we will investigate whether and how gradient gating in (9) can prevent oversmoothing. For simplicity, we set m = 1 to consider only scalar node features (extension to vector node features is straightforward). Moreover, we assume coupling functions of the form F(X) = A(X)X, expressed element-wise as (see also <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref>; Rusch et al. <ref type="formula" target="#formula_1">(2022a))</ref>,</p><formula xml:id="formula_11">(F(X)) i = j?Ni A(X i , X j )X j .<label>(12)</label></formula><p>Here, A(X) is a matrix-valued function whose form covers many commonly used coupling functions stemming from the graph attention (GAT, where A ij = A(X i , X j ) is learnable) or convolution operators (GCN, where A ij is fixed). Furthermore, the matrices are right stochastic, i.e., the entries satisfy</p><formula xml:id="formula_12">0 ? A ij ? 1, j?Ni A ij = 1.<label>(13)</label></formula><p>Finally, as the multi-rate feature of (9) has no direct bearing on the oversmoothing problem, we focus on the contribution of the gradient feedback term. To this end, we deactivate the multi-rate aspects and assume that? i = X i for all i ? V, leading to the following form of <ref type="formula" target="#formula_8">(9)</ref>:</p><formula xml:id="formula_13">dX i (t) dt = ? i (t) ? ? ? ? ? j?Ni A ij X j (t) ? ? ? X i (t) ? ? , ?t ? 0, ? i (t) = tanh ? ? j?Ni X j (t) ? X i (t) p ? ? .<label>(14)</label></formula><p>Lack of G 2 can lead to oversmoothing. We first consider the case where the Gradient Gating is switched off by setting p = 0 in (14). This yields a standard GNN in which node features are evolved through message passing between neighboring nodes, without any explicit information about graph gradients. We further assume that the activation function is ReLU i.e., ?(x) = max(x, 0). Given this setting, we have the following proposition on oversmoothing:</p><p>Proposition 3.3. Assume the underlying graph G is connected. For any c ? 0, let X * i ? c, for all i ? V be a (zero-Dirichlet energy) steady state of the ODEs (14). Moreover, assume no Gradient Gating (p = 0 in <ref type="formula" target="#formula_0">(14)</ref>) and</p><formula xml:id="formula_14">A ij (c, c) = A ji (c, c), and A ij (c, c) ? a, 1 ? i, j ? v,<label>(15)</label></formula><p>with 0 &lt; a ? 1 and that there exists at least one node denoted w.l.o.g. with index 1 such that X 1 (t) ? c, for all t ? 0. Then, the steady state X * i = c, for all i ? V, of (14) is exponentially stable.</p><p>Proposition 3.2 implies that without gradient gating (G 2 ), (9) can lead to oversmoothing. The proof, presented in SM C.1 relies on analyzing the time-evolution of small perturbations around the steady state c and showing that these perturbations decay exponentially fast in time (see <ref type="formula" target="#formula_0">(19)</ref>).</p><p>G 2 prevents oversmoothing. We next investigate the effect of Gradient Gating in the same setting of Proposition 3.3. The following Proposition shows that gradient gating prevents oversmoothing:</p><p>Proposition 3.4. Assume the underlying graph G is connected. For any c ? 0 and for all i ? V, let X * i ? c be a (zero-Dirichlet energy) steady state of the ODEs <ref type="formula" target="#formula_0">(14)</ref>. Moreover, assume Gradient Gating (p &gt; 0) and that the matrix A in <ref type="formula" target="#formula_0">(14)</ref> satisfies <ref type="formula" target="#formula_0">(15)</ref> and that there exists at least one node denoted w.l.o.g. with index 1 such that X 1 (t) ? c, for all t ? 0. Then, the steady state X * i = c, for all i ? V is not exponentially stable.</p><p>The proof, presented in SM C.2 clearly elucidates the role of gradient gating by showing that the energy associated with the quasi-linearized evolution equations (SM Eqn. <ref type="formula" target="#formula_1">(20)</ref>) is balanced by two terms (SM Eqn. <ref type="formula" target="#formula_1">(22)</ref>), both resulting from the introduction of gradient gating by setting p &gt; 0 in <ref type="formula" target="#formula_0">(14)</ref>. One of them is of indefinite sign and can even cause growth of perturbations around a steady state c. The other decays initial perturbations. However, the rate of this decay is at most polynomial (SM Eqn. <ref type="formula" target="#formula_1">(27)</ref>). For instance, the decay is merely linear for p = 2 and slower for higher values of p. Thus, the steady state c cannot be exponentially stable and oversmoothing is prevented. This justifies the intuition behind gradient gating, namely, if oversmoothing occurs around a node i, i.e., lim n?? j?Ni X n i ? X n j = 0, then the corresponding rate ? n i goes to zero (at a faster rate), such that the underlying hidden node feature X i stops getting updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we present an experimental study of G 2 on both synthetic and real datasets. We use G 2 with three different coupling functions: GCN (Kipf &amp; Welling, 2017), GAT <ref type="bibr" target="#b52">(Velickovic et al., 2018)</ref> and GraphSAGE <ref type="bibr" target="#b25">(Hamilton et al., 2017)</ref>. The code can be found at https://github.com/tkrusch/gradientgating. Effect of G 2 on Dirichlet energy. Given that oversmoothing relates to the decay of Dirichlet energy (11), we follow the experimental setup proposed by <ref type="bibr" target="#b45">Rusch et al. (2022a)</ref> to probe the dynamics of the Dirichlet energy of Gradient-Gated GNNs, defined on a 2-dimensional 10 ? 10 regular grid with 4-neighbor connectivity. The node features X are randomly sampled from U([0, 1]) and then propagated through a 1000-layer GNN with random weights. We compare GAT, GCN and their gradient-gated versions (G 2 -GAT and G 2 -GCN) in this experiment. <ref type="figure" target="#fig_2">Fig. 2</ref> depicts on log-log scale the Dirichlet energy of each layer's output with respect to the layer number. We clearly observe that GAT and GCN oversmooth as the underlying Dirichlet energy converges exponentially fast to zero, resulting in the node features becoming indistinguishable. In practice, the Dirichlet energy for these architectures is ? 0 after just ten hidden layers. On the other hand, and as suggested by the theoretical results of the previous section, adding G 2 decisively prevents this behavior and the Dirichlet energy remains (near) constant, even for very deep architectures (up to 1000 layers).</p><p>G 2 for very deep GNNs. Oversmoothing inhibits the use of large number of GNN layers. As G 2 is designed to alleviate oversmoothing, it should allow very deep architectures. To test this assumption, we reproduce the experiment considered in <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref>: a node-level classification task on the Cora dataset using increasingly deeper GCN architectures. In addition to   G 2 , we also compare with two recently proposed mechanisms to alleviate oversmoothing, DropEdge <ref type="bibr" target="#b42">(Rong et al., 2020)</ref> and <ref type="bibr">GraphCON (Rusch et al., 2022a)</ref>. The results are presented in <ref type="figure" target="#fig_3">Fig. 3</ref>, where we plot the test accuracy for all the models with the number of layers ranging from 2 to 128. While a plain GCN seems to suffer the most from oversmoothing (with the performance rapidly deteriorating after 8 layers), GCN+DropEdge as well as GCN+GraphCON are able to mitigate this behavior to some extent, although the performance eventually starts dropping (after 16 and 64 layers, respectively). In contrast, G 2 -GCN exhibits a small but noticeable increase in performance for increasing number of layers, reaching its peak performance for 128 layers. This experiment suggests that G 2 can indeed be used in conjunction with deep GNNs, potentially allowing performance gains due to depth. G 2 for multi-scale node-level regression. We test the multi-rate nature of G 2 on node-level regression tasks, where the target node values exhibit multiple scales. Due to a lack of widely available node-level regression tasks, we propose regression experiments based on the Wikipedia article networks Chameleon and Squirrel, <ref type="bibr" target="#b43">(Rozemberczki et al., 2021)</ref>. While Chameleon and Squirrel are already widely used as heterophilic node-level classification tasks, the original datasets consist of continuous node targets (average monthly web-page traffic). We normalize the provided webpage traffic values for every node between 0 and 1 and note that the resulting node values exhibit values on a wide range of different scales ranging between 10 ?5 and 1 (see <ref type="figure">Fig. 4</ref>). <ref type="table" target="#tab_0">Table 1</ref> shows the test normalized mean-square error (mean and standard deviation based on the ten pre-defined splits in <ref type="bibr" target="#b40">Pei et al. (2020)</ref>) for two standard GNN architectures (GCN and GAT) with and without G 2 . We observe from <ref type="table" target="#tab_0">Table 1</ref> that adding G 2 to the baselines significantly reduces the error, demonstrating the advantage of using multiple update rates.</p><formula xml:id="formula_15">10 ?1 E(X n ) G 2 -GCN GCN G 2 -GAT GAT</formula><p>G 2 for varying homophily (Synthetic Cora). We test G 2 on a node-level classification task with varying levels of homophily on the synthetic Cora dataset <ref type="bibr" target="#b59">Zhu et al. (2020)</ref>. Standard GNN models are known to perform poorly in heterophilic settings. This can be seen in <ref type="figure">Fig. 5</ref>, where we present the classification accuracy of GCN and GAT on the synthetic-Cora dataset with a level of homophily varying between 0 and 0.99. While these models succeed in the homophilic case (reaching nearly perfect accuracy), their performance drops to ? 20% when the level of homophily approaches 0. Adding G 2 to GCN or GAT mitigates this phenomenon: the resulting models reach a test accuracy of over 80%, even in the most heterophilic setting, thus leading to a four-fold increase in the accuracy of the underlying GCN or GAT models. Furthermore, we notice an increase in performance even in the homophilic setting. Moreover, we compare with a state-of-the-art model GGCN <ref type="bibr" target="#b55">(Yan et al., 2021)</ref>, which has been recently proposed to explicitly deal with heterophilic graphs. From <ref type="figure">Fig. 5</ref> we observe that G 2 performs on par and slightly better than GGCN in strongly heterophilic settings. Heterophilic datasets. In <ref type="table" target="#tab_1">Table 2</ref>, we test the proposed framework on several real-world heterophilic graphs (with a homophily level of ? 0.30) <ref type="bibr" target="#b40">(Pei et al., 2020;</ref><ref type="bibr" target="#b43">Rozemberczki et al., 2021)</ref> and benchmark it against baseline models GraphSAGE <ref type="bibr" target="#b25">(Hamilton et al., 2017)</ref>, <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b52">(Velickovic et al., 2018)</ref> and MLP <ref type="bibr" target="#b23">(Goodfellow et al., 2016)</ref>, as well as recent state-of-the-art models on heterophilic graph datasets, i.e., GGCN <ref type="bibr" target="#b55">(Yan et al., 2021)</ref>, GPRGNN <ref type="bibr" target="#b12">(Chien et al., 2020b)</ref>, H2GCN <ref type="bibr" target="#b59">(Zhu et al., 2020)</ref>, FAGCN <ref type="bibr" target="#b4">(Bo et al., 2021)</ref>, MixHop (Abu-El-Haija et al., 2019), GCNII , Geom-GCN <ref type="bibr" target="#b40">(Pei et al., 2020)</ref>, PairNorm <ref type="bibr" target="#b57">(Zhao &amp; Akoglu, 2019)</ref>. We can observe that G 2 added to GCN, GAT or GraphSAGE outperforms all other methods (in particular recent methods such as GGCN, GPRGNN, H2GCN that were explicitly designed to solve heterophilic tasks). Moreover, adding G 2 to the underlying base GNN model improves the results on average by 45.75% for GAT, 45.4% for GCN and 18.6% for GraphSAGE. Finally, we wish to highlight that G 2 -GraphSAGE outperforms the current state-of-the-art on the Squirrel dataset by over 16.5%. Large-scale graphs. Given the exceptional performance of G 2 -Graph-SAGE on small and medium sized heterophilic graphs, we test the proposed G 2 (applied to GraphSAGE, i.e., G 2 -GraphSAGE) on large-scale datasets.</p><p>To this end, we consider three different experiments based on large graphs from <ref type="bibr" target="#b33">Lim et al. (2021a)</ref>, which range from highly heterophilic (homophily level of 0.07) to fairly homophilic (homophily level of 0.61). The sizes range from large graphs with ?170K nodes and ?1M edges to a very large graph with ?3M nodes and ?14M edges. <ref type="table" target="#tab_2">Table 3</ref> shows the results of G 2 -GraphSAGE together with other standard GNNs, as well as recent state-of-the-art models, i.e., MLP <ref type="bibr" target="#b23">(Goodfellow et al., 2016)</ref>, <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b52">(Velickovic et al., 2018</ref><ref type="bibr">), MixHop (Abu-El-Haija et al., 2019</ref>, LINK(X) <ref type="bibr" target="#b34">(Lim et al., 2021b)</ref>, GCNII , APPNP <ref type="bibr" target="#b29">(Klicpera et al., 2018)</ref>, GloGNN <ref type="bibr" target="#b31">(Li et al., 2022)</ref>, GPR-GNN <ref type="bibr" target="#b11">(Chien et al., 2020a)</ref> and ACM-GCN <ref type="bibr" target="#b36">(Luan et al., 2021)</ref>. We can see that G 2 -GraphSAGE significantly outperforms current state-of-the-art (by up to 13%) on the two heterophilic graphs (i.e., snap-patents and arXiv-year). Moreover, G 2 -GraphSAGE is on-par with the current state-of-the-art on the homophilic graph dataset genius.</p><p>We conclude that the proposed gradient gating method can successfully be scaled up to large graphs, reaching state-of-the-art performance, in particular on heterophilic graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Gating. Gating is a key component of our proposed framework. The use of gating (i.e., the modulation between 0 and 1) of hidden layer outputs has a long pedigree in neural networks and sequence modeling. In particular, classical recurrent neural network (RNN) architectures such as LSTM <ref type="bibr" target="#b27">(Hochreiter &amp; Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b13">(Cho et al., 2014)</ref> rely on gates to modulate information propagation in the RNN. Given the connections between RNNs and early versions of GNNs <ref type="bibr" target="#b58">(Zhou et al., 2019)</ref>, it is not surprising that the idea of gating has been used in designing GNNs <ref type="bibr" target="#b5">Bresson &amp; Laurent (2017)</ref>; <ref type="bibr" target="#b32">Li et al. (2016)</ref>. However, to the best of our knowledge, the use of local graph-gradients to further modulate gating in order to alleviate the oversmoothing problem is novel, and so is its theoretical analysis.</p><p>Multi-scale methods. The multi-rate gating procedure used in G 2 is a particular example of multi-scale mechanisms. The use of multi-scale neural network architectures has a long history. An early example is <ref type="bibr" target="#b26">Hinton &amp; Plaut (1987)</ref>, who proposed a neural network with each connection having a fast changing weight for temporary memory and a slow changing weight for long-term learning. The classical convolutional neural networks <ref type="bibr">(CNNs, LeCun et al. (1989)</ref>) can be viewed as multi-scale architectures for processing multiple spatial scales in images <ref type="bibr" target="#b3">(Bai et al., 2020)</ref>. Moreover, there is a close connection between our multi-rate mechanism (4) and the use of multiple time scales in recently proposed sequence models such as <ref type="bibr">UnICORNN (Rusch &amp; Mishra, 2021)</ref> and long expressive memory (LEM) <ref type="bibr" target="#b46">(Rusch et al., 2022b)</ref>.</p><p>Neural differential equations. Ordinary and partial differential equations (ODEs and PDEs) are playing an increasingly important role in designing, interpreting, and analyzing novel graph machine learning architectures <ref type="bibr" target="#b2">Avelar et al. (2019)</ref>; <ref type="bibr" target="#b41">Poli et al. (2019)</ref>; <ref type="bibr" target="#b60">Zhuang et al. (2020);</ref><ref type="bibr" target="#b53">Xhonneux et al. (2020)</ref>. <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref> designed attentional GNNs by discretizing parabolic diffusion-type PDEs. Di <ref type="bibr" target="#b16">Giovanni et al. (2022)</ref> interpreted GCNs as gradient flows minimizing a generalized version of the Dirichlet energy. <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref> applied a non-Euclidean diffusion equation ("Beltrami flow") yielding a scheme with adaptive spatial derivatives ("graph rewiring"), and <ref type="bibr" target="#b51">Topping et al. (2021)</ref> studied a discrete geometric PDE similar to Ricci flow to improve information propagation in GNNs. <ref type="bibr">Eliasof et al. (2021)</ref> proposed a GNN framework arising from a mixture of parabolic (diffusion) and hyperbolic (wave) PDEs on graphs with convolutional coupling operators, which describe dissipative wave propagation. Finally, <ref type="bibr" target="#b45">Rusch et al. (2022a)</ref> used systems of nonlinear oscillators coupled through the associated graph structure to rigorously overcome the oversmoothing problem. In line with these works, one contribution of our paper is a continuous version of G 2 (9), which we used for a rigorous analysis of the oversmoothing problem. Understanding whether this system of ODEs has an interpretation as a known physical model is a topic for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have proposed a novel framework, termed G 2 , for efficient learning on graphs. G 2 builds on standard MPNNs, but seeks to overcome their limitations. In particular, we focus on the fact that for standard MPNNs such as GCN or GAT, each node (in every hidden layer) is updated at the same rate. This might inhibit efficient learning of tasks where different node features would need to be updated at different rates. Hence, we equip a standard MPNN with gates that amount to a multi-rate modulation for the hidden layer output in (4). This enables multiple rates (or scales) of flow of information across a graph. Moreover, we leverage local (graph) gradients to further constrain the gates. This is done to alleviate oversmoothing where node features become indistinguishable as the number of layers is increased.</p><p>By combining these ingredients, we present a very flexible framework (dubbed G 2 ) for graph machine learning wherein any existing MPNN hidden layer can be employed as the coupling function and the multi-rate gradient gating mechanism can be built on top of it. Moreover, we also show that G 2 corresponds to a time-discretization of a system of ODEs (9). By studying the (in)-stability of the corresponding zero-Dirichlet energy steady states we rigorously prove that gradient gating can mitigate the oversmoothing problem, paving the way for the use of very deep GNNs within the G 2 framework. In contrast, the lack of gradient gating is shown to lead to oversmoothing.</p><p>We also present an extensive empirical evaluation to illustrate different aspects of the proposed G 2 framework. Starting with synthetic, small-scale experiments, we demonstrate that i) G 2 can prevent oversmoothing by keeping the Dirichlet energy constant, even for a very large number of hidden layers, ii) this feature allows us to deploy very deep architectures and to observe that the accuracy of classification tasks can increase with increasing number of hidden layers, iii) the multi-rate mechanism significantly improves performance on node regression tasks when the node features are distributed over a range of scales, and iv) G 2 is very accurate at classification on heterophilic datasets, witnessing an increasing gain in performance with increasing heterophily.</p><p>This last feature was more extensively investigated, and we observed that G 2 can significantly outperform baselines as well as recently proposed methods on both benchmark medium-scale and large-scale heterophilic datasets, achieving state-of-the-art performance. Thus, by a combination of theory and experiments, we demonstrate that the G 2 -framework is a promising approach for learning on graphs.</p><p>Future work. As future work, we would like to better understand the continuous limit of G 2 , i.e., the ODEs (9), especially in the zero spatial-resolution limit and investigate if the resulting continuous equations have interesting geometric and analytical properties. Moreover, we would like to use G 2 for solving scientific problems, such as in computational chemistry or the numerical solutions of PDEs. Finally, the promising results for G 2 on large-scale graphs encourage us to use it for even larger industrial-scale applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for: Gradient Gating for Deep Multi-Rate Learning on Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional experiments</head><p>In this section, we describe additional empirical results to complement those in the main text.</p><p>On the multi-rate effect of G 2 . Here, we analyze the performance of G 2 on the multi-scale node-level regression task of the main text. As we see in the main text, G 2 applied to GCN or GAT outperforms their plain counterparts (GCN and GAT) on the multi-scale node-level regression task by more than 50% on Chameleon and more than 100% on Squirrel. The question therefore arises whether this better performance can be explained by the multi-rate nature of gradient gating.</p><p>To empirically analyse this, we begin by adding a control parameter ? to G 2 (4) as follows,</p><formula xml:id="formula_16">X n = (1 ? (? n ) ? ) X n?1 + (? n ) ? ?(F ? (X n?1 , G)).</formula><p>Clearly, setting ? = 1 recovers the original gradient gating message-passing update,</p><formula xml:id="formula_17">X n = (1 ? ? n ) X n?1 + ? n ?(F ? (X n?1 , G)),</formula><p>while setting ? = 0 disables any explicit multi-rate behavior and a plain message-passing scheme is recovered, X n = ?(F ? (X n?1 , G)).</p><p>Note that by continuously changing ? from 0 to 1 controls the level of multi-rate behavior in the proposed gradient gating method. In <ref type="figure">Fig. 6</ref> we plot the test NMSE of the best performing G 2 -GCN and G 2 -GAT on the Chameleon multi-scale node-level regression task for increasing values of ? ? [10 ?3 , 1] in log-scale. We can see that the test NMSE monotonically decreases (lower error means better performance) for both G 2 -GCN and G 2 -GAT for increasing values of ?, i.e., increasing level of multi-rate behavior. We can conclude that the multi-rate behavior of G 2 is instrumental in successfully learning multi-scale regression tasks.  <ref type="figure">Figure 6</ref>: Test NMSE on the multi-scale chameleon node-level regression task of G 2 -GCN and G 2 -GAT for continuously decreasing level of multi-rate behavior. On the sensitivity of performance of G 2 to the hyperparameter p. The proposed gradient gating model implicitly depends on the hyperparameter p, which defines the multiple rates ? , i.e., ? n =?(F ? (X n?1 , G)),</p><formula xml:id="formula_18">? n ik = tanh ? ? j?Ni |? n ik ?? n jk | p ? ? .</formula><p>While any value p &gt; 0 can be used in practice, a standard hyperparameter tuning procedure (see B for the training details) on p has been applied in every experiment included in this paper. Thus, it is natural to ask how sensitive the performance of G 2 is with respect to different values of the hyperparameter p.</p><p>To answer this question, we trained different G 2 -GraphSAGE models on the Texas as well as the Wisconsin graph datasets for different values of p ? [1, 5]. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the resulting performance of G 2 -GraphSAGE. We can see that different values of p do not significantly change the performance of the model. However, including the hyperparameter p to the hyperparameter fine-tuning procedure will further improve the overall performance of G 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details</head><p>All small and medium-scale experiments have been run on NVIDIA GeForce RTX 2080 Ti, GeForce RTX 3090, TITAN RTX and Quadro RTX 6000 GPUs. The large-scale experiments have been run on Nvidia Tesla A100 (40GiB) GPUs.</p><p>All hyperparameters were tuned using random grid search. <ref type="table" target="#tab_3">Table 4</ref> shows the rounded hyperparameter p in G 2 (4) of each best performing network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Mathematical Details</head><p>In this section, we provide proofs for Propositions 3.3 and 3.4 in the main text. We start with the following technical result which is necessary in the subsequent proofs.</p><p>A Poincare Inequality on Connected Graphs. Poincare inequalities for functions <ref type="bibr" target="#b18">(Evans, 2010)</ref> bound function values in terms of their gradients. Similar bounds on node values in terms of graph-gradients can be derived and a particular instance is given below, Proposition C.1. Let G = (V, E) be a connected graph and the corresponding (scalar) node features are denoted by y i ? R, for all i ? V. Let y 1 = 0. Then, the following bound holds,</p><formula xml:id="formula_19">i?V y 2 i ? d? 1 i?V j?Ni |y j ? y i | 2 ,<label>(16)</label></formula><p>where d = max i?V deg(i) and ? 1 is the eccentricity of the node 1.</p><p>Proof. Fix a node i ? V. By assumption, the graph G is connected. Hence, there exists a path connecting i and the node 1. Denote the shortest path as P(i, 1). This path can be expressed in terms of the nodes i,1 with 0 ? ? ?, where 0 i,1 = 1 and ? i,1 = i. For any , we require i,1 ? ( + 1) i,1 . Moreover, ? i,1 is the graph distance between the nodes i and 1 and ? 1 = max i?V ? i,1</p><p>is the eccentricity of the node 1. Given the node feature y i , we can rewrite it as,</p><formula xml:id="formula_20">y i = y 1 + ??1 =0 y ( +1)i,1 ? y i,1 = ??1 =0 y ( +1)i,1 ? y i,1 ,</formula><p>as by assumption y 1 = 0.</p><p>Using Cauchy-Schwartz inequality on the previous identity yields,</p><formula xml:id="formula_21">y 2 i ? ? 1 ??1 =0 y ( +1)i,1 ? y i,1 2 .</formula><p>Summing the above inequality over i ? V and using the fact that i,1 ? ( + 1) i,1 , we obtain the desired Poincare inequality (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Proposition 3.3 of Main Text</head><p>Proof. By the definition of exponential stability, we consider a small perturbation around the steady state c and study whether this perturbation grows or decays in time. To this end, define the perturbation as,X</p><formula xml:id="formula_22">i = X i ? c, 1 ? i ? v.<label>(17)</label></formula><p>A tedious but straightforward calculation shows that these perturbations evolve by the following linearized system of ODEs,</p><formula xml:id="formula_23">dX i (t) dt = j?Ni A ij (c, c) X j ?X i , ?t, ?i ? V.<label>(18)</label></formula><p>Multiplyingx i to both sides of (18) yields,</p><formula xml:id="formula_24">X i dX i (t) dt = j?Ni A ij (c, c)X i X j ?X i , ? dX 2 i (t) dt = j?Ni A ij (c, c) X 2 j ?X 2 i ? j?Ni A ij (c, c) X j ?X i 2 .</formula><p>Summing the above identity over all nodes i ? V yields,</p><formula xml:id="formula_25">d dt i?VX 2 i (t) = i?V j?Ni A ij (c, c) X 2 j ?X 2 i ? i?V j?Ni A ij (c, c) X j ?X i 2 = 1 2 i?V j?Ni (A ij (c, c) ? A j,i (c, c)) =0 (15) X 2 j ?X 2 i ? 1 2 i?V j?Ni (A ij (c, c) + A j,i (c, c)) =2Aij (15) X j ?X i 2 , = ? i?V j?Ni A ij (c, c) X j ?X i 2 , ? ?a i?V j?Ni X j ?X i 2 , (by (15)), ? ? a d? 1 i?VX 2 i .</formula><p>Here, the last inequality comes from applying the Poincare inequality (16) for the perturbationsX and from the fact that by assumptionX 1 = 0. Applying Gr?nwall's inequality yields,</p><formula xml:id="formula_26">i?VX 2 i (t) ? i?VX 2 i (0)e ? a d? 1 t .<label>(19)</label></formula><p>Thus, the initial perturbations around the steady state c are damped down exponentially fast and the steady state c is exponentially stable implying that this architecture will lead to oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Proposition 3.4 of Main Text</head><p>Proof. As in the proof of Proposition 3.3, we consider small perturbations of form (17) of the steady state c and investigate how these perturbations evolve in time. Assuming that the initial perturbations are small, i.e., that there exists an 0 &lt; &lt;&lt; 1 such that max i?V |x i (0)| ? , we perform a straightforward calculation to obtain that the perturbations (for a short time) evolve with the following quasi-linearized system of ODEs,</p><formula xml:id="formula_27">dX i (t) dt = ? i (t) j?Ni A ij (c, c) X j ?X i , ?i ? V, ? i (t) = j?Ni |X j (t) ?X i (t)| p , ?i ? V.<label>(20)</label></formula><p>Note that we have used the fact that ? (x) = 1 and tanh (0) = 1 in obtaining <ref type="formula" target="#formula_1">(20)</ref> from <ref type="formula" target="#formula_0">(14)</ref>. Next, we multiplyx i to both sides of (20) to obtain,</p><formula xml:id="formula_28">X i dX i (t) dt = j?Ni A ij (c, c)? iXi X j ?X i , ? dX 2 i (t) dt = j?Ni A ij (c, c)? i X 2 j ?X 2 i ? j?Ni A ij (c, c)? i X j ?X i 2<label>(21)</label></formula><p>Trivially,</p><formula xml:id="formula_29">|X j ?X i | p ? ? i , ?j ? N i , ?i.</formula><p>Applying this inequality to the last line of the identity <ref type="formula" target="#formula_0">(21)</ref>, we obtain,</p><formula xml:id="formula_30">dX 2 i (t) dt ? j?Ni A ij (c, c)? i X 2 j ?X 2 i ? j?Ni A ij (c, c) X j ?X i p+2 .</formula><p>Summing the above inequality over i ? V leads to, <ref type="formula" target="#formula_0">(15)</ref>).</p><formula xml:id="formula_31">d dt i?VX 2 i (t) ? i?V j?Ni A ij (c, c)? i X 2 j ?X 2 i ? i?V j?Ni A ij (c, c) X j ?X i p+2 ? 1 2 i?V j?Ni A ij (c, c) (? i ? ? j ) X 2 j ?X 2 i (A ij = A j,i ) ? a i?V j?Ni X j ?X i p+2 (from</formula><p>Therefore, we have the following inequality,</p><formula xml:id="formula_32">d dt i?VX 2 i (t) ? T 1 ? T 2 , T 1 = 1 2 i?V j?Ni A ij (c, c) (? i ? ? j ) X 2 j ?X 2 i T 2 = a i?V j?Ni X j ?X i p+2 .<label>(22)</label></formula><p>We analyze the differential inequality (22) by starting with the term T 1 in (22). We observe that this term does not have a definite sign and can be either positive or negative. However, we can upper bound this term in the following manner. Given that the right-hand side of the ODE system (20) is Lipschitz continuous, the well-known Cauchy-Lipschitz theorem states that the solutionsx Remark C.2. We note that the Proposition 3.4 assumes a certain structure of the matrix A in (14). A careful perusal of the proof presented above reveal that this assumptions can be further relaxed. To start with, if the matrix A(c, c) is not symmetric, then there will be an additional term in the inequality <ref type="formula" target="#formula_1">(22)</ref>, which would be proportional to A ij ? A ji . This term will be of indefinite sign and can cause further growth in the perturbations of the steady state c. In any case, it can only further destabilize the quasi-linearized system. The assumption that the entries of A are uniformly positive amounts to assuming positivity of the weights of the underlying GNN layer. This can be replaced by requiring that the corresponding eigenvalues are uniformly positive. If some eigenvalues are negative, this will cause further instability and only strengthen the conclusion of lack of (exponential) stability. Finally, the assumption that one node is not perturbed during the quasi-linearization is required for the Poincare inequality (16). If this is not true, an additional term, of indefinite sign, is added to the inequality <ref type="formula" target="#formula_1">(22)</ref>. This term can cause further growth of the perturbations and will only add instability to the system. Hence, all the assumptions in Proposition 3.4 can be relaxed and the conclusion of lack of exponential stability of the zero-Dirichlet energy steady state still holds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Continuous limit of G 2 . It has recently been shown (see Avelar et al. (2019); Poli et al. (2019); Zhuang et al. (2020); Xhonneux et al. (2020); Chamberlain et al. (2021a); Eliasof et al. (2021); Chamberlain et al. (2021b); Topping et al. (2021); Rusch et al. (2022a) and references therein)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Dirichlet energy E(X n ) of layer-wise node features X n propagated through a GAT, GCN and their gradient gated versions (G 2 -GAT, G 2 -GCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Test accuracies of GCN with gradient gating (G 2 -GCN) as well as plain GCN and GCN combined with other methods on the Cora dataset for increasing number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Histogram of the target node values of the Chameleon and Squirrel node-level regression tasks. Test accuracy of GCN and GAT with / without gradient gating (G 2 ) on synthetic Cora with a varying level of true label homophily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Test accuracies of G 2 -GraphSAGE on Texas and Wisconsin graph datasets for varying values of p in (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Normalized test MSE on multiscale node-level regression tasks. -GAT 0.136 ? 0.029 0.069 ? 0.029</figDesc><table><row><cell></cell><cell>Chameleon</cell><cell>Squirrel</cell></row><row><cell>#Nodes</cell><cell>2,277</cell><cell>5,201</cell></row><row><cell>#Edges</cell><cell>31,421</cell><cell>198,493</cell></row><row><cell cols="3">GCN G 2 -GCN 0.137 ? 0.033 0.070 ? 0.028 0.207 ? 0.039 0.143 ? 0.039 GAT 0.207 ? 0.038 0.143 ? 0.039 G 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on heterophilic graphs. The three best performing methods are highlighted in red (First), blue (Second), and violet (Third). -GraphSAGE 87.57 ? 3.86 87.84 ? 3.49 37.14 ? 1.01 64.26 ? 2.38 71.40 ? 2.38 86.22 ? 4.90</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Film</cell><cell>Squirrel</cell><cell>Chameleon</cell><cell>Cornell</cell></row><row><cell>Hom level</cell><cell>0.11</cell><cell>0.21</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>0.30</cell></row><row><cell>#Nodes</cell><cell>183</cell><cell>251</cell><cell>7,600</cell><cell>5,201</cell><cell>2,277</cell><cell>183</cell></row><row><cell>#Edges</cell><cell>295</cell><cell>466</cell><cell>26,752</cell><cell>198,493</cell><cell>31,421</cell><cell>280</cell></row><row><cell>#Classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>GGCN GPRGNN H2GCN FAGCN MixHop GCNII Geom-GCN PairNorm GraphSAGE GCN GAT MLP G 2 -GAT G 2 -GCN G 2</cell><cell cols="6">84.86 ? 4.55 86.86 ? 3.29 37.54 ? 1.56 55.17 ? 1.58 71.14 ? 1.84 85.68 ? 6.63 78.38 ? 4.36 82.94 ? 4.21 34.63 ? 1.22 31.61 ? 1.24 46.58 ? 1.71 80.27 ? 8.11 84.86 ? 7.23 87.65 ? 4.98 35.70 ? 1.00 36.48 ? 1.86 60.11 ? 2.15 82.70 ? 5.28 82.43 ? 6.89 82.94 ? 7.95 34.87 ? 1.25 42.59 ? 0.79 55.22 ? 3.19 79.19 ? 9.79 77.84 ? 7.73 75.88 ? 4.90 32.22 ? 2.34 43.80 ? 1.48 60.50 ? 2.53 73.51 ? 6.34 77.57 ? 3.83 80.39 ? 3.40 37.44 ? 1.30 38.47 ? 1.58 63.86 ? 3.04 77.86 ? 3.79 66.76 ? 2.72 64.51 ? 3.66 31.59 ? 1.15 38.15 ? 0.92 60.00 ? 2.81 60.54 ? 3.67 60.27 ? 4.34 48.43 ? 6.14 27.40 ? 1.24 50.44 ? 2.04 62.74 ? 2.82 58.92 ? 3.15 82.43 ? 6.14 81.18 ? 5.56 34.23 ? 0.99 41.61 ? 0.74 58.73 ? 1.68 75.95 ? 5.01 55.14 ? 5.16 51.76 ? 3.06 27.32 ? 1.10 31.52 ? 0.71 38.44 ? 1.92 60.54 ? 5.30 52.16 ? 6.63 49.41 ? 4.09 27.44 ? 0.89 36.77 ? 1.68 48.36 ? 1.58 61.89 ? 5.05 80.81 ? 4.75 85.29 ? 3.31 36.53 ? 0.70 28.77 ? 1.56 46.21 ? 2.99 81.89 ? 6.40 84.59 ? 5.55 87.65 ? 4.64 37.30 ? 0.87 46.48 ? 1.41 64.12 ? 1.96 87.30 ? 4.84 84.86 ? 3.24 87.06 ? 3.19 37.09 ? 1.16 39.62 ? 2.91 55.83 ? 2.88 86.49 ? 5.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on large-scale datasets.</figDesc><table><row><cell></cell><cell cols="2">snap-patents arXiv-year</cell><cell>genius</cell></row><row><cell>Hom level</cell><cell>0.07</cell><cell>0.22</cell><cell>0.61</cell></row><row><cell>#Nodes</cell><cell>2,923,922</cell><cell>169,343</cell><cell>421,961</cell></row><row><cell>#Edges</cell><cell>13,975,788</cell><cell>1,166,243</cell><cell>984,979</cell></row><row><cell>#Classes</cell><cell>5</cell><cell>5</cell><cell>2</cell></row><row><cell>MLP GCN GAT MixHop LINKX LINK GCNII APPNP GloGNN GPR-GNN ACM-GCN</cell><cell cols="3">31.34 ? 0.05 45.65 ? 0.04 45.37 ? 0.44 52.16 ? 0.09 61.95 ? 0.12 56.00 ? 1.34 90.77 ? 0.27 36.70 ? 0.21 86.68 ? 0.09 46.02 ? 0.26 87.42 ? 0.37 46.05 ? 0.51 55.80 ? 0.87 51.81 ? 0.17 90.58 ? 0.16 60.39 ? 0.07 53.97 ? 0.18 73.56 ? 0.14 37.88 ? 0.69 47.21 ? 0.28 90.24 ? 0.09 32.19 ? 0.07 38.15 ? 0.26 85.36 ? 0.62 62.09 ? 0.27 54.68 ? 0.34 90.66 ? 0.11 40.19 ? 0.03 45.07 ? 0.21 90.05 ? 0.31 55.14 ? 0.16 47.37 ? 0.59 80.33 ? 3.91</cell></row><row><cell>G</cell><cell></cell><cell></cell><cell></cell></row></table><note>2 -GraphSAGE 69.50 ? 0.39 63.30 ? 1.84 90.85 ? 0.64</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Rounded hyperparameter p in G 2 of each best performing network.</figDesc><table><row><cell></cell><cell cols="9">Texas Wisconsin Film Squirrel Chameleon Cornell snap-patents arXiv-year genius</cell></row><row><cell>G 2 -GAT</cell><cell>3.06</cell><cell>1.68</cell><cell>1.23</cell><cell>3.48</cell><cell>3.54</cell><cell>3.54</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>G 2 -GCN</cell><cell>3.93</cell><cell>2.92</cell><cell>3.79</cell><cell>1.99</cell><cell>1.08</cell><cell>3.87</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>G 2 -GraphSAGE</cell><cell>4.47</cell><cell>1.14</cell><cell>2.89</cell><cell>3.04</cell><cell>2.00</cell><cell>3.27</cell><cell>1.60</cell><cell>3.40</cell><cell>4.40</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>The research of TKR and SM was performed under a project that has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant Agreement No. 770880). MM would like to acknowledge the IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. MB is supported in part by ERC Grant No. 724228 (LEMAN).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>depend continuously on the initial data. Given that max i?V |X i (0)| ? &lt;&lt; 1 and the bounds on the hidden states (1), there exists a time t &gt; 0 such that</p><p>Using the definitions of ? and the right stochasticity of the matrix A, we easily obtain the following bound,</p><p>where d = max i?V deg(i).</p><p>On the other hand, the term T 2 in (22) is clearly positive. Hence, the solutions of resulting</p><p>will clearly decay in time. The key question is whether or not the decay is exponentially fast. We answer this question below.</p><p>To this end, we have the following calculation using the H?lder's inequality, i?V j?Ni</p><p>Observing thatX 1 = 0 by assumption, we can applying the Poincare inequality (16) in the above inequality to further obtain,</p><p>Hence, from the definition of T 2 (22), we have,</p><p>Therefore, the differential inequality (24) now reduces to,</p><p>The differential inequality (26) can be explicitly solved to obtain,</p><p>From <ref type="formula">(27)</ref>, we see that the initial perturbations decay but only algebraically at a rate of t ? 2 p in time. For instance, the decay is only linear in time for p = 2 and even slower for higher value of p.</p><p>Combining the analysis of the terms T 1,2 in the differential inequality <ref type="formula">(22)</ref>, we see that the one of the terms can lead to a growth in the initial perturbations whereas the second term only leads to polynomial decay. Even if the contribution of the term T 1 ? 0, the decay of initial perturbations is only polynomial. Thus, the steady state c is not exponentially stable.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Discrete and continuous deep residual learning over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H C</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GRAND: graph neural diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">I</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1407" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Beltrami flow and neural diffusion on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Traffic Prediction with Graph Neural Networks in Google Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10991</idno>
		<title level="m">Graph neural networks as gradient flows</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Eliasof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Treister</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Utilizing graph machine learning within drug discovery and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothish</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrude</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Jeremy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNN</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the cognitive science society</title>
		<meeting>the ninth annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caihua</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07308</idno>
		<title level="m">Finding global homophily in graph neural networks when meeting heterophily</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lek-Heng</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Hodge laplacians on graphs. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Is heterophily a real nightmare for graph neural networks to do node classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: all we have is low pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unicornn: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9168" to="9178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph-coupled oscillator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="18888" to="18909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long expressive memory for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Roch</forename><surname>Vlimant</surname></persName>
		</author>
		<title level="m">Graph neural networks in particle physics. Machine Learning: Science and Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21001</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Encoding labeled graphs by labeling RAAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonina</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14522</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Introduction to nonlinear dynamical systems and chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Louis-Pascal A C Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Continuous graph neural networks. In ICML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<title level="m">Graph neural networks: a review of methods and applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ordinary differential equations on graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
