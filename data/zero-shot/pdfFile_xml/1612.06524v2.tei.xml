<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore 3D human pose estimation from a single RGB image. While many approaches try to directly predict 3D pose from image measurements, we explore a simple architecture that reasons through intermediate 2D pose predictions. Our approach is based on two key observations (1) Deep neural nets have revolutionized 2D pose estimation, producing accurate 2D predictions even for poses with self-occlusions (2) "Big-data"sets of 3D mocap data are now readily available, making it tempting to "lift" predicted 2D poses to 3D through simple memorization (e.g., nearest neighbors). The resulting architecture is straightforward to implement with off-the-shelf 2D pose estimation systems and 3D mocap libraries. Importantly, we demonstrate that such methods outperform almost all state-of-theart 3D pose estimation systems, most of which directly try to regress 3D pose from 2D measurements.</p><p>Inferring 3D structure from 2D correspondences is also a 1 arXiv:1612.06524v2 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inferring 3D human pose from image measurements is classic task in computer vision, dating back to the iconic work of Hogg <ref type="bibr" target="#b10">[11]</ref> and O'Rourke and Badler <ref type="bibr" target="#b21">[22]</ref>. Such a technology has immediate applications in various tasks such as action understanding, surveillance, human-robot interaction, and motion capture, to name a few. As such, it has a long and storied history. We refer the reader to various surveys for a broad overview of the popular topic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Previous approaches often make use of a highly sensored environment, including video streams <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>, multiview cameras <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, depth images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27]</ref>. In this work, we focus on the "pure" and challenging setting of recovering 3D body pose with a single 2D RGB image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Our key insight to the problem is leveraging recent advances in 2D image understanding, made possible through the undeniable impact of deep learning. While originally explored for coarse recognition tasks such as image classification, recent methods have extended such network architectures to "fine-grained" human pose estimation, where the task is formulated as one of 2D heatmap Output 3D Pose <ref type="figure">Figure 1</ref>. Overview of our approach for 3D pose estimation: given an input image, first estimate a 2D pose and then estimate its depth by matching to a library of 3D poses. The final prediction is given by the colored skeleton, while the ground-truth is shown in gray. Our approach works surprisingly well because 2D pose estimation is accurate even during occlusions (as illustrated by both wrists above), suggesting that 2D pose estimates need only be refined by adding depth values.</p><p>prediction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>. One of the long standing challenges in 2D human pose estimation has been estimating poses under self-occlusions. Indeed, reasoning about occlusions has been one of the underlying motivations for working in a 3D coordinate frame rather than 2D. But one of our salient conclusions is that state-of-the-art methods do a surprisingly good job of 2D pose estimation even under occlusion. Given this observation, the remaining challenge is predicting depth values for the estimated 2D joints.</p><p>well-studied problem in computer vision, often addressed in multiview setting as structure from motion. In the context of monocular human pose estimation, the relevant cues seem to be semantic rather than geometric. One can estimate 3D postures from a 2D skeleton based on high-level knowledge derived from anthropometric, kinematic, and dynamic constraints. Inspired by the success of data-driven architectures, we explore a simple non-parametric encoding of such high-level constraints: given a 3D pose library, we generate a large number of 2D projections (from virtual camera views). Given this training set of paired (2D,3D) data and predictions from a 2D pose estimation algorithm, we report back the depths from the 3D pose associated with the closest matching 2D example from our library. Our entire pipeline is summarized in <ref type="figure">Fig. 1</ref>. Generalization: One desirable property of our twostage approach is generalization. Due to the difficulty of annotation in 3D, training datasets with 3D labels are typically collected in a lab environment, while 2D datasets tend to be more diverse. Our two-stage pipeline makes use of different training sets for different stages, resulting a system that can predict 3D poses from "in-the-wild" images.</p><p>Evaluation: Though we present qualitative results on in-the-wild-imagery, we also perform an extensive quantitative evaluation of our method on widely benchmarked 3D human-pose datasets, such as Human3.6M <ref type="bibr" target="#b12">[13]</ref>. We follow standard train/test protocol splits, but our analysis reveals that there has been inconsistent reporting in the literature, both in terms of test sets and evaluation criteria. To make our results as transparent as possible, we report performance for all metrics and splits we could find. One of our surprising findings is the impressive performance of our simple pipeline: we outperform essentially all prior work on all metrics. Our entire pipeline, even given the non-parametric matching step, returns a 3D pose given a 2D image in under 200ms (160ms for 2D estimation by a CNN, 26ms for exemplar matching with a training library of 200,000 poses). Finally, to promote future progress, we perform an exhaustive analysis of additional baselines with upper bounds that reveal the continued benefit of working with intermediate 2D representations and data-driven encoding of 3D constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Here we review related works on 3D human pose prediction most relevant to our approach.</p><p>(Deep) Regression: Most existing work that makes use of deep features tends to formulate the problem as a direct 2D image to 3D pose regression task. Li et al. <ref type="bibr" target="#b16">[17]</ref> use deep learning to train a regression model to predict 3D pose directly from images. Tekin et al. <ref type="bibr" target="#b29">[30]</ref> integrate spatiotemporal features via an image sequence to learn regression model for 3D pose mapping. We provide both a theoretical and empirical analysis that suggests that 2D pose may be a useful intermediate representation.</p><p>Intermediate 2D pose: Other approaches have explored pipelines that use 2D poses as an intermediate result. Most focus on the second-stage that lifts 2D estimates to 3D. This is classically treated as a constrained optimization problem who's objective minimizes the 2D reprojection error of an unknown 3D pose and unknown camera <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2]</ref>. The optimization problem is often subject to kinematic constraints <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29]</ref>, and sometimes 3D poses are assumed to live a in low-dimensional subspace to better condition the optimization <ref type="bibr" target="#b36">[37]</ref>. Such optimization-based approaches could be sensitive to initialization and local minima, and often require expensive constrained solvers. We use datadriven matching, that when combined with a simple closedform warping algorithm, yields a fast and accurate 3D solution.</p><p>Exemplar-based: Previous work has also explored example-based methods, dating back at least to <ref type="bibr" target="#b25">[26]</ref>. A central challenge is generalization to novel poses outside the training set. <ref type="bibr" target="#b13">[14]</ref> propose matching upper and lower bodies individually, to allow for novel compositions at test-time. <ref type="bibr" target="#b34">[35]</ref> adapt exemplars to better match image measurements with an energy minimization approach. <ref type="bibr" target="#b24">[25]</ref> synthesize new 2D images with image-based rendering. Other methods also warp 3D exemplars to 2D image descriptors, often based on shape-context <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> or silhouette features <ref type="bibr" target="#b4">[5]</ref>. In our work, we show that a modest number of exemplars (200,000), combined with a simple closed-form algorithm for warping a 3D exemplar to exactly project to 2D pose estimates, outperforms more complex methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we describe our method for estimating 3D human pose given a single RGB image. We make use of a probabilistic formulation over variables including the image I, the 3D pose X ? R N ?3 , and the 2D pose x ? R N ?2 , where N is the number of articulated joints. We write the joint probability as:</p><formula xml:id="formula_0">p(X, x, I) = p(X|x, I) ? p(x|I) ? p(I)<label>(1)</label></formula><p>where the above makes no limiting assumptions by itself.</p><p>Conditional independence: Let us now assume that the 3D pose X is conditionally independent of image I given the 2D pose x. This is equivalent to the implication that given a 2D skeleton, the prediction of its corresponding 3D skeleton would not be affected by 2D image measurements. While this is not quite true (we show a counter example in <ref type="figure" target="#fig_2">Fig. 2)</ref>, it seems to be a reasonable first-order approximation. Moreover, this factorization still allows for p(x|I) to be arbitrarily complex, which is likely needed to accurately model complex interactions between 2D projections  We show the output of our system given the ground-truth 2D pose, with the (incorrect) best-matching 3D exemplar on the right (visualized from a novel viewpoint, where the estimated camera is drawn as a view frustum). Our experiments suggest that such cases are rare, and that much of the time 3D can be inferred from 2D projections. and image features during occlusions. Given this conditional Independence, one can write:</p><formula xml:id="formula_1">p(X, x, I) = p(X|x) NN ? p(x|I) CNN ?p(I)<label>(2)</label></formula><p>We tackle the second term with a image-based CNN that predicts 2D keypoint heatmaps. We tackle the first term with a non-parametric nearest-neighbor (NN) model. We describe each term in turn below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image-Based 2D Pose Estimation</head><p>Given the above Independence assumption, we would first like to predict 2D pose given image measurements. We model the conditional of 2D pose given an image as</p><formula xml:id="formula_2">P (x|I) = CN N (I)<label>(3)</label></formula><p>where we assume CNN is a nonlinear function that returns N 2D heatmaps (or marginal distributions over the location of individual joints). We make use of convolutional pose machines (CPMs) <ref type="bibr" target="#b32">[33]</ref>, which return precisely N heatmaps for individual body joints. We normalize the heatmaps so that they can be interpreted as marginal distributions for each joint. CPM is a near-state-of-the-art pose estimation system (88.5% PCKh on MPII dataset <ref type="bibr" target="#b3">[4]</ref>, quite close to the state-of-the-art value of 90.9% <ref type="bibr" target="#b20">[21]</ref>). Note the offthe-shelf CPM model was trained on MPII dataset, which is a somewhat limited dataset in that annotations are provided through manual inspection. We fine-tune this model on the large scale Human3.6M <ref type="bibr" target="#b12">[13]</ref> training set, which contains annotations acquired by a mocap system (allowing for larger-scale labeling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Nonparametric 3D shape model</head><p>We model P (X|x) with a non-parametric nearest neighbor model. We will follow a notational convention where X = [X, Y, Z] and x = [x, y]. Assume that we have library X i X i * <ref type="figure">Figure 3</ref>. On the left, we show the 3D exemplar X i that best matches the ground-truth 2D pose x. While the overall pose is roughly correct, the arms and legs are bent incorrectly. By simply copying the depth values from the exemplar (and copying the (x, y) values from the 2D pose under a weak perspective model, as given in <ref type="formula">(6)</ref>), we can obtain a warped exemplar X * i that better matches the 2D pose. of 3D poses {X i } paired with a particular camera projection matrix {M i }, such that the associated 2D poses are given by {M i (X i )}. If we want to consider multiple cameras for a single 3D pose, we add another copy of the 3D pose with a different camera matrix to our library. We define a distribution over 3D poses based on reprojection error:</p><formula xml:id="formula_3">P (X = X i |x) ? e ? 1 ? 2 ||Mi(Xi)?x|| 2 (4)</formula><p>where the MAP estimate is given by the 1-nearest neighbor (1NN). We explore two extensions to the above basic framework.</p><p>Virtual cameras: We can further reduce the squared reprojection error by searching over small perturbations of each camera. This involves solving a camera resectioning problem <ref type="bibr" target="#b8">[9]</ref>, where an iterative solver can be initialized with M i :</p><formula xml:id="formula_4">M * i = argmin M ||M (X i ) ? x|| 2<label>(5)</label></formula><p>In practice, we construct a shortlist of k candidates that score well according to <ref type="bibr" target="#b3">(4)</ref>, and resort them according to optimal camera matrix. We found that optimizing over cameras produced a small but noticeable improvement in our experiments. Unless otherwise specified, we choose k = 10 in our experiments. Warped exemplars: Much previous work on exemplars introduce methods for warping exemplars to better match the 2D pose estimates, often formulated as an inverse kinematics optimization problem. We describe an extremely lightweight method for doing so here. We first align the 3D exemplar to the camera-coordinate system used to compute the projection x. This is done with a 3D rigid transformation given by the camera extrinsics encoded in M i (or M * i ). In practice we use a training set {X i } where 3D exemplars are already aligned to their projections {x i }, implying that extrinsics in M i reduce to an identity matrix (which is the case for the Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref>, since 3D poses are specified in camera coordinates of their associated image projections). Given this alignment, we simply replace the (X i , Y i ) exemplar coordinates with their scaled 2D counterparts (x, y) under a weak perspective camera model:</p><formula xml:id="formula_5">X * i = sx sy Z i , where s = average(Z i ) f (6)</formula><p>where f is the focal length of the camera (given by the intrinsics in M i ) and average(Z i ) is the average depth of the 3D joints. Such weak-perspective approximations are commonly used to initialize algorithms for perspective (PnP) camera calibration <ref type="bibr" target="#b17">[18]</ref>, and will be reasonable when the depth variation of the human skeleton is small relative to the overall distance to the camera. Our results suggest that such closed-form solutions for 3D warping rival the accuracy of complex energy minimization methods (see <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In our experiments, we test a variety of variations of our proposed pipeline.</p><p>Qualitative results: We first present some qualitative results. <ref type="figure" target="#fig_4">Fig. 4</ref> shows results on challenging examples from subject S11 of Human3.6M. We choose examples with selfocclusions and sitting poses. To demonstrate the accuracy of the 3D predictions, we visualize novel viewpoints. We then apply the proposed method on Leeds Sports Pose(LSP) dataset <ref type="bibr" target="#b14">[15]</ref> to test cross-dataset generalization. We posit that our pipeline will generalize across image variation (due to the underlying robustness of our 2D pose estimation system) but maybe limited in the 3D estimates due to the library used (from Human3.6M). Importantly, our approach produces plausible 3D poses even when the activity class is not included in Human3.6M. This implies that our method can reliably estimate 3D poses in the wild!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation protocols</head><p>We use Human3.6M for quantitative evaluation and analysis. It appears that multiple train/test splits have been used in the literature, as well as different approaches to computing mean per joint position error (MPJPE), measured in millimeters. We summarize them here.</p><p>Protocol 1: In <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, the entire dataset was partitioned into six training subjects (S1, S5, S6, S7, S8, S9), and one testing subject (S11). Evaluation is performed on every 64th frame of S11's video clips. In this configuration, there are total 1.8 million 3D poses available in the training set. MPJPE between the ground truth 3D pose and the estimated 3D pose is computed by first aligning poses with a rigid transformation <ref type="bibr" target="#b15">[16]</ref>.</p><p>Protocol 2: Others <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> use five subjects (S1, S5, S6, S7, S8) for training, and two subjects (S9, S11) for testing. We follow <ref type="bibr" target="#b36">[37]</ref>'s setup that downsamples the videos from 50 fps to 10 fps. Here, MPJPE is evaluated without a rigid transformation, following the original h36m protocol: both the ground-truth and predicted 3D pose is centered with respect to a root joint (ie. pelvis). In contrast to Protocol 1, this evaluation can be sensitive to a single poorly-predicted joint, particularly if it is the root <ref type="bibr" target="#b12">[13]</ref>.</p><p>To compare to published performance numbers, we use the appropriate protocol as needed. From our own experience, we find Protocol 1 to be simpler and more intuitive, and so focus on it for our diagnostic evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to state-of-the-art (Protocol 1)</head><p>Final system: <ref type="table" target="#tab_0">Table 1</ref> compare MPJPE for each activity class. Our approach clearly outperforms <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b24">[25]</ref>. ("Ours" in the tables of comparison throughout the experiment refers to the warped exemplar X * described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>Performance given ground-truth 2D: A common diagnostic is evaluating performance given ground-truth 2D poses, written as gt. <ref type="table">Table 2</ref> shows that our simple matching + warping outperforms <ref type="bibr" target="#b34">[35]</ref>, who use a complex iterative algorithm for matching and warping exemplars to image evidence. Our diagnostics will later show that even matching exemplars without warping outperforms prior art, indicating the remarkable power of a simple NN baseline.</p><p>Size of trainset: <ref type="table">Table 3</ref> shows the MPJPE versus the training data size. Since approaches deal with 2D and 3D sources differently, we list both sizes. Yasin et al. <ref type="bibr" target="#b34">[35]</ref> project multiple 2D poses from each 3D exemplar (with virtual cameras) to create 2D poses for matching, and Rogez et al. <ref type="bibr" target="#b24">[25]</ref> directly synthesize 2D images for training. Our approach makes use of the default training data in Hu-man3.6M, where each 3D pose is paired with a single 2D projection. We max out performance with a modest pose library of 180k 3D-2D pairs, but produce competitive accuracy even for 18k. The slight increase in MPJPE for larger training sets seems to be related to noise from 2D pose estimation, since we observe a monotonically decrease when ground truth 2D poses are given <ref type="figure">(Fig. 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-art (Protocol 2)</head><p>Final system: <ref type="table">Table 4</ref> provides the comparison to <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b29">[30]</ref> using Protocol 2. Note that in both these works, temporal smoothness was exploited by taking a short image sequences as input. Even though we do not use temporal information, our system is quite close to state-of-the-art. A qualitative comparison to <ref type="bibr" target="#b36">[37]</ref> is also provided in <ref type="figure">Fig. 5</ref>.</p><p>Performance given ground-truth 2D: Our strong performance in <ref type="figure">Fig. 5</ref> might be attributed to better 2D pose estimation. Therefore, we investigate the case given ground truth 2D pose, following Zhou's diagnostic protocol <ref type="bibr" target="#b36">[37]</ref>: evaluate MPJPE up to a 3D rigid body transformation including scale, only on the first 30 seconds of the first cam-  era in Human3.6M. For a fair comparison, we make use the same training set of 3D-2D training data for both methods. The results are shown in <ref type="table" target="#tab_3">Table 5</ref>. With a shortlist of k = 10 matches, camera resectioning (5) and exemplar warping (6) produces a slightly lower error than <ref type="bibr" target="#b36">[37]</ref>'s approach without a 3D prior. Qualitative results are provided in <ref type="figure">Fig. 6</ref>. Our approach produces lower 2D reprojection error, while Zhou's method appears to suffers from the restriction of 3D poses to a low-dimensional subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Diagnostics</head><p>We now perform an extensive set of diagnostics to reveal the strength of our individual components, as well as upperbound analysis that is useful for guiding future work. For simplicity, we restrict ourselves to Protocol 1.  <ref type="bibr" target="#b34">[35]</ref> by Protocol 1 given 2D ground truth. Our approach is clearly state-of-the-art, indicating the effectiveness of our simple approach to NN matching and warping.  <ref type="table">Table 3</ref>. Comparison to <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b24">[25]</ref> under different amounts of training data, under Protocol 1. Our approach yields the best performance at the source size of 180k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Per Joint Position Error (MPJPE), in mm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of warping:</head><p>We evaluate the benefits of warping (X * vs X) in <ref type="table">Table 6</ref>. It is clear that warping exemplars X * is a simple and effective approach to reducing error. Quite surprisingly, even without warping, simply matching to a set of 3D exemplar projections outperforms the state-of-theart (see <ref type="table" target="#tab_0">Table 1</ref> &amp; <ref type="table">Table 6</ref>)! To analyze an upper-bound for our warping approach, we combine 2D estimates (x, y) with depth values Z given by the ground-truth 3D pose Z GT . In the last row, the performance of combining ground truth depth Z GT with x is listed as a reference baseline. This suggests that one can still dramatically lower error by 2X even when continuing to use the output of current 2D pose estimation systems.</p><p>Warping given ground-truth 2D: Next, we compute the error for the case that ground truth 2D pose is given, as shown in <ref type="table" target="#tab_1">Table 7</ref>. We write |gt to emphasize that methods now have access to 2D ground-truth pose estimates. We first note that matching unwarped examples rivals the accuracy of state-of-the-art (see <ref type="table" target="#tab_1">Table 2 &amp; Table 7</ref>). This again suggests the remarkable power a simple NN baseline based on matching 2D projections. That said, warping still improves results by a considerable margin. A qualitative example is provided in <ref type="figure">Fig. 3</ref>.</p><p>Warping given optimal exemplar match: It is natural to ask what is the upper-bound on performance given our training set of (3D,2D) pairs. We first compute the optimal exemplar that minimizes 3D reprojection error (up to a rigid body transformation) to the true 3D test pose. We write the index of this best match from the training set as i = GT . We would like to see the effect of warping given this optimal match. We analyze this combination in <ref type="table">Table 8</ref>. This suggests that, in principle, error can still be significantly reduced (by almost 2X) even given our fixed library of 3D poses. However, it is not clear that this is obtainable given our pipeline because it may require image evidence to select this optimal 3D exemplar (violating the conditional independence assumption from <ref type="formula" target="#formula_1">(2)</ref>).</p><p>Effect of trainset size: An important aspect to investigate is the influence of database size. Here we investigate the error versus the number of exemplars in the database. <ref type="figure">Fig. 7</ref> evaluates performance versus a random fraction of our overall database. As expected, more data results in lower error, though diminishing results are observed (even in log scale). This is reasonable since training data is extracted from videos captured at 50 fps, implying that correlations over frames might limit the benefit of additional frames. We see that convergence is also effected by the quality of the 2D pose estimates: error given ground-truth 2D poses plateaus at 5 ? 10 5 , while 2D pose estimates plateau even sooner at 2 ? 10 5 . We posit that a more restricted 3D pose prior (implicitly enforced by a small  <ref type="table">Table 4</ref>. Comparison to <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b29">[30]</ref> by Protocol 2. Our results are close to state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zhou's results Our results</head><p>Zhou's results Our results <ref type="figure">Figure 5</ref>. Qualitative comparison of Zhou <ref type="bibr" target="#b36">[37]</ref> with our results. Our results are generally more accurate, but both methods struggle with left/right limb ambiguities (e.g., the second row). While much of our improved performance comes from better 2D pose estimation, we still compare favorably when using the same groundtruth 2D pose estimates ( <ref type="figure">Fig. 6</ref> and <ref type="table" target="#tab_3">Table 5</ref>).</p><p>randomly-sampled 3D library) helps given inaccurate 2D pose estimates. But in either case, exemplar-based 3D matching is effective even for modestly-size training sets <ref type="bibr">(200,</ref><ref type="bibr">000)</ref>. This analysis appears to suggest that better 2D pose estimates are needed to take advantage of "bigger" 3D datasets.</p><p>Since the joint prediction error is not a normal distribution, we also plot median error in <ref type="figure">Fig. 8</ref>. We see that the median is generally lower than mean error, and the difference between the two becomes smaller when ground truth 2D or 3D is given. This may suggest that errors are often  <ref type="figure">Figure 6</ref>. Qualitative comparison of Zhou <ref type="bibr" target="#b36">[37]</ref> with our results, given access to the same ground-truth 2D pose. While both 3D estimates are plausible, Zhou's tends to produce higher 2D reprojection error since 3D poses are restricted to lie in a subspace (e.g., the incorrectly-articulated head).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Avg. MPJPE Ramakrishna <ref type="bibr" target="#b23">[24]</ref>|gt, multi-frame 89.50 Dai <ref type="bibr" target="#b5">[6]</ref>|gt, multi-frame 72.98 Zhou <ref type="bibr" target="#b36">[37]</ref>|gt, single-frame 50.04 Zhou <ref type="bibr" target="#b36">[37]</ref>|gt, multi-frame 49.64 X * |gt, k = 1, single-frame 51.06 X * |gt, k = 10, single-frame 49.55  <ref type="table">Table 6</ref>. Given the predicted 2D pose x, warped exemplars X * outperform unwarped exemplars X by a reasonable margin. We also compute an upper-bound for warped exemplars that uses (x, y) estimates from the predicted pose and z estimates from the groundtruth 3D pose. The dramatic error reduction suggests that significant further improvement is possible by improving upon our 3D matching. Importantly, this improvement is realizable even given existing 2D pose estimation systems.</p><p>due to a single incorrect joint prediction, which would significantly impact average error but not the median.  <ref type="table" target="#tab_1">Table 7</ref>. We compare matching to exemplars X and warped exemplars X * given ground-truth 2D pose estimates. This suggests that our simple closed-form warping approach would be even more effective with better 2D pose estimates.</p><p>Prediction Avg. Median X|GT 60.11 55.36 X * |GT 37.32 33.91 <ref type="table">Table 8</ref>. We analyze performance given the optimal matching 3D training exemplar "GT" (in terms of 3D error wrt the ground-truth test 3D pose). Simply reporting this optimal match produces an error of 60mm, around 10mm lower than the actual match found given an ideal 2D pose-estimation system ( <ref type="table" target="#tab_1">Table 7)</ref>. Warping this exemplar X * |GT significantly improves accuracy. This suggests that our overall 3D matching stage could still be significantly improved even given the current size of the library of 3D poses. <ref type="figure">Figure 7</ref>. Mean MPJPE by Protocol 1 versus size of the 3D pose library. We explore diagnostic variants using previously-introduced notation. In general, MPJPE decreases with a larger library. The error saturates at 2 ? 10 5 when using CNN-predicted 2D poses "|x", but saturates later at 5 ? 10 5 when using ground-truth 2D poses "|gt". The results suggest that with better 2D pose estimates, our exemplar matching would benefit from larger training data.</p><p>ization of exemplar-matching, <ref type="table">Table 9</ref> quantitatively evaluates accuracy on HumanEva-I <ref type="bibr" target="#b27">[28]</ref> given a model trained on Human3.6M. These results suggest that the 3D exemplars from HumanEva do generalize, and that generalization is significantly improved through our warping procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an simple approach to 3D human pose estimation by performing 2D pose estimation, followed by 3D exemplar matching. The simplicity and efficiency of our method, combined with its state-of-the-art performance on both benchmark datasets and unconstrained "in-the-wild" <ref type="figure">Figure 8</ref>. Median MPJPE by Protocol 1 versus database size. Median error is lower than mean error in <ref type="figure">Fig. 7</ref>, suggesting that a few joints are responsible for a large mean error. Other trends follow the mean error curves from <ref type="figure">Fig. 7</ref>  <ref type="table">Table 9</ref>. We evaluate a Human3.6M-trained model on HumanEva (under Protocol 1). To isolate the impact of 3D matching, we use ground-truth 2D keypoints. As a point of comparison, average error on Human3.6M test is 70.93 (unwarped) and 57.5 (warped) ( <ref type="table">Table 9</ref>). These results suggest that 3D exemplars do generalize across datasets, and importantly, warping significantly increases the amount of generalization. Note that the two datasets use different definitions of skeletons, implying that learning a mapping should reduce error even further.</p><p>imagery, suggests that such simple baselines should be used for future benchmarking in 3D pose estimation. A notable advantage of intermediate 2D representations is modular training -2D datasets (which are typically larger and more diverse because of ease of annotation) can be used to train the initial-image processing module, while 3D motion capture data can be used to train the subsequent 3D-reasoning module. This allows our system to take immediate advantage of advances in 2D pose estimation, such as multi-body analysis <ref type="bibr" target="#b6">[7]</ref>. Our results also suggest that 3D inference is, in some sense, "all about 2D", at least in the case of articulated objects. Indeed, one of our surprising findings was the high performance of 2D pose estimation systems even under occlusions, suggesting that 2D estimates can in fact be reliably estimated without directly reasoning about depth. Given such reliable 2D estimates, we show that one can efficiently impute depth through simple memorization and warping of a 3D pose library.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Input image with the 2D pose estimation by CPM The retrieved 3D pose using the 2D pose as query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>A failure case where the 3D pose is not conditionally independent of the image given the 2D pose: p(X|x, I) = p(X|x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>We show qualitative results on Human3.6M-test (top) and LSP-test (bottom). Our method produces plausible results for challenging images with self-occlusions and extreme poses, and can generalize to activities and poses not in the train set (Human3.6M-train).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Zhou</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison to<ref type="bibr" target="#b34">[35]</ref> by Protocol 1. Our results are clearly state-of-the-art. Please see text for more details.</figDesc><table><row><cell>Method</cell><cell cols="2">Direction Discuss</cell><cell>Eat</cell><cell>Greet</cell><cell>Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell><cell>SitDown</cell></row><row><cell>Yasin [35]</cell><cell>88.4</cell><cell>72.5</cell><cell cols="2">108.5 110.2</cell><cell>97.1</cell><cell>81.6</cell><cell>107.2</cell><cell>119.0</cell><cell>170.8</cell></row><row><cell>Rogez [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>71.63</cell><cell>66.60</cell><cell cols="2">74.74 79.09</cell><cell>70.05</cell><cell>67.56</cell><cell>89.30</cell><cell>90.74</cell><cell>195.62</cell></row><row><cell>Method</cell><cell>Smoke</cell><cell>Photo</cell><cell cols="4">Wait Walk WalkDog WalkPair</cell><cell>Avg.</cell><cell>Median</cell><cell>-</cell></row><row><cell>Yasin [35]</cell><cell>108.2</cell><cell>142.5</cell><cell>86.9</cell><cell>92.1</cell><cell>165.7</cell><cell>102.0</cell><cell>108.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Rogez [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>83.46</cell><cell>93.26</cell><cell cols="2">71.15 55.74</cell><cell>85.86</cell><cell>62.51</cell><cell>82.72</cell><cell>69.05</cell><cell>-</cell></row><row><cell>Method</cell><cell cols="2">Direction Discuss</cell><cell>Eat</cell><cell>Greet</cell><cell>Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell><cell>SitDown</cell></row><row><cell>Yasin [35]</cell><cell>60.0</cell><cell>54.7</cell><cell>71.6</cell><cell>67.5</cell><cell>63.8</cell><cell>61.9</cell><cell>55.7</cell><cell>73.9</cell><cell>110.8</cell></row><row><cell>X  *  |gt (Ours)</cell><cell>53.27</cell><cell>46.75</cell><cell cols="2">58.63 61.21</cell><cell>55.98</cell><cell>58.13</cell><cell>48.85</cell><cell>55.60</cell><cell>73.41</cell></row><row><cell>Method</cell><cell>Smoke</cell><cell>Photo</cell><cell cols="4">Wait Walk WalkDog WalkPair</cell><cell>Avg.</cell><cell>Median</cell><cell>-</cell></row><row><cell>Yasin [35]</cell><cell>78.9</cell><cell>96.9</cell><cell>67.9</cell><cell>47.5</cell><cell>89.3</cell><cell>53.4</cell><cell>70.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>60.25</cell><cell>76.05</cell><cell cols="2">62.19 35.76</cell><cell>61.93</cell><cell>51.08</cell><cell>57.50</cell><cell>51.93</cell><cell>-</cell></row></table><note>Table 2. Comparison to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>shows that even simple NN matching produces an average accuracy of 70.93,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>3D pose estimation accuracy given ground-truth 2D poses, under Protocol 2. Here, k is the number of candidate exemplar extracted in the shortlist that are subsequently processed by searching over virtual cameras. Our single-frame results with k = 10 outperforms all prior art, including those which make use of multiframe temporal cues.</figDesc><table><row><cell cols="3">Prediction Avg. Median</cell></row><row><cell>X|x</cell><cell>85.52</cell><cell>75.04</cell></row><row><cell>X  *  |x</cell><cell>82.72</cell><cell>69.05</cell></row><row><cell cols="2">[sx Z GT ] 43.86</cell><cell>30.19</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose from silhouettes by relevance vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple prior-free method for non-rigid structure-from-motion factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Computational Studies of Human Motion: Tracking and Motion Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view 3d human pose estimation in complex environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Model-based vision: a program to see a walking person. Image and Vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down reasoning with convolutional latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05699</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d human pose reconstruction using millions of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BMVC</title>
		<meeting>the BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and globally convergent pose estimation from video images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mjolsness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A survey of computer vision-based human motion capture. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-based image analysis of human motion using constraint propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;rourke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR Workshops</title>
		<meeting>the IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling 3d human poses from uncalibrated monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I. Dong</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
