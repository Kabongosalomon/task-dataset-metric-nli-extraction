<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Paliwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libing</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Kalantari</surname></persName>
						</author>
						<title level="a" type="main">Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computational Photography</term>
					<term>Raw Video Denoising</term>
					<term>Gradient Mask</term>
					<term>Adversarial Training</term>
					<term>Multi-Stage architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a learning-based approach for denoising raw videos captured under low lighting conditions. We propose to do this by first explicitly aligning the neighboring frames to the current frame using a convolutional neural network (CNN). We then fuse the registered frames using another CNN to obtain the final denoised frame. To avoid directly aligning the temporally distant frames, we perform the two processes of alignment and fusion in multiple stages. Specifically, at each stage, we perform the denoising process on three consecutive input frames to generate the intermediate denoised frames which are then passed as the input to the next stage. By performing the process in multiple stages, we can effectively utilize the information of neighboring frames without directly aligning the temporally distant frames. We train our multi-stage system using an adversarial loss with a conditional discriminator. Specifically, we condition the discriminator on a soft gradient mask to prevent introducing high-frequency artifacts in smooth regions. We show that our system is able to produce temporally coherent videos with realistic details. Furthermore, we demonstrate through extensive experiments that our approach outperforms state-of-the-art image and video denoising methods both numerically and visually.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C APTURING videos of dynamic scenes in low-light environments, such as at night or indoors, is inevitable. Videography in these situations is challenging because the number of photons arriving at camera sensors is low, resulting in severe noise in the captured videos. In addition to reduction in visual quality, the existence of noise negatively affects the subsequent video processing tasks. Hence, video denoising plays an important role in video processing, especially under low lighting conditions.</p><p>With decades of studies in image denoising, a large number number of non-learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and learning-based techniques <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> have been developed to remove additive white Gaussian noise (AWGN). However, the noise in final sRGB images is transformed from raw images through an image signal processor (ISP) pipeline and is more complicated than AWGN. Therefore, the performance of these approaches on real-world images is limited. The most recent approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> address this issue by using more sophisticated noise models and performing the denoising on raw images.</p><p>While these approaches can successfully denoise a single image, they are not effective at denoising videos, producing results with severe flickering artifacts. This is mainly because they do not take the information of neighboring frames into consideration. A few approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> propose to tackle this application by considering the neighboring frames when removing AWGN from an sRGB video frame, but their performance on real-world videos is sub-optimal.</p><p>Most recently, a couple of approaches address this problem by performing the denoising on raw video frames. Chen et al. <ref type="bibr" target="#b24">[25]</ref> propose to train a single frame denoising network on static videos and enforce the results of consecutive frames to be similar. However, their method does not utilize the information of adjacent frames and, thus, severe flickering artifacts still occur in videos with large motions. Yue et al. <ref type="bibr" target="#b25">[26]</ref> incorporate the information of neighboring frames by implicitly aligning their features using deformable convolutions <ref type="bibr" target="#b26">[27]</ref>. However, this alignment component cannot be directly supervised as it registers the feature maps implicitly. Therefore, their approach is not able to effectively use the information of neighboring frames, producing results with blurriness and flickering.</p><p>To address this problem, we propose a novel learningbased approach that denoises each frame by explicitly aligning the neighboring frames. Specifically, we first align the neighboring frames to the current frame and then fuse the registered frames to obtain the final denoised frame. Furthermore, since aligning the temporally distant neighboring frames is difficult, we propose to perform the two processes of alignment and fusion in multiple stages. At each stage, we use three consecutive frames to denoise the middle frame. By repeating this process in each stage, we can effectively utilize the information of multiple neighboring frames to produce each denoised frame. With our multistage approach, the alignment is always performed between the immediate neighboring and current frames, thereby alleviating the difficulty of aligning temporally distant frames.</p><p>We train our multi-stage system in an end to end manner on a large set of synthetically generated noisy and clean videos. To encourage the network to produce sharp frames with realistic details, we propose to train the network using an adversarial loss function. Furthermore, to prevent the network from producing high-frequency artifacts in the smooth regions, we condition the discriminator on a soft gradient mask.</p><p>We demonstrate the superiority of our framework through comprehensive comparisons against the state-ofthe-art denoising methods on a variety of synthetic and real scenes. Moreover, we show the effectiveness of our design choices through extensive ablation studies.</p><p>Our contributions are three-fold:</p><p>? A multi-stage flow-based video denoiser which can effectively utilize the information of multiple neighboring frames to help denoise the center frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose to train our network in an adversarial manner using a discriminator which is conditioned on a soft gradient mask. Our system produces results with realistic details without introducing highfrequency artifacts in the smooth regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We demonstrate that our approach outperforms state-of-the-art image and video denoising methods on a variety of real and synthetic test scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the image and video denoising methods, including non-learning and learningbased approaches. Since we use an adversarial loss to train our network, we also provide a brief overview of generative adversarial networks (GANs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Denoising</head><p>The problem of image denoising plays an important role in image processing and computer vision and has been the subject of extensive research in the past. Traditional approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref> reconstruct a clean image from a noisy image by taking advantage of either local statistics or self-similarity of the input image. In recent years, learning-based algorithms <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> have significantly advanced the field by harnessing the power of deep neural networks. However, the aforementioned approaches perform denoising on sRGB images for removing AWGN and are not able to generalize to real-world images. To address this problem, Chen et al. <ref type="bibr" target="#b15">[16]</ref> train a network on pairs of real noisy and clean raw images. Specifically, they capture the noisy image in low lighting conditions and obtain the ground truth clean image by capturing a long exposure image of the scene. Guo et al. <ref type="bibr" target="#b32">[33]</ref> and Brooks et al. <ref type="bibr" target="#b16">[17]</ref> train their networks on synthetic raw images generated by unprocessing existing sRGB images. To generate the input, they accurately model the camera noise and add it to the synthetically generated raw images. Although these methods demonstrate promising performance on image denoising, they are not able to effectively denoise videos as they do not exploit the information of neighboring frames. Therefore, they are generally not able to produce temporally coherent results.</p><p>Several approaches propose to generate a denoised image from burst images <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. These approaches utilize the information of multiple images to reconstruct a single denoised image and could be potentially used for generating denoised videos. However, these burst photography algorithms are mainly designed for input images with small motion, while the motion in typical videos are generally large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Denoising</head><p>Compared to image denoising, video denoising is a more challenging problem which is relatively under-explored. Existing non-learning video denoising methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> generate results by grouping similar patches and then jointly filtering them. In recent years, learningbased video denoising methods have demonstrated promising results. Chen et al. <ref type="bibr" target="#b19">[20]</ref> use a simple fully connected RNN to perform video denoising on sRGB frames. Xue et al. <ref type="bibr" target="#b20">[21]</ref> propose to handle this application by aggregating information from registered frames using learned task-oriented flows. We also explicitly register the neighboring frames, but we do so through a multi-stage system to be able to effectively align the temporally distant frames. Claus et al. <ref type="bibr" target="#b21">[22]</ref> propose a CNN video denoiser using a combination of spatial and temporal filtering without prior knowledge of the noise distribution. Tassano et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> propose a two-step cascaded architecture without explicit motion estimation. We also propose to perform the denoising in multiple stages, but explicitly align the neighboring images to effectively utilize their information. Moreover, all the denoising blocks in our multi-stage denoiser use shared weights, reducing the number of trainable parameters in our system.</p><p>Unfortunately, these approaches operate on the sRGB images and are specifically designed to remove AWGN from sRGB images, reducing their performance on realworld videos. Recently, a few methods propose to perform denoising on raw videos to be able to effectively denoise real-world videos. Chen et al. <ref type="bibr" target="#b24">[25]</ref> propose to train a single frame denoising network on low-light static videos with the long exposure image of the same scene as ground truth. To achieve temporal coherency, they enforce the results of consecutive frames to be similar. However, their method does not utilize the information of adjacent frames and, thus, severe flickering artifacts still occur in videos with large motions. Yue et al. <ref type="bibr" target="#b25">[26]</ref> propose to use deformable convolutions <ref type="bibr" target="#b26">[27]</ref> to implicitly align features in neighboring frames. However, directly supervising the alignment component is not possible as the feature maps are implicitly registered. Therefore, their approach still produces results with blurriness and flickering due to the failure of effectively using the information of neighboring frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generative Adversarial Networks</head><p>Since the introduction of generative adversarial network (GAN) by Goodfellow et al. <ref type="bibr" target="#b41">[42]</ref>, it has been widely used to produce visually convincing images for computer vision tasks, such as image synthesis <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, image translation <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, and image super-resolution <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. More related to our application, Chen et al. <ref type="bibr" target="#b14">[15]</ref> leverage a GAN to model the noise in real images. GANs have also been used to remove noise from Monte Carlo rendered images <ref type="bibr" target="#b52">[53]</ref>, tomography images <ref type="bibr" target="#b53">[54]</ref>, and real images <ref type="bibr" target="#b54">[55]</ref>. In this paper, we use adversarial loss for video denoising and propose to condition the discriminator on a soft gradient to reduce the high-frequency artifacts in the smooth areas.  <ref type="figure">Fig. 1</ref>. The multi-stage denoiser progressively denoises raw noisy sequence I 0 [t?N :t+N ] until we obtain the final denoised raw frame I N t . The ISP <ref type="bibr" target="#b25">[26]</ref> converts the raw frame, I N t , to sRGB, L N t . Note that, each denoiser in our system, generates the output from three consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>The goal of our system is to denoise a real raw video sequence, while preserving the sharp features of the scene. To do this, we generate each denoised frame, I d t , by utilizing the information of 2N + 1 noisy consecutive frames, I n [t?N :t+N ] , where N &gt; 1. We separate the raw bayer pattern of each input image into an image with four channels <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, before passing it to our denoiser. Once a denoised raw frame is obtained using our system, we generate the sRGB frame through the pretrained image signal processor (ISP) by Yue et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>We divide the video denoising task into two stages of alignment and fusion, and model them using two convolutional neural networks (CNNs). During alignment, the neighboring frames are aligned with the reference frame. The aligned images are then fused to synthesize the denoised output during the fusion stage. To effectively utilize the information of the neighboring frames, we perform the denoising process in multiple stages. We train our networks using an adversarial loss with a conditional discriminator to ensure producing high-quality videos with realistic details. The overview of our system is shown in <ref type="figure">Fig. 1</ref> and we discuss each stage in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Stage Denoiser</head><p>As discussed, we would like to generate the denoised frame, I d t , by first aligning the 2N neighboring frames to the central frame, I n t , and then fusing the 2N + 1 registered frames. However, as the temporal distance between the neighboring and reference frames increases, the alignment quality deteriorates due to complex large object and camera motion (see <ref type="figure" target="#fig_3">Fig. 3</ref>).</p><p>We address this problem by proposing a multi-stage denoiser which denoises the input sequence by processing a smaller sub-sequence of overlapping inputs at each stage, as shown in <ref type="figure">Fig. 1</ref>. In our system, the entire sequence is progressively denoised, gradually improving the denoised result until we obtain the final denoised frame. Specifically, at each stage, we use three consecutive frames, , are recursively used as input to the next stage until we obtain the final denoised frame at the N th stage, I N t = I d t . Note that, by performing the denoising process in N stages, the denoised frame is obtained by combining the information of 2N + 1 neighboring frames.</p><formula xml:id="formula_0">I i?1 [k?1:k+1] ,</formula><p>In this paper, we generate the results using a two-stage denoiser and, thus, utilize the information of 5 (2N + 1 where N = 2) consecutive frames to reconstruct each denoised frame. The first stage outputs the intermediate denoised sequence</p><formula xml:id="formula_1">I 1 [t?1:t+1] from the noisy input sequence I 0 [t?2:t+2] .</formula><p>The intermediate sequence with three frames is again fed to the denoiser to obtain the final denoised frame,</p><formula xml:id="formula_2">I d t .</formula><p>Our approach can effectively utilize the information of temporally distant frames by performing the alignment and fusion on only three consecutive frames at multiple stages. Note that, Tassano et al. <ref type="bibr" target="#b23">[24]</ref> also propose a two-stage denoiser, however, each denoising block in their approach is modeled by a single network. In contrast, we break down the denoising block into two stages of alignment and fusion. By explicitly aligning the neighboring neighboring frames we are able to utilize their information more effectively. Furthermore, unlike Tassano et al., we use denoisers (flow and fusion networks) with shared weights across different stages. By doing so, we reduce the number of trainable parameters and implicitly ensure that the input frames are denoised progressively at multiple stages. Below we discuss the two stages of each denoising block, i.e., alignment and fusion.</p><p>Alignment The goal of alignment is to register the three consecutive input frames by generating a pair of flows, F k?k?1 and F k?k+1 , from the reference frame, I i k , to the previous and next frame, I i k?1 and I i k+1 , respectively. The flows are then used to warp the two neighboring frames to produce a set of aligned frames. We use an iterative flow network architecture, proposed by Teed et al. <ref type="bibr" target="#b55">[56]</ref>, which essentially generates a coarse flow in the first pass and then enhances it in following passes. This flow network calculates a 128 channel contextual map at 1/8 resolution for the reference frame to assist with flow estimation. We upsample this contextual map by a factor of 8 (using Teed et al.'s learned upsampling module) and pass it to the fusion network so that the network is cognizant of a scene's context such as object boundaries. A flow generated by our network along with a warped image is shown in <ref type="figure">Fig. 4 (left)</ref>.</p><p>Fusion Here, we merge the output of alignment network,? i [k?1:k+1] , with the help of reference frame's contextual information, to generate the denoised frame, I i+1 k . For fusion, we utilize a residual CNN comprised of recursive residual group (RRG) blocks, proposed by Zamir et al. <ref type="bibr" target="#b17">[18]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The RRG block utilizes attention blocks <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> to identify important features, while discarding trivial information like artifacts caused by inaccurate warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss</head><p>Most of the existing denoising techniques use a pixel-wise loss <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b58">[59]</ref>, e.g., L 1 , to train their system. Unfortunately, while a network trained using a pixel-wise loss is able to remove the noise, it does so by blurring </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-stage</head><p>Multi-stage Ground Truth out the details. Therefore, we instead propose to train our system with an adversarial loss to reconstruct denoised videos with realistic details, while effectively denoising the smooth regions. Our adversarial loss is defined as follows:</p><formula xml:id="formula_3">min G max D L adv (G, D) + ? f L feat (G, D) + ? r L recn (G) + ? p L prcp (G) ,<label>(1)</label></formula><p>where D and G denote the discriminator and generator, respectively. We use the network proposed by Wang et al. <ref type="bibr" target="#b59">[60]</ref> as our discriminator and the generator is our multistage denoiser. Moreover, L adv , L feat , L recn , and L prcp are adversarial, feature, reconstruction, and perceptual losses, respectively. Finally, the corresponding weights are set to ? f = 1e ?2 , ? r = 1e ?1 , and ? p = 5e ?3 . Next, we discuss each loss in detail. Adversarial Loss L adv An adversarial loss adds realistic details to the output, thus, giving a natural feel to the synthesized videos. Ideally, we want to enhance the details in the textured areas, while effectively denoising smooth areas. However, by using a na?ve adversarial loss, our system introduces high-frequency artifacts in the smooth areas of the generated frames. This is mainly because the discriminator in this case, interprets the high-frequency artifacts in the smooth areas as real textures.</p><p>To avoid this issue, we propose to condition the discriminator on a soft mask that identifies the textured areas (see <ref type="figure">Fig. 4</ref>). In this way, the discriminator is able to detect the generated frames with high-frequency artifacts in smooth areas as fake, which consequently encourages the generator to remove such artifacts.</p><p>To obtain the mask, we first compute vertical and horizontal gradients of the ground truth grayscale frame. We then filter the two gradients using a box filter of size 3, before computing the gradient magnitude, f . Finally, we  <ref type="figure">Fig. 4</ref>. On the left, we show two consecutive noisy frames and the estimated flow from frame 1 to 2. We also show the warped version of frame 2 using the estimated flow. Note that, we estimate the flow and perform the warping on raw images, however, we convert the warped image to sRGB domain for visualization. On the right, we show a ground truth frame with its corresponding gradient mask, f .</p><p>obtain the gradient mask f by highlighting the gradients as follows:</p><formula xml:id="formula_4">f = tanh(f /?),<label>(2)</label></formula><p>where ? is a normalizing parameter and is set to ? = 0.8 ? 2 in our experiments. This map is used along with the fake and real images as inputs to the discriminator, guiding the network to enhance details in the highlighted regions, while effectively denoising the smooth regions. In our system, we use the hinge adversarial loss <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, defined as follows:</p><formula xml:id="formula_5">L D = ? E[min(0, ?1 + D(L c t , f ))] ? E[min(0, ?1 ? D(G(I n [t?N :t+N ] ), f ))], L G = ? ? g E[D(G(I n [t?N :t+N ] ), f )],<label>(3)</label></formula><p>where L c t is the clean ground truth frame (L denotes frames in sRGB domain), f is the ground truth gradient mask, and ? g is the weight of the generator term which we set to 5e ?5 .</p><p>Feature Loss L feat We use a feature loss proposed by Wang et al. <ref type="bibr" target="#b59">[60]</ref> to stabilize the adversarial training. It is defined as:</p><formula xml:id="formula_6">L feat = T i=1 1 N i D i (L c t , f ) ? D i (G(I n [t?N :t+N ), f ) 1 , (4)</formula><p>where D i (.) and T denote the discriminator's i th -layer's output and total number of discriminator layers, respectively. We use the 4-layer discriminator defined by Wang et al. <ref type="bibr" target="#b59">[60]</ref> with spectral normalization <ref type="bibr" target="#b61">[62]</ref>. Reconstruction loss L recn We use an L 1 reconstruction loss in both sRGB and raw domains with a weighting parameter, ? = 0.5, defined as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VBM4D Noisy</head><p>Ours GT FastDVDnet DIDN RViDeNet KPN <ref type="figure">Fig. 5</ref>. Comparison against state-of-the-art image and video denoising methods on CANYON and MARKET scenes. We use the method of Brooks et al. <ref type="bibr" target="#b16">[17]</ref> to produce the input raw noisy videos. Zoom into the electronic version to see the differences. The videos for DIDN, RViDeNet, and our approach are provided in the supplementary video. </p><formula xml:id="formula_7">L recn = I c t ? I d t 1 + ? L c t ? L d t 1 .<label>(5)</label></formula><p>Perceptual loss L prcp We use a VGG-based perceptual loss, as proposed by Chen and Koltun <ref type="bibr" target="#b63">[64]</ref>, to improve the texture in the results. The perceptual loss is defined as:</p><formula xml:id="formula_8">L prcp = N i=1 1 M i ||V (i) (L c t ) ? V (i) (L d t )|| 1 ,<label>(6)</label></formula><p>where V (i) denotes the i th -layer with M i elements of the VGG network <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Training the two networks in our multi-stage system in an end to end manner is difficult. Therefore, we perform the training in two stages. First, we train the flow network separately to ensure it can effectively align the noisy input images. To perform the training, we use an L 1 warping loss, L w , between clean sRGB reference and warped adjacent frames in addition to a total variation smoothing loss, L tv . We apply the combination of warping and TV losses to each iterations using the weighting scheme similar to Teed. et al <ref type="bibr" target="#b55">[56]</ref>:</p><formula xml:id="formula_9">L flow = N i=1 ? i?N (?L i w + L i tv ),<label>(7)</label></formula><p>where ? = 0.8 and ? = 100. Here, ? i?N increasingly weights the latter flows of the iterative network to ensure effective coarse to fine flow calculation. Once the flow network is trained, we use it in our multi-stage denoiser and train the fusion network by minimizing the loss in Eq. 1. Currently, there is a lack of large scale high-quality FHD video dataset that can be used to train video restoration techniques. Therefore, we use youtube-dl to download 200 4K resolution videos from various YouTube channels. Most of the 4K YouTube videos are made up of smaller clips pieced together. From these, we extract around 500 highquality clips with good lighting and downsample them to 1080p to mitigate the noise and compression artifacts. Most of the videos are footage captured by enthusiasts using drones or personal cameras. Therefore, our dataset has a variety of scenes with urban/rural landscape containing rigid (e.g., vehicles) and non-rigid (e.g., humans, animals, etc.) motion, as well as diverse camera motion. Next, we extract 16,600 training samples each containing 8 consecutive frames. We augment the data using random horizontal and vertical flipping, and random change in input video sequence direction. This dataset is crucial for learning a powerfull model with the ability to generalize to a variety of scenes with different conditions.</p><p>To train our flow network, we randomly choose two frames from the sample and randomly crop them to patches of size 384 ? 384. To further augment the data, we apply PyTorch's color jitter with parameters (hue = 0.2/?, contrast = 0.2, brightness = 0.2, saturation = 0.2). The jitter is applied to the two frames separately with a probability of 0.2. Otherwise the same jitter augmentation is applied to both the frames. For training our fusion network in our multi-stage denoiser, we use patches of size 96 ? 96 from 5 consecutive frames in our training samples.</p><p>The synthetic training dataset is composed of sRGB frames but our goal is to train a network to denoise raw   video sequences. Therefore, to generate realistic raw noisy data, we use the unprocessing pipeline proposed by Brooks et al. <ref type="bibr" target="#b16">[17]</ref> to convert the sRGB frames to raw. We then add shot and read noise to the raw frames using Poisson P and N Gaussian distribution <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref> as follows:</p><formula xml:id="formula_10">x p ? ? 2 s P(y p /? 2 s ) + N (0, ? 2 r ),<label>(8)</label></formula><p>where x p is the noisy observation and y p is the true intensity at pixel p. Moreover, ? r and ? s are read and shot noise parameters. We use the same set of shot and read parameters as RViDeNet <ref type="bibr" target="#b25">[26]</ref> to be able to fairly compare with their approach on synthetic and real test scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>We implemented our model in PyTorch and used Adam <ref type="bibr" target="#b65">[66]</ref> for training. We train the alignment network for 400k iterations using a learning rate of 3e-4, optimizer parameters, ? 1 = 0.9 and ? 2 = 0.999, and batch size of 5. For adversarial training, we use the two time-scale update rule (TTUR) by Heusel et al. <ref type="bibr" target="#b66">[67]</ref> for better convergence. The generator and discriminator learning rates are set to 1e-4 and 4e-4, respectively, with optimizer parameters, ? 1 = 0.0 and ? 2 = 0.9, and batch size of 4. We decrease the learning rates by a factor of 10 after 200k iterations and train it for a total of 400k iterations which takes 5 days on a RTX 2080 Ti GPU.</p><p>The source code along with our synthetic test set is available at https://github.com/avinashpaliwal/MaskDnGAN. We compare against image denoising approach of Yu et al. <ref type="bibr" target="#b58">[59]</ref> (DIDN), burst image denoising approach of Mildenhall et al. <ref type="bibr" target="#b33">[34]</ref> (KPN) and video denoising methods by Maggioni et al. <ref type="bibr" target="#b18">[19]</ref> (VBM4D), Tassano et al. <ref type="bibr" target="#b23">[24]</ref> (FastDVDnet) and Yue et al. <ref type="bibr" target="#b25">[26]</ref> (RViDeNet). For all the approaches, we use the source codes provided publicly. Moreover, for fair comparisons, we retrain DIDN, KPN, FastDVDnet, and RViDeNet with all their losses on our synthetic dataset. We use a kernel size of K = 15 for KPN. Finally, we set the number of input frames in KPN and RViDeNet to 5 during retraining to have the same input as ours. Note that, we do not provide the noise level information as input to KPN and FastDVDnet since none of the other approaches, including ours, utilize such information. We obtain the ? parameter for VBM4D using a grid search to achieve the best PSNR on the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Videos</head><p>We create a synthetic test set of 12 high-quality 1080p videos extracted from 4K YouTube videos under Creative Commons license. <ref type="table">Table.</ref> 1 shows the quantitative comparison of our system against other state-of-the-art approaches in terms of PSNR, SSIM <ref type="bibr" target="#b67">[68]</ref>, and LPIPS <ref type="bibr" target="#b68">[69]</ref>. Our adversarial multi-stage approach outperforms the other methods, especially in terms of perceptual quality, as demonstrated by the LPIPS metric.</p><p>Next, we compare our approach against the other methods on two of these synthetic scenes in <ref type="figure">Fig. 5</ref>. In both scenes, VBM4D is unable to effectively denoise the videos. In the CANYON scene (top), the camera is moving backwards and, thus, the bottom areas have large motions, while the top regions (horizon) remain relatively static. FastDVDnet and RViDeNet are able to align objects in the top regions and generate sharper results in comparison to DIDN, a single image denoising approach. On the other hand, both FastDVDnet and RViDeNet suffer in regions with significant motion and generate slightly blurrier results compared to DIDN. Although KPN produces sharper results than Fast-DVDnet and RViDeNet, it is not able to reproduce accurate  <ref type="bibr" target="#b25">[26]</ref>. For each scene, we show the results on two consecutive frames. Note that, while DIDN reconstructs some of the details, they are not consistent across consecutive frames, as indicated by the green arrows.</p><p>color and texture in very noisy regions. Our method is able to generate sharper results with consistent color in all regions. The MARKET scene (bottom) contains both object and camera motions. Other approaches are unable to generate sharp details, while our method is able to recover the texture on the moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real dataset</head><p>Here, we show comparisons on the real test set provided by Yue et al. <ref type="bibr" target="#b25">[26]</ref> which is divided into 5 indoor and 10 outdoor scenes. The indoor scenes are captured in a controlled environment with ground truth frames, while the outdoor scenes are captured without ground truth. Both test sets consist of five ISO levels, covering a variety of different noise levels. Each indoor scene is a video sequence of 7 frames and we compute the quantitative results only on the valid frames, i.e., frames 3 to 5. As shown in <ref type="table">Table.</ref> 2, our approach significantly outperforms the other methods according to all metrics in both sRGB and raw domains.</p><p>We show comparisons against other approaches on two scenes from the indoor test set in <ref type="figure" target="#fig_5">Fig. 6</ref>. The TRAIN scene (top) has a static background with the blue train moving horizontally. VBM4D denoises the train but is unable to reproduce the background texture. DIDN performs well in the smooth regions of the train, but blurs the details of the textured areas like the grapes. FastDVDnet, and RViDeNet are unable to align the adjacent frames and generate artifacts on the moving train, but are able to combine frames in static regions (e.g., grapes) to reconstruct the details. Furthermore, FastDVDnet generates artifacts in saturated regions (see supplementary pdf). Here, KPN produces comparable result to ours as it can effectively combine the images in static regions (top inset) and denoise the smooth areas on the train. In the PIKACHU scene (bottom), the box at the center with Pikachu is being horizontally rotated while the background is static. VBM4D retains some noise in the form of color artifacts. DIDN removes the noise, but also blurs out the details. KPN cannot effectively combine the images and produces artifacts on the textured regions (top inset), but effectively denoises the smooth regions (bottom inset). FastDVDnet, and RViDeNet have alignment artifacts as seen in both the textured areas and smooth regions around the mouth. In comparison, our method produces results without visible artifacts and reconstructs the details by effectively combining all the input frames.</p><p>Next, we compare our approach against the other methods on two outdoor scenes by showing the results on two consecutive frames ( <ref type="figure" target="#fig_6">Fig. 7)</ref>. VBM4D is not only unable to reconstruct the details, but also preserves some of the input noise. DIDN produces results with more details compared to FastDVDnet and RViDeNet, but the textures change in the consecutive frames (see the green arrows) resulting in flickering artifacts. Similarly, KPN generates sharper frames but the texture changes between consecutive frames, especially in the PAVILION scene where the colors and texture vary drastically. FastDVDnet and RViDeNet mitigate the flickering but also smooth out the details in the process. Our result has more details, while being consistent across the two frames (see supplementary video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Temporal Consistency</head><p>We numerically evaluate the temporal consistency of our approach against the other approaches in <ref type="table" target="#tab_5">Table 3</ref>, in addition to the visual comparison shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. We use a flow based warping error defined by Lai et al. <ref type="bibr" target="#b69">[70]</ref>. Here, we use the flows computed on clean frames to warp the adjacent denoised frames, L d t and L d t+1 , to the current frame. The average warping error (E w ) is then computed by averaging the masked L 1 error between the warped adjacent frames. In terms of temporal consistency, our approach outperforms the other approaches by a large margin while generating sharper results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Inference Performance</head><p>We compare the inference time of our approach against the other approaches in <ref type="table">Table 4</ref>. While DIDN, KPN and FastD- VDNet are faster than our approach, they produce results with significantly lower quality. Compared to RViDeNet, our method is considerably faster and produces better results. The majority of timing in our approach is spent during flow computations. Therefore, we can cut down our timing to 1.25s by reusing the flows computed in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We demonstrate the effect of each component in our model on the denoising quality in <ref type="table" target="#tab_6">Tables 5 and 6</ref>. Increasing the number of inputs from 3 to 5, improves the performance in terms of both, image quality and temporal consistency, showing that our system effectively combines information from the additional frames. Switching to the 2-stage model (Ours5M) further improves performance and temporal consistency, thereby demonstrating the effectiveness of the multi stage architecture. As seen, going beyond two stages to three stages (Ours7M) reduces the performance as training such system is difficult. Next we show that while training the system on our full loss, including the adversarial term, improves the perceptual quality (LPIPS), it lowers the PSNR and SSIM due to poor performance in smooth regions (high frequency artifacts). Conditioning the discriminator with the gradient mask gives the best PSNR and SSIM, while further improving the perceptual quality. Finally, we show the inference time for different number of stages in <ref type="table" target="#tab_7">Table 7</ref>. While the number of networks used to generate a single denoised frame increase exponentially with the number of stages (3+1 for 2-stage; 5+3+1 for 3stages), the amortized cost of inference increases linearly with the number of stages, as shown in <ref type="table" target="#tab_7">Table 7</ref>. This is because most of the computation from the previous frame can be reused in computation of the current denoised frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Limitations</head><p>Our approach has some limitations. First, our model is unable to effectively denoise videos with noise levels outside the training noise distribution, as shown in <ref type="figure">Fig. 9</ref>. Nevertheless, our approach still produces better results than DIDN and RViDeNet, demonstrating better generalization ability of our method. Moreover, in some cases (see <ref type="figure">Fig. 8</ref>), our adversarial system oversharpens the images (left) or hallucinate details (right) that do not exist in the ground truth image. However, this is not a major issue as our results are visually plausible. Finally, compared to single-stage, our  multi-stage approach is more computationally demanding, especially as the number of stages increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION &amp; FUTURE WORK</head><p>We present a novel deep learning approach for raw video denoising. We propose a multi-stage denoiser with a denoiser block further divided into alignment and fusion stages. The proposed denoiser alleviates the need to directly align temporally distant frames in the noisy video sequence. We further propose an adversarial training strategy where we condition the discriminator on a gradient mask. Our adversarial approach effectively removes noise, while preserving the details in the scene. We demonstrate that our approach significantly outperforms state-of-the-art methods on both synthetic and real scenes, both visually and numerically in terms of several quantitative metrics. In the future, it would be interesting to explore the possibility of using our system for the other video enhancement tasks like video super resolution.</p><p>Ground Truth Ground Truth Ours Ours <ref type="figure">Fig. 8</ref>. On the left, we show an example where our system produces sharper image than the ground truth. On the right, we show a case where our method hallucinate details that do not exist in the ground truth image. Nevertheless, our results are still visually pleasing.</p><p>GT Noisy DIDN RViDeNet Ours KPN FastDVDNet <ref type="figure">Fig. 9</ref>. For noise outside the training distribution (ISO 38400 in this case), our model introduces artifacts in the smooth regions. However, our result is still much better than other approaches at extracting the underlying texture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>A. Paliwal, L. Zeng and N.K. Kalantari are with the Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX 77843. E-mail: {avinashpaliwal, libingzeng, nimak}@tamu.edu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the denoiser block in the multi-stage denoiser. The two-input flow network separately processes the two adjacent frame pairs, (I i k?1 , I i k ) and (I i k+1 , I i k ), and generates a pair of flows which are used to warp the adjacent frames to I i k . The fusion network then combines the aligned frames to synthesize the intermediate denoised frame, I i+1 k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of the single-stage approach against our multi-stage technique when 5 consecutive input frames are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison against state-of-the-art image and video denoising methods on real indoor scenes TRAIN and PIKACHU by Yue at al.<ref type="bibr" target="#b25">[26]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison against state-of-the-art image and video denoising methods on real outdoor scenes POOL and PAVILION by Yue at al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>32.64dB 31.94dB 32.80dB 31.91dB 25.74dB 34.29dB 28.69dB 34.33dB 33.30dB 34.31dB 34.47dB 25.61dB Ours 36.29dB 28.79dB</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Numerical comparisons on our synthetic test set.</figDesc><table><row><cell></cell><cell cols="2">Raw</cell><cell></cell><cell>sRGB</cell><cell></cell></row><row><cell></cell><cell cols="2">PSNR SSIM</cell><cell cols="2">PSNR SSIM</cell><cell>LPIPS</cell></row><row><cell>Noisy</cell><cell>32.53</cell><cell>0.811</cell><cell>31.82</cell><cell>0.829</cell><cell>0.2727</cell></row><row><cell>VBM4D [19]</cell><cell>-</cell><cell>-</cell><cell>33.41</cell><cell>0.898</cell><cell>0.1982</cell></row><row><cell>DIDN [59]</cell><cell>35.27</cell><cell>0.888</cell><cell>36.54</cell><cell>0.952</cell><cell>0.0700</cell></row><row><cell>KPN [34]</cell><cell>39.24</cell><cell>0.967</cell><cell>36.85</cell><cell>0.959</cell><cell>0.0612</cell></row><row><cell>FastDVDnet [24]</cell><cell>-</cell><cell>-</cell><cell>36.12</cell><cell>0.942</cell><cell>0.0921</cell></row><row><cell>RViDeNet [26]</cell><cell>40.23</cell><cell>0.973</cell><cell>36.94</cell><cell>0.957</cell><cell>0.0698</cell></row><row><cell>Ours</cell><cell>41.09</cell><cell>0.979</cell><cell>37.96</cell><cell>0.967</cell><cell>0.0424</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>36.10dB 32.95dB 36.36dB 34.68dB 27.25dB 36.44dB 30.67dB 35.95dB 34.59dB 35.96dB 37.34dB 25.22dB Ours 37.72dB 28.07dB</head><label></label><figDesc></figDesc><table><row><cell>Noisy</cell><cell>VBM4D</cell><cell>DIDN</cell><cell>KPN</cell><cell>FastDVDnet</cell><cell>RViDeNet</cell><cell>Ours</cell><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Results on real indoor test set by Yue et al.<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell></cell><cell cols="2">Raw</cell><cell></cell><cell>sRGB</cell><cell></cell></row><row><cell></cell><cell cols="2">PSNR SSIM</cell><cell cols="2">PSNR SSIM</cell><cell>LPIPS</cell></row><row><cell>Noisy</cell><cell>31.99</cell><cell>0.733</cell><cell>31.72</cell><cell>0.751</cell><cell>0.4688</cell></row><row><cell>VBM4D [19]</cell><cell>-</cell><cell>-</cell><cell>34.46</cell><cell>0.908</cell><cell>0.2076</cell></row><row><cell>DIDN [59]</cell><cell>35.67</cell><cell>0.853</cell><cell>39.72</cell><cell>0.977</cell><cell>0.0477</cell></row><row><cell>KPN [34]</cell><cell>43.06</cell><cell>0.986</cell><cell>39.77</cell><cell>0.979</cell><cell>0.0443</cell></row><row><cell>FastDVDnet [24]</cell><cell>-</cell><cell>-</cell><cell>37.45</cell><cell>0.958</cell><cell>0.0791</cell></row><row><cell>RViDeNet [26]</cell><cell>43.63</cell><cell>0.987</cell><cell>39.74</cell><cell>0.978</cell><cell>0.0451</cell></row><row><cell>Ours</cell><cell>43.96</cell><cell>0.988</cell><cell>40.40</cell><cell>0.981</cell><cell>0.0357</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Temporal consistency evaluation using average warping error<ref type="bibr" target="#b69">[70]</ref> on the synthetic test set.</figDesc><table><row><cell></cell><cell cols="5">DIDN KPN FastDVDnet RViDeNet Ours</cell></row><row><cell>Ew (x 10 ?4 )</cell><cell>8.81</cell><cell>7.99</cell><cell>8.34</cell><cell>7.96</cell><cell>7.02</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 4</cell><cell></cell><cell></cell></row><row><cell cols="6">Comparison of inference time for generating a 1080p frame on RTX</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2080 Ti.</cell><cell></cell><cell></cell></row><row><cell>VBM4D</cell><cell>DIDN</cell><cell>KPN</cell><cell cols="2">FastDVDNet RViDeNet</cell><cell>Ours</cell></row><row><cell>124.90s</cell><cell>1.20s</cell><cell>0.89s</cell><cell>0.74s</cell><cell>9.82s</cell><cell>1.47s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Evaluating the effect of components in our model on the denoising quality.</figDesc><table><row><cell>Component</cell><cell cols="2">Raw PSNR SSIM</cell><cell cols="2">sRGB PSNR SSIM</cell><cell>LPIPS</cell></row><row><cell>3-Input (1-stage)</cell><cell>40.87</cell><cell>0.978</cell><cell>37.67</cell><cell>0.965</cell><cell>0.0432</cell></row><row><cell>5-input (1-stage)</cell><cell>40.97</cell><cell>0.979</cell><cell>37.84</cell><cell>0.967</cell><cell>0.0423</cell></row><row><cell>5-input (2-stage)</cell><cell>41.09</cell><cell>0.979</cell><cell>37.96</cell><cell>0.967</cell><cell>0.0424</cell></row><row><cell>7-input (3-stage)</cell><cell>40.96</cell><cell>0.978</cell><cell>37.83</cell><cell>0.967</cell><cell>0.0418</cell></row><row><cell>5-input (2-stage)</cell><cell>40.89</cell><cell>0.978</cell><cell>37.79</cell><cell>0.966</cell><cell>0.0542</cell></row><row><cell>+ Adversarial</cell><cell>40.58</cell><cell>0.977</cell><cell>37.44</cell><cell>0.964</cell><cell>0.0439</cell></row><row><cell>+ Gradient Mask</cell><cell>41.09</cell><cell>0.979</cell><cell>37.96</cell><cell>0.967</cell><cell>0.0424</cell></row><row><cell></cell><cell cols="2">TABLE 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Evaluating the effect of components in our model on the temporal</cell></row><row><cell></cell><cell cols="2">consistency.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours3</cell><cell>Ours5</cell><cell>Ours5M</cell><cell cols="2">Ours7M</cell></row><row><cell>Ew (x 10 ?4 )</cell><cell>7.37</cell><cell>7.07</cell><cell>7.02</cell><cell></cell><cell>7.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Inference time (seconds) for multi-stage architecture per frame.</figDesc><table><row><cell>N-stage model</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>inference time (s)</cell><cell>0.84</cell><cell>1.47</cell><cell>2.30</cell><cell>3.04</cell><cell>5.18</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the reviewers for their comments and suggestions. The synthetic test dataset was collected from YouTube channels Video Library -No copyright Footage, Le Monde en Vid?o and Underway, all under Creative Commons (CC) license.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of gaussians in the wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flexisp: A flexible camera image processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paj?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive multi-column deep neural networks with application to robust image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising via cnns: An adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Divakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cycleisp: Real image restoration via improved data synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2696" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep RNNs for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXXIX</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9971</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Videnn: Deep blind video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dvdnet: A fast network for deep video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1805" to="1809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards real-time deep video denoising without flow estimation</title>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1354" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seeing motion in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3185" to="3194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised raw video denoising with a benchmark dataset on dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2301" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pl?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Basis prediction networks for effective burst denoising with large kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A nonlocal bayesian image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1665" to="1688" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlocal video denoising, simplification and inpainting using discrete regularization on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghoniem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chahir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2445" to="2455" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient video denoising based on dynamic nonlocal means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video denoising using low rank tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Vision (ICMV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10341</biblScope>
			<biblScope unit="page" from="162" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Patch-based video denoising with optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Lisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miladinovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2573" to="2586" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local classspecific and global image-level generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7870" to="7879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GestureGAN for hand gesture-to-gesture translation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference -MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multichannel attention selection GAN with cascaded semantic guidance for cross-view image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2417" to="2426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">To learn image superresolution, use a GAN to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CT super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-CIRCLE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Vannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ESRGAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial monte carlo denoising with conditioned auxiliary feature modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DN-GAN: Denoising generative adversarial networks for speckle noise reduction in optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">101632</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A generative adversarial network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="16" to="517" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep iterative down-up cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2095" to="2103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Geometric gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep and hierarchical implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno>abs/1702.08896</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
