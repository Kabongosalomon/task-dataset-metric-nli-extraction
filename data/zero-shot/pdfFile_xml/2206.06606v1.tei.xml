<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Astock: A New Dataset and Automated Stock Trading based on Stock-specific News Analyzing Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinan</forename><surname>Zou</surname></persName>
							<email>jinan.zou@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyao</forename><surname>Cao</surname></persName>
							<email>haiyao.cao@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
							<email>lingqiao.liu@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Lin</surname></persName>
							<email>yuhao.lin01@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
							<email>ehsan.abbasnejad@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javen</forename><forename type="middle">Qinfeng</forename><surname>Shi</surname></persName>
							<email>javen.shi@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Astock: A New Dataset and Automated Stock Trading based on Stock-specific News Analyzing Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural Language Processing(NLP) demonstrates a great potential to support financial decisionmaking by analyzing the text from social media or news outlets. In this work, we build a platform to study the NLP-aided stock auto-trading algorithms systematically. In contrast to the previous work, our platform is characterized by three features: (1) We provide financial news for each specific stock. <ref type="formula">(2)</ref> We provide various stock factors for each stock. (3) We evaluate performance from more financial-relevant metrics. Such a design allows us to develop and evaluate NLP-aided stock auto-trading algorithms in a more realistic setting. In addition to designing an evaluation platform and dataset collection, we also made a technical contribution by proposing a system to automatically learn a good feature representation from various input information. The key to our algorithm is a method called semantic role labeling Pooling (SRLP), which leverages Semantic Role Labeling (SRL) to create a compact representation of each news paragraph. Based on SRLP, we further incorporate other stock factors to make the final prediction. In addition, we propose a self-supervised learning strategy based on SRLP to enhance the out-of-distribution generalization performance of our system. Through our experimental study, we show that the proposed method achieves better performance and outperforms all the baselines' annualized rate of return as well as the maximum drawdown of the CSI300 index and XIN9 index on real trading. Our Astock dataset and code are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Stock prediction has been an attractive task for a long time, and it is still challenging since the stochasticity of the market and behavior patterns of participators are fluctuating and elusive. Stock forecasting based on Natural Lan- * Both authors contributed equally ? Corresponding Author  guage Processing (NLP) techniques is a promising solution since text information, e.g., tweets, financial news etc., is strongly correlated with the stock prices. However, the NLPbased stock forecasting research is still scattered without unified definitions, benchmark datasets, clear articulations of the tasks, which severaly hinders progress of this field. Existing approaches are usually based on market sentiment analysis <ref type="bibr" target="#b9">[Xu and Cohen, 2018;</ref><ref type="bibr" target="#b1">Cheng and Li, 2021]</ref> and use news to predict the related securities' price on the following trading day(s) <ref type="bibr" target="#b9">[Zhang et al., 2017;</ref><ref type="bibr" target="#b8">Li et al., 2020</ref>]. Despite the limited success in those studies, the existing works are still far from realistic for two reasons: Firstly, previous methods ignore the financial factors, which plays a key role in practical trading. Secondly, these models are evaluated only on intermediate performance metric, e.g., stock movement prediction accuracy. It is unclear how well they can support a practical trading system to make sufficient profit.</p><formula xml:id="formula_0">A1 V ? ? ? ? ? ? ? ? ? ? ? ? A0</formula><p>To address the problems above, we construct a China Ashares market dataset with news and stock factors called Astock. Specifically, we annotate all occurrences of the three trading actions (long, preserve, short) in 40,963 news originated from Tushare 1 with a valid official license, which describes the major financial events. The dataset also includes various stock factors to build a realistic system. Based on As-tock, we establish a semantic role labeling pooling (SRLP) to build a compact representation for stock-specific news and predict the stock movement. This work also explores how to leverage a self-supervised method better to upgrade the SRLP method, which achieves better performance for classification and high domain generalization ability.</p><p>In experiments, we further propose a realistic trading platform that outperforms the state-of-the-art text classification baseline's average returns and Sharpe Ratios over the CSI300 index and XIN9 index of testing period from January 2021 to November 2021. Specifically, we analyze the profitability of the proposed strategy based on stock movement prediction result for real trading as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The primary contributions of this work can be summarized as follows:</p><p>? We construct a brand new Chinese stock prediction task dataset with stock-specific news and stock factors. ? Our SRLP characterizes the key attributes of financial events, which is convenient for incorporating other stock factors and further creating a self-supervised module on top of the SRLP method. Our self-supervised SRLP method obtains competitive stock movement prediction and out-of-distribution (OOD) generalization results. ? We further evaluate algorithm performance on realworld trading from more financial-relevant metrics. By conducting extensive experimental studies, we show that our self-supervised SRLP achieves remarkable performance on these metrics. Furthermore, we observe that the proposed trading strategies work well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based Stock Prediction</head><p>In recent years, the use of text-based information, especially news and social media, has significantly improved the performance of stock prediction tasks and these methods usually rely on text-based features and sentiment analysis to forecast stock movements <ref type="bibr" target="#b9">[Xu and Cohen, 2018;</ref><ref type="bibr" target="#b8">Hu et al., 2018;</ref><ref type="bibr" target="#b6">Ding et al., 2015]</ref>. However, These approaches assume that the real-trading distribution was the same as the training distribution, which is not realistic as it is difficult to generalize to future trading. By contrast, our self-supervised SRL approach pays closer attention to the quality and comprehensiveness of the news, which could help with out-of-distribution generalization on the realistic trading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Role Labeling and Self-Supervised Learning Approach</head><p>Semantic role labeling (SRL) aims to disclose the predicateargument structure of a given sentence, which could provide a clear overlay that uncovers the underlying semantics of text <ref type="bibr" target="#b2">[Conia et al., 2021]</ref>. However, previous stock movement prediction methods <ref type="bibr" target="#b9">[Xu and Cohen, 2018;</ref><ref type="bibr" target="#b8">Hu et al., 2018;</ref><ref type="bibr" target="#b6">Ding et al., 2015]</ref> adopted the word or sentence level representation to predict the stock movement. Due to the lack of abstract information of the news, these approaches can overfit the training data and fail to distinguish the key features of news. To deal with this problem, we used the SRL's characteristics for extracting a clear overlay that uncovers the underlying semantics of news.</p><p>Recently, self-supervised learning has become a very popular technique in the training stage of NLP, which generates labels without any human intervention and learns common language representations. Some researches <ref type="bibr" target="#b8">[Im et al., 2021;</ref><ref type="bibr" target="#b9">Zheng et al., 2021]</ref> have proven that self-supervised learning strengthens the generalization ability for models as it improves the performance in many tasks. The stock prediction task aims to explore a realistic method to predict the stock movement with comprehensive and reasonable information in the China stock market. To this end, it is important to have minute-level price information in the dataset and we are motivated to collect one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dateset Creation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard of news and stock factors collection</head><p>There are two main components in our dataset: News and stock factors for the China stock market. In terms of news data, there are 40,963 pieces of listed company news, including company announcements and company-related news from July 2018 to November 2021. The news data are split into two parts: the In-distribution split and the out-ofdistribution split. The in-distribution split is from July 2018 to December 2020 for training and testing where the training set occupies by 80%, and the validation set and test set occupy 10% respectively. The out-of-distribution split is selected from January 2021 to November 2021, which is used for OOD generalization testing. Every piece of news includes its published time and a corresponding news summary. Factor investing is an investment approach that involves targeting quantifiable firm characteristics or factors that can explain the differences in stock returns. Factor-based strategies may help investors meet particular investment objectives-such as potentially improving returns or reducing risk over the long term. Our Astock dataset covers the 24 stock factors on each stock of the China A-shares including Dividend yield, Total share, Circulated share, Free Float share, Market Capitalization, Price-earning ratio, PE for Trailing Twelve Months, Price/book value ratio, Price-to-sales Ratio, Price to Sales ratio, Circulate Market Capitalization, Open price, High price,Low price, Close price , Previous close price, Price change, Percentage of change, Volume, Amount, Turn over rate, Turn over rate for circulated Market Capitalization, Volume ratio. Furthermore, We compare Astock with several widely used stock prediction datasets in <ref type="table" target="#tab_1">Table 1</ref>. The value is reflected in the following aspects: (1) Astock provides financial news for each specific stock over the entire China A-shares market. (2) Astock provides various stock factors for each stock. (3) Astock provides minute-level historical prices for the news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task Formulation</head><p>We divide the automated trading system into two tasks: stock movement classification and simulated trading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-based stock movement classification</head><p>The goal of the stock movement classification task is to classify the effects of the input information. We measure the impact of each piece of company news by the stock return rate. In this paper, the news is annotated by the stock return rate r , and three cases are considered in our annotation: outperforming, neutral, and underperforming as shown in Equation 1. We further model the stock movement by classifying it into three categories. The ground truth for those categories can be derived from r. Specifically, we follow the following rules to categorize the data into three classes after ranking all the news by r, which aims to find the most strong signal of the stock movement, and to reduce the disturbance of noises comparing to dividing the data evenly. After the domain experts gave us the advice and the experiments with different thresholds was conducted, we set 20% as the threshold where the tunable parameters a, b, c, and d are 20, 40, 60, and 20, respectively.</p><formula xml:id="formula_1">label = ? ? ? outperf orming, if r ranked top a% neutral, if r ranked top b% ? c% underperf orming, if r ranked bottom d%</formula><p>(1) where r is the return rate of the news. We randomly select 80% of the in-distribution dataset as the training set, and the other 20% is split evenly into validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulated Trading</head><p>Stock movement prediction accuracy may not necessarily translate to a profitability of an auto-trading system. To further investigate how the stock prediction can benefit for the actual trading practice, we employ a practical trading strategy based on the stock movement prediction results and evaluate various metrics for the trading actions. The trading strategy details can be found at our github page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>This section describes the technical contribution of this work: a novel system for stock movement prediction. Our system consists of two major components: semantic role labeling pooling method and a self-supervised learning based on SRLP, we will elaborate on those two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Role Labelling Pooling</head><p>In this work, we propose to leverage the off-the-shelf semantic role labeling, i.e., <ref type="bibr">Propbank [Kingsbury and Palmer, 2003]</ref>, to pool the output embeddings of a pre-trained language model to construct an alternative representation. The rationale is that the semantic roles in Propbank, i.e., verb (V), proto-agent (A0), and proto-patient (A1), are generalpurposed and are also strongly associated with the event arguments. We show an example for semantic role labeling for financial news in <ref type="figure" target="#fig_1">Figure 3</ref>.  More specifically, we first use the Language Technology Platform (LTP)  to automatically mark the semantic roles from the sentences of an entire piece of news and then select V, A0, and A1 to represent the roles for each sentence. Secondly, we process each sentence with a pretrained language model to obtain a sequence of output embeddings {s 1 , s 2 , ? ? ? , s n }. We use V, A 0 and A 1 to denote the indices of tokens corresponding to the V, A0, A1 components. At last, we perform pooling for embeddings with their indices falling into V, A 0 and A 1 . We call this scheme Semantic Role Labelling Pooling SRLP in short. Taking A0 as an example, the SRLP feature for A0 is</p><formula xml:id="formula_2">e A0 = 1 |A 0 | i?A0 s i<label>(2)</label></formula><p>For a sentence with N sets of V, A0 and A1, we concatenate e A0 ,e A1 ,e V of each sentence and the financial factor F of the stock-of-interest into a data matrix:</p><formula xml:id="formula_3">E = ? ? ? e 1 V ... e t V ... e N V e 1 A0 ... e t A0 ... e N A0 e 1 A1 ... e t A1 ... e N A1 F ... F ... F, ? ? ?<label>(3)</label></formula><p>where E is output of the above process. Each column of E, denoted as e j , is the concatenation of e j V , e j A0 , e j A1 and F. E is then processed by a Transformer encoder in the same way as the standard text classification to generate the stock movement prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-Supervised Learning based on SRLP</head><p>Besides standard supervised training loss for stock movement classification, in this work, we further propose to use a selfsupervised training task as an auxiliary task to train the network. For stock prediction, good generalization is highly desirable since the training data is usually sampled from a period different from the test period. A significant problem in practice is to ensure that our model generalizes to scenarios different from the training set. We further create a selfsupervised learning method on top of the SRLP. Recent studies <ref type="bibr">[Mohseni et al., 2020;</ref><ref type="bibr" target="#b7">Hendrycks et al., 2019]</ref> have shown that incorporating a self-supervised learning task along with the supervised training task could lead to better generalization. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the self-supervised task is defined as predicting the position of one randomly masked SRL role from all the roles of SRL in a piece of news. Intuitively, the self-supervised learning task should be designed to encourage the favorable properties of features. In this work, we propose to randomly mask one pooled embedding, i.e., e j V , e j A0 or e j A1 , from a randomly selected sentence, and then ask the network to identify the masked embedding from a pool of candidate embeddings. Such a cloze-style task encourages the network to perform reasoning over other unmasked cues to work out the missing item. We hypothesize that such a reasoning capability is beneficial for understanding the financial news and thus helps stock movement prediction.</p><p>Formally, we randomly select a e j from E and then select one element from e j = { e j V , e j A0 , e j A1 }, after that we replace the selected element with an all-zero vector, indicating a "mask" operation. Taking masked V at the t-th sentence as an example, we denote the E after this mask operation as E .</p><formula xml:id="formula_4">E = ? ? ? e 1 V ... M ... e N V e 1 A0 ... e t A0 ... e N A0 e 1 A1 ... e t A1 ... e N A1 F ... F ... F ? ? ?</formula><p>Then we feed E into the transformer to obtain a query vector sequence q ? R d q = T ransf ormer(E ) <ref type="bibr">[:, t]</ref> where [:, t] means extract the t-th column of the vector sequences calculated by the transformer. The unmasked SRLP-V features (or SRLP-A0, SRLP-A1 features, depending on which type of SRLP feature is chosen) is also send to an encoder to calculate candidate key vectors: Formally, K is defined as:</p><formula xml:id="formula_5">K = [f V (e 1 V ), ? ? ? , f v (e t V ), ? ? ? , f V (e N V )] ? R d?N where f V</formula><p>is an encoder specified for encoding V-type SRLP feature. Then the query vector is compared against each column vector in K and is expected to have the highest matching score at the t-th location. This process could be implemented via matrix multiplication and the softmax operation:</p><formula xml:id="formula_6">P SSL = Softmax(qK)</formula><p>(4) and we hope the highest probability entry in Eq. 4 is at the t-th dimension. This requirement could be enforced via the cross-entropy loss. Finally, the training loss for the models is</p><formula xml:id="formula_7">L = ?L CLS + (1 ? ?)L SSL</formula><p>(5) where L CLS is the cross-entropy loss for the text classification and L SSL is the cross-entropy loss on the self-supervised learning prediction P SSL . ? here is a trade-off parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct experiments to evaluate the performance of the proposed model. We conduct experiments on two different splits of our dataset for each model: Indistribution split and out-of-distribution split. We also feed the prediction result of our method to the proposed trading strategy to analyze the profitability through backtesting on real-world stock data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation metrics</head><p>We evaluated the stock movement prediction and simulated trading performance. For the Stock movement prediction, we applied the Accuracy, F1 Score, Recall and Precision as evaluation metrics. For simulated trading, we applied the Annualized Rate of Return, Maximum Drawdown and Sharpe Ration as evaluation metrics based on our simulated trading strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Methods</head><p>We re-implement the current state-of-art stock movement prediction models as baselines, including StockNet <ref type="bibr" target="#b9">[Xu and Cohen, 2018]</ref>, HAN Stock <ref type="bibr" target="#b8">[Hu et al., 2018]</ref>. StockNet <ref type="bibr" target="#b9">[Xu and Cohen, 2018</ref>] is a stock temporally-dependent movement prediction model which also uses Twitter data and price information to predict two classes for stock movement. Hybrid Attention Networks (HAN) Stock <ref type="bibr" target="#b8">[Hu et al., 2018]</ref> is a stock trend prediction model based on a sequence of recent related news to predict three classes for stock movement task, which are the same as ours. We also construct baselines by formulating the stock movement prediction problem as text classification and use four strong pre-trained Chinese language models as backbones such as XLNet-base-Chinese <ref type="bibr" target="#b3">[Cui et al., 2020]</ref>, Sentiment Knowledge Enhanced pre-trained language model(SKEP) <ref type="bibr" target="#b8">[Tian et al., 2020]</ref>, Bert Chinese <ref type="bibr" target="#b4">[Devlin et al., 2019]</ref> and RoBERTa WWM Ext <ref type="bibr" target="#b3">[Cui et al., 2020]</ref>. For the above four pre-trained language models, we extract sentence embedding from the [CLS] token and attach a threeway classifier to predict the stock movements. In addition, we also compare the CSI300 index, XIN9 index 2 against the proposed method when analyzing the profitability of the proposed system. For our methods, we used RoBERTa WWM Ext as our backbone PLM since the remarkable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Stock Movement Evaluation</head><p>We first compare different methods on the task of stock movement prediction. We conduct experiments on two different splits of our dataset: In-distribution split and out-ofdistribution split. In the in-distribution split, both training and testing data are sampled from the same period while the outof-distribution split uses data from different periods to construct the training and testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-distribution evaluation</head><p>The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. From the results, we made the following two observations:</p><p>1. If only text information is used, the proposed SRLP approach achieves the state-of-the-art performance. Interestingly, we find that SRLP achieves superior performance when further combines the stock factors. It outperforms RoBERTa WWM Ext (News+Factors) by more than 2%. We postulate that this is because the compact representation in SRLP make incorporation of stock factors easier. Note that the proposed way of incorporating stock factors (see Section 3.1) does not only introduce extra modalities for the stock movement prediction but also could make the text analysis module adaptive to the stock factors. This could be useful to model the scenario like the effect of a similar event could result in a different impact on the stock movement for a different type of company.</p><p>2. The proposed self-supervised SRLP can further boost the performance of SRLP. In the best setting of selfsupervised SRLP, i.e., with V being masked, self-supervised SRLP achieves more than 1% improvement over SRLP.</p><p>The improvement is even larger when the stock factors are provided, showing more than 2% improvement over SRLP (News+Factors), achieving 66.89% prediction accuracy. This validates the effectiveness of the proposed self-supervised learning approach. Interestingly, we observe that masking A0 and A1 usually will not bring improvement in contrast to the case of masking V. Note that V encodes the type of an event, and the argument is encoded by A0 or A1. It seems that predicting the type of events is a more effective self-supervised learning task than working on the argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-distribution evaluation</head><p>In the experiments above, the training data and testing data are sampled from the same period. Thus the distributions of training data and testing data are similar. For real-world applications, the stock movement prediction model is applied to future data unseen at the training time. Hence, it is critical to evaluate the model in such an out-of-distribution setting. To this end, we construct a new training/testing split by using the data from July 2018 to December 2020 as training data and the data from January 2021 to November 2021 for the testing data. We first conduct an evaluation on the stock movement prediction task, and the results are shown in <ref type="table" target="#tab_4">Table  3</ref>. From the results, we can see that the proposed method is still comparably competitive over other baselines. In this section, we discuss the possible profitability of the proposed strategy in real-world trading. We use our trading strategy to conduct trading simulation (backtesting) on stock data from January 2021 to November 2021 using the stock movement prediction result of our model trained from July 2018 to December 2020 as mentioned in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Profitability Evaluation in Real-world</head><p>In <ref type="table" target="#tab_5">Table 4</ref>, we show that our self-supervised SRLP model achieves a remarkable annualized rate of return of 13.85% , which surpasses the previous baselines and market index XIN9 and CSI300. The resulting baseline HAN Stock [Hu  <ref type="bibr" target="#b9">[Xu and Cohen, 2018]</ref> achieve an annualized rate of return of -13.5% and -22.42% respectively, and the market XIN9 index and CSI300 were overall declining in 2021, which obtains -15.38 % and -9.34% respectively. In addition, our self-supervised learning method also obtains the lowest Maximum Drawdown of -3.6% and the highest Sharpe Ratio of 40.93% , which significantly outperforms the previous methods and indicates that our self-supervised could successfully achieve higher expected returns while remaining relatively less risky as shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we study the problem of NLP-based stock prediction and build a platform with a new dataset, AStock, featured by: (1) Large number of stocks, stock-relevant news.</p><p>(2) Availability of various financial factors.</p><p>(3) Financialrelevent metrics for Evaluation. The Platform is based on two novel techniques. One leverages Propbank-style semantic role labeling results to create compact news representation. Building on top of this representation, the other technique is a customized self-supervised learning training strategy for improving generalization performance. We demonstrate that the proposed method achieves superior performance over other baselines through extensive experiments in both in-distribution and out-of-distribution settings. Also, by feeding our prediction to a practical simulated trading, our method achieves better profitability in backtesting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the automated stock trading system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>A Semantic role labeling example for a piece of news.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>-specific news ..............................Overall framework of our approach, including a domain adapted pre-trained model (RoBERTa WWM Ext), Semantic Roles Pooling, transformer layer, self-supervised module (left part), and the supervised module (right part). The green arrow represents a duplicate for the SRLP. The final result is generated from the stock movement classifier, and the total loss is obtained from the self-supervised SRLP part and supervised stock movement classification part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparison between Astock and other existing widelyused stock prediction dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Num of Stock Text Source</cell><cell>Price-level</cell><cell>Stock Factors</cell></row><row><cell>DMFT's dataset</cell><cell>50</cell><cell></cell><cell>Daily</cell></row><row><cell>[Zhang et al., 2017] EMNLP 17'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>StockNet's dataset</cell><cell>88</cell><cell>Twitter</cell><cell></cell></row><row><cell>[Xu and Cohen, 2018] ACL 18'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dingxia's dataset</cell><cell>500</cell><cell>News</cell><cell>Daily</cell></row><row><cell>[Ding et al., 2014] EMNLP 14'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Trade the event 's dataset</cell><cell></cell><cell>News</cell><cell></cell></row><row><cell>[Zhou et al., 2021] ACL 21'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>3680</cell><cell>Stock News</cell><cell>Minute-level when news published Daily-level for all the stocks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Original text: ??????Career HK??????199??,?? ?????1.103%?Translation: Career HK, a shareholder of Kerui international, reduced 1.99 million shares of the company, accounting for 1.103% of the total share capital of the company</figDesc><table><row><cell>A0</cell><cell>V</cell><cell>A1</cell></row><row><cell>??????Career HK</cell><cell>??</cell><cell>????199??</cell></row><row><cell>Career HK, a shareholder of Kerui international</cell><cell>reduced</cell><cell>1.99 million shares</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The stock movement classification performance(%) of in-distribution evaluation on our scheme and others demonstrates the effectiveness of our self-supervised SRL method.indicates that the model adopted this Semantic role's pooling information.indicates that the method does not adopt this semantic role's pooling.indicates that semantic role's pooling is masked.</figDesc><table><row><cell>Model</cell><cell>Resource</cell><cell cols="7">Semantic Role Accuracy F1 Score Recall Precision A0 V A1</cell></row><row><cell>StockNet [Xu and Cohen, 2018] ACL 18'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.72</cell><cell>44.44</cell><cell>46.68</cell><cell>47.65</cell></row><row><cell>HAN Stock [Hu et al., 2018] ICWSDM 18'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.35</cell><cell>56.61</cell><cell>57.20</cell><cell>58.41</cell></row><row><cell>Bert Chinese [Devlin et al., 2019]NAACL 19'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.11</cell><cell>58.99</cell><cell>59.20</cell><cell>59.07</cell></row><row><cell>ERNIE-SKEP [Tian et al., 2020] ACL 20'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.66</cell><cell>60.66</cell><cell>60.59</cell><cell>61.85</cell></row><row><cell>XLNET Chinese [Cui et al., 2020]EMNLP 20'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.14</cell><cell>61.19</cell><cell>61.09</cell><cell>61.60</cell></row><row><cell>RoBERTa WWM Ext [Cui et al., 2020]EMNLP 20'</cell><cell>News</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.34</cell><cell>61.48</cell><cell>61.32</cell><cell>61.97</cell></row><row><cell></cell><cell>News + Factors</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.49</cell><cell>62.54</cell><cell>62.51</cell><cell>62.59</cell></row><row><cell>Our SRLP</cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>61.76</cell><cell>61.69</cell><cell>61.62</cell><cell>61.87</cell></row><row><cell></cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>64.79</cell><cell>64.85</cell><cell>64.79</cell><cell>65.26</cell></row><row><cell>Our Self-supervised SRLP</cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>61.07</cell><cell>61.11</cell><cell>61.11</cell><cell>61.11</cell></row><row><cell></cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>62.36</cell><cell>62.32</cell><cell>62.43</cell><cell>62.64</cell></row><row><cell></cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>62.42</cell><cell>62.46</cell><cell>62.44</cell><cell>62.62</cell></row><row><cell></cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>62.15</cell><cell>62.15</cell><cell>62.15</cell><cell>62.59</cell></row><row><cell></cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>61.34</cell><cell>61.23</cell><cell>61.46</cell><cell>61.30</cell></row><row><cell></cell><cell>News</cell><cell></cell><cell></cell><cell></cell><cell>62.97</cell><cell>63.05</cell><cell>62.93</cell><cell>63.47</cell></row><row><cell>Our Self-supervised SRLP</cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>64.59</cell><cell>64.62</cell><cell>64.63</cell><cell>64.65</cell></row><row><cell>with Factors</cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>66.82</cell><cell>66.81</cell><cell>66.90</cell><cell>66.82</cell></row><row><cell></cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>65.54</cell><cell>65.53</cell><cell>65.62</cell><cell>65.50</cell></row><row><cell></cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>65.34</cell><cell>65.21</cell><cell>65.43</cell><cell>65.43</cell></row><row><cell></cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>65.27</cell><cell>65.35</cell><cell>65.24</cell><cell>65.77</cell></row><row><cell></cell><cell>News + Factors</cell><cell></cell><cell></cell><cell></cell><cell>66.89</cell><cell>66.92</cell><cell>66.95</cell><cell>66.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: The comparison (%) of the out-of-distribution evalua-</cell></row><row><cell cols="5">tion on stock movement classification with StockNet, RoBERTa-</cell></row><row><cell cols="5">WWM Ext, HAN Stock method and our method from 1/1/2021 to</cell></row><row><cell>12/11/2021.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Accuracy F1 Score Recall Precision</cell></row><row><cell>StockNet[Xu and Cohen, 2018]</cell><cell>44.35</cell><cell>42.52</cell><cell>45.42</cell><cell>45.82</cell></row><row><cell>HAN Stock[Hu et al., 2018]</cell><cell>53.41</cell><cell>53.33</cell><cell>53.69</cell><cell>54.53</cell></row><row><cell>RoBERTa WWM Ext[Cui et al., 2020]</cell><cell>60.15</cell><cell>60.08</cell><cell>59.89</cell><cell>60.78</cell></row><row><cell>Self-supervised SRLP(V masked)+Factors</cell><cell>64.09</cell><cell>63.95</cell><cell>63.90</cell><cell>64.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparison of profitability test on</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Maximum Draw-</cell></row><row><cell cols="6">down(%), Annualized Rate of Return(%), and Sharpe Ratio Rate(%)</cell></row><row><cell cols="6">with strong baselines, XIN9, CSI300 and our proposed method from</cell></row><row><cell>1/1/2021 to 12/11/2021.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Maximum Drawdown</cell><cell>?</cell><cell>Annualized Rate of Return ?</cell><cell>Sharpe Ratio</cell><cell>?</cell></row><row><cell>XIN9</cell><cell>-15.85</cell><cell></cell><cell>-15.38</cell><cell cols="2">-32.01</cell></row><row><cell>CSI300</cell><cell>-14.40</cell><cell></cell><cell>-9.34</cell><cell cols="2">-32.99</cell></row><row><cell>StockNet[Xu and Cohen, 2018]</cell><cell>-7.40</cell><cell></cell><cell>-22.42</cell><cell cols="2">-177.65</cell></row><row><cell>HAN Stock[Hu et al., 2018]</cell><cell>-7.38</cell><cell></cell><cell>-13.50</cell><cell cols="2">-55.84</cell></row><row><cell>RoBERTa WWM Ext[Cui et al., 2020]</cell><cell>-3.83</cell><cell></cell><cell>1.35</cell><cell cols="2">-16.31</cell></row><row><cell>Self-supervised SRLP(V masked) with Factors</cell><cell>-3.60</cell><cell></cell><cell>13.85</cell><cell>40.93</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 4: The comparison for the real trading performance on Return Rate, Draw Down Rate with CSI300 index, XIN9, Roberta WWM Ext, HAN Stock, StockNet and our proposed method from 1/1/2021 to 12/11/2021 et al., 2018] and StockNet</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Equivalent to the Standard and Poor's 500 (S&amp;P 500) or the Dow Jones Industrial Average (DJIA) in the US stock market</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Che</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11616</idno>
		<title level="m">Wanxiang Che, Yunlong Feng, Libo Qin, and Ting Liu. N-ltp: A open-source neural chinese language technology platform with pretrained models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling the momentum spillover effect for stock prediction via attribute-driven graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invero-xl: Making cross-lingual semantic role labeling accessible with intelligible verbs and roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Conia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting pre-trained models for Chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using structured events to predict stock price movement: An empirical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for event-driven stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The role of text-extracted investor sentiment in chinese stock price prediction with the enhancement of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<editor>Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online; Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4067" to="4076" />
		</imprint>
	</monogr>
	<note>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trade the event: Corporate events detection for newsbased event-driven trading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen ; Yumo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining<address><addrLine>Melbourne, Australia; Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
	<note>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
