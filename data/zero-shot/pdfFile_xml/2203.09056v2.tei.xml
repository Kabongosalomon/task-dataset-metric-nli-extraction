<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Table Detection and Structure Recognition from Heterogeneous Document Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chixiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of EEIS</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Table Detection and Structure Recognition from Heterogeneous Document Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Table detection</term>
					<term>Table structure recognition</term>
					<term>Corner detection</term>
					<term>Spatial CNN</term>
					<term>Grid CNN</term>
					<term>Split-and-merge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new table detection and structure recognition approach named RobusTabNet to detect the boundaries of tables and reconstruct the cellular structure of each table from heterogeneous document images. For table detection, we propose to use CornerNet as a new region proposal network to generate higher quality table proposals for Faster R-CNN, which has significantly improved the localization accuracy of Faster R-CNN for table detection. Consequently, our table detection approach achieves state-of-the-art performance on three public table detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by only using a lightweight ResNet-18 backbone network.</p><p>Furthermore, we propose a new split-and-merge based table structure recognition approach, in which a novel spatial CNN based separation line prediction module is proposed to split each detected table into a grid of cells, and a Grid CNN based cell merging module is applied to recover the spanning cells. As the spatial CNN module can effectively propagate contextual information across the whole table image, our table structure recognizer can robustly recognize tables with large blank spaces and geometrically distorted (even curved) tables. Thanks to these two techniques, our table structure recognition approach achieves state-of-the-art performance on three public benchmarks, including SciTSR, PubTabNet and cTDaR TrackB2-Modern. Moreover, we have further demonstrated the advantages of our approach in recognizing tables with complex structures, large blank spaces, as well as geometrically distorted or even curved shapes on a more challenging in-house dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tables are a prevalent means of representing and communicating structured data, which are widely used in diverse types of documents including financial statements, scientific papers, invoices, purchasing orders, etc.</p><p>With the explosive growth of the number of documents, automatic table detection and structure recognition techniques are eagerly desired to reconstruct tables from document images, which can facilitate many downstream applications, such as information retrieval <ref type="bibr" target="#b0">[1]</ref> and question answering <ref type="bibr" target="#b1">[2]</ref>. The aim of table detection is to detect the boundaries of tables, while the aim of table structure recognition (TSR) is to reconstruct the cellular structure of each detected table, i.e., identifying the coordinates of each cell bounding box as well as its row and column spanning information. Both table detection and structure recognition are unsolved problems due to the following challenges. First, tables in documents may have complex structures and diverse styles (erratic use of ruling lines). For example, in financial reports, some borderless tables may have complex hierarchical header structures, contain many empty or spanning cells, or have extremely large/small blank spaces between neighboring columns. Some neighboring tables may be very close to each other, making it hard to determine whether they should be merged or not. In invoices, tables may have different sizes, e.g., some line-item tables may only contain two rows and some others may span multiple pages. Second, tables cells may contain diverse contents, ranging from a single character to a set of more complex page objects such as paragraphs, tables, figures, formulas, etc. Third, some background objects in documents like figures, graphics, flow charts and structurally laid out texts, may have similar textures as tables, which poses another challenge for reduction of false alarms. In forms, some tables may be embedded in other more complex tabular objects (e.g., nested tables), which makes table boundaries ambiguous. Moreover, many camera-captured document images are of poor image quality, and tables contained in them may be distorted (even curved) or contain artifacts or noises, which makes table detection and structure recognition even more difficult.</p><p>In recent years, the success of deep learning in various computer vision applications has motivated researchers to explore deep neural networks for detecting tables and recognizing table structures from document images. These deep learning based table detection and structure recognition approaches have substantially outperformed traditional rule or statistical machine learning based methods in terms of both accuracy and capability <ref type="bibr" target="#b2">[3]</ref>. Most deep learning based table detection approaches (e.g., <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>) treat table as a specific object and borrow various CNN-based object detection and segmentation frameworks, like Faster R-CNN <ref type="bibr" target="#b11">[12]</ref>, Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>, and Cascade R-CNN <ref type="bibr" target="#b13">[14]</ref>, to solve the table detection problem. With the help of some effective techniques like more powerful backbone networks and deformable convolution operations, these CNN based table detection methods, especially CDeC-Net <ref type="bibr" target="#b10">[11]</ref>, have achieved superior performance on many public table detection benchmark datasets. Despite this, the localization accuracy of these methods is still far from satisfactory. For instance, although CDeC-Net has leveraged the Cascade R-CNN model <ref type="bibr" target="#b13">[14]</ref> to improve table detection accuracy, its detection accuracy still drops a lot when the Intersection-over-Union (IoU) threshold is increased from 0.5 to 0.9 during evaluation (Table VIII in <ref type="bibr" target="#b10">[11]</ref>). As the localization accuracy of table detection will significantly affect the performance of the following TSR task, more effective techniques to improve the localization accuracy of these CNN based table detection methods are still desired. For table structure recognition, deep learning based methods (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>) have already made great progress towards recognizing tables with complex structures and diverse styles. Recent best performing table structure recognition approaches, like TabStruct-Net <ref type="bibr" target="#b21">[22]</ref> and LGPMA <ref type="bibr" target="#b22">[23]</ref>, typically use CNNbased object detection or segmentation models like Mask R-CNN to detect table cells first, then adopt some cell grouping/clustering algorithms to predict row/column relationships between the detected cells. Although these methods have achieved very high accuracy on benchmark datasets like SciTSR <ref type="bibr" target="#b24">[25]</ref> and PubTabNet <ref type="bibr" target="#b26">[26]</ref>, they still cannot be directly applied to geometrically distorted or even curved tables as they rely on an assumption that tables are axis-aligned.</p><p>In some real-world application scenarios like the "Insert data from picture" feature <ref type="bibr" target="#b1">2</ref> in Excel, document images may be captured by mobile cameras. In these camera-captured images, it is inevitable that tables are geometrically distorted. However, existing benchmark datasets haven't taken this important scenario into account as images in these datasets are either captured by scanners or converted from digital PDF files. Thus, more research is needed to find out new table structure recognition approaches robust to geometrically distorted or even curved tables.</p><p>In this paper, we propose a new table detection and structure recognition approach named RobusTabNet to overcome the abovementioned challenges. For table detection, we use CornerNet <ref type="bibr" target="#b27">[27]</ref> as a new region proposal network for Faster R-CNN, which generates table proposals by detecting and grouping corner points. With these corner-based high quality region proposals, our approach achieves superior performance even with a very lightweight backbone network, i.e., ResNet-18 <ref type="bibr" target="#b28">[28]</ref>. For TSR, we present a new split-and-merge based approach and propose two effective techniques to significantly improve its capability. First, we propose a novel spatial CNN <ref type="bibr" target="#b29">[29]</ref> based separation line prediction module to split each detected table into a grid of cells. As the spatial CNN can effectively propagate contextual information across the whole table image, our separation line prediction algorithm can improve the robustness of our table structure recognizer to tables with large blank spaces and distorted or even curved shapes. Second, we propose a Grid CNN based cell merging module to recover the wrongly split cells, especially the spanning cells. In this module, the whole table is compactly represented as a grid so that a simple CNN based cell merging module can achieve higher accuracy than Relation Network or Graph Convolutional Network (GCN) based methods. With these new techniques, the proposed RobusTabNet has achieved state-of-the-art performance on both table detection (cTDaR TrackA <ref type="bibr" target="#b30">[30]</ref>, PubLayNet <ref type="bibr" target="#b31">[31]</ref> and IIIT-AR-13K <ref type="bibr" target="#b32">[32]</ref>) and structure recognition (SciTSR <ref type="bibr" target="#b24">[25]</ref>, PubTabNet <ref type="bibr" target="#b26">[26]</ref> and cTDaR TrackB2-Modern <ref type="bibr" target="#b30">[30]</ref>) public benchmarks. We have further validated the robustness of our approach to tables with complex structures, large blank spaces, as well as distorted or even curved shapes on a more challenging in-house dataset.</p><p>The main contributions of this paper are as follows:</p><p>? <ref type="table">We present a new table detector by using Corner-Net as a new region proposal network for Faster R-CNN to achieve high table localization</ref> accuracy. Compared with RPN <ref type="bibr" target="#b11">[12]</ref>, the percentage of well-localized proposals (IoU&gt;0.9) in the positive samples (IoU&gt;0.7) from CornerNet is much higher, which contributes to better end-to-end table detection performance.</p><p>? We present a new split-and-merge based table structure recognizer, which is robust to geometrically distorted or even curved tables. To this end, a new spatial CNN based separation line prediction approach is proposed to robustly predict curvilinear separation lines from distorted or even curved tables, while a Grid CNN module is proposed to recover spanning cells efficiently and effectively.</p><p>? Our proposed Rule-based methods are among the earliest approaches for locating tables inside documents. These methods usually exploit visual clues (e.g., text-block arrangement <ref type="bibr" target="#b33">[33]</ref>, or horizontal and vertical lines <ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref>), keywords <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>, or formal templates <ref type="bibr" target="#b39">[39]</ref> to detect tables in particular scenarios. We refer readers to <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41]</ref> for a more detailed summarization of these conventional approaches. Rule-based methods usually require extensive manual efforts to design heuristic rules and tune hyper-parameters. To reduce the dependence on heuristics, lots of statistical machine learning based approaches have been proposed, e.g., <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>. Although these methods have improved table detection accuracy significantly, they still rely on handcrafted features, which limit their generalization ability. A comprehensive review of these statistical machine learning based methods can be found in <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Deep learning based methods</head><p>With the rapid development of deep learning, numerous CNN based table detection methods have been proposed and outperformed traditional methods by a big margin in terms of both accuracy and capability. These methods can be roughly classified into three categories: object detection based methods, semantic segmentation based methods, and bottom-up methods.</p><p>Object detection based methods. These methods adapt state-of-the-art top-down object detection or instance segmentation frameworks to solve the table detection problem. Initially, Hao et al. <ref type="bibr" target="#b3">[4]</ref>, Yi et al. <ref type="bibr" target="#b45">[45]</ref>, and Oliveira et al. <ref type="bibr" target="#b46">[46]</ref> adopted R-CNN <ref type="bibr" target="#b47">[47]</ref> for table detection first, but the performance of these methods was limited by the traditional region proposal generation methods, which relied on the heuristic rules and handcrafted features. Later, more advanced object detectors, like Fast R-CNN <ref type="bibr" target="#b48">[48]</ref>, Faster R-CNN <ref type="bibr" target="#b11">[12]</ref>, YOLO <ref type="bibr" target="#b49">[49]</ref>, RetinaNet <ref type="bibr" target="#b50">[50]</ref>, Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, were explored by Vo et al. <ref type="bibr" target="#b4">[5]</ref>, Gilani et al. <ref type="bibr" target="#b5">[6]</ref>, Schreiber et al. <ref type="bibr" target="#b14">[15]</ref>, Huang et al. <ref type="bibr" target="#b6">[7]</ref>, Zheng et al. <ref type="bibr" target="#b7">[8]</ref>, Saha et al. <ref type="bibr" target="#b8">[9]</ref>, Prasad et al. <ref type="bibr" target="#b9">[10]</ref> and Agarwal et al. <ref type="bibr" target="#b10">[11]</ref>, to detect tables (as well as other page objects like figures and formulas) from document images, respectively. The accuracy of these detectors for table detection could be improved further by adding some effective techniques. For example, Gilani et al. <ref type="bibr" target="#b5">[6]</ref>, Arif et al. <ref type="bibr" target="#b51">[51]</ref>, and Prasad et al. <ref type="bibr" target="#b9">[10]</ref> proposed to use image transformation techniques, e.g., distance transforms, coloration and dilation, to enhance input document images or augment the used training sets so that additional clues could be provided to the detectors. Siddiqui et al. <ref type="bibr" target="#b52">[52]</ref> incorporated deformable convolution and deformable RoI Pooling operations <ref type="bibr" target="#b53">[53]</ref> into Faster R-CNN to make the model more robust to geometric transformations. Agarwal et al. <ref type="bibr" target="#b10">[11]</ref> employed a more powerful backbone network, i.e., a composite backbone network <ref type="bibr" target="#b54">[54]</ref> with deformable convolution filters, to push the accuracy of Cascade Mask R-CNN further. Although this method achieved state-of-the-art performance on several benchmark datasets (e.g., <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref><ref type="bibr" target="#b57">[57]</ref>), it suffered from high computation complexity and memory usage.</p><p>To improve the localization accuracy, Sun et al. <ref type="bibr" target="#b58">[58]</ref> adopted Faster R-CNN to detect table boxes and the corresponding corner boxes simultaneously and used a post-processing algorithm to adjust table boundaries according to the detected corners. However, the corner boxes are manually predefined small boxes, and the size has no explicit meaning, which leads to higher miss detection rate for corners <ref type="bibr" target="#b58">[58]</ref>.</p><p>Semantic segmentation based methods. These methods (e.g., <ref type="bibr" target="#b59">[59]</ref><ref type="bibr" target="#b60">[60]</ref><ref type="bibr" target="#b61">[61]</ref><ref type="bibr" target="#b62">[62]</ref>) treat table detection as a semantic segmentation problem and leverage existing semantic segmentation frameworks like FCN <ref type="bibr" target="#b63">[63]</ref> to predict a pixel-level segmentation mask first, and then group table pixels into tables. Yang et al. <ref type="bibr" target="#b59">[59]</ref> proposed a multimodal FCN for page segmentation to detect tables and other page objects, in which both visual features from images and linguistic features from the content of underlying texts are leveraged to improve segmentation accuracy. He et al. <ref type="bibr" target="#b60">[60]</ref> proposed a multi-scale multi-task FCN to predict two sets of segmentation masks for text-block/table/figure regions and their corresponding contours first. After refined by a conditional random field (CRF) model, these segmentation masks are then input to a post-processing module to obtain table regions. Kavasidis et al. <ref type="bibr" target="#b61">[61]</ref> proposed a saliency-based FCN performing multi-scale reasoning on visual cues followed by a fully connected CRF for localizing tables and charts in digital/digitized documents.</p><p>Bottom-up methods. Most bottom-up methods model each document image as a graph, where each node represents a page object (e.g., word, text-line) and each edge represents a neighboring relationship between two page objects, and then formulate table detection as a graph labeling problem. Li et al. <ref type="bibr" target="#b64">[64]</ref> used traditional layout analysis methods to generate line regions first, then applied two hybrid CNN-CRF models to classify them into four classes (text, formula, table, figure) and predict whether each pair of line regions belong to a same cluster, respectively. After that, regions belonging to the same class and the same cluster were merged to get page objects. Riba et al. <ref type="bibr" target="#b65">[65]</ref> and Hole?ek et al. <ref type="bibr" target="#b66">[66]</ref> took text regions (words or text-lines) as nodes and generated a visibility or neighborhood graph to represent the underlying structure of each input document first, then used graph neural networks to perform node and edge classification. After that, connected subgraphs where the nodes are classified as table were extracted as tables. Recently, Li et al. <ref type="bibr" target="#b67">[67]</ref> proposed to consider document layout analysis as a text-based sequence labeling problem and leveraged pre-trained language models to classify each word into a pre-defined page object category, including table. These bottom-up methods depend on certain assumptions like availability of accurate word/text-line bounding boxes as additional inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Table structure recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Traditional methods</head><p>Early table structure recognition methods were mainly based on handcrafted features and heuristic rules. These methods (e.g., <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b68">[68]</ref><ref type="bibr" target="#b69">[69]</ref><ref type="bibr" target="#b70">[70]</ref><ref type="bibr" target="#b71">[71]</ref>) are mostly applied to simple table structures or specific data formats, such as PDF files. To reduce the dependence on heuristics, a few statistical machine learning based methods were proposed later, e.g., <ref type="bibr" target="#b72">[72]</ref>. A comprehensive review of these traditional methods can be found in <ref type="bibr" target="#b44">[44]</ref>. These traditional methods usually make strong assumptions about table layouts and rely on domain-specific heuristics, which limit their generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Deep learning based methods</head><p>Recently, there is a trend to leverage deep learning models to solve the TSR problem. These methods can be roughly divided into three categories: row/column extraction based methods, image-to-markup generation based methods and bottom-up methods.</p><p>Row/column extraction based methods. These methods usually adopt object detection or semantic segmentation frameworks to detect rows and columns from a table image first, then intersect the detected rows and columns to generate a grid of cells. DeepDeSRT <ref type="bibr" target="#b14">[15]</ref> is the first to apply FCN based semantic segmentation models to the TSR task. They adopted two FCN models to segment tables into rows and columns first, and then used post-processing algorithms to deal with spurious detection fragments as well as severed and conjoined structures. However, this vanilla FCN based row/column segmentation method cannot robustly predict complete segmentation masks for rows and columns when tables contain large blank spaces <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. To alleviate this problem, Siddiqui et al. <ref type="bibr" target="#b15">[16]</ref> and Tensmeyer et al. <ref type="bibr" target="#b16">[17]</ref> pooled features along rows and columns of pixels on some intermediate feature maps, which enabled their FCN models to leverage much wider contextual information to improve row/column segmentation accuracy. Instead of relying on FCN, Khan et al. <ref type="bibr" target="#b17">[18]</ref> proposed to use sequential models like bi-directional gated recurrent unit networks (GRU) to scan pre-processed table images from topto-bottom and left-to-right to identify row and column separators. Siddiqui et al. <ref type="bibr" target="#b18">[19]</ref> proposed to formulate the problem of row/column identification in a tabular structure as an object detection problem instead of a semantic segmentation problem, and leveraged three object detection models, namely deformable Faster R-CNN, deformable R-FCN and deformable FPN, to detect the bounding boxes of rows and columns from tables directly. Hashmi et al. <ref type="bibr" target="#b19">[20]</ref> adopted another object detection model, i.e., Mask R-CNN with optimized anchors, to further improve row/column detection accuracy. All the abovementioned methods, except Tensmeyer et al. <ref type="bibr" target="#b16">[17]</ref>, didn't take spanning cells into consideration and can only recover the basic grid structures of tables. To deal with spanning cells, Tensmeyer et al. <ref type="bibr" target="#b16">[17]</ref> presented the SPLERGE method, which used a Split model to produce the grid structure of an input table first, and then used a Merge model to predict which grid elements should be merged to recover spanning cells. Differing from this two-stage paradigm, Zou et al. <ref type="bibr" target="#b73">[73]</ref> proposed a onestage approach to segmenting the real row and column separators directly to avoid over-splitting spanning cells. Although these methods have achieved promising results on some benchmark datasets, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b55">55]</ref>, they cannot be directly applied to distorted or even curved tables as they rely on an implicit assumption that tables are axis-aligned.</p><p>Image-to-markup generation based methods. These methods treat table recognition as an image-tomarkup generation problem and adopt existing imageto-markup models to directly convert each source table image into target presentational markup that fully describes its structure and cell contents. Deng et al. <ref type="bibr" target="#b74">[74]</ref> constructed a new dataset TABLE2LATEX-450K and proposed to use an attentional encoder-decoder model to convert tables into LaTeX source codes. Li et al. <ref type="bibr" target="#b57">[57]</ref> defined a set of HTML tags to describe table structures only and presented a new table benchmark dataset known as TableBank. Zhong et al. <ref type="bibr" target="#b26">[26]</ref> introduced another large scale table benchmark dataset PubTabNet, which contains 568k table images with corresponding structured HTML representation, and introduced an attention-based encoder-dual-decoder architecture to recognize table structures and cell contents simultaneously. These methods rely on a large amount of training data and still struggle with big and complex tables <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b57">57]</ref>.</p><p>Bottom-up methods. One group of bottom-up methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b76">76]</ref> treat words or cell contents as nodes in a graph and use graph neural networks to predict whether each sampled node pair is in a same cell, row, or column. These methods rely on an assumption that the bounding boxes of words or cell contents are available as additional inputs, which are not easy to obtain from table images directly. To eliminate this assumption, another group of methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> proposed to detect the bounding boxes of table cells directly. After cell detection, Zheng et al. [8] and Qiao et al. <ref type="bibr" target="#b22">[23]</ref> designed some rules to cluster cells into rows and columns. In order to improve the accuracy of both cell detection and cell clustering, Raja et al. <ref type="bibr" target="#b21">[22]</ref> introduced a novel loss function that modeled the inherent alignment of cells in the cell detection network, and a graph-based problem formulation to build associations between the detected cells. However, this method still fails to handle tables containing a large number of empty cells and distorted tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our table extraction approach, RobusTabNet, consists of two deep learning models, i.e., a table detector and a table structure recognizer. For each input image, we first use our table detector to detect all tables within it and crop them from the original image. Then, each cropped table image is resized to an appropriate resolution and fed into the table structure recognizer to reconstruct its cellular structure. Finally, the recognition results are mapped back onto the original image. An outline of our approach and the expected outputs are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Details of our table detector and structure recognizer will be introduced in Section 3.2 and Section 3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CornerNet-FRCN based table detector</head><p>Existing CNN-based table detection methods typically use RPN to generate table proposals. We find that the percentage of well-localized table proposals (IoU&gt;0.9) in the positive samples (IoU&gt;0.7) generated by RPN is not high enough, which is an important reason for the unsatisfactory localization accuracy of these table detectors (see analysis in Section 5.3.2). To address this issue, we propose to use CornerNet to detect the top-left and bottom-right corners of all table bounding boxes first and then group each pair of top-left and bottom-right corners to obtain table proposals. As table corners can be precisely inferred from ruling lines and alignment of cell contents in tables, the positive proposals (IoU&gt;0.7) generated by our approach will be of higher localization accuracy, which can improve the localization accuracy of our table detector effectively. After that, we use a simple Fast R-CNN module to reject non-table proposals and refine the bounding boxes of remaining positive proposals further.</p><p>The overall architecture of our approach is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. There are three core modules: 1) A CNN backbone network that is responsible for computing a shared convolutional feature map; 2) A CornerNet based region proposal generation module, which detects the top-left and bottom-right corners of the tables and enumerates all the potential table proposals; 3) A Fast R-CNN (FRCN) module, which is used to prune non-table proposals and refine the bounding boxes of remaining table proposals. For the sake of efficiency, a ResNet-18 network with dilations in "Conv5" is used as the backbone network, and the stride of the output feature map, named Dilated-C5, is 16 pixels. We further use a 1 ? 1 convolutional layer to reduce the channel dimension of Dilated-C5 from 512 to 64 for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">CornerNet as region proposal network</head><p>CornerNet <ref type="bibr" target="#b27">[27]</ref> detects an object as a pair of keypoints, i.e., the top-left and bottom-right corners of the bounding box. It uses a convolutional network to predict two sets of heatmaps to represent the locations of the top-left and bottom-right corners of different object categories respectively, as well as an embedding vector for each detected corner such that the distance between the embeddings of two corners from the same object is small. To produce tighter bounding boxes, the network also predicts offsets to slightly adjust the locations of the corners. With the predicted heatmaps, embeddings and offsets, a simple post-processing algorithm is applied to obtain the final bounding boxes. However, Duan et al. <ref type="bibr" target="#b77">[77]</ref> find that the performance of the abovementioned corner grouping method is restricted by its relatively weak ability of referring to the global information of an object. Therefore, in this work, we abandon the embedding vectors and adopt CornerNet as a new region proposal network for Faster R-CNN by detecting and exhaustively grouping corner points.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we append a 3 ? 3 convolutional layer with the stride of 1 on Dilated-C5 to generate a new feature map Dilated-C5', on which two sibling branches are attached for detecting top-left and bottom-right corners, respectively. Taking the top-left corner detection branch as an example, we first use a top-left corner pooling module, composed of a top pooling and a left pooling operator <ref type="bibr" target="#b27">[27]</ref>, to aggregate context information. The context enhanced feature map is fused with the original feature map in a residual connection manner. Then, a detection module is attached to this feature map and performs dense perpixel prediction of top-left corners. Specifically, let p i and q j be a pixel on the feature map and raw image with the coordinates of (p x i , p y i ) and (q x j , q y j ), respectively. We define that p i is corresponding to q j if</p><formula xml:id="formula_0">p x i = q x j s and p y i = ? ? ? ? ? ? ? ? ? q y j s ? ? ? ? ? ? ? ? ? ,<label>(1)</label></formula><p>where s denotes the stride of the feature map. If p i is corresponding to q j and q j is a top-left corner point, the detection module will give p i a "top-left corner" label 6 and predict the corresponding offset ? i defined by</p><formula xml:id="formula_1">? i = ? ? ? ? ? ? ? q x j s ? q x j s , q y j s ? ? ? ? ? ? ? ? ? ? q y j s ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ,<label>(2)</label></formula><p>to adjust the location of the corner to compensate for the quantization error caused by network downsampling. As depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, the detection module contains two parallel branches, a 3 ? 3 convolutional layer followed by a 1 ? 1 convolutional layer in each branch, for corner/non-corner classification and corner offset regression, respectively. Furthermore, if a pair of false corner detections are close to the corresponding ground truth corner locations, they can still produce a box that highly overlaps with the ground-truth box. Therefore, during training, we reduce the penalty of the negative locations within a radius r of the positive location, and the radius r is determined by the size of the ground-truth box. We refer readers to <ref type="bibr" target="#b27">[27]</ref> for more details about the selection of r. Once r is determined, the amount of penalty reduction is given by an unnormalized 2D Gaussian, e ? x 2 +y 2 2? 2 , whose center is at the positive location and ? is set as r/3.</p><p>To generate table proposals, we first apply nonmaximal suppression (NMS) by using a 3 ? 3 max pooling layer on the corner heatmaps. Then top-K top-left and bottom-right corners are extracted from the heatmaps, which are further filtered by a score threshold, C th . The locations of the remaining corners are adjusted by the corresponding predicted offsets. Then, we take all the valid combinations, i.e., the x and y coordinates of the top-left corner are smaller than that of the bottom-right corner, as table proposals, so that a high recall rate is retained. After that, we use the standard NMS algorithm with an IoU threshold of 0.7 to remove redundant proposal boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Fast R-CNN</head><p>Given the extracted region proposals, we adopt a Fast R-CNN module to reject negative (non-table) proposals and refine the bounding boxes of positive (table) proposals. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, for each proposal, we first adopt an RoI Align algorithm <ref type="bibr" target="#b12">[13]</ref> to extract a 7 ? 7 ? 64 feature descriptor from the proposal box on the Dilated-C5 feature map. Then, it is fed into two 1,024-d fully connected ( f c) layers (each followed by a ReLU activation function) before the final <ref type="table">table/nontable classification and bounding box regression layers.</ref> During training, a proposal is assigned a positive label if it has an IoU over 0.7 with any ground-truth bounding box, or a negative label if it has IoU lower than 0.5 for all ground-truth bounding boxes. An online hard example mining (OHEM) method is adopted to select an equal number of hard positive and hard negative samples to train the Fast R-CNN module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Split-and-merge based table structure recognizer</head><p>After table detection, each detected table is cropped from the raw image and resized to an appropriate size to ensure that there is enough inter-line spacing for separation line prediction. Then, each resized table image is fed into a table structure recognizer to reconstruct its cellular structure. The flowchart of our table structure recognizer is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Given a cropped table image, a spatial CNN based separation line prediction module is used to predict a row separator mask and a column separator mask first. Then, a connected component analysis (CCA) based line generation algorithm is used to extract all row and column separation lines from the predicted separator masks, which are intersected to generate a grid of cells. After that, a Grid CNN based cell merging module is adopted to merge wrongly split cells into spanning cells. The separation line prediction module and cell merging module share a same CNN backbone network and are trained jointly. For the sake of efficiency, we adopt the Feature Pyramid Network (FPN) <ref type="bibr" target="#b78">[78]</ref>, which is built on top of ResNet-18, as the backbone network. Details of the separation line prediction and cell merging modules are described in Section 3.3.1 and 3.3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Spatial CNN based separation line prediction</head><p>Some important visual clues like ruling lines and alignment of cell contents provide useful hints to indicate whether a separation line exists at a position within a table. However, the ResNet-FPN backbone cannot embed such useful visual clues into the features of pixels in large blank regions of borderless tables effectively, because each feature vector on the output convolutional feature map only contains local context information extracted from its effective receptive field. Based on such convolutional feature map, it is hard for the following separation line segmentation module to predict separation lines from large blank spaces in borderless tables robustly, because the feature vector of each pixel in these blank regions does not contain enough information to determine whether a separator line passes through this pixel or not. To address this issue, we propose to use spatial CNN modules <ref type="bibr" target="#b29">[29]</ref> to enhance the feature representation of each pixel on the convolutional feature map by propagating contextual information across the whole feature map in left-right or top-bottom directions.</p><p>The overall architecture of our spatial CNN based separation line prediction module is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Given an input image X ? R H?W?3 , we adopt the FPN backbone network to generate a shared convolutional feature map P 2 ? R H 4 ? W 4 ?C , where C represents the number of channels and is set to 64 in our experiments. Then, two parallel semantic segmentation branches are attached to P 2 to predict a row separator mask? row and a column separator mask? col , respectively. Taking the row separation line prediction branch as an example, we add a 3 ? 3 convolutional layer and three repeated down-sampling blocks, each composed of a sequence of a 1 ? 2 max-pooling layer, a 3 ? 3 convolutional layer and a ReLU activation function, after P 2 sequentially to generate a down-sampled feature map P 2 ? R H 4 ? W 32 ?C , which is taken as the input of two cascaded spatial CNN modules. The first spatial CNN module divides the feature map into W 32 slices along the width direction, which are denoted as S w = {s w i ? R H 4 ?1?C |i ? N, i = 1, 2, ..., W 32 } then propagates the information from the leftmost slice s w 1 to the rightmost slice s w W/32 with convolution operators. Specifically, the leftmost slice s w 1 is convolved by a convolution kernel with the kernel size of 9 ? 1 (9 and 1 represent kernel height and width respectively) and its output feature map is merged with its right slice s w 2 by element-wise addition. This procedure is done iteratively so that the information can be propagated from the leftmost slice to the rightmost slice effectively. The second spatial CNN module uses the same method to propagate the information from the rightmost slice s w W/32 to the leftmost slice s w 1 . In this way, each pixel in the output feature map can leverage the structural information from both sides to enhance its feature representation ability. Finally, this contextenhanced feature map is up-sampled by a factor of 4 with a bilinear interpolation operation to generate an output feature map P out ? R H? W 8 ?C , on which a 1 ? 1 convolutional layer followed by a sigmoid activation function is attached to predict a row separator mask S row ? R H? W 8 ?1 . The architecture of the column separation line prediction branch is similar to the row separation line prediction branch, except that the downsampling is performed along the height direction and the two spatial CNN modules propagate information from the topmost slice to the bottommost slice and from the bottommost slice to the topmost slice, respectively.</p><p>To generate the ground-truth (GT) row and column separator masks, the row and column separation lines of each table as well as the bounding boxes of textlines in each cell are annotated ( <ref type="figure" target="#fig_4">Fig. 5(a)</ref>). Following SPLERGE <ref type="bibr" target="#b16">[17]</ref>, we calculate the GT separator masks by maximizing the size of the separation regions without intersecting any non-spanning cell contents, as shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>. Specifically, for each annotated row separation line, we move it upwards and downwards respectively until it touches a text box that belongs to a non-spanning cell to obtain its corresponding row separation region. The similar procedure can also be applied to generate column separation regions. After this step, if the thickness of a separation region is less than 8 pixels, we will further expand it to make sure that its thickness is at least 8 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Cell generation</head><p>After separation line prediction, we first binarize the predicted row and column separator heatmaps with a classification score threshold, S th . Then, we extract the connected components (CCs) from the segmentation masks, which represent detected separators. With these CCs, we can extract all row and column separation lines as well as their corresponding line thicknesses. Taking row separation line generation as an example, we first apply the findContours method in OpenCV <ref type="bibr" target="#b79">[79]</ref> to the binarized row separator mask to obtain the contours of row CCs. Then for each row CC, we use a polynomial curve fitting algorithm to fit a function y = f (x) from its contour points, which approximates the center line of this row CC. To compute the corresponding line thickness, a vertical scan-line is used to traverse the related row CC mask from left to right with a stride of 8 pixels. On each scanned pixel column, a line segment can be obtained by intersecting the scan-line with the upper and lower boundaries of the row CC. By averaging the lengths of all the line segments, we can estimate the line thickness of the separation line, denoted by lw. Then, we translate the fitted separation line y = f (x) upwards and downwards respectively to generate two border lines, y = f (x) + lw 2 and y = f (x)? lw 2 . Similarly, the column separation lines can also be generated by rotating the column separator heatmap with 90 degrees before running this algorithm. Finally, we intersect all the translated row lines with column lines to calculate all intersection points, from which we can extract all the shrunk cell boxes, e.g., the blue box in <ref type="figure" target="#fig_5">Fig. 6</ref>, and arrange them in a grid manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Grid CNN based cell merging</head><p>Based on the compact grid representation, we introduce a Grid CNN module to aggregate context information effectively with several stacked convolution layers to improve cell merging accuracy. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, we first use an RoI Align algorithm to extract a 7 ? 7 ? 64 feature descriptor from the bounding box of each cell, which is then fed into a 2-hidden-layer fully connected ( f c) neural network with 512 nodes at each layer to generate a 512-d feature vector. Assume that the detected cells are arranged in M rows and N columns, then the corresponding feature vectors will construct a new feature map F grid ? R M?N?512 . Each pixel in F grid corresponds to a cell generated by the split model. F grid is then convolved by three 3 ? 3 convolutional layers to obtain an enhanced feature map F grid ? R M?N?512 . Finally, we use a relation network <ref type="bibr" target="#b80">[80]</ref> to predict whether each pair of adjacent pixels on F grid , which corresponds to each pair of adjacent cells in the input table image, should be merged or not. Here, we only consider 4-adjacency neighborhood relations to construct relational pairs.</p><p>The architecture of the relation network is shown in the blue dashed box in <ref type="figure" target="#fig_5">Fig. 6</ref>. Given a pair of adjacent pixels in F grid , p i and p j , whose corresponding feature vectors are F p i grid and F p j grid and corresponding cell bounding boxes are b i and b j respectively, we extract a feature representation x i j to encode the appearance compatibility and spatial compatibility of their corresponding cells. Specifically, x i j is constructed by concatenating the appearance features F p i grid and 9 F p j grid and the spatial compatibility feature l i j of b i and b j , i.e., x i j = [F p i grid ; l i j ; F p j grid ]. The spatial compatibility feature l i j is used to measure the relative scale and location relationships between b i and b j . Following Zhang et al. <ref type="bibr" target="#b80">[80]</ref>, let b i j denote the union bounding box of b i and b j , then l i j is defined as an 18-d vector concatenating three 6-d vectors, which indicate the box delta of b i and b j , b i and b i j , b j and b i j , respectively. Given two bounding boxes</p><formula xml:id="formula_2">b i = {x i , y i , w i , h i } and b j = {x j , y j , w j , h j }, their box delta is defined as ?(b i , b j ) = (t i j x , t i j y , t i j w , t i j h , t ji x , t ji y )</formula><p>where each dimension is given by</p><formula xml:id="formula_3">t i j x = x i ? x j /w i , t i j y = y i ? y j /h i , t i j w = log w i /w j , t i j h = log h i /h j ,<label>(3)</label></formula><p>t ji x = x j ? x i /w j , t ji y = y j ? y i /h j . A binary classifier is applied on the feature representation to predict whether each pair of cells should be merged or not. It is implemented with a 2-hidden-layer MLP with 512 nodes at each hidden layer and a sigmoid activation node at its output layer. Note that in the inference stage, each pair of cells is predicted twice for the inputs x i j and x ji , and the maximum value is taken as the final merging score for this pair of cells.</p><p>In the training stage, we use detected cells from the split model to generate positive and negative relational pairs for training the cell merging module. As illustrated in <ref type="figure" target="#fig_4">Fig. 5(c-d)</ref>, given all the ground-truth cell boxes, a detected cell box b det is assigned to a ground-truth cell box b gt if the following condition is satisfied, i.e.,</p><formula xml:id="formula_4">Area(b det ? b gt ) Area(b det ) &gt; 0.5,<label>(4)</label></formula><p>where Area(b det ) and Area(b det ? b gt ) denote the area of b det and the area of the overlap between b det and b gt , respectively. Then, each detected cell is paired with each of its 4-connected cells to construct candidate relational pairs. If two cells in a relational pair are assigned to a same ground-truth cell box, we give this relational pair a positive label, otherwise a negative label. During training, we ignore all the negative relational pairs that contain cells not assigned to any ground-truth cell, and then adopt an OHEM method to select hard samples to train the cell merging module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Loss Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Table detection</head><p>Loss for CornerNet based region proposal network.</p><p>There are two sibling output layers for each corner detection branch, i.e., a corner/non-corner classification layer and an offset regression layer. The multi-task loss function can be defined as follows: <ref type="bibr" target="#b4">(5)</ref> where N t and N c denote the number of tables and the number of corners in a mini-batch respectively, c i and c * i are the predicted and "ground-truth" labels for the i-th pixel on the heatmap, c * i has been augmented with the unnormalized Gaussians to reduce the penalty around the ground-truth locations, L det (c i , c * i ) is a variant of focal loss as in <ref type="bibr" target="#b27">[27]</ref> for classification tasks, t j and t * j are predicted and ground-truth 2-d coordinate offsets defined by Eq. 1 and Eq. 2 for the j-th corner, <ref type="bibr" target="#b11">[12]</ref> for regression tasks. Loss for Fast R-CNN. There are two sibling output layers for the Fast R-CNN module, i.e., a table/nontable classification layer and a quadrilateral bounding box regression layer. The multi-task loss function is defined as follows: <ref type="bibr" target="#b5">(6)</ref> where N is the number of sampling region proposals (including N f g positive ones), k i and k * i are predicted and ground-truth labels for the i-th sampling region proposal respectively, L cls (k i , k * i ) is a cross-entropy loss for classification tasks, b j and b * j are predicted and ground-truth 8-d normalized coordinate offsets as stated in <ref type="bibr" target="#b81">[81]</ref> for the j-th positive region proposal, L reg (b j , b * j ) is an L 1 loss for regression tasks.</p><formula xml:id="formula_5">L corner = 1 N t i L det (c i , c * i ) + 1 N c j L o f f (t j , t * j ),</formula><formula xml:id="formula_6">L o f f (t j , t * j ) is a Smooth-L 1 loss</formula><formula xml:id="formula_7">L f rcn = 1 N i L cls (k i , k * i ) + 1 N f g j L reg (b j , b * j ),</formula><p>Total loss for table detector. With the definitions of L corner and L f rcn , the training loss for the table detector can be defined as follows:</p><formula xml:id="formula_8">L detector = ? corner ? L corner + L f rcn ,<label>(7)</label></formula><p>where ? corner is a loss-balancing parameter, and we set ? corner = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Table structure recognition</head><p>Loss for spatial CNN based separation line prediction. There are two branches in the separation line prediction module for row and column separator prediction, respectively. The total loss of this module is the sum of the losses of two branches. Let N row and N col denote the number of sampling pixels for row and column separator prediction branch respectively, {R i , C j } and {R * i , C * j } be the predicted and groundtruth labels for the i-th sampling pixel on the row separator heatmap and the j-th sampling pixel on the column separator heatmap respectively, and L(R i , R * i ) and L(C j , C * j ) be the binary cross-entropy loss for classification tasks. Based on these definitions, the loss function for the separation line prediction module can be defined as follows:</p><formula xml:id="formula_9">L split = 1 N row i L(R i , R * i ) + 1 N col j L(C j , C * j ). (8)</formula><p>Loss for Grid CNN based cell merging. Let N p be the number of selected relational pairs for cell merging, r i and r * i be the predicted and ground-truth labels for the i-th relational pair, and L(r i , r * i ) be a binary crossentropy loss for classification tasks. The loss function for the cell merging module is defined as follows:</p><formula xml:id="formula_10">L merge = 1 N p i L(r i , r * i ).<label>(9)</label></formula><p>Total loss for table structure recognizer. With the definitions of L split and L merge , the training loss for the table structure recognizer can be defined as follows:</p><formula xml:id="formula_11">L recognizer = L split + L merge .<label>(10)</label></formula><p>5. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and evaluation protocols</head><p>We conduct comprehensive experiments on three table detection benchmark datasets, including cTDaR TrackA <ref type="bibr" target="#b30">[30]</ref>, PubLayNet <ref type="bibr" target="#b31">[31]</ref> and IIIT-AR-13K <ref type="bibr" target="#b32">[32]</ref>, and three table structure recognition datasets, including SciTSR <ref type="bibr" target="#b24">[25]</ref>, PubTabNet <ref type="bibr" target="#b26">[26]</ref> and cTDaR TrackB2-Modern <ref type="bibr" target="#b30">[30]</ref>, to evaluate the performance of our table detection and structure recognition approaches, respectively. We follow the evaluation protocols defined by the authors to make our results comparable to the ones reported by other methods. Moreover, to demonstrate the advantage of our TSR approach in dealing with geometrically distorted tables, we have also collected a much more challenging in-house dataset which contains many distorted or even curved tables.</p><p>cTDaR TrackA <ref type="bibr" target="#b30">[30]</ref> contains both historical and modern document images.</p><p>The historical subset contains hand-drawn tables and handwritten texts, including 600 images for training and 199 images for testing. The modern subset contains printed PDF documents, including 600 images for training and 240 images for testing. It adopts the weighted average (WAvg.) F1-score as evaluation metric, which is calculated with the IoU thresholds of 0.6, 0.7, 0.8 and 0.9.</p><p>PubLayNet <ref type="bibr" target="#b31">[31]</ref> is a high-quality dataset for document layout analysis, which contains 335,703 images for training, 11,245 images for validation and 11,405 images for testing. We use it for table detection performance evaluation, and only use the images containing at least a table for model training (86,460 images). Since the annotations of the testing set are not released, we only report results on the validation set. The COCO evaluation protocol is used as the evaluation metric of this dataset.</p><p>IIIT-AR-13K <ref type="bibr" target="#b32">[32]</ref> is introduced for graphical object detection in annual reports, which contains 9,333 images for training, 1,955 images for validation and 2,120 images for testing. This dataset is used for table detection performance evaluation only. The PASCAL VOC evaluation protocol is used as the evaluation metric of this dataset.</p><p>SciTSR <ref type="bibr" target="#b24">[25]</ref> contains 12,000 training images and 3,000 testing images cropped from scientific papers. To evaluate the performance of different methods on complicated tables, authors also extract all the 716 complicated tables from the test set as a test subset, called SciTSR-COMP. The adjacency relation-based evaluation metric, which is used in ICDAR-2013 table competition <ref type="bibr" target="#b55">[55]</ref>, is employed as the evaluation metric of this dataset.</p><p>PubTabNet <ref type="bibr" target="#b26">[26]</ref> contains 500,777 training images, 9,115 validating images and 9,138 testing images. This dataset contains a large number of three-lines tables with empty or spanning cells. Since the annotations of testing set are not released, we only report results on the validation set. The authors proposed a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition task, which can identify both table structure recognition and OCR errors. Some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> have proposed a modified TEDS metric, denoted as TEDS-Struct, to evaluate table structure recognition accuracy only by ignoring OCR errors. We also use the TEDS-Struct metric to evaluate our table structure recognition approach on this dataset.</p><p>cTDaR TrackB2-Modern <ref type="bibr" target="#b30">[30]</ref> contains no images for training, but 100 images with annotations are provided as testing data. To evaluate our approach on this dataset, we manually labeled the structures of tables in the cTDaR TrackA modern subset, which contains 600 training images. The annotations will be released publicly to facilitate future research in this area. It has been checked that there is no overlap between the 600 training images and the 100 testing images. The adjacency relation-based metric 3 is used as the evaluation metric of this dataset. During evaluation, the convex hull of the content is used to represent a cell. Note that <ref type="table" target="#tab_5">both table region detection and table structure</ref> recognition have to be done on this dataset.</p><p>Private Dataset. Our in-house dataset is composed of 9,000 training images and 700 testing images. Most images in this dataset are captured by cameras so that many tables in this dataset are skewed or even curved. Sample images in this dataset are shown in <ref type="figure" target="#fig_0">Fig. 10</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref>. We use the same adjacency relation-based metric as cTDaR TrackB to evaluate our table structure recognition approach on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>For both table detector and structure recognizer, the weights of ResNet-18 related layers are initialized with a pre-trained ResNet-18 model for the ImageNet classification task. The weights of newly added layers are initialized with a Gaussian distribution of mean 0 and standard deviation 0.01. The models are optimized by a standard SGD algorithm with a momentum of 0.9 and weight decay of 0.0005. Unless otherwise specified, all the models are trained for 15K iterations with a base learning rate of 0.032, which is divided by 10 at 10K and 13K iterations, respectively. The table detection model for PubLayNet is trained with a 3? training schedule because of the larger amount of data. The TSR models for SciTSR and PubTabNet are trained for 12 epochs. Besides, we apply synchronized batch normalization across multiple GPUs to stabilize the training.</p><p>We implement our approach based on PyTorch 4 v1.6.0 and conduct experiments on a workstation with 8 Nvidia V100 GPUs. In each training iteration, we sample 4 images for each GPU. In the training phase of our table detector, since the number of positive proposals is small, we add some synthesized samples by introducing random jittering to ground-truth boxes. Then, for each image, we select a mini-batch of 32 hard positive and 32 hard negative proposals for the FRCN detector. We adopt a multi-scale training strategy during training. While keeping the aspect ratio, the shorter side of each selected training image is randomly rescaled to a number in {320, 416, 512, 608, 704, 800}. Moreover, when training our table detector on cTDaR TrackA, we also rotate training images by a random angle in {0 ? , 90 ? , 180 ? , 270 ? } with ?5 ? angle jitter for data augmentation. In the training phase of our table structure recognizer, we use the cropped table images for training and the shorter side of each 4 https://pytorch.org/ selected training image is randomly rescaled to a number in {416, 512, 608, 704, 800} while keeping the aspect ratio. For each image, we sample a minibatch of 1,024 row/column separator pixels and 1,024 background pixels for each separation line prediction branch. Furthermore, we select a mini-batch of 64 hard positive and 64 hard negative cell pairs for the cell merging module.</p><p>In the testing phase of table detection, the shorter side of each testing image is rescaled to be 512 pixels with the longer side not exceeding 1,024 pixels. We set the number of selected corners (top-K) as 100 with a corner score threshold C th as 0.3. We apply the standard NMS algorithm with an IoU threshold of 0.3 on the detected tables to suppress redundant detections. For TSR testing, we rescale the longer side of each cropped table image to be 1,024 pixels while keeping the aspect ratio, except for the SciTSR dataset where the cropped images are not resized. The binarization score threshold S th is set as 0.8. The grid cells from the split model are merged based on the merging scores with a threshold of 0.8. Abovedmentioned hyper-parameters are tuned on our in-house dataset, and we directly apply them to other datasets without further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on table detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Comparisons with prior arts</head><p>We compare our table detection approach with other most competitive methods on cTDaR TrackA, PubLayNet and IIIT-AR-13K. All the results of our approach are based on single-model and single-scale testing. The results are listed in <ref type="table" target="#tab_1">Table 1</ref>, <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>. On cTDaR TrackA, our approach achieves the best WAvg. F1-score of 94.9%, outperforming other methods by a notable margin. Furthermore, it is noted that our approach has achieved the best F1scores at higher IoU thresholds, e.g., 92.9% vs. 91.5% with the IoU threshold at 0.9, which demonstrates the superiority of our approach on high precision table localization. On PubLayNet, our model with the ResNet-18 backbone network can even substantially outperform the Mask R-CNN model with the ResNeXt-101 backbone network by improving the AP 0.5:0.95 from 96.0% to 97.0%, and significantly improving the AP 0.95 from 81.4% to 92.0%. Similarly, on IIIT-AR-13K, our model can also substantially outperform the Mask R-CNN model with the ResNet-101 backbone network by improving the AP from 97.6% to 98.2% on the validation set and from 96.5% to 97.7% on the testing set, respectively.</p><p>To push the table detection performance of the Cascade Mask R-CNN    framework on public benchmarks, Agarwal et al. <ref type="bibr" target="#b10">[11]</ref> employed a more powerful backbone network, i.e., dual backbone ResNeXt-101 with deformable convolution filters. However, its performance is still inferior to ours on cTDaR TrackA and PubLayNet. The superior performance achieved on these public benchmark datasets shows the effectiveness and robustness of our approach. Some qualitative results of our approach on these datasets are presented in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Ablation study</head><p>CornerNet vs. RPN for table proposal generation. To compare the proposed CornerNet based table proposal generation algorithm with RPN <ref type="bibr" target="#b11">[12]</ref>, we evaluate their recall rates with top-50 proposals on PubLayNet first. The quantitative results are given in <ref type="table" target="#tab_4">Table 4</ref>, which shows that although these two methods can achieve a similar recall rate at the IoU threshold of 0.7, the CornerNet based method can significantly outperform RPN under a higher IoU threshold 0.9, i.e., 97.8% vs. 89.3%. Then, we further evaluate the end-to-end performance and the comparison results are given in the last two rows of <ref type="table" target="#tab_2">Table 2</ref>. We can find that the performance of RPN based table detector is inferior to our CornerNet based detector, which shows that the quality of the proposals generated by CornerNet is better than RPN. To reveal the relation between proposal quality and end-to-end detection accuracy, we further compute the maximum IoU between each proposal and all the GT boxes, and the corresponding statistical results are shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. We find that there are more proposals from RPN within the IoU range of (0.7, 0.9]. As the proposals in this range will also be taken as positive samples during the training of Fast R-  CNN, these low quality proposals will also have high classification scores and survive from the NMS step, which will degrade the end-to-end performance when evaluating at high IoU thresholds. Compared with RPN, the percentage of well-localized proposals (IoU&gt;0.9) in the positive samples (IoU&gt;0.7) from CornerNet is much higher (96.3% vs. 48.1%), which contributes to better end-to-end  <ref type="table" target="#tab_6">Table 5</ref>), our approach has achieved state-of-the-art performance with the best F1score of 99.3% and 98.7% on the full testing set and the complicated subset, respectively. Moreover, our approach shows negligible performance degradation on the complicated subset, which demonstrates its robustness to tables with complex structures. Similarly, on PubTabNet (see <ref type="table" target="#tab_7">Table 6</ref>), our approach has also achieved the best TEDS-Struct score of 97.0%. It is noted that, the recent best performing method LGPMA <ref type="bibr" target="#b22">[23]</ref> (the winner of ICDAR 2021 Competition on  Scientific Literature Parsing Task B <ref type="bibr" target="#b83">[83]</ref>) has leveraged an important task constraint, namely tables are axisaligned, to achieve higher accuracy. So, it cannot be directly applied to distorted tables. Our approach doesn't rely on such kind of assumptions but still achieves higher accuracy. On cTDaR TrackB2-Modern, our table detector and table structure recognizer are combined together to conduct end-to-end evaluation.</p><p>Since the outputs of our approach are cell boxes rather than convex hulls of cell contents, for the sake of fair comparison, we use the same text detection algorithm as CascadeTabNet <ref type="bibr" target="#b9">[10]</ref> to detect texts in each image and then assign them to table cells if 80% of a text box is located in a cell box. As shown in <ref type="table" target="#tab_8">Table 7</ref>, our approach surpasses previous methods by a large margin. Some qualitative results of our approach on these datasets are presented in <ref type="figure" target="#fig_8">Fig. 9</ref>.</p><p>To further validate the robustness of our approach to distorted or even curved table images, we conducted experiments on the in-house dataset and compared our table structure recognizer with SPLERGE. As shown in <ref type="table" target="#tab_9">Table 8</ref>, our approach outperforms SPLERGE significantly by improving the WAvg. F1-score from 63.8% to 94.6%. Some qualitative results of our approach on this challenging dataset are presented in <ref type="figure" target="#fig_0">Fig. 10</ref>, from which we can observe that our table structure recognizer can work robustly under various challenging conditions such as tables without ruling lines, tables with empty or spanning cells and distorted or even curved shapes.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Ablation study</head><p>Influence of kernel width in spatial CNN modules. We investigate the influence of the kernel width in the four spatial CNN modules to TSR accuracies. The experimental results on the manually labeled cTDaR modern subset are shown in <ref type="table" target="#tab_10">Table 9</ref>. Here, the kernel width determines the number of pixels from which a pixel could receive messages directly. Consistent with the observations in <ref type="bibr" target="#b29">[29]</ref>, increasing the kernel width improves the performance up to a saturation point (k = 9), and then the performance slightly decreases. Therefore, we set the kernel width as 9 for all the other experiments.</p><p>Effectiveness of spatial CNN based separation line prediction. We compare our spatial CNN based message passing method with two previously used methods in the TSR field, i.e., projection networks <ref type="bibr" target="#b16">[17]</ref> and Bi-GRU <ref type="bibr" target="#b17">[18]</ref>. Moreover, as self-attention operations are also known to be good at aggregating global context information, we also select a representative one, i.e., criss-cross attention <ref type="bibr" target="#b84">[84]</ref>, for comparison. All models are trained with the same hyper-parameters for fair comparison and tested on our challenging in-house dataset. We have also implemented a baseline model, i.e., removing the spatial CNN modules directly from our TSR model. The quantitative results of these variants are given in <ref type="table" target="#tab_1">Table 10</ref> and some qualitative results are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. The experimental results show that the performance of other message passing methods are obviously inferior to our spatial CNN based method, especially for tables with large blank spaces or curved tables, which can demonstrate the effectiveness of our spatial CNN based separation line prediction method.</p><p>Effectiveness of Grid CNN based cell merging. We further compare the proposed Grid CNN based cell merging method with other two visual relationship prediction based methods <ref type="bibr" target="#b85">[85]</ref>, which are based on relation network and GCN respectively, on the in-house dataset to demonstrate the effectiveness of Grid CNN for cell merging. The experimental results are listed in <ref type="table" target="#tab_1">Table 11</ref>, from which we can find that the cell merging module can significantly improve the performance of our TSR model (90.9% vs. 94.6%). Moreover, the   last three rows show that the proposed Grid CNN based cell merging method is more effective than the relation network based (93.2% vs. 94.6%) and the GCN based (94.0% vs. 94.6%) methods. Based on our observations, due to the grid arrangement of cell features, Grid CNN can leverage context information effectively with several stacked convolution layers to improve cell merging accuracy, leading to improved robustness to tables with hierarchical spanning cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Limitations of our approach</head><p>Although the proposed RobusTabNet shows superior capability in most scenarios as demonstrated in the previous experiments, it still has some limitations. For example, our current table detector still struggles with nearby tables, and our table structure recognizer is not robust enough to cells with multi-line contents. Some failure examples are presented in <ref type="figure" target="#fig_0">Fig. 12</ref>. Furthermore, <ref type="figure" target="#fig_0">Figure 12</ref>: Some typical failure cases, including the detection of nearby tables and the structure recognition of cells with multi-line contents.</p><p>our TSR approach will fail on some extremely dense tables, because the predicted segmentation masks of nearby separation lines could be overlapped. Note that these difficulties are common challenges for other stateof-the-art methods. Finding effective solutions to these problems will be our future work. Moreover, since the tables in existing datasets are mostly with black lines/letters and white backgrounds, the effectiveness and generalization ability of our approach on tables with different types of backgrounds, text fonts and line colors need to be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>In this paper, we introduce a new table detection and structure recognition approach named RobusTabNet to extract tables from heterogeneous document images. For table detection, we use CornerNet as a new region proposal network for Faster R-CNN, which can leverage more precise corner points generated from heatmaps to improve table localization accuracy. For table structure recognition, we propose two effective techniques to significantly improve the capability of the split-andmerge paradigm, i.e., spatial CNN based separation line prediction and Grid CNN based cell merging. As the spatial CNN can effectively propagate contextual information across the whole table image, improved robustness can be achieved to tables with large blank spaces and curved tables. Moreover, as the whole table is compactly represented as a grid, a simple but effective Grid CNN can be used to achieve excellent cell merging accuracy. Consequently, the proposed RobusTabNet has achieved state-of-the-art performance on both table detection (cTDaR TrackA, PubLayNet and IIIT-AR-13K) and structure recognition (SciTSR, PubTabNet and cTDaR TrackB2-Modern) public benchmarks. We have further validated the robustness of our approach to tables with complex structures, large blank spaces, as well as distorted or even curved shapes on a more challenging in-house dataset.</p><p>For future work, we will study how to leverage header analysis techniques to disambiguate nearby tables. Furthermore, we will also explore how to incorporate textual information into our Grid CNN module to improve the robustness of our table structure recognizer to cells with multi-line contents. To achieve more robust structure recognition of dense tables, we will study effective technologies for adaptive scaling. As for latency reduction, we will explore an end-to-end solution for <ref type="table">table extraction, where the table detector  and the table structure recognizer can share a same  backbone network.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An outline of our table extraction approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of our CornerNet-FRCN based table detection approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Flowchart of our table structure recognition approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Overall architecture of our spatial CNN based separation line prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of the ground-truth generation for table structure recognition. (a) Annotated text boxes and separation lines; (b) Expanded separation lines; (c) Ground-truth cell boxes, including spanning cells; (d) If a pair of neighboring shrunk cells (blue boxes) detected by the split model are assigned to a same ground-truth cell box, we will give this pair a positive label, otherwise a negative label, to train the cell merging module. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>A schematic view of the Grid CNN based cell merging module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of our table detector. (a-b) are from cTDaR TrackA, (c) is from PubLayNet, and (d) is from IIIT-AR-13K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The distribution of the IoU between proposals and GT boxes, where the x-axis represents the ranges of IoU, and the y-axis represents the ratio of the number of proposals in the corresponding IoU range between the two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results of our table structure recognition approach. (a-b) are from SciTSR, (c-d) are from PubTabNet, (e-f) are cropped from cTDaR TrackB2-Modern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results on the in-house dataset. 1 st row: original images; 2 nd row: results from SPLERGE<ref type="bibr" target="#b16">[17]</ref>; 3 rd row: results from our table structure recognizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Some comparison examples from different message passing methods for separation line prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Table detection performance comparison on ICDAR2019 cTDaR TrackA. * indicates that the results are from<ref type="bibr" target="#b30">[30]</ref> </figDesc><table><row><cell>Methods</cell><cell>IoU@0.6(%) P R F1</cell><cell>IoU@0.7(%) P R F1</cell><cell>IoU@0.8(%) P R F1</cell><cell>IoU@0.9(%) WAvg. P R F1 F1(%)</cell></row><row><cell>Applica-robots  *</cell><cell cols="4">90.3 90.1 90.2 88.4 88.1 88.2 82.6 82.4 82.5 54.6 54.4 54.5 77.0</cell></row><row><cell>ABC Fintech  *</cell><cell cols="4">87.4 78.5 82.7 86.3 77.5 81.7 84.1 75.5 79.6 76.8 69.0 72.7 78.6</cell></row><row><cell>Lenovo Ocean  *</cell><cell cols="4">91.8 90.2 90.1 90.8 89.2 90.0 88.5 87.0 87.7 82.9 81.5 82.2 87.7</cell></row><row><cell>NLPR-PAL  *</cell><cell cols="4">97.1 97.5 97.3 96.0 96.4 96.2 93.6 94.0 93.8 86.5 86.9 86.7 92.9</cell></row><row><cell>TableRadar  *</cell><cell cols="4">97.6 96.4 97.0 96.6 95.4 96.0 95.8 93.2 95.1 90.8 89.7 90.2 94.2</cell></row><row><cell>CDeC-Net[11]</cell><cell cols="4">98.0 93.9 95.9 97.7 93.6 95.6 97.1 93.0 95.0 93.4 89.5 91.5 94.3</cell></row><row><cell>RPN+FRCN</cell><cell cols="4">97.8 94.8 96.3 97.3 94.3 95.7 96.3 93.3 94.7 92.4 89.5 90.9 94.1</cell></row><row><cell cols="5">Ours (CornerNet+FRCN) 98.4 94.0 96.1 98.2 93.9 96.0 97.7 93.3 95.4 95.0 90.8 92.9 94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Table detection performance comparison on the validation set of PubLayNet. * indicates that the results are from<ref type="bibr" target="#b31">[31]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>AP 0.5:0.95</cell><cell>AP 0.75</cell><cell>AP 0.95</cell></row><row><cell>Faster R-CNN  *</cell><cell>ResNeXt-101</cell><cell>95.4</cell><cell>97.8</cell><cell>77.8</cell></row><row><cell>Mask R-CNN  *</cell><cell>ResNeXt-101</cell><cell>96.0</cell><cell>97.8</cell><cell>81.4</cell></row><row><cell>CDeC-Net[11]</cell><cell>Dual ResNeXt-101</cell><cell>96.7</cell><cell>-</cell><cell>-</cell></row><row><cell>RPN+FRCN</cell><cell>ResNet-18</cell><cell>96.0</cell><cell>97.5</cell><cell>87.0</cell></row><row><cell>Ours (CornerNet+FRCN)</cell><cell>ResNet-18</cell><cell>97.0</cell><cell>97.8</cell><cell>92.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Table detectionperformance comparison on IIIT-AR-13K. * indicates that the results are from<ref type="bibr" target="#b32">[32]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>P</cell><cell>Validation Set(%) R F1</cell><cell>AP</cell><cell>P</cell><cell>Testing Set(%) R F1</cell><cell>AP</cell></row><row><cell>Faster R-CNN  *</cell><cell cols="4">ResNet-101 95.7 92.6 94.2 95.5</cell><cell cols="3">95.1 92.3 93.7 93.9</cell></row><row><cell>Mask R-CNN  *</cell><cell cols="4">ResNet-101 98.2 96.6 97.4 97.6</cell><cell cols="3">97.1 97.1 97.1 96.5</cell></row><row><cell>Ours (CornerNet+FRCN)</cell><cell>ResNet-18</cell><cell cols="3">98.6 98.3 98.5 98.2</cell><cell cols="3">99.0 97.8 98.4 97.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Table proposal generation quality comparison on PubLayNet.</figDesc><table><row><cell>Methods</cell><cell cols="4">IoU@0.6 Recall(%) Recall(%) Recall(%) Recall(%) IoU@0.7 IoU@0.8 IoU@0.9</cell></row><row><cell>RPN</cell><cell>99.5</cell><cell>99.3</cell><cell>98.8</cell><cell>89.3</cell></row><row><cell>CornerNet</cell><cell>99.5</cell><cell>99.4</cell><cell>99.2</cell><cell>97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table detection</head><label>detection</label><figDesc></figDesc><table><row><cell>performance. These</cell></row><row><cell>experimental results demonstrate the superiority of our</cell></row><row><cell>CornerNet based table proposal generation algorithm</cell></row><row><cell>for achieving higher localization accuracy and better</cell></row><row><cell>end-to-end table detection results.</cell></row><row><cell>5.4. Experiments on table structure recognition</cell></row><row><cell>5.4.1. Comparisons with prior arts</cell></row><row><cell>We compare our table structure recognition approach</cell></row><row><cell>with other most competitive methods on SciTSR,</cell></row><row><cell>PubTabNet and cTDaR TrackB2-Modern. On SciTSR</cell></row><row><cell>and SciTSR-COMP (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>TSR performance comparison on SciTSR and SciTSR-COMP. * indicates that the results are from<ref type="bibr" target="#b24">[25]</ref>.</figDesc><table><row><cell>Methods</cell><cell>P</cell><cell>SciTSR(%) R</cell><cell>F1</cell><cell>SciTSR-COMP(%) P R F1</cell></row><row><cell>Adobe  *</cell><cell cols="3">93.0 78.4 85.1</cell><cell>90.1 71.7 79.8</cell></row><row><cell cols="4">DeepDeSRT[15]  *  90.6 88.7 89.0</cell><cell>86.3 83.1 84.6</cell></row><row><cell>Tabby[70]  *</cell><cell cols="3">92.6 92.0 92.1</cell><cell>89.2 87.2 88.2</cell></row><row><cell cols="4">TabStruct-Net[22] 92.7 91.3 92.0</cell><cell>90.9 88.2 89.5</cell></row><row><cell>GraphTSR[25]</cell><cell cols="3">95.9 94.8 95.3</cell><cell>96.4 94.5 95.5</cell></row><row><cell>SEM[82]</cell><cell cols="3">97.7 96.5 97.1</cell><cell>96.8 94.7 95.7</cell></row><row><cell>LGPMA[23]</cell><cell cols="3">98.2 99.3 98.8</cell><cell>97.3 98.7 98.0</cell></row><row><cell>Ours</cell><cell cols="3">99.4 99.1 99.3</cell><cell>99.0 98.4 98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: TSR performance comparison on the validation set of</cell></row><row><cell>PubTabNet.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">TEDS(%) TEDS-Struct(%)</cell></row><row><cell>EDD[26]</cell><cell>88.3</cell><cell>-</cell></row><row><cell>TabStruct-Net[22]</cell><cell>-</cell><cell>90.1</cell></row><row><cell>GTE[8]</cell><cell>-</cell><cell>93.0</cell></row><row><cell>LGPMA[23]</cell><cell>94.6</cell><cell>96.7</cell></row><row><cell>Ours</cell><cell>-</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>TSR Performance comparison on ICDAR2019 cTDaR TrackB2-Modern. * indicates that the results are from<ref type="bibr" target="#b30">[30]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="3">IoU@0.6(%) P R F1</cell><cell cols="3">IoU@0.7(%) P R F1</cell><cell cols="3">IoU@0.8(%) P R F1</cell><cell cols="3">IoU@0.9(%) P R F1</cell><cell>WAvg. F1(%)</cell></row><row><cell>Zou et al.[73]</cell><cell cols="3">18.8 10.1 13.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.7</cell><cell>0.9</cell><cell>1.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NLPR-PAL*</cell><cell cols="3">32.2 42.1 36.5</cell><cell cols="3">26.9 35.1 30.5</cell><cell cols="3">17.2 22.5 19.5</cell><cell>3.1</cell><cell>4.0</cell><cell>3.5</cell><cell>20.6</cell></row><row><cell cols="4">CascadeTabNet[10] 49.9 39.0 43.8</cell><cell cols="3">40.3 31.5 35.4</cell><cell cols="3">21.6 16.9 19.0</cell><cell>4.1</cell><cell>3.2</cell><cell>3.6</cell><cell>23.2</cell></row><row><cell>GTE[8]</cell><cell>-</cell><cell>-</cell><cell>38.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.8</cell></row><row><cell>Ours</cell><cell cols="3">76.4 76.8 76.6</cell><cell cols="3">71.3 71.6 71.4</cell><cell cols="3">58.1 58.4 58.3</cell><cell cols="3">25.7 25.8 25.8</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>TSR performance comparison on the in-house dataset.</figDesc><table><row><cell>Methods</cell><cell>IoU@0.6(%) P R F1</cell><cell>IoU@0.7(%) P R F1</cell><cell>IoU@0.8(%) P R F1</cell><cell>IoU@0.9(%) P R F1</cell><cell>WAvg. F1(%)</cell></row><row><cell cols="2">SPLERGE [17] 75.9 55.2 63.9</cell><cell>75.8 55.1 63.8</cell><cell>75.7 55.0 63.7</cell><cell>75.7 55.0 63.7</cell><cell>63.8</cell></row><row><cell>Ours</cell><cell>94.9 94.5 94.7</cell><cell>94.8 94.4 94.6</cell><cell>94.8 94.4 94.6</cell><cell>94.7 94.3 94.5</cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of the kernel width for spatial CNN modules.</figDesc><table><row><cell>Kernel Width (k)</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell></row><row><cell cols="7">WAvg. F1-score (%) 93.9 94.8 95.3 95.8 96.0 95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of different message passing methods.</figDesc><table><row><cell>Message Passing</cell><cell cols="3">IoU@0.6(%)</cell><cell cols="3">IoU@0.7(%)</cell><cell cols="3">IoU@0.8(%)</cell><cell cols="3">IoU@0.9(%)</cell><cell>WAvg.</cell></row><row><cell>Methods</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>F1(%)</cell></row><row><cell>No message passing</cell><cell cols="3">92.9 92.2 92.5</cell><cell cols="3">92.8 92.1 92.4</cell><cell cols="3">92.7 92.0 92.4</cell><cell cols="3">92.7 91.9 92.3</cell><cell>92.4</cell></row><row><cell cols="4">Projection Networks [17] 93.8 92.6 93.2</cell><cell cols="3">93.7 92.5 93.1</cell><cell cols="3">93.6 92.5 93.0</cell><cell cols="3">93.5 92.4 93.0</cell><cell>93.0</cell></row><row><cell>Bi-GRU (2 layers) [18]</cell><cell cols="3">93.7 92.7 93.2</cell><cell cols="3">93.7 92.6 93.1</cell><cell cols="3">93.6 92.5 93.1</cell><cell cols="3">93.6 92.5 93.0</cell><cell>93.1</cell></row><row><cell>CC Attention [84]</cell><cell cols="3">94.1 93.7 93.9</cell><cell cols="3">94.1 93.6 93.8</cell><cell cols="3">94.0 93.5 93.8</cell><cell cols="3">94.0 93.5 93.7</cell><cell>93.8</cell></row><row><cell cols="4">Spatial CNN (Proposed) 94.9 94.5 94.7</cell><cell cols="3">94.8 94.4 94.6</cell><cell cols="3">94.8 94.4 94.6</cell><cell cols="3">94.7 94.3 94.5</cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of different cell merging methods.</figDesc><table><row><cell>Cell Merging</cell><cell cols="3">IoU@0.6(%)</cell><cell cols="3">IoU@0.7(%)</cell><cell cols="3">IoU@0.8(%)</cell><cell cols="3">IoU@0.9(%)</cell><cell>WAvg.</cell></row><row><cell>Methods</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>F1(%)</cell></row><row><cell>No cell merging</cell><cell cols="3">91.7 90.5 91.1</cell><cell cols="3">91.6 90.4 91.0</cell><cell cols="3">91.5 90.3 90.9</cell><cell cols="3">91.5 90.2 90.9</cell><cell>90.9</cell></row><row><cell cols="4">Relation Network [85] 93.5 93.1 93.3</cell><cell cols="3">93.4 93.0 93.2</cell><cell cols="3">93.3 93.0 93.1</cell><cell cols="3">93.3 92.9 93.1</cell><cell>93.2</cell></row><row><cell>GCN [85]</cell><cell cols="3">94.2 94.0 94.1</cell><cell cols="3">94.1 93.9 94.0</cell><cell cols="3">94.1 93.8 94.0</cell><cell cols="3">94.0 93.8 93.9</cell><cell>94.0</cell></row><row><cell cols="4">Grid CNN (Proposed) 94.9 94.5 94.7</cell><cell cols="3">94.8 94.4 94.6</cell><cell cols="3">94.8 94.4 94.6</cell><cell cols="3">94.7 94.3 94.5</cell><cell>94.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://support.microsoft.com/en-us/office/ insert-data-from-picture-3c1bb58d-2c59-4bc0-b04a-\ a671a6868fd7</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/cndplab-founder/ctdar measurement tool</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tableseer: Automatic table metadata extraction and searching in digital libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries</title>
		<meeting>the 7th ACM/IEEE-CS Joint Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Table cell search for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web</title>
		<meeting>the International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="771" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Current status and performance analysis of table recognition in document images with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="87663" to="87685" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A table detection method for pdf documents based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble of deep object detectors for page object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Ubiquitous Information Management and Communication</title>
		<meeting>the International Conference on Ubiquitous Information Management and Communication</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Table detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="771" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A yolo-based table detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="813" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graphical object detection in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gadpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapadni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sultanpure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cdec-net: Composite deformable cascade network for table detection in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9491" to="9498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1483" to="1498" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation for table structure recognition in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1397" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M D</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
	<note>Table structure extraction with bi-directional gated recurrent unit networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeptabstr: Deep learning based table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Fateh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T R</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guided table structure recognition through anchor optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="113521" to="113534" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking table recognition using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m">Table structure recognition using top-down and bottom-up cues</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lgpma: Complicated table structure recognition with local and global pyramid mask alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="99" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive scaling for archival table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="80" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04729</idno>
		<title level="m">Complicated table structure recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image-based table recognition: Data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="564" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lang</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1510" to="1515" />
		</imprint>
	</monogr>
	<note>Icdar 2019 competition on table detection and recognition (ctdar)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Publaynet: Largest dataset ever for document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iiit-ar-13k: A new dataset for graphical object detection in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lipps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="216" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The t-recs table recognition and analysis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic table detection in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Danatsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition and Image Analysis</title>
		<meeting>the IEEE International Conference on Pattern Recognition and Image Analysis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Table recognition and understanding from pdf files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1143" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A hybrid method for table detection from document image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>In-Seop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soo-Hyung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR Asian Conference on Pattern Recognition</title>
		<meeting>the IAPR Asian Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tupaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alam</surname></persName>
		</author>
		<title level="m">Extracting tabular information from text files</title>
		<meeting><address><addrLine>Medford, USA 1</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>EECS Department, Tufts University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Table detection in document images using header and trailer patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Harit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Eighth Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic table ground truth generation and a background-analysis-based table structure extraction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Phillipst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="528" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of table recognition: Models, observations, transformations, and inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tableprocessing paradigms: A research survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Embley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="86" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trainable table location in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cesarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="236" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning rich hidden markov models in document analysis: Table location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C E</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="843" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Design of an end-to-end method to extract information from tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="171" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cnn based page object detection in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast cnn-based document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A B</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Viana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1173" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<title level="m">Table detection in document images using foreground and background features</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Decnt: Deep deformable cnn for table detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="74151" to="74161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cbnet: A novel composite backbone network architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11653" to="11660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orsi</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1449" to="1453" />
		</imprint>
	</monogr>
	<note>Icdar 2013 table competition</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on page object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1417" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Tablebank: Table benchmark for image-based table detection and recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1918" to="1925" />
		</imprint>
	</monogr>
	<note>Proceedings of the Language Resources and Evaluation Conference</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Faster r-cnn based table detection combining corner locating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5315" to="5324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multiscale multi-task fcn for semantic page segmentation and table detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A saliency-based convolutional neural network for table and chart detection in digitized documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Analysis and Processing</title>
		<meeting>the International Conference on Image Analysis and Processing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="292" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tablenet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Page object detection from pdf document images by deep structured prediction and supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3627" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Table detection in business document images by message passing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Terrades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rusticus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llad?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">108641</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hole?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baudi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Klinger</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition Workshops</title>
		<meeting>the International Conference on Document Analysis and Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="158" to="164" />
		</imprint>
	</monogr>
	<note>Table understanding in structured documents</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Docbank: A benchmark dataset for document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="949" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Identifying and understanding tabular material in compound documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="405" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itonori</surname></persName>
		</author>
		<title level="m">Table structure recognition based on textblock arrangement and ruled line position</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Configurable table structure recognition in untagged pdf documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shigarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikhailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Symposium on Document Engineering</title>
		<meeting>the 2016 ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="119" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Texus: A unified framework for extracting and understanding tables in pdf documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="895" to="918" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Table structure understanding and its performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1479" to="1497" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A deep semantic segmentation model for imagebased table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Signal Processing</title>
		<meeting>the IEEE International Conference on Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="274" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Challenges in endto-end neural scientific table recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="894" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Gfte: Graphbased financial table extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="644" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
	<note>Res2tim: Reconstruct syntactic structures from table images</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The opencv library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal: Software Tools for the Professional Programmer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="120" to="123" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5686" />
		</imprint>
	</monogr>
	<note>Relationship proposal networks</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An anchor-free region proposal network for faster r-cnn-based text detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="327" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Split, embed and merge: An accurate table structure recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page">108565</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Icdar 2021 competition on scientific literature parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="605" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Relatext: Exploiting visual relationships for arbitrary-shaped scene text detection with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107684</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
