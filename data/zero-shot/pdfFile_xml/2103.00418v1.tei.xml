<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Logic Embeddings for Complex Query Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Luus</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Riegel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndivhuwo</forename><surname>Makondo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabang</forename><surname>Lebese</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gray</surname></persName>
						</author>
						<title level="a" type="main">Logic Embeddings for Complex Query Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Answering logical queries over incomplete knowledge bases is challenging because: 1) it calls for implicit link prediction, and 2) brute force answering of existential first-order logic queries is exponential in the number of existential variables. Recent work of query embeddings provides fast querying, but most approaches model set logic with closed regions, so lack negation. Query embeddings that do support negation use densities that suffer drawbacks: 1) only improvise logic, 2) use expensive distributions, and 3) poorly model answer uncertainty. In this paper, we propose Logic Embeddings, a new approach to embedding complex queries that uses Skolemisation to eliminate existential variables for efficient querying. It supports negation, but improves on density approaches: 1) integrates well-studied t-norm logic and directly evaluates satisfiability, 2) simplifies modeling with truth values, and 3) models uncertainty with truth bounds. Logic Embeddings are competitively fast and accurate in query answering over large, incomplete knowledge graphs, outperform on negation queries, and in particular, provide improved modeling of answer uncertainty as evidenced by a superior correlation between answer set size and embedding entropy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reasoning over knowledge bases is fundamental to Artificial Intelligence, but still challenging since most knowledge graphs (KGs) such as DBpedia <ref type="bibr" target="#b1">(Bizer et al., 2009</ref>), Freebase <ref type="bibr">(Bollacker et al., 2008)</ref>, and NELL <ref type="bibr" target="#b4">(Carlson et al., 2010)</ref> are often large and incomplete. Answering complex queries is an important use of KGs, but missing facts makes queries unanswerable under normal inference. <ref type="figure">Figure 1</ref> shows an example of handling a logic query representing the natural language question "Which films star Golden Globe winners that have not also won an Oscar?". Answering this query 1 IBM Research. https://github.com/francoisluus/KGReasoning LaTeX template (credit: ICML). Copyright 2021 by authors. involves multiple steps of KG traversal and existential firstorder logic (FOL) operations, each producing intermediate entities. We consider queries involving missing facts, which means there is uncertainty about these intermediates that complicates the task. Two main approaches to answering such multi-hop queries involving missing facts are (i) sequential path search and (ii) query embeddings. Sequential path search grows exponentially in the number of hops, and requires approaches like reinforcement learning <ref type="bibr" target="#b8">(Das et al., 2017)</ref> or beam search <ref type="bibr" target="#b0">(Arakelyan et al., 2020</ref>) that have to explicitly track intermediate entities. Query embeddings prefer composition over search, for fast (sublinear) inference and tractable scaling to more complex queries. While relation functions have to learn knowledge, composition can otherwise use inductive bias to model logic operators directly to alleviate learning difficulty.</p><p>Query embeddings need to (a) predict missing knowledge, (b) model logic operations, and (c) model answer uncertainty. Query2Box models conjunction as intersection of boxes, but is unable to model negation as the complement of a closed region is not closed <ref type="bibr" target="#b20">(Ren et al., 2020)</ref>. Beta embeddings of <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> model conjunction as weighted interpolation of Beta distributions and negation as inversion of density, but improvise logic and depend on neural versions of logic conjunction for better accuracy. <ref type="bibr" target="#b15">(Hamilton et al., 2018)</ref> models entities as points so are unable to naturally express uncertainty, while Query2Box uses poorly differentiable geometric shapes unsuited to uncertainty calculations. Beta embeddings naturally model uncertainty with densities and do support complex query embedding, although its densities have no closed form and entropy calculations are expensive.</p><p>Beta embeddings <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> are the first query embedding that supports negation and models uncertainty, however it (1) abruptly converts first-order logic to set logic, (2) only improvises set logic with densities, (3) requires expensive Beta distribution (no closed form), and (4) dissimilarity uses divergence that needs integration.</p><p>We present our logic embeddings to address these issues with (1) formulation of set logic in terms of first-order logic, (2) use of well-studied logic, (3) simple representation with truth bounds, and (4) fast, symmetric dissimilarity measure. Involute logical negation of latent propositions that characterize subsets -a direct approach.</p><p>Weighted t-norm with attention performs set intersection logically on truth bounds.</p><p>Locality-sensitive hashing to find neighbors, also evaluates logical satisfiability directly. Which films star Golden Globe winners that have not also won an Oscar? <ref type="figure">Figure 1</ref>. Logic embeddings perform real-valued logic on latent propositions (latents), an array of truth bounds that describes any subset of entities. (a) A learnt Skolem function maps latents of singleton Oscar to latents of maximal subset of Oscar winners, and similarly for GoldenGlobe; (b) Complement of a subset is logical negation of latents that identify non-Oscar winners; (c) Intersection of subsets is logical conjunction of latents that identify "Golden Globe winners that have not also won an Oscar"; (d) q(T ) = 1 ? D(T, A) measures logic satisfiability of candidate answer set directly, while nearest neighbors to intermediate embeddings can provide some explainability. <ref type="figure">Figure 2</ref>. Computation graph for <ref type="figure">Figure 1</ref> with product t-norm for intersect. Nodes are truth vectors that identify entity subsets. Embedding query logic reduces to a simple vectorized calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Starring Winner</head><formula xml:id="formula_0">! ! ? ? 1 ? ! ! ? 1 ? ! " ! ? 1 ? ! negation intersect</formula><p>Logic embeddings are a compositional query embedding with inductive bias of real-valued logic, for answering (with uncertainty) existential (?) FOL multi-hop logical queries over incomplete KGs. It represents entity subsets with arrays of truth bounds on latent propositions that describe and compress their features and relations. This allows us to directly use real-valued logic to filter and identify answers. Truth bounds [l, u] : 0 ? l ? u ? 1 from <ref type="bibr" target="#b23">(Riegel et al., 2020)</ref> express uncertainty about truths, stating it can be a value range (e.g. unknown [0, 1]). Sum of bound widths model uncertainty, which correlates to answer size. Now intersection is simply conjunction (?) of bounds to retain only shared propositions, union is disjunction (?) to retain all propositions, and complement is negation (?) of bounds to find subsets with opposing propositions.</p><p>The novelty of logic embeddings is that it (a) performs set logic with real-valued logic, (b) characterizes subsets with truth bounds, and (c) correlates bounds with uncertainty. Its benefits are (a) improved accuracy with well-studied tnorms, (b) faster, simplified calculations, and (c) improved prediction of answer size.</p><p>Our contributions of logic embeddings make several advances to address issues of poor logic and uncertainty modeling, and computational expense of current methods:  </p><formula xml:id="formula_1">? ? ? ? ? ? ? ! ! " ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Direct logic. Defines Skolem set logic via maximal</head><p>Skolemisation of first-order logic to embed queries, where proximity of logic embeddings directly evaluates logic satisfiability of first-order logic queries.</p><p>2. Improved logic. Performs set intersection as logic conjunction over latent propositions with t-norms, often used for intersection of fuzzy sets. <ref type="bibr" target="#b18">(Mostert &amp; Shields, 1957)</ref> decomposition provide weak, strong, and nilpotent conjunctions, which show higher accuracy than idempotent conjunction via density interpolation of <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020</ref> Our contributions also include (i) introduction of Skolem set logic in Section 2 to enable lifted inference, (ii) definition of logic embeddings in Section 3 to enable logic query composition, (iii) implementation details of method in Section 4, and (iv) detailed evaluation in Section 5 and new cardinality prediction experiment showing benefit of truth bounds. ? , .</p><p>, ? , ? ? ,</p><formula xml:id="formula_2">! " ? ? # ? # ! " " # ? ? negation intersect</formula><p>Dependency graph <ref type="figure">Figure 4</ref>. Computation and dependency graph for the pin query in <ref type="table" target="#tab_4">Table 1</ref>, we map FOL to Skolem set logic with simple rules. <ref type="figure">Figure 5</ref>. A maximum Skolem function relates subset c via r(c, t) to the largest subset t = {t 1 , t 2 , t 3 }, and not to smaller subsets, e.g.</p><formula xml:id="formula_3">! ! " " # # " " $ $ " " $ " # " ! " $ # !</formula><formula xml:id="formula_4">{t 1 } or {t 1 , t 2 }, even if these satisfy r (v, t ) : v ? c, t ? t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Querying with Skolem set logic</head><p>We define an existential first-order language L whose signature contains a set of functional symbols F and a set of predicate symbols R. The alphabet of L includes the logic symbols of conjunction (?), disjunction (?), negation (?), and existential quantification (?). The semantics of L interprets the domain of discourse as a family of subsets C ? 2 V , where 2 V denotes the power set of a set of entities V. This allows for lifted inference where variables and terms map to entity subsets, which are single elements in the discourse.</p><p>Sentences of L express relational knowledge, e.g. a binary predicate r ? R relates two unordered subsets via function r : C?C ? [0, 1] that evaluates to a real truth value. However, the underlying knowledge is predicated under a different signature on domain of discourse V, relating entities via r : V ? V ? {False, True}. Subsets c, t ? C are related via r(c, t), with the union t = ? ?v?c t over all underlying propositions r (v, t ) for each entity v ? c (see <ref type="figure">Figure 5</ref>).</p><p>Existential quantification in L results in sentences that are always true in the underlying interpretation 1 , because the nullset is present in C. We introduce maximal quantification via a modified Skolem function to make L useful.</p><p>Definition 1 (Maximal Skolem function). A maximal Skolem function f r ? F : C ? C assigns the maximal subset c to an existentially quantified variable T so that ?T.r(a, T ) ? r(a, f r (a)) (equisatisfiable). The function receives input element a ? C and outputs the related subset c over V with the largest cardinality, so that f r (a) = c : |c | ? |c|, c, ?c ? C.  <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> in first-order logic and Skolem set logic forms. Notation: intersection (i), union (u), predicate (p), and negation (n). See <ref type="figure">Figure 4</ref> for example of converting pin to Skolem set logic.</p><p>First-order logic Skolem set logic</p><formula xml:id="formula_5">1p ?T. p(a, T ) fp(a) 2p ?V, T. p(a, V ) ? q(V, T ) fq(fp(a)) 3p ?V, W, T. p(a, V ) ? q(V, W ) fr(fq(fp(a))) ? r(W, T ) 2i ?T. p(a, T ) ? q(b, T ) fp(a) ? fq(b) 3i ?T. p(a, T ) ? q(b, T ) ? r(c, T ) fp(a)?fq(b)?fr(c) pi ?V, T. p(a, V ) ? q(V, T ) ? r(b, T ) fq(fp(a)) ? fr(b) ip ?V, T. [p(a, V )?q(b, V )]?r(V, T ) fr(fp(a) ? fq(b)) 2in ?T. p(a, T ) ? ?q(b, T ) fp(a) ? ?fq(b) 3in ?T. p(a, T ) ? q(b, T ) ? ?r(c, T ) fp(a)?fq(b)??fr(c) pin ?V, T. p(a, V )?q(V, T )??r(b, T ) fq(fp(a)) ? ?fr(b) pni ?V, T. p(a, V )??q(V, T )?r(b, T ) ?fq(fp(a)) ? fr(b) inp ?V, T. [p(a, V ) ? ?q(b, V )] fr(fp(a) ? ?fq(b)) ? r(V, T ) 2u ?T. p(a, T ) ? q(b, T ) fp(a) ? fq(b) up ?V, T. [p(a, V )?q(b, V )]?r(V, T ) fr(fp(a) ? fq(b))</formula><p>Formulae in L such as r(a, T ) can thus convert to sentences ?T.r(a, T ) by quantification over free variables, and the largest satisfying assignment to target variable T : r(a, T ) obtained via maximal Skolemisation f r (a) then subsumes all valid groundings in the underlying interpretation over V. "Relation following" of <ref type="bibr" target="#b6">(Cohen et al., 2020)</ref> is similar where r(a, T ) = {t | ?v ? a : r (v, t )}. Normal Skolem functions are different as they only map to single entities.</p><p>Skolem set logic. This is set logic that involves Skolem functions that substitute related subsets. We introduce notation for maximal Skolemisation of sentences in L that represents set logic on Skolem terms in their underlying interpretation over V. Evaluation in L of conjunction ?, disjunction ?, and negation ? correspond to intersection, union, and complement in Skolem set logic, respectively. 2 Sentence forms and their Skolem set logic representations (?, ? extends trivially to more inputs) for target variable T , given anchor elements a, b (assigned subsets) and relation predicates (r,. . .,z) include these conversion rules:</p><p>? Relation: ?T.r(a, T ) gives f r (a).</p><p>? Negation: ?T.?r(a, T ) gives ?f r (a).</p><formula xml:id="formula_6">? Conjunction: ?T.r(a, T ) ? q(b, T ) gives f r (a) ? f q (b). ? Disjunction: ?T.r(a, T ) ? q(b, T ) gives f r (a) ? f q (b). ? Multi-hop: ?T.r(a, V 1 ) ? q(V 1 , V 2 ) ? . . . ? z(V n?1 , T )</formula><p>has chain-like relations that give f z ? ? ? (f q (f r (a))).</p><p>Query formulae. We consider query formulae defined over V with m anchor entities a i ? V, a single free target variable T , and n bound variables V j . Query answering assigns the subset {t ? V} ? C of entities to T that satisfy ?V 1 , V 2 , . . . , V n . q(a 1 , a 2 , . . . , a m , V 1 , . . . , V n , T ).</p><p>(1)</p><p>We recast formulae into C by converting anchor entities to singleton subsets, we then quantify T to maximally Skolemise the sentence and derive its Skolem set logic term f (a 1 , a 2 , . . . , a m ) for T given the anchor entities.</p><p>The dependency graph of formula (1) consists of vertices {a 1 , . . . , a m , V 1 , . . . , V n , T } and a directed edge for each vertex pair (x, y) related inside the formula, e.g. via r(x, y). Queries are valid when the dependency graph is a single-sink acyclic graph with anchor entities as source nodes <ref type="bibr" target="#b15">(Hamilton et al., 2018)</ref>, and an equivalent to the original formula can then be recovered from Skolem set logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Logic Embeddings</head><p>Our approach for positive inference on queries of form (1) require only Skolem set logic, but should also support:</p><p>1. Lifted inference: Inference over subsets of entities, needing fewer actions than with single-entity inference;</p><p>2. Knowledge integration: Underlying single-entity knowledge over V integrates over subsets from C;</p><p>3. Generalization: Use of subset similarities to predict absent knowledge with an uncertainty measure.</p><p>The powerset C over entities V from a typical KB is extremely large, so discrete approaches to achieve above with non-uniform subset representations are likely intractable.</p><p>Set embeddings. We consider set embeddings that map C to a continuous space M, so these images of subsets approximately preserve their relationships from C (Sun &amp; Nielsen, 2019). It has metric properties such as the volume of subsets, not usually considered by graph embeddings. Set embeddings have the following properties:</p><p>? Uniform: Enables standard parameterization, simplifies memory structures and related computation;</p><p>? Continuous: Differentiable, enables optimization;</p><p>? Permutation-invariant: Subset elements unordered;</p><p>? Uncertainty: Subset size corresponds to entropy;</p><p>? Proximity: Relatively preserves subset dissimilarities.</p><p>Definition 2 (Logic embeddings). Logic embeddings are set embeddings that characterize subsets with latent propositions, and perform set logic on subsets via logic directly over their latent propositions.</p><p>Logic embeddings inherit the aforementioned properties and benefits of set embeddings, but are also:</p><p>? Logical: Logic over truth values in embeddings performs set logic, and proximity correlates with satisfiability;</p><p>? Contextual: Latent propositions integrate select knowledge depending on the subset;</p><p>? Open-world: Accepts and integrates unknown or partially known knowledge and inferences.</p><p>Logic embeddings also share query embedding advantages of efficient answering, generalization, and full logic support:</p><p>? Fast querying: Obtains answers closest to query embedding in sublinear time, unlike subgraph matching with exponential time in query size <ref type="bibr" target="#b7">(Dalvi &amp; Suciu, 2007)</ref>.</p><p>? General querying: Generalizes to unseen query forms.</p><p>? Implicit prediction: Implicitly imputes missing relations, and avoids exhaustive link prediction (De Raedt, 2008) subgraph matching requires and scales poorly on.</p><p>? Natural modeling: Supports intuitive set intersection, unlike point embeddings <ref type="bibr" target="#b15">(Hamilton et al., 2018)</ref>.</p><p>? Uncertainty: Models answer size with embedding entropy <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> or truth bounds (ours).</p><p>? Fundamental support: Handles negation (and disjunction via De Morgan's law), unlike box embeddings where complements are not closed regions (and union resorts to disjunctive normal form) <ref type="bibr" target="#b20">(Ren et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent propositions.</head><p>A logic embedding keeps truth values with associate distribution p X ? M on latent propositions of features and properties that characterize and distinguish subset X ? V. Subset entities x ? X may share a similar relation r(x, Y ) to a particular subset Y , where latent propositions can integrate such identifying relations. Logic embeddings need to be contextual given the limited embedding capacity, as only some relations may be relevant to define a particular subset.</p><p>Uncertainty. We represent volume in embedding space with lower and upper bounds [l, u] on truth values, to express uncertainty and allow correlation of embedding entropy of a subset with its cardinality. We use truth bounds of <ref type="bibr" target="#b23">(Riegel et al., 2020)</ref> that admit the open-world assumption and have probabilistic semantics to interpret known (l ? u), unknown ([0, 1]) and contradictory states (l &gt; u, not considered here).</p><formula xml:id="formula_7">The logic embedding for X ? V is an n-tuple S X = ([l i , u i ] : l i , u i ? [0, 1]) n i=1 of lower and upper bound pairs (l i ? u i ) that represents an n-tuple p X = (P i ) n i=1 of uniform distributions P i = U (l i , u i )</formula><p>, which omits contradiction l i &gt; u i . The chain rule for differential entropy H(p X ) = H(P 1 , . . . , P n ) of the embedding distribution applies and gives an upper-bound in terms of components</p><formula xml:id="formula_8">H(P i ) = log(u i ? l i ), where H(P 1 , . . . , P n ) = n i=1 H(P i |P 1 , . . . , P i?1 ) ? n i=1 H(P i ).</formula><p>Condition 1 (Uncertainty axiom). Set embeddings should (approx.) satisfy ?X ? C: entropy H(p X ) is a monotonically increasing function of H(U X ), where U X is a uniform distribution over elements of X (Sun &amp; Nielsen, 2019).</p><p>We measure adherence to the uncertainty axiom with correlation between subset size |X| and entropy upper-bound</p><formula xml:id="formula_9">n i=1 H(P i ) or total truth interval width n i=1 (u i ? l i ), and by predicting |X| from h X = [H(P i )] n i=1</formula><p>. Proximity. Subsets with high overlap should embed close by, whereas little to no overlap should result in relatively distant embeddings. We now review the proximity axiom.</p><formula xml:id="formula_10">Condition 2 (Proximity axiom). ?(X, X ) ? C 2 : D(p X p X ) should positively correlate with D(U X U X ), given information divergence D (Sun &amp; Nielsen, 2019). Relative entropy is an important divergence where the fam- ily of f -divergences D f (p|q) = p(x)f (q(x)/p(x))dx typically include log(p(x)) or p(x) ?1 terms over finite sup- port x ? X . Uniform distributions U (l, u)</formula><p>in logic embeddings may not cover a [0, 1] support, and may result in undefined divergence. Therefore, we measure dissimilarity D(S X , S X ) ? [0, 1] between logic embeddings of subsets (X, X ) with the expected mean of L 1 -norms of truth bounds, where</p><formula xml:id="formula_11">D(S X , S X ) = n i=1 l i ? l i + u i ? u i 2n .<label>(2)</label></formula><p>Condition 3 (Satisfiability axiom). Substitution instance q(X ) of first-order logic formula q(T ) has satisfiability 1 ? D(S X , S X ), where target variable T has answer X.</p><p>Query q(T ) is true for answer X, since 1?D(S X , S X ) = 1, but candidate satisfiability q(X ) can reduce to minimum 0 (false), depending on the dissimilarity between X and X.</p><p>Set logic. Conjunction and disjunction of latent propositions of subsets perform their intersection and union, respectively, unlike information-geometric set embeddings that interpolate distributions. De Morgan's law replaces disjunction a ? b with conjunction and negations ?(?a??b).</p><formula xml:id="formula_12">Involute negation of S = ?(?S) = ([l i , u i ]) n i=1 describes comple- ment 3 ?S = ([1?u i , 1?l i ]) n i=1</formula><p>. We use continuous t-norm : [0, 1] k ? [0, 1] to perform generalized conjunction for real-valued logic, and calculate S = k j=1 S j as</p><formula xml:id="formula_13">S = (l (1) i , . . . , l (k) i ), (u (1) i , . . . , u (k) i ) n i=1</formula><p>.</p><p>(3) <ref type="bibr" target="#b18">(Mostert &amp; Shields, 1957)</ref> decompose any continuous tnorm into Archimedean t-norms, namely minimum/G?del</p><formula xml:id="formula_14">min (t) = min(t 1 , . . . , t k ), product prod (t) = k j=1 t j ,</formula><p>and ?ukasiewicz luk (t) = max(0, 1 ? k j=1 (1 ? t j )), which we evaluate separately to consider all prime aspects.</p><p>Contextual. Limited capacity requires intersection to reintegrate latent propositions contextually via a weighted t-norm: 1) continuous function 4 (w, t) of weights w and truths t; 2) behaves equal to unweighted case if weights are 1; and 3) w j = 0 removes input j, and weights are in [0, 1].</p><formula xml:id="formula_15">min (w, t) = k j=1 t j w j e ?tj k j=1 w j e ?tj (4) prod (w, t) = k j=1 t wj j (5) luk (w, t) = max 0, 1 ? k j=1 w j (1 ? t j )<label>(6)</label></formula><p>Weight w</p><formula xml:id="formula_16">(v) j for (l (v) j , u (v) j )</formula><p>, the j th truth bounds in input v, depends on bounds of all conjunction inputs via attention, starting with function g as</p><formula xml:id="formula_17">g (v) j = g(l (v) 1 , . . . , l (v) n , u (v) 1 , . . . , u (v) n ).<label>(7)</label></formula><p>Softargmax over all the conjunction inputs yields a score s</p><formula xml:id="formula_18">(v) j = exp(g (v) j )/ k h=1 exp(g (h) j ) which normalizes after w (v) j = s (v) j / max(s (1) j , . . . , s (k) j ) to ensure max weight 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>Query embedding. We calculate a logic embedding for a single-sink acyclic query with Skolem set logic over anchor entities. We keep vectors {r ? R d } for relation embeddings, and {x ? [0, 1] 2d } for logic embeddings of all entities, where x = [l 1 , . . . , l d , u 1 , . . . , u d ]. To measure the "cost" of modeling uncertainty by tracking bounds we also test point truth embeddings (l = u), where x = [t 1 , . . . , t 2d ].</p><p>We parameterize our Skolem function f r (</p><formula xml:id="formula_19">x) = f (r, x) with F 1 ? R 3d?h , F 2 ? R h?h , and F 3 ? R h?2d to relate x to y = [y l , y l + y u (1 ? y l )], where [y l , y u ] = f (r, x) = ? (max (0, max (0, [r, x]F 1 ) F 2 ) F 3 ) activates sigmoid.</formula><p>Set logic has attention that uses g(x) = max (0, xG 1 ) G 2 with parameter matrices G 1 ? R 2d?2d and G 2 ? R 2d?d .</p><p>Cardinality prediction. We predict 5 the cardinality |X| of subset X from the entropy vector h X of its logic embedding</p><formula xml:id="formula_20">with ? ? ? (max (0, max (0, h X H 1 ) H 2 ) H 3 ) scaled by ?, where H 1 ? R d? d 4 , H 2 ? R d 4 ? d 16 , and H 3 ? R d 16 ?1 .</formula><p>Query answering. Our objective is to embed a query q relatively close to its answers {y} and far from negative samples {z}. We train model parameters 6 of 1) entity logic embeddings, 2) relation embeddings, 3) Skolem function, and 4) t-norm attention, to minimize query answering loss</p><formula xml:id="formula_21">? log ?(? ? D(y, q)) ? k j=1 1 k log ?(D(z j , q) ? ?). (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We primarily compare against Beta embeddings (BETAE <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref>) that also support arbitrary FOL queries and negation, where our logic embeddings (LOGICE with ?ukasiewicz t-norm) show improved 1) generalization, 2) reasoning, 3) uncertainty modeling, and 4) training speed.</p><p>Datasets. We use two complex logical query datasets from: Q2B with 9 query structures <ref type="bibr" target="#b20">(Ren et al., 2020)</ref>, and BETAE that adds 5 for negation <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020</ref>  <ref type="bibr">, 2017)</ref>. <ref type="table" target="#tab_4">Table 1</ref> shows the first-order logic and Skolem set logic forms of the 14 query templates.</p><p>We separately follow the evaluation procedures of above Q2B and BETAE datasets. 7 Training omits ip/pi/2u/up (Table 1) to test handling of unseen query forms. Negation is challenging with 10x less queries than conjunctive ones. 8</p><p>Generalization.</p><p>Queries have at least one link prediction task to test generalization, where withheld data contain goal answers. We measure Hits@k and mean reciprocal rank (MRR) of these non-trivial answers that do not appear in train/valid data.  <ref type="bibr" target="#b15">(Hamilton et al., 2018)</ref> on almost all query forms in <ref type="table" target="#tab_6">Table 4</ref>, and further improves with point truths. 9 LOGICE also answers negation queries more accurately than BETAE for most query forms in <ref type="table" target="#tab_8">Table 2</ref>. CQD-Beam does not handle negation nor uncertainty and is expensive as it grounds candidate entities explicitly, yet LOGICE generalizes better and more efficiently in <ref type="table" target="#tab_7">Table 5</ref>(a) on FB15k-237 and NELL.</p><p>Reasoning. Logical entailment on queries without missing links tests how faithful deductive reasoning is. We thus train on all splits and measure entailment accuracy in  FB15k-237 FB15k NELL Model 2in 3in inp pin pni avg avg avg LOGICE 4.9 8.2 7.7 3.6 3.5 5.6 12.5 6.2 +bounds 4.9 8.0 7.3 3.6 3.5 5.5 11.7 6.3 BETAE 5.1 7.9 7.4 3.6 3.4 5.4 11.8 5.9 its sketch method has worse faithfulness than LOGICE with point truths for FB15k and NELL, also EmQL does not support negation nor models uncertainty like LOGICE.</p><p>Compare logics. LOGICE can intersect via minimum, product, or ?ukasiewicz t-norms, which perform similarly (?1%) in <ref type="table" target="#tab_9">Table 3</ref>, while all outperform BETAE. ?ukasiewicz provides superior uncertainty modeling, so is the default choice for LOGICE. Attention via weighted t-norm improves LOGICE accuracy (+9.6%), where one hypothesis is better use of limited embedding capacity through learning weighted combinations of latent propositions. However, BETAE improves by avg. +12.3% with a similar attention mechanism so has greater dependence on it, possibly because it devises intersection as interpolation of densities, whereas LOGICE uses established real-valued logic via t-norms. In particular, the BETAE intersect is idempotent while LOGICE offers weak and strong conjunctions of which ?ukasiewicz offers nilpotency.</p><p>Uncertainty modeling. Correlation between differential entropy n i=1 H(P i ) (upper-bound) and answer size (uncertainty, number of entities) is significantly higher in LOGICE than BETAE using both Spearman's rank correlation and    Pearson's correlation coefficient in <ref type="table">Table 6</ref>. Both significantly outperform uncertainty of Q2B with L 1 box size.</p><p>Total truth interval width n i=1 (u i ? l i ) of LOGICE correlates better to answer size in most cases than BETAE, and offers direct use of the probabilistic semantics of truth bounds to simplify uncertainty modeling. Note that minimizing query answering loss in Eq. (8) does not directly optimize answer cardinality, so LOGICE naturally models uncertainty only as by-product of learning to answer.</p><p>Cardinality prediction. Above evaluation aggregates entropy, but element-wise entropies h X = [H(P i )] n i=1 contain more information that we use for explicit answer size prediction. We train a regression classifier to map provided uncertainties h X to answer size |X|, and measure mean absolute error s ? |X| /|X| of size prediction s. <ref type="table" target="#tab_11">Table 7</ref> shows reduced cardinality prediction error of 83% with LOGICE, compared to avg. 96% with BETAE, indicating more informative uncertainties with LOGICE. However, these errors are still quite large, possibly because the main training objective does not directly optimize uncertainties for cardinality prediction.</p><p>Training speed. BETAE uses the Beta distribution with no closed form that requires integration to compute entropy and dissimilarity. In contrast, LOGICE uses simple truth bounds with fast entropy and dissimilarity calculations. <ref type="figure" target="#fig_2">Figure 6</ref> shows that LOGICE with any t-norm trains 2-3x faster than BETAE, with the exact same compute resources, optimizer and learning rate. The LOGICE training curve also appears smooth and monotonic, compared to disrupted learning progress with BETAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related work</head><p>In addition to the overview of query embeddings in the Introduction, we also relate our work to 1) tensorized logic, 2) querying with t-norms, and 3) lifted inference. 10 Tensorized logic. Distributed representations like embeddings can enable generalization and efficient inference that symbolic logic lacks. Logic tensors of <ref type="bibr" target="#b12">(Grefenstette, 2013)</ref> express truths as specific tensors and map entities to one-hot vectors with full-rank matrices, but only memorize facts. Matrix factorization reduces one-hot vectors to low dimensions to enable generalization and efficiency while optimizing logic constraints, but can scale exponentially in the number of query variables <ref type="bibr">(Rockt?schel et al., 2015)</ref>.</p><p>Logic Tensor Networks learn a real vector per entity and even Skolem functions that map to entity features, but has weak inductive bias as it needs to learn predicates to perform logic <ref type="bibr">(Serafini &amp; Garcez, 2016)</ref>. In contrast, logic embeddings support uncertainty and only has to learn Skolem functions to express knowledge and generalize, with direct logic on latent truths and logic satisfiability via distance.</p><p>Querying with t-norms. Triangular norms allow for differentiable composition of scores, often in the context of expensive search. <ref type="bibr" target="#b13">(Guo et al., 2016)</ref> jointly embed KGs and logic rules via t-norm of scores, but only for simple rules. <ref type="bibr" target="#b0">(Arakelyan et al., 2020)</ref> combine scores from a pretrained link predictor via t-norms repeatedly to search for an answer while tracking intermediaries, whereas logic embeddings perform vectorized t-norm to directly embed answers.</p><p>Lifted inference. Many probabilistic inference algorithms accept first-order specifications, but perform inference on a mostly propositional level by instantiating first-order constructs <ref type="bibr" target="#b11">(Friedman et al., 1999;</ref><ref type="bibr" target="#b22">Richardson &amp; Domingos, 2006)</ref>. In contrast, lifted inference operates directly on firstorder representations, manipulating not only individuals but also groups of individuals, which has the potential to significantly speed up inference <ref type="bibr" target="#b10">(de Salvo Braz et al., 2007)</ref>. We need to reason about entities we know about, as well as those we know exist but with unknown properties. Logic embeddings perform a type of lifted inference as it does not have to ground out the theory or reason separately for each entity, but can perform logic inference directly on subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Embedding complex logic queries close to answers is efficient but presents several difficulties in set theoretic modeling of uncertainty and full existential FOL, where Euclidean geometry and probability density approaches suffer deficiency and computational expense. Our logic embeddings overcome these difficulties by converting set logic into direct real-valued logic. We execute FOL queries logically, and not through Venn diagram models like other embeddings, yet we achieve efficiency of lifted inference over subsets.</p><p>Main limitations include the need for more training data than search-based methods, although we have strong inductive bias of t-norm logic to reduce sample size dependence. Future work will consider negation attending to applied relations of its input to benefit from context like the t-norms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended Related Work</head><p>Link prediction. Reasoning over knowledge bases is fundamental to Artificial Intelligence, but still challenging since most knowledge graphs (KGs) such as DBpedia <ref type="bibr" target="#b1">(Bizer et al., 2009)</ref>, Freebase <ref type="bibr">(Bollacker et al., 2008)</ref>, and NELL <ref type="bibr" target="#b4">(Carlson et al., 2010)</ref> are often large and incomplete. Answering complex queries is an important use of KGs, but missing facts makes queries unanswerable under normal inference. KG embeddings are popular for predicting facts, learning entities as vectors and relations between them as functions in vector space, like translation <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref> or rotation <ref type="bibr">(Sun et al., 2019)</ref>.</p><p>Link prediction uncovers similar behavior of entities, and semantic similarity between relations (e.g. birthplace predicts nationality). Path queries involve multi-hop reasoning (e.g. country of birthcity of person), where compositional learning embeds queries close to answer entities with fast (sublinear) neighbor search <ref type="bibr" target="#b14">(Guu et al., 2015)</ref>. In contrast, sequential path search grows exponentially in the number of hops, and requires approaches like reinforcement learning <ref type="bibr" target="#b8">(Das et al., 2017)</ref> or beam search <ref type="bibr" target="#b0">(Arakelyan et al., 2020)</ref> that have to explicitly track intermediate entities.</p><p>Path-based methods. A simple approach to complex query answering represents first-order logical queries as a directed graph corresponding to the reasoning path to be followed. Such path-based methods are characterized by carrying out a sub-graph matching strategy in their pursuit for solving complex queries. However, they fail to deal with queries with missing relations and cannot scale to large KGs as the complexity of sub-graph matching grows exponentially in the query size. Several works aim at addressing the former by imputing missing relations <ref type="bibr" target="#b14">(Guu et al., 2015;</ref><ref type="bibr" target="#b16">Hong et al., 2018)</ref>, leading into a denser KG with high computational demand.</p><p>Query embeddings. Recent approaches aim to address the two issues by learning embeddings of the query such that entities that answer the query are close to the embeddings of the query and answers can be found by fast nearest neighbor searches. Such approaches implicitly impute missing relations and also lead to faster querying compared to subgraph matching. Here, logical queries as well as KG entities are embedded into a lower-dimensional vector space as geometric shapes such as points <ref type="bibr" target="#b15">(Hamilton et al., 2018)</ref>, boxes <ref type="bibr" target="#b20">(Ren et al., 2020)</ref> and distributions with bounded support <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref>.</p><p>Compared to point-based embeddings, boxes and distributions naturally model sets of entities they enclose, with set operations on those sets corresponding to logical operations (e.g., set intersection corresponds to the conjunction operator), and thus iteratively executing set operations results in logical reasoning. Furthermore, box and distribution-based embeddings allow handling the uncertainty over the queries.</p><p>Majority of early embedding based approaches are limited to a subset of first-order logic consisting of existential quantification and conjunctions, with a few recent papers supporting the so-called existential positive first-order (EPFO) queries <ref type="bibr" target="#b20">(Ren et al., 2020;</ref><ref type="bibr" target="#b0">Arakelyan et al., 2020)</ref> that additionally include disjunctions. The work by <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> is the first to handle the full set of first-order logic including negation.</p><p>Recent path-based approaches utilize knowledge graph embeddings to learn to tractably traverse the graph in the embedding space <ref type="bibr" target="#b17">(Lou et al., 2020)</ref> or using pre-trained black boxes for link prediction <ref type="bibr" target="#b0">(Arakelyan et al., 2020)</ref>. Neural Subgraph Matching <ref type="bibr" target="#b17">(Lou et al., 2020)</ref> uses order embeddings to embed the query and KG graphs into a lowerdimensional space, and efficiently performs subgraph matching directly in the embedding space. This has the potential to impute missing relations and has been shown to be orders of magnitude faster than standard sub-graph matching approaches on subgraph matching benchmarks. However, it has not been applied to complex query answering problems. <ref type="bibr" target="#b0">(Arakelyan et al., 2020)</ref> use a pre-trained, black-box, neural link predictor to reduce sample complexity and scale to larger KGs, and was shown to be effective on EPFO queries, but does not support the full set of first-order logic queries.</p><p>Lifted inference. Many probabilistic inference algorithms accept first-order specifications, but perform inference on a mostly propositional level by instantiating first-order constructs <ref type="bibr" target="#b11">(Friedman et al., 1999;</ref><ref type="bibr" target="#b22">Richardson &amp; Domingos, 2006)</ref>. In contrast, lifted inference operates directly on firstorder representations, manipulating not only individuals but also groups of individuals, which has the potential to significantly speed up inference <ref type="bibr" target="#b10">(de Salvo Braz et al., 2007)</ref>.</p><p>Variable elimination of non-observed non-query variables is the basis of several lifted inference algorithms <ref type="bibr" target="#b19">(Poole, 2003;</ref><ref type="bibr" target="#b10">de Salvo Braz et al., 2007)</ref>, and strongly resembles the lifting lemma which simulates ground resolution because it is complete and then lifts the resolution proof to the firstorder world <ref type="bibr" target="#b5">(Chang &amp; Lee, 2014)</ref>.</p><p>Theorem proving produces a potentially unbounded number of resolutions on grounded representations by performing unification and resolution on clauses with free variables, operating directly on the first-order representation. However, reasoning over incomplete knowledge requires generalization where particular facts about some individuals could apply with uncertainty to a similar group, thus predicting missing facts.</p><p>We need to reason about entities we know about, as well as entities we know exist but which have unknown properties. Logic embeddings perform a type of lifted inference as it does not have to ground out the theory or reason separately for each entity, but can perform logic inference directly with compact representations of smooth sets of entities. <ref type="table">Table 9</ref> gives logic query forms from <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> with corresponding Skolem normal form and Skolem set logic. Note that Skolem normal form uses conventional set logic symbols inside predicates, to avoid confusion with logic symbols composing predicates. Subsequent Skolem set logic resumes use of logic symbols, as the actual operations are logical over vectors of truth values. <ref type="table" target="#tab_4">Table 10</ref> gives the relation and entity counts, as well as the train/valid/test split edge counts for the three datasets used. <ref type="table" target="#tab_4">Table 11</ref> gives the number of generated queries for the Q2B query datasets, and <ref type="table" target="#tab_4">Table 12</ref> gives the query counts for the BETAE query datasets.  <ref type="table" target="#tab_4">Table 15</ref> gives full numbers on Pearson's correlation coefficient for all three KGs. <ref type="table" target="#tab_4">Table 16</ref> gives answer size prediction mean absolute error for all three KGs. <ref type="table" target="#tab_4">Table 17</ref> provides generalization and entailment scores for all three KGs. FB15k-237 LOGICE 4.9 8.2 7.7 3.6 3.5 5.6 +bounds 4.9 8.0 7.3 3.6 3.5 5.5 BETAE 5.1 7.9 7.4 3.6 3.4 5.4 NELL995 LOGICE 5.3 7.5 11.1 3.3 3.8 6.2 +bounds 5.3 7.8 11.1 3.3 3.8 6.3 BETAE 5.1 7.8 10.0 3.1 3.5 5.9 <ref type="table">Table 9</ref>. Complex logical query structures of <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> in first-order logic and Skolem set logic form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query forms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results</head><p>First-order logic Skolem normal form Skolem set logic 1p ?T. p(a, T ) p(a, fp(a)) fp(a) 2p ?V, T. p(a, V ) ? q(V, T ) p(a, fp(a)) ? q(fp(a), fq(fp(a))) fq(fp(a)) 3p</p><p>?V, W, T. p(a, V ) ? q(V, W ) ? r(W, T ) p(a, fp(a)) ? q(fp(a), fq(fp(a))) fr(fq(fp(a))) ? r(fq(fp(a)), fr(fq(fp(a))))  <ref type="table" target="#tab_4">804   Table 11</ref>. Number of queries in Q2B dataset generated for different query structures (see <ref type="bibr" target="#b20">(Ren et al., 2020)</ref>  <ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref>      </p><formula xml:id="formula_22">2i ?T. p(a, T ) ? q(b, T ) p(a, fp(a)) ? q(b, fq(b)) fp(a) ? fq(b) 3i ?T. p(a, T ) ? q(b, T ) ? r(c, T ) p(a, fp(a)) ? q(b, fq(b)) ? r(c, fr(c)) fp(a) ? fq(b) ? fr(c) pi ?V, T. p(a, V ) ? q(V, T ) ? r(b, T ) p(a, fp(a)) ? q(fp(a), fq(fp(a))) ? r(b, fr(b)) fq(fp(a)) ? fr(b) ip ?V, T. [p(a, V ) ? q(b, V )] ? r(V, T ) [p(a, fp(a)) ? q(b, fq(b))] fr(fp(a) ? fq(b)) ? r(fp(a) ? fq(b), fr(fp(a) ? fq(b))) 2in ?T. p(a, T ) ? ?q(b, T ) p(a, fp(a)) ? ?q(b, fq(b)) fp(a) ? ?fq(b) 3in ?T. p(a, T ) ? q(b, T ) ? ?r(c, T ) p(a, fp(a)) ? q(b, fq(b)) ? ?r(c, fr(c)) fp(a) ? fq(b) ? ?fr(c) pin ?V, T. p(a, V ) ? q(V, T ) ? ?r(b, T ) p(a, fp(a)) ? q(fp(a), fq(fp(a))) ? ?r(b, fr(b)) fq(fp(a)) ? ?fr(b) pni ?V, T. p(a, V ) ? ?q(V, T ) ? r(b, T ) p(a, fp(a)) ? ?q(fp(a), fq(fp(a))) ? r(b, fr(b)) ?fq(fp(a)) ? fr(b) inp ?V, T. [p(a, V ) ? ?q(b, V )] ? r(V, T ) [p(a, fp(a)) ? ?q(b, fq(b))] fr(fp(a) ? ?fq(b)) ? r(fp(a) ? ?fq(b), fr(fp(a) ? ?fq(b))) 2u ?T. p(a, T ) ? q(b, T ) p(a, fp(a)) ? q(b, fq(b)) fp(a) ? fq(b) up ?V, T. [p(a, V ) ? q(b, V )] ? r(V, T ) [p(a, fp(a)) ? q(b, fq(b))] fr(fp(a) ? fq(b)) ? r(fp(a) ? fq(b), fr(fp(a) ? fq(b)))</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Intuition behind truth bounds for Figure 1: propositions identify features, relations substitute propositions, negation flips truths, intersect retains common features. The final query embedding is closer to propositions of film2/4 (answer set) than film1/3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Test accuracy vs. training time comparison (V100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FB15k</head><label></label><figDesc>0.67 0.64 0.60 0.63 0.64 0.55 0.58 0.70 0.59 0.49 0.55 0.70 0.61 LOGICE bounds 0.51 0.56 0.48 0.68 0.67 0.62 0.40 0.56 0.61 0.30 0.34 0.58 0.53 BETAE 0.42 0.55 0.56 0.59 0.61 0.60 0.54 0.71 0.60 0.35 0.45 0.64 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FB15k</head><label></label><figDesc>43 0.53 0.53 0.53 0.49 0.46 0.45 0.66 0.54 0.46 0.55 0.63 0.52 BETAE 0.24 0.40 0.43 0.40 0.39 0.40 0.40 0.52 0.51 0.26 0.35 0.46 0.40 Q2B 0.07 0.21 0.31 0.36 0.29 0.24 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2103.00418v1 [cs.AI] 28 Feb 2021</figDesc><table><row><cell>= ?</cell><cell>, GoldenGlobe ? ?</cell><cell>, Oscar ?</cell><cell>,</cell><cell>= ?</cell><cell>, ? ?</cell><cell>, ?</cell><cell>,</cell><cell>= 1 ?</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">dissimilarity ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d) Fast querying</cell><cell></cell></row><row><cell cols="2">Reasons with subsets, substitutes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with largest related subset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Transitions FOL to Skolem set logic.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). 3. Direct uncertainty. Truth bounds naturally model uncertainty and have fast entropy calculation. Both entropy and bounds width show superior correlation to answer size. 4. Improved measure. Measures dissimilarity with simple L 1 -norm, which improves training speed and accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Complex logical query structures of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>tests both disjunctive normal form (DNF) and De Morgan's form (DM) for unions (2u/up), but we only report DNF elsewhere as it outperforms DM. LOGICE with bounds generalizes better than BETAE, Q2B, and GQE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>(b). LOGICE on average reasons more faithfully than BETAE, Q2B, and GQE baselines on all datasets.</figDesc><table /><note>EmQL is a query embedding that specifically optimises faithful reasoning (Sun et al., 2020), and thus outperforms all other methods in Table 5(b). However, EmQL without</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Test MRR results (%, higher better) of LOGICE and BE-TAE on answering queries with negation (BETAE dataset). 9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .</head><label>3</label><figDesc>Validation MRR averages (%, higher better) for LOGICE with various t-norms and BETAE on training queries (BETAE datasets), where i, n, and p are all query forms containing intersection, negation, or relation components, respectively.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell cols="2">NELL995</cell></row><row><cell></cell><cell>i</cell><cell>n</cell><cell>p</cell><cell>all</cell><cell>i</cell><cell>n</cell><cell>p</cell><cell>all avg</cell></row><row><cell cols="6">No bounds with attention (LOGICE)</cell><cell></cell><cell></cell></row><row><cell>luk</cell><cell cols="8">14.4 5.0 13.2 15.8 18.8 6.5 19.5 21.5 18.7</cell></row><row><cell>min</cell><cell cols="8">14.4 5.0 13.2 15.7 18.6 6.6 19.4 21.5 18.6</cell></row><row><cell>prod</cell><cell cols="8">14.4 5.0 13.3 15.8 18.7 6.6 19.4 21.5 18.7</cell></row><row><cell cols="5">Bounds with attention ( +bounds)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>luk</cell><cell cols="8">13.9 5.0 12.9 15.3 18.7 6.5 19.0 21.3 18.3</cell></row><row><cell>min</cell><cell cols="8">13.9 5.0 12.7 15.2 18.5 6.6 19.0 21.3 18.2</cell></row><row><cell>prod</cell><cell cols="8">14.0 5.0 13.0 15.4 18.6 6.7 19.1 21.3 18.4</cell></row><row><cell cols="9">BETAE 13.7 4.8 12.6 15.0 17.5 6.1 17.0 19.6 17.3</cell></row><row><cell cols="4">Bounds with no attention</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>luk</cell><cell cols="8">12.7 5.1 11.7 14.1 16.1 6.8 16.7 18.9 16.5</cell></row><row><cell>min</cell><cell cols="8">13.0 5.4 11.8 14.4 16.4 7.1 16.8 19.1 16.8</cell></row><row><cell>prod</cell><cell cols="8">12.9 5.3 11.9 14.3 16.5 7.1 17.0 19.2 16.8</cell></row><row><cell cols="9">BETAE 11.6 5.0 11.7 13.3 15.4 5.7 14.8 17.5 15.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .Table 6 .</head><label>46</label><figDesc>Test MRR results (%, higher better) of LOGICE, BETAE, Q2B and GQE on answering EPFO (?, ?, ?) queries (BETAE dataset). 9 Spearman's rank correlation and Pearson's correlation coefficient (higher better) between learned embedding (diff. entropy and truth interval width for LOGICE, diff. entropy for BETAE, L1 box size for Q2B) and the number of answers of queries (BETAE dataset). 9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Generalization on FB15k-237</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k NELL</cell></row><row><cell>Model</cell><cell>1p</cell><cell></cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell cols="4">2u DNF DM DNF DM up</cell><cell>avg</cell><cell>avg</cell><cell>avg</cell></row><row><cell>LOGICE</cell><cell cols="11">41.3 11.8 10.4 31.4 43.9 23.8 14.0 13.4 13.1 10.2</cell><cell>9.8</cell><cell>22.3</cell><cell>44.1</cell><cell>28.6</cell></row><row><cell cols="11">+bounds 40.5 11.4 10.1 29.8 42.2 22.4 13.4 13.0 12.9</cell><cell>9.8</cell><cell>9.6</cell><cell>21.4</cell><cell>40.8</cell><cell>28.0</cell></row><row><cell>BETAE</cell><cell cols="10">39.0 10.9 10.0 28.8 42.5 22.4 12.6 12.4 11.1</cell><cell>9.7</cell><cell>9.9</cell><cell>20.9</cell><cell>41.6</cell><cell>24.6</cell></row><row><cell>Q2B</cell><cell>40.6</cell><cell></cell><cell>9.4</cell><cell>6.8</cell><cell cols="5">29.5 42.3 21.2 12.6 11.3</cell><cell>-</cell><cell>7.6</cell><cell>-</cell><cell>20.1</cell><cell>38.0</cell><cell>22.9</cell></row><row><cell>GQE</cell><cell>35.0</cell><cell></cell><cell>7.2</cell><cell>5.3</cell><cell cols="4">23.3 34.6 16.5 10.7</cell><cell>8.2</cell><cell>-</cell><cell>5.7</cell><cell>-</cell><cell>16.3</cell><cell>28.0</cell><cell>18.6</cell></row><row><cell cols="16">Table 5. Hits@3 results (higher better) on the Q2B datasets testing (a) generalization and (b) reasoning faithfulness. 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">(a) Generalization on FB15k-237 (Q2B datasets)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k NELL</cell><cell></cell></row><row><cell cols="2">Model</cell><cell></cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>ip</cell><cell>pi</cell><cell>2u</cell><cell>up</cell><cell>avg</cell><cell>avg</cell><cell>avg</cell><cell></cell></row><row><cell cols="2">LOGICE</cell><cell></cell><cell>46.1</cell><cell cols="9">28.6 24.8 34.8 46.5 12.0 23.7 27.7 21.1 29.5</cell><cell>54.9</cell><cell>39.3</cell><cell></cell></row><row><cell cols="2">+bounds</cell><cell></cell><cell>45.0</cell><cell cols="9">26.6 23.0 32.0 44.1 11.1 22.1 25.5 20.4 27.7</cell><cell>50.3</cell><cell>38.6</cell><cell></cell></row><row><cell cols="2">EmQL</cell><cell></cell><cell>37.7</cell><cell cols="6">34.9 34.3 44.3 49.4 40.8 42.3</cell><cell>8.7</cell><cell cols="2">28.2 35.8</cell><cell>49.5</cell><cell>46.8</cell><cell></cell></row><row><cell cols="3">CQD-Beam</cell><cell>51.2</cell><cell cols="9">28.8 22.1 35.2 45.7 12.9 24.9 28.4 12.1 29.0</cell><cell>68.0</cell><cell>37.5</cell><cell></cell></row><row><cell cols="2">BETAE</cell><cell></cell><cell>43.1</cell><cell cols="9">25.3 22.3 31.3 44.6 10.2 22.3 26.6 18.0 27.1</cell><cell>51.4</cell><cell>33.8</cell><cell></cell></row><row><cell cols="2">Q2B</cell><cell></cell><cell>46.7</cell><cell cols="9">24.0 18.6 32.4 45.3 10.8 20.5 23.9 19.3 26.8</cell><cell>48.4</cell><cell>30.6</cell><cell></cell></row><row><cell cols="2">GQE</cell><cell></cell><cell>40.5</cell><cell cols="4">21.3 15.5 29.8 41.1</cell><cell>8.5</cell><cell cols="4">18.2 16.9 16.3 23.1</cell><cell>38.7</cell><cell>24.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Entailment on FB15k-237 (Q2B datasets)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k NELL</cell><cell></cell></row><row><cell cols="2">LOGICE</cell><cell></cell><cell>81.5</cell><cell cols="9">54.2 46.0 58.1 67.1 28.5 44.0 66.6 40.8 54.1</cell><cell>65.5</cell><cell>85.3</cell><cell></cell></row><row><cell cols="2">+bounds</cell><cell></cell><cell>73.7</cell><cell cols="9">46.4 38.9 49.8 61.5 22.0 37.2 54.6 35.1 46.6</cell><cell>58.4</cell><cell>80.1</cell><cell></cell></row><row><cell cols="2">EmQL</cell><cell></cell><cell cols="10">100.0 99.5 94.7 92.2 88.8 91.5 93.0 94.7 93.7 94.2</cell><cell>91.4</cell><cell>98.8</cell><cell></cell></row><row><cell cols="2">? sketch</cell><cell></cell><cell>89.3</cell><cell cols="9">55.7 39.9 62.9 63.9 51.9 54.7 53.8 44.7 57.4</cell><cell>55.5</cell><cell>82.5</cell><cell></cell></row><row><cell cols="2">BETAE</cell><cell></cell><cell>77.9</cell><cell cols="9">52.6 44.5 59.0 67.8 23.5 42.2 63.7 35.1 51.8</cell><cell>60.6</cell><cell>80.2</cell><cell></cell></row><row><cell cols="2">Q2B</cell><cell></cell><cell>58.5</cell><cell cols="9">34.3 28.1 44.7 62.1 11.7 23.9 40.5 22.0 36.2</cell><cell>43.7</cell><cell>51.1</cell><cell></cell></row><row><cell cols="2">GQE</cell><cell></cell><cell>56.4</cell><cell cols="9">30.1 24.5 35.9 51.2 13.0 25.1 25.8 22.0 31.6</cell><cell>43.7</cell><cell>49.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Spearman's rank correlation on FB15k-237</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k NELL</cell></row><row><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell>2in</cell><cell>3in</cell><cell>inp</cell><cell>pin</cell><cell>pni</cell><cell>avg</cell><cell>avg</cell><cell>avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .</head><label>7</label><figDesc>Answer size prediction mean absolute error (%, lower better) with embedding entropy components for LOGICE and BETAE, and box size components for Q2B (BETAE dataset). 9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Answer size prediction error on FB15k-237</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k NELL</cell></row><row><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell cols="6">2in 3in inp pin pni avg</cell><cell>avg</cell><cell>avg</cell></row><row><cell>LOGICE</cell><cell>78</cell><cell>83</cell><cell>86</cell><cell>82</cell><cell>94</cell><cell>89</cell><cell>86</cell><cell>81</cell><cell>79</cell><cell>81</cell><cell>81</cell><cell>81</cell><cell>83</cell><cell>87</cell><cell>80</cell></row><row><cell>BETAE</cell><cell>111</cell><cell>96</cell><cell>97</cell><cell>97</cell><cell>97</cell><cell>95</cell><cell>97</cell><cell>97</cell><cell>95</cell><cell>97</cell><cell>97</cell><cell>98</cell><cell>98</cell><cell>95</cell><cell>95</cell></row><row><cell>Q2B</cell><cell cols="7">191 101 100 310 780 263 103</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>north American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.1119-1129, 2015.    Serafini, L. and Garcez, A. d. Logic tensor networks: Deep learning and logical reasoning from data and knowledge.</figDesc><table><row><cell>arXiv preprint arXiv:1606.04422, 2016.</cell></row><row><cell>Sun, H., Arnold, A. O., Bedrax-Weiss, T., Pereira, F., and</cell></row><row><cell>Cohen, W. W. Faithful embeddings for knowledge base</cell></row><row><cell>queries. Advances in Neural Information Processing</cell></row><row><cell>Systems, 33, 2020.</cell></row><row><cell>Sun, K. and Nielsen, F. Information-geometric set embed-</cell></row><row><cell>dings (igse): From sets to probability distributions. arXiv</cell></row><row><cell>preprint arXiv:1911.12463, 2019.</cell></row><row><cell>Sun, Z., Deng, Z.-H., Nie, J.-Y., and Tang, J. Rotate: Knowl-</cell></row><row><cell>edge graph embedding by relational rotation in complex</cell></row><row><cell>space. In International Conference on Learning Repre-</cell></row><row><cell>sentations (ICLR), 2019.</cell></row><row><cell>Toutanova, K. and Chen, D. Observed versus latent features</cell></row><row><cell>for knowledge base and text inference. In Proceedings</cell></row><row><cell>of the 3rd Workshop on Continuous Vector Space Models</cell></row><row><cell>and their Compositionality, 2015.</cell></row><row><cell>Xiong, W., Hoang, T., and Wang, W. Y. Deeppath: A</cell></row><row><cell>reinforcement learning method for knowledge graph rea-</cell></row><row><cell>soning. In Empirical Methods in Natural Language Pro-</cell></row><row><cell>cessing (EMNLP), 2017.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc>compares test MRR results for LOGICE and BETAE on different datasets. Table 13 compares test MRR results for queries without negation. Table 14 provides full details of Spearman's rank correlation for all three KGs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 .</head><label>8</label><figDesc>Test MRR results (%) of LOGICE and BETAE on answering queries with negation.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>2in 3in inp pin pni avg</cell></row><row><cell>FB15k</cell><cell cols="2">LOGICE 15.1 14.2 12.5 7.1 13.4 12.5</cell></row><row><cell></cell><cell cols="2">+bounds 14.0 13.4 11.9 6.6 12.4 11.7</cell></row><row><cell></cell><cell>BETAE</cell><cell>14.3 14.7 11.5 6.5 12.4 11.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 .</head><label>10</label><figDesc>Dataset statistics according to<ref type="bibr" target="#b20">(Ren &amp; Leskovec, 2020)</ref> with training, validation and test edge splits.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Relations Entities Training Edges Validation Edges Test Edges Total Edges</cell></row><row><cell>FB15k</cell><cell>1,345</cell><cell>14,951</cell><cell>483,142</cell><cell>50,000</cell><cell>59,071</cell><cell>592,213</cell></row><row><cell>FB15k-237</cell><cell>237</cell><cell>14,505</cell><cell>272,115</cell><cell>17,526</cell><cell>20,438</cell><cell>310,079</cell></row><row><cell>NELL995</cell><cell>200</cell><cell>63,361</cell><cell>114,213</cell><cell>14,324</cell><cell>14,267</cell><cell>142,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 .</head><label>12</label><figDesc>). Number of queries in BETAE dataset generated for different query structures (see</figDesc><table><row><cell></cell><cell cols="2">Training</cell><cell cols="2">Validation</cell><cell></cell><cell>Test</cell></row><row><cell>Dataset</cell><cell>1p</cell><cell>others</cell><cell>1p</cell><cell>others</cell><cell>1p</cell><cell>others</cell></row><row><cell>FB15k</cell><cell cols="6">273,710 273,710 59,097 8,000 67,016 8,000</cell></row><row><cell cols="7">FB15k-237 149,689 149,689 20,101 5,000 22,812 5,000</cell></row><row><cell>NELL995</cell><cell cols="6">107,982 107,982 16,927 4,000 17,034 4,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>).</figDesc><table><row><cell></cell><cell>Training</cell><cell></cell><cell cols="2">Validation</cell><cell></cell><cell>Test</cell></row><row><cell>Dataset</cell><cell cols="2">1p/2p/3p/2i/3i 2in/3in/inp/pin/pni</cell><cell>1p</cell><cell>others</cell><cell>1p</cell><cell>others</cell></row><row><cell>FB15k</cell><cell>273,710</cell><cell>27,371</cell><cell cols="4">59,097 8,000 67,016 8,000</cell></row><row><cell>FB15k-237</cell><cell>149,689</cell><cell>14,968</cell><cell cols="4">20,101 5,000 22,812 5,000</cell></row><row><cell>NELL995</cell><cell>107,982</cell><cell>10,798</cell><cell cols="4">16,927 4,000 17,034 4,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 .</head><label>13</label><figDesc>Test MRR results (%) of LOGICE, BETAE, Q2B and GQE on answering EPFO (?, ?, ?) queries (BETAE data).Table 14. Spearman's rank correlation (higher better) between learned embedding (diff. entropy and truth intervals for LOGICE, diff. entropy for BETAE, box size for Q2B) and the number of answers of queries.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell cols="4">2u DNF DM DNF DM up</cell><cell>avg</cell></row><row><cell>FB15k</cell><cell></cell><cell>LOGICE</cell><cell cols="12">72.3 29.8 26.2 56.1 66.3 42.7 32.6 43.4 37.1 27.5 26.7 44.1</cell></row><row><cell></cell><cell></cell><cell cols="13">+bounds 67.8 27.1 24.6 51.4 61.3 41.3 30.1 38.1 33.7 25.1 23.8 40.8</cell></row><row><cell></cell><cell></cell><cell>BETAE</cell><cell cols="12">65.1 25.7 24.7 55.8 66.5 43.9 28.1 40.1 25.0 25.2 25.4 41.6</cell></row><row><cell></cell><cell></cell><cell>Q2B</cell><cell cols="8">68.0 21.0 14.2 55.1 66.5 39.4 26.1 35.1</cell><cell>-</cell><cell>16.7</cell><cell>-</cell><cell>38.0</cell></row><row><cell></cell><cell></cell><cell>GQE</cell><cell cols="8">54.6 15.3 10.8 39.7 51.4 27.6 19.1 22.1</cell><cell>-</cell><cell>11.6</cell><cell>-</cell><cell>28.0</cell></row><row><cell cols="3">FB15k-237 LOGICE</cell><cell cols="10">41.3 11.8 10.4 31.4 43.9 23.8 14.0 13.4 13.1 10.2</cell><cell>9.8</cell><cell>22.3</cell></row><row><cell></cell><cell></cell><cell cols="10">+bounds 40.5 11.4 10.1 29.8 42.2 22.4 13.4 13.0 12.9</cell><cell>9.8</cell><cell>9.6</cell><cell>21.4</cell></row><row><cell></cell><cell></cell><cell>BETAE</cell><cell cols="9">39.0 10.9 10.0 28.8 42.5 22.4 12.6 12.4 11.1</cell><cell>9.7</cell><cell>9.9</cell><cell>20.9</cell></row><row><cell></cell><cell></cell><cell>Q2B</cell><cell>40.6</cell><cell>9.4</cell><cell>6.8</cell><cell cols="5">29.5 42.3 21.2 12.6 11.3</cell><cell>-</cell><cell>7.6</cell><cell>-</cell><cell>20.1</cell></row><row><cell></cell><cell></cell><cell>GQE</cell><cell>35.0</cell><cell>7.2</cell><cell>5.3</cell><cell cols="4">23.3 34.6 16.5 10.7</cell><cell>8.2</cell><cell>-</cell><cell>5.7</cell><cell>-</cell><cell>16.3</cell></row><row><cell cols="2">NELL995</cell><cell>LOGICE</cell><cell cols="12">58.3 17.7 15.4 40.5 50.4 27.3 19.2 15.9 14.4 12.7 11.8 28.6</cell></row><row><cell></cell><cell></cell><cell cols="13">+bounds 57.6 16.9 14.6 40.8 50.2 26.5 18.0 15.3 13.5 12.0 11.3 28.0</cell></row><row><cell></cell><cell></cell><cell>BETAE</cell><cell cols="9">53.0 13.0 11.4 37.6 47.5 24.1 14.3 12.2 11.0</cell><cell>8.5</cell><cell>8.6</cell><cell>24.6</cell></row><row><cell></cell><cell></cell><cell>Q2B</cell><cell cols="8">42.2 14.0 11.2 33.3 44.5 22.4 16.8 11.3</cell><cell>-</cell><cell>10.3</cell><cell>-</cell><cell>22.9</cell></row><row><cell></cell><cell></cell><cell>GQE</cell><cell cols="2">32.8 11.9</cell><cell>9.6</cell><cell cols="4">27.5 35.2 18.4 14.4</cell><cell>8.5</cell><cell>-</cell><cell>8.8</cell><cell>-</cell><cell>18.6</cell></row><row><cell>Dataset</cell><cell cols="2">Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell>2in</cell><cell>3in</cell><cell>inp</cell><cell>pin</cell><cell>pni</cell><cell>avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 .</head><label>15</label><figDesc>Pearson correlation coefficient (higher better) between learned embedding (diff. entropy for LOGICE and BETAE, box size for Q2B) and the number of answers of queries.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell>2in</cell><cell>3in</cell><cell>inp</cell><cell>pin</cell><cell>pni</cell><cell>avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 .</head><label>17</label><figDesc>Detailed Hits@3 results for all the Query2Box datasets.</figDesc><table><row><cell cols="2">Generalization</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>ip</cell><cell>pi</cell><cell>2u</cell><cell>up</cell><cell>avg</cell></row><row><cell>FB15k</cell><cell>LOGICE</cell><cell>81.0</cell><cell cols="9">51.9 46.3 62.5 73.2 28.4 47.8 65.3 37.6 54.9</cell></row><row><cell></cell><cell>+bounds</cell><cell>76.4</cell><cell cols="9">47.9 43.3 56.6 67.1 25.0 42.6 57.9 35.9 50.3</cell></row><row><cell></cell><cell>EmQL</cell><cell>42.4</cell><cell cols="6">50.2 45.9 63.7 70.0 60.7 61.4</cell><cell>9.0</cell><cell cols="2">42.6 49.5</cell></row><row><cell></cell><cell>BETAE</cell><cell>75.8</cell><cell cols="9">46.0 41.8 62.5 74.3 24.3 48.0 60.2 29.2 51.4</cell></row><row><cell></cell><cell>Q2B</cell><cell>78.6</cell><cell cols="9">41.3 30.3 59.3 71.2 21.1 39.7 60.8 33.0 48.4</cell></row><row><cell></cell><cell>GQE</cell><cell>63.6</cell><cell cols="9">34.6 25.0 51.5 62.4 15.1 31.0 37.6 27.3 38.7</cell></row><row><cell cols="2">FB15k-237 LOGICE</cell><cell>46.1</cell><cell cols="9">28.6 24.8 34.8 46.5 12.0 23.7 27.7 21.1 29.5</cell></row><row><cell></cell><cell>+bounds</cell><cell>45.0</cell><cell cols="9">26.6 23.0 32.0 44.1 11.1 22.1 25.5 20.4 27.7</cell></row><row><cell></cell><cell>EmQL</cell><cell>37.7</cell><cell cols="6">34.9 34.3 44.3 49.4 40.8 42.3</cell><cell>8.7</cell><cell cols="2">28.2 35.8</cell></row><row><cell></cell><cell>BETAE</cell><cell>43.1</cell><cell cols="9">25.3 22.3 31.3 44.6 10.2 22.3 26.6 18.0 27.1</cell></row><row><cell></cell><cell>Q2B</cell><cell>46.7</cell><cell cols="9">24.0 18.6 32.4 45.3 10.8 20.5 23.9 19.3 26.8</cell></row><row><cell></cell><cell>GQE</cell><cell>40.5</cell><cell cols="4">21.3 15.5 29.8 41.1</cell><cell>8.5</cell><cell cols="4">18.2 16.9 16.3 23.1</cell></row><row><cell>NELL995</cell><cell>LOGICE</cell><cell>64.5</cell><cell cols="9">36.4 36.6 41.4 54.6 14.9 26.0 51.4 27.9 39.3</cell></row><row><cell></cell><cell>+bounds</cell><cell>63.9</cell><cell cols="9">35.1 35.5 40.7 54.4 14.2 25.2 50.3 27.6 38.6</cell></row><row><cell></cell><cell>EmQL</cell><cell>41.5</cell><cell cols="9">40.4 38.6 62.9 74.5 49.8 64.8 12.6 35.8 46.8</cell></row><row><cell></cell><cell>BETAE</cell><cell>58.7</cell><cell cols="9">29.6 30.7 36.1 50.0 11.0 22.9 44.1 21.1 33.8</cell></row><row><cell></cell><cell>Q2B</cell><cell>55.5</cell><cell cols="9">26.6 23.3 34.3 48.0 13.2 21.2 36.9 16.3 30.6</cell></row><row><cell></cell><cell>GQE</cell><cell>41.8</cell><cell cols="4">23.1 20.5 31.8 45.4</cell><cell>8.1</cell><cell cols="4">18.8 20.0 13.9 24.8</cell></row><row><cell cols="2">Entailment</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>ip</cell><cell>pi</cell><cell>2u</cell><cell>up</cell><cell>avg</cell></row><row><cell>FB15k</cell><cell>LOGICE</cell><cell>88.4</cell><cell cols="9">64.0 57.9 70.8 80.6 41.0 59.0 76.6 51.0 65.5</cell></row><row><cell></cell><cell>+bounds</cell><cell>82.9</cell><cell cols="9">57.3 51.9 62.5 73.0 34.1 51.5 67.2 45.5 58.4</cell></row><row><cell></cell><cell>EmQL</cell><cell>98.5</cell><cell cols="9">96.3 91.1 91.4 88.1 87.8 89.2 88.7 91.3 91.4</cell></row><row><cell></cell><cell>? sketch</cell><cell>85.1</cell><cell cols="9">50.8 42.4 64.4 66.1 50.4 53.8 43.2 42.7 55.5</cell></row><row><cell></cell><cell>BETAE</cell><cell>83.2</cell><cell cols="9">57.3 51.0 71.1 81.4 32.7 56.9 70.4 41.0 60.6</cell></row><row><cell></cell><cell>Q2B</cell><cell>68.0</cell><cell cols="9">39.4 32.7 48.5 65.3 16.2 32.9 61.4 28.9 43.7</cell></row><row><cell></cell><cell>GQE</cell><cell>73.8</cell><cell cols="9">40.5 32.1 49.8 64.7 18.9 36.1 47.2 30.4 43.7</cell></row><row><cell cols="2">FB15k-237 LOGICE</cell><cell>81.5</cell><cell cols="9">54.2 46.0 58.1 67.1 28.5 44.0 66.6 40.8 54.1</cell></row><row><cell></cell><cell>+bounds</cell><cell>73.7</cell><cell cols="9">46.4 38.9 49.8 61.5 22.0 37.2 54.6 35.1 46.6</cell></row><row><cell></cell><cell>EmQL</cell><cell cols="10">100.0 99.5 94.7 92.2 88.8 91.5 93.0 94.7 93.7 94.2</cell></row><row><cell></cell><cell>? sketch</cell><cell>89.3</cell><cell cols="9">55.7 39.9 62.9 63.9 51.9 54.7 53.8 44.7 57.4</cell></row><row><cell></cell><cell>BETAE</cell><cell>77.9</cell><cell cols="9">52.6 44.5 59.0 67.8 23.5 42.2 63.7 35.1 51.8</cell></row><row><cell></cell><cell>Q2B</cell><cell>58.5</cell><cell cols="9">34.3 28.1 44.7 62.1 11.7 23.9 40.5 22.0 36.2</cell></row><row><cell></cell><cell>GQE</cell><cell>56.4</cell><cell cols="9">30.1 24.5 35.9 51.2 13.0 25.1 25.8 22.0 31.6</cell></row><row><cell>NELL995</cell><cell>LOGICE</cell><cell>96.2</cell><cell cols="9">90.7 84.1 84.1 89.5 65.2 76.0 94.7 87.1 85.3</cell></row><row><cell></cell><cell>+bounds</cell><cell>94.1</cell><cell cols="9">86.0 78.7 80.4 87.1 53.6 68.5 90.9 81.2 80.1</cell></row><row><cell></cell><cell>EmQL</cell><cell>99.0</cell><cell cols="9">99.0 97.1 99.7 99.6 98.7 98.9 98.8 98.5 98.8</cell></row><row><cell></cell><cell>? sketch</cell><cell>94.5</cell><cell cols="9">77.4 52.9 97.4 97.5 88.1 90.8 70.4 73.5 82.5</cell></row><row><cell></cell><cell>BETAE</cell><cell>94.3</cell><cell cols="9">88.2 76.2 84.0 90.2 46.6 68.8 92.5 81.4 80.2</cell></row><row><cell></cell><cell>Q2B</cell><cell>83.9</cell><cell cols="9">57.7 47.8 49.9 66.3 19.9 29.6 73.7 31.0 51.1</cell></row><row><cell></cell><cell>GQE</cell><cell>72.8</cell><cell cols="9">58.0 55.2 45.9 57.3 24.8 34.2 59.0 40.7 49.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Universal quantification is not supported as interpretation in the underlying knowledge is contradictory, since useful relations cannot all be true for both empty and non-empty subsets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Skolem set logic reuses operators ?, ?, ? to signify that it performs direct real-valued logic on truth vectors of its terms. Skolem set logic usages are noted to avoid confusion with FOL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Truth bounds can share intervals with their negations, therefore complements can share latent propositions, which supports the diverse partitioning that complex queries require.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use non-monotonic smoothmin (? = ?10) for weighted minimum t-norm and set l = u = (l + u)/2 when l &gt; u. 5 1:1 train:test, ? = 10 3 , 250 epochs, Adam opt. (lr = 10 ?4 ).6  Hyperparameters include d = 400, h = 1600, ? = 0.375, k = 128 random negative samples, 512 batch size, 450k epochs, Adam optimizer (lr = 10 ?4 ). Pytorch on 1x NVIDIA Tesla V100.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/francoisluus/KGReasoning 8 Please see appendix for statistics of datasets. 9 Please see appendix for full results on FB15k and NELL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Please see appendix for an extended related work section.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arakelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cochez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03459</idno>
		<title level="m">Complex query answering with neural link predictors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia-a crystallization point for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of web semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD international conference on Management of data (SIGMOD)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Symbolic logic and mechanical theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siegler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06115</idno>
		<title level="m">Scalable neural methods for reasoning with a symbolic knowledge base</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient query evaluation on probabilistic databases. VLDB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Logical and relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lifted first-order probabilistic inference. Statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Salvo Braz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1300" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5823</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Answering graph pattern queries via knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03092</idno>
		<title level="m">Neural subgraph matching</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the structure of semigroups on a compact manifold with boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Mostert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Shields</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Annals of Mathematics</publisher>
			<biblScope unit="page" from="117" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">First-order probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beta embeddings for multi-hop logical reasoning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Query2box</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Riegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Makondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Akhalwaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13155</idno>
		<title level="m">Logical neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference of the</title>
		<meeting>the 2015 conference of the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
