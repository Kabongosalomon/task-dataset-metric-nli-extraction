<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-DIMENSIONAL EDGE-BASED AUDIO EVENT RELATIONAL GRAPH REPRESENTATION LEARNING FOR ACOUSTIC SCENE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbo</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyang</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Song</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Botteldooren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-DIMENSIONAL EDGE-BASED AUDIO EVENT RELATIONAL GRAPH REPRESENTATION LEARNING FOR ACOUSTIC SCENE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Acoustic scene classification</term>
					<term>audio event</term>
					<term>graph representation learning</term>
					<term>multi-dimensional edge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing deep learning-based acoustic scene classification (ASC) approaches directly utilize representations extracted from spectrograms to identify target scenes. However, these approaches pay little attention to the audio events occurring in the scene despite they provide crucial semantic information. This paper conducts the first study that investigates whether real-life acoustic scenes can be reliably recognized based only on the features that describe a limited number of audio events. To model the task-specific relationships between coarse-grained acoustic scenes and fine-grained audio events, we propose an event relational graph representation learning (ERGL) framework for ASC. Specifically, ERGL learns a graph representation of an acoustic scene from the input audio, where the embedding of each event is treated as a node, while the relationship cues derived from each pair of event embeddings are described by a learned multidimensional edge feature. Experiments on a polyphonic acoustic scene dataset show that the proposed ERGL achieves competitive performance on ASC by using only a limited number of embeddings of audio events without any data augmentations. The validity of the proposed ERGL framework proves the feasibility of recognizing diverse acoustic scenes based on the event relational graph. Our code is available on our homepage (https://github.com/Yuanbo2020/ERGL).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Acoustic scene classification (ASC) aims to classify an audio clip from various sources in real scenarios into a predefined semantic label (e.g., park, mall, or bus) <ref type="bibr" target="#b0">[1]</ref>. ASC provides a broad description of the surrounding environment to assist intelligent agents in quickly understanding the general picture of the environment, and thus is beneficial for various applications, such as sound source recognition <ref type="bibr" target="#b1">[2]</ref>, elderly well-being assistance <ref type="bibr" target="#b2">[3]</ref>, and audio-visual scene recognition <ref type="bibr" target="#b3">[4]</ref>.</p><p>Typical deep learning-based ASC methods usually consist of three steps: first, they convert the input time-domain audio stream into a time-frequency spectrogram as its acoustic features. Then, the obtained acoustic features are fed to neural networks to automatically generate task-orientated representations. Finally, the classifier recognizes the acoustic scene of the input audio stream based on such high-level representations. For example, the paper <ref type="bibr" target="#b4">[5]</ref> utilizes a CNNbased method with mel spectrograms of input audio for ASC, where attention-based pooling layers are used to reduce the dimension of the representation. The spatial pyramid pooling approach is used by CNN in <ref type="bibr" target="#b5">[6]</ref> to provide various resolutions for ASC. Except for mel spectrograms, wavelet-based deep scattering spectrum <ref type="bibr" target="#b6">[7]</ref> is introduced in ASC to exploit higher-order temporal information of acoustic features by convolutional recurrent neural networks (CRNN) with bidirectional gated recurrent units. Given the intrinsic relationship between acoustic scenes and audio events, some studies jointly analyze scenes and events relying on multi-task learning (MTL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. To further mine the implicit relational information between coarse-grained scenes and embedded fine-grained events, a relation-guided ASC <ref type="bibr" target="#b10">[11]</ref> is proposed to guide the model to bidirectionally fuse scene-event relations for mutually beneficial scene and event classification.</p><p>However, most of the aforementioned approaches do not specifically consider the important semantically meaningful information in the acoustic scene (i.e., audio events). It is difficult to explain what types of cues in the audio stream are utilized by these approaches to recognize the acoustic scene. Meanwhile, it is natural for humans to recognize acoustic scenes based on the semantically meaningful audio events contained in them, where the occurring events and their relationships vary in different acoustic scenes <ref type="bibr" target="#b11">[12]</ref>. This paper proposes to deep learn a pair of multi-dimensional edge-based graph to represent each audio event in an end-to-end manner, which contains not only the activation of audio events (they are treated as nodes in the graph) in the audio signal but also their task-specific relationships (represented as edges). Then, the scene-dependent event relational graph is fed into a gated graph convolutional network (Gated GCN) to extract scene-related cues for classification. This result shows that by relying only on several explicit audio event embeddings, the proposed ERGL can successfully build scene-dependent event relational graphs and effectively distinguish scenes. The paper is organized as follows. Section 2 introduces the proposed ERGL. Section 3 describes the dataset, experimental setup, and analyzes results. Section 4 draws conclusions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AUDIO EVENT RELATIONAL GRAPH</head><p>This paper presents a novel audio scene-event relationship modeling approach that learns a unique scene-related event relational graph from audio clips in an end-to-end manner. It first learn a set of embeddings, where each embedding (v i ) contains the i-th audio event-related information, and is treated as the i-th node in the event relational graph. Inspired by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, we further learn a pair of multi-dimensional edge features e i,j and e j,i to explicitly describe multiple taskspecific relationship cues between each pair of nodes v i and v j . As a result, the obtained graph explicitly describes the occurrence of a set of pre-defined audio events and task-specific relationships among them in the given audio scene. Finally, the obtained event relational graph consisting of n nodes and n ? n edges is fed into a GatedGCN <ref type="bibr" target="#b14">[15]</ref> for ASC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio event node feature learning</head><p>To obtain node embeddings that describe audio events' occurrence in the given audio clip, we propose an audio event node feature generation model derived from the PANN <ref type="bibr" target="#b15">[16]</ref> which shows excellent performance in recognizing audio events. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the spectrogram of the audio clip is first fed into a set of convolutional blocks (ConvBlocks), each of which contains two convolutional layers with kernels of size 3 ? 3, a batch normalization, and a ReLU activation <ref type="bibr" target="#b15">[16]</ref>. Then, a fully-connected (FC) layer with 2048 units is used to generate a joint representation for all audio events. Different from the PANN, we further employ n FC layers with 64 units to individually learn n embeddings, where each embedding describes a unique pre-defined audio event.</p><p>During training, each learned event embedding v i is fed into the audio event classifier to individually predict the target event's occurrence probability p vi , where Mean Squared Error (MSE) loss is used to measure the distance between p vi and the label y vi (i.e., L event = M SE(p vi , y vi )). Specifically, to train our audio event node feature generation module, we use the PANN that is pre-trained on Audioset <ref type="bibr" target="#b16">[17]</ref>, which contains 527 classes of audio events, to generate pseudo-labels for all events. It produces a 527-dimensional soft pseudolabel y = [y v1 , y v2 , y v3 , ..., y v527 ] for each audio stream, describing the occurrence probabilities of 527 classes of predefined audio events. Since real-world audio scene datasets rarely have all 527 classes of events, i.e., the number of occurred events would be much less than 527, we rank all events by accumulating their occurrence probabilities in all training data, and only use a set of top-ranked (Top n) audio events with the highest overall probability to describe each audio scene. As a result, each audio event graph contains n nodes and n ? n edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Audio event relational edge feature learning</head><p>Once all audio event (node) representations are produced, we propose a multi-dimensional edge learning (MEL) module to explicitly extract task-specific relationships between each pair of events, i.e., we deeply learn a pair of multidimensional edge features to connect each pair of nodes. Here, our hypothesis is that the co-occurrence patterns of all event pairs may include key clues for audio scene classification. For a pair of nodes, the MEL module in <ref type="figure" target="#fig_0">Fig. 1</ref> consists of a node-context relationship modeling (NCM) sub-module and a node-node relationship modeling (NNM) sub-module. NCM first learns the task-specific relationship cues between each node (event) and the global context (scene), generating a scene-aware event feature from each node. Then, NNM further models the relationship between each pair of scene-aware event features, to generate the final multi-dimensional edge feature describing the scene-aware relationship between each pair of nodes.</p><p>NCM. For each nodes v i , NCM conducts cross-attention <ref type="bibr" target="#b17">[18]</ref> between it and the global contextual representation v all that concatenated all node features, where nodes v i is used as the query, and v all is employed as the key and value.</p><formula xml:id="formula_0">NCM(Q, K) = ?(QW q (KW k ) T / d k )KW v (1) S i = NCM(v i , v all )<label>(2)</label></formula><p>where ? is softmax, W q , W k , W v are learnable weights, and d k is a factor equal to the number of K's channels. As a re-sult, the obtained representations S i (i = 1, 2, ? ? ? , n) encode scene-aware cues for each audio event.</p><p>NNM. After obtaining all scene-aware event features from the NCM, the NNM module further models the relationship between each pair of them, to generate final multidimensional edge features for the graph. In particular, the NNM consists of a cross-attention operation and global average polling (GAP) layer, which takes a pair of the scene-aware event features S i and S j ) as the input, and a pair of multidimensional edge features e i,j and e j,i as output. Specifically, the NNM first conducts:</p><formula xml:id="formula_1">R i,j = NNM(S j , S i ), R j,i = NNM(S i , S j )<label>(3)</label></formula><p>where R i,j encodes S j -related cues in S i , and correspondingly, R j,i encodes S i -related cues in S j . Next, R i,j and R j,i are fed into a GAP layer to obtain the multi-dimensional edge feature vectors e i,j and e j,i .</p><formula xml:id="formula_2">e i,j = GAP(R i,j ), e j,i = GAP(R j,i )<label>(4)</label></formula><p>Consequently, the learned edge features e i,j and e j,i capture multiple task-specific cues for scene classification, which relate to both event nodes v i and v j , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ASC based on event relational graph</head><p>Once the event relational graph that contains n node embeddings v = {v 1 , v 2 , ? ? ? , v n } and n ? n multi-dimensional directed edge representations e = {e 1,1 , ? ? ? , e i,j , ? ? ? , e n,n } are obtained, we feed it to the gated graph convolution network (GCN) <ref type="bibr" target="#b14">[15]</ref> for audio scene classification</p><p>Since the model contains U GCN layers, its output is G U = (v U , e U ), which is a graph with the same topology as G 0 . The i-th node represents the activation state of the i-th event in the scene. The latent node features in G U are concatenated as the scene representation and input to the final scene classification layer. Cross entropy (CE) is used as the loss function in ASC between the scene prediction p s and the scene true label y s , L scene = CE(p s , y s ). Hence, the total loss of the proposed model is L = L event + L scene .</p><p>In this paper, U is an important parameter in graph representation learning, and U defaults to 2. The effect of U on the model will be explored in the experiments discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset, Baseline, Experiments Setup, and Metric</head><p>In this paper, TUT Urban Acoustic Scenes 2018 development dataset (UAS) <ref type="bibr" target="#b18">[19]</ref> with 8640 10-seconds clips is used. UAS from real life contains 10 classes of acoustic scenes totaling 24 hours. UAS does not contain labels for events. Thus, to obtain the event labels used in Sec. 2.1, pre-trained PANN 1 <ref type="bibr" target="#b15">[16]</ref> is used to annotate each audio clip with 527 classes of event pseudo-labels. In training, following <ref type="bibr" target="#b18">[19]</ref>, about 30% of training samples are assigned to form the validation set.</p><p>A typical CNN-based approach <ref type="bibr" target="#b18">[19]</ref> to ASC is used as the baseline. In addition to the baseline, this paper also presents <ref type="bibr" target="#b0">1</ref> Model Cnn14 16k mAP=0.438.pth: https://zenodo.org/record/3987831 the performance of other methods based on attention, multitemporal features, and scene-event joint learning.</p><p>Following <ref type="bibr" target="#b15">[16]</ref>, the log mel spectrogram with 64 bins is used as the acoustic feature, which is extracted by the Short-Time Fourier Transform with a Hamming window of size 1024 and a hop size of 320 samples. Dropout and normalization are used in training to prevent over-fitting of the model. A batch size of 64 and AdamW optimizer <ref type="bibr" target="#b19">[20]</ref> with a learning rate of 1e-3 are used to minimize the loss. Systems are trained on a GPU card (Tesla V100-SXM2-32GB) for 300 epochs. The average accuracy (Acc) <ref type="bibr" target="#b0">[1]</ref> is used as the performance metric. A higher Acc indicates a better performance of scene recognition. Please visit our homepage given in the Abstract for more details, source code, and some visual supplements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results and Analysis</head><p>Number n of target audio events. We first evaluate the impact of the choice of n, i.e. the number of top events used in the model, on the performance of the proposed ERGL for scene classification. To this end, <ref type="table" target="#tab_1">Table 1</ref> shows the results of ERGL on ASC for different values of n. The accuracy of the model does not increase monotonically as n increases. The reason may be that as the number of events n increases, the number of nodes in the graph increases linearly, but the number of edges in the graph grows in n 2 , which sharply increases the burden for learning the multi-dimensional edge features with the MEL module. The increased parameters do not provide more useful information to the model, but increase the learning burden of the model and reduce its performance.</p><p>ERGL achieves the best result when n equals 25. This means that these top 25 classes of audio events can accurately and efficiently describe the 10 different classes of scenes ('airport', 'bus', 'metro', 'metro station', 'park', 'public square', 'shopping mall', 'street pedestrian', 'street traffic', 'tram') in the used dataset. These top 25 classes of audio events are, in order, ['Speech', 'Vehicle', 'Music', 'Silence', 'Animal', 'Train', 'Bird', 'Inside, small room', 'Raindrop', 'Insect', 'Clip-clop', 'Rain', 'Outside, rural or natural', 'Rain on surface', 'Rail transport', 'White noise', 'Outside, urban or manmade', 'Railroad car, train wagon', 'Boat, Water vehicle', 'Mouse', 'Horse', 'Tick', 'Car', 'Mechanical fan', 'Patter']. So n = 25 will be used in the following experiments.</p><p>Number U of GCN layers. <ref type="table" target="#tab_2">Table 2</ref> explores ERGL performance under different numbers of graph convolutional layers. The results in <ref type="table" target="#tab_2">Table 2</ref> illustrate that increasing the number of layers of GCN does not lead to better results. The reason for this may be that the 2-layer Gated GCN already achieves a good balance between model performance and computational efficiency on the graph consisting of embeddings of 25 classes of audio events in this paper, and also that adding extra layers would make the network deeper and harder to train. Subsequent experiments will continue to use U equal to 2. Ablation study of MEL. The expected role of MEL is to exploit multi-dimensional features to represent the edge of each pair of event nodes in scene-dependent event relational graphs. To investigate the practical efficacy of MEL, we conduct an ablation experiment on MEL under the same model structure and training conditions. The result for ERGL without MEL is 73.55%?1.94%, while that of ERGL equipped with MEL is 78.08%?2.01%, which illustrates that MEL that provides the model with scene-dependent multi-dimensional edge features does benefit the model.</p><p>Comparison with prior non-ensemble ASC methods 2 . <ref type="table" target="#tab_3">Table 3</ref> shows the well-performing CNN <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b20">[21]</ref> and CRNN <ref type="bibr" target="#b21">[22]</ref> with self-attention to capture global features. Attention is also used in pooling layers to reduce the dimensionality of CNN features <ref type="bibr" target="#b4">[5]</ref>. Furthermore, multiple layer temporal feature (MLTF) <ref type="bibr" target="#b22">[23]</ref> is used to try to capture the dynamic temporal information of the audio signal efficiently. And waveletbased spectrum <ref type="bibr" target="#b6">[7]</ref> is used to exploit higher-order scene information for ASC. PANN <ref type="bibr" target="#b15">[16]</ref> is also estimated, and is explored in 2 modes referring to transfer learning <ref type="bibr" target="#b23">[24]</ref>. In the fixed mode, PANN's parameters are not updated during training, and the scene is classified using prior knowledge of 527 classes of events learned from Audioset. In the fine-tuning mode, PANN's parameters are updated using existing knowledge in capturing scene information.</p><p>The fixed-mode PANN result is comparable to Baseline, indicating that PANN with just audio event knowledge has a certain discriminative ability for scenes. In contrast, ERGL using just audio event embedding significantly improves scene classification accuracy even though these event embeddings are learned from pseudo labels without verification. Overall, the proposed end-to-end EGRL without any data enhancement methods shows a competitive result compared to other methods. This illustrates that ERGL proposed in this paper can effectively discriminate different scenes by relying only on event embeddings, proving that a scene-dependent event relational graph built solely on the semantic information of events to identify different scenes is effective.</p><p>Comparison with scene-event joint analysis methods. The ERGL proposed in this paper infers target scenes based on the implicit event relational graph in the scene, which is a sene-event joint analysis method. <ref type="table" target="#tab_4">Table 4</ref> compares ERGL with other scene-event joint analysis methods. In <ref type="table" target="#tab_4">Table 4</ref>, #1 based on the same latent space for scene and event classification performed the worst. The reason may be that real-life coarse-grained scenes and fine-grained events differ at the semantic level and the feature space. Based on MTL, <ref type="bibr" target="#b8">[9]</ref> attempts to use shared base features and separated taskdependent high-level features to identify scenes and events.</p><p>[10] is based on a conditional loss from one-way scene-event to analyze scenes and events jointly. RGASC <ref type="bibr" target="#b10">[11]</ref> exploits the scene-event relationship to guide the two-tower model to bidirectionally fuse scene-event information to achieve mutually beneficial scene-event classification. The proposed ERGL relies only on the representation of events to achieve one-way event-to-scene inference, and deduces target scenes based on the corresponding implicit event relational graph. It is worth noting that ERGL, which only needs 25 classes of audio event information, outperforms RGASC using 527 classes of audio event information, which shows the effectiveness of ERGL. Overall, the proposed ERGL achieves competitive and promising results, demonstrating the feasibility of inferring acoustic scenes based on the event relational graph. Scene and event jointly classification <ref type="bibr" target="#b7">[8]</ref> 52.35 2 MTL-based event and scene analysis <ref type="bibr" target="#b8">[9]</ref> 61.69 3</p><p>Conditional scene and event recognition <ref type="bibr" target="#b9">[10]</ref> 66.39 4</p><p>Relation-guided ASC (RGASC) <ref type="bibr" target="#b10">[11]</ref> 77.35 5</p><p>The proposed ERGL 78.08</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper explores the feasibility of building scene-dependent event relational graphs to recognize various acoustic scenes. The proposed ERGL represents each acoustic scene by learning a set of audio event embeddings as graph node features and specifically producing a pair of scene-aware multidimensional edge features to describe the event relationships.</p><p>Experimental results show that ERGL achieves competitive ASC performance on a real-life dataset. Future work will analyze and visualize the bidirectional multi-dimensional edges learned by ERGL to clarify scene-event relationships and build a scene-event dynamic relationship model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The framework of scene-dependent event relational graph learning (ERGL) for ASC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Acc (%) of the proposed method at different top n values.</figDesc><table><row><cell>#</cell><cell>Top n</cell><cell>Acc (%)</cell><cell>#</cell><cell>Top n</cell><cell>Acc (%)</cell></row><row><cell>1</cell><cell>10</cell><cell>74.75 ? 2.61</cell><cell>6</cell><cell>150</cell><cell>75.39 ? 1.83</cell></row><row><cell>2</cell><cell>25</cell><cell>78.08 ? 2.01</cell><cell>7</cell><cell>200</cell><cell>74.66 ? 1.40</cell></row><row><cell>3</cell><cell>50</cell><cell>76.02 ? 2.08</cell><cell>8</cell><cell>250</cell><cell>74.81 ? 1.92</cell></row><row><cell>4</cell><cell>75</cell><cell>75.42 ? 1.45</cell><cell>9</cell><cell>300</cell><cell>73.97 ? 2.31</cell></row><row><cell>5</cell><cell>100</cell><cell>75.30 ? 1.57</cell><cell>10</cell><cell>400</cell><cell>72.63 ? 1.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Acc (%) of the proposed method at different U layers.</figDesc><table><row><cell>#</cell><cell>U</cell><cell>Acc (%)</cell><cell>#</cell><cell>U</cell><cell>Acc (%)</cell></row><row><cell>1</cell><cell>1</cell><cell>74.53 ? 2.12</cell><cell>5</cell><cell>5</cell><cell>75.44 ? 2.03</cell></row><row><cell>2</cell><cell>2</cell><cell>78.08 ? 2.01</cell><cell>6</cell><cell>6</cell><cell>74.91 ? 1.88</cell></row><row><cell>3</cell><cell>3</cell><cell>76.19 ? 1.86</cell><cell>7</cell><cell>7</cell><cell>74.52 ? 1.69</cell></row><row><cell>4</cell><cell>4</cell><cell>76.33 ? 1.95</cell><cell>8</cell><cell>8</cell><cell>74.00 ? 1.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of non-ensemble systems on UAS dataset.</figDesc><table><row><cell>System</cell><cell>Model structure</cell><cell>Acc (%)</cell></row><row><cell>PANN [16] (Fixed mode)</cell><cell>VGG-like CNN</cell><cell>56.9</cell></row><row><cell>Baseline [19]</cell><cell>CNN</cell><cell>59.7</cell></row><row><cell>CNN from Surrey [21]</cell><cell>CNN</cell><cell>68.0</cell></row><row><cell>NNF CNNEns [25]</cell><cell>CNN and nearest neighbor filters</cell><cell>69.3</cell></row><row><cell>Model with Attention [22]</cell><cell>CRNN with Self-attention</cell><cell>70.8</cell></row><row><cell>ABCNN [5]</cell><cell>Attention-Based CNN</cell><cell>72.6</cell></row><row><cell>PANN (Fine-tuning mode)</cell><cell>VGG-like CNN</cell><cell>73.8</cell></row><row><cell>MLTF [23]</cell><cell>CNN and SVM</cell><cell>75.3</cell></row><row><cell>Wavelet-based spectrum [7]</cell><cell>CRNN</cell><cell>76.6</cell></row><row><cell>Proposed ERGL</cell><cell>CNN and Graph Learning</cell><cell>78.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ASC results of scene-event joint analysis methods.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Acc (%)</cell></row><row><cell>1</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Results are from DCASE2018 T1A website. The proposed ERGL only uses one end-to-end model with one type of acoustic feature and without data augmentation, whereas the top 3 methods in T1A are ensembles of multiple models with multiple features, so the top 3 results are omitted inTable 3.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational analysis of sound scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic recognition of urban sound sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Defr?ville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Engineering Society Convention 120. Audio Engineering Society</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Positive technology for elderly well-being: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Raffaella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-visual scene classification via contrastive event-object alignment and semantic-based fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Botteldooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MMSP, 2022</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural networks for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCASE 2018 Workshop</title>
		<meeting>DCASE 2018 Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using spatial pyramid pooling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Basbug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICSC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="128" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-level attention model with deep scattering spectrum for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICMEW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="396" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards joint sound scene and polyphonic sound event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nolasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1236" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint analysis of sound events and acoustic scenes using multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TIS</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="301" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scenedependent acoustic event detection with scene conditioning and fake-scene-conditioned loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="646" to="650" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation-guided acoustic scene classification aided with event embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Botteldooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN, 2022</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The acoustic summary as a tool for representing urban sound environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oldoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Coensel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bockstael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Baets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Botteldooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Landscape and Urban Planning</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1239" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning graph representation of person-specific cognitive processes from audiovisual behaviours for automatic personality recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13570</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PANNs: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Pw</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multidevice dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DCASE 2018 challenge surrey cross-task convolutional neural network baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DCASE Workshop</title>
		<meeting>the DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="217" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-attention mechanism based system for dcase2018 challenge task1 and task4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Proceedings of DCASE Challenge</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using multi-layered temporal pooling based on deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE Challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer learning for improving singing-voice detection in polyphonic instrumental music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH, 2020</title>
		<imprint>
			<biblScope unit="page" from="1236" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="34" to="38" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
