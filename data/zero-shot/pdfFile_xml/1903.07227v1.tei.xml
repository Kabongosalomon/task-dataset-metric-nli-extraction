<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COUNTERPOINT BY CONVOLUTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
							<email>cooijmans.tim@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck Equal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COUNTERPOINT BY CONVOLUTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition.</p><p>Our model is an instance of orderless NADE <ref type="bibr" target="#b35">[36]</ref>, which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from <ref type="bibr" target="#b39">[40]</ref> yields better samples than ancestral sampling, based on both log-likelihood and human evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Counterpoint is the process of placing notes against notes to construct a polyphonic musical piece. <ref type="bibr" target="#b8">[9]</ref> This is a challenging task, as each note has strong musical influences on its neighbors and notes beyond. Human composers have developed systems of rules to guide their compositional decisions. However, these rules sometimes contradict each other, and can fail to prevent their users from going down musical dead ends. Statistical models of music, which is our current focus, is one of the many computational approaches that can help composers try out ideas more quickly, thus reducing the cost of exploration <ref type="bibr" target="#b7">[8]</ref>.</p><p>Whereas previous work in statistical music modeling has relied mainly on sequence models such as Hidden Markov Models (HMMs <ref type="bibr" target="#b1">[2]</ref>) and Recurrent Neural Networks (RNNs <ref type="bibr" target="#b30">[31]</ref>), we instead employ convolutional neural networks due to their invariance properties and em- phasis on capturing local structure. Nevertheless, they have also been shown to successfully model large-scale structure <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Moreover, convolutional neural networks have shown to be extremely versatile once trained, as demonstrated by a variety of creative uses such as Deep-Dream <ref type="bibr" target="#b28">[29]</ref> and style transfer <ref type="bibr" target="#b9">[10]</ref>.</p><p>We introduce COCONET, a deep convolutional model trained to reconstruct partial scores. Once trained, CO-CONET provides direct access to all conditionals of the form p(x i | x C ) where C selects a fragment of a musical score x and i / ? C is in its complement. COCONET is an instance of deep orderless NADE <ref type="bibr" target="#b35">[36]</ref>, which learns an ensemble of factorizations of the joint p(x), each corresponding to a different ordering. A related approach is the multi-prediction training of deep Boltzmann machines (MP-DBM) <ref type="bibr" target="#b11">[12]</ref>, which also gives a model that can predict any subset of variables given its complement.</p><p>However, the sampling procedure for orderless NADE treats the ensemble as a mixture and relies heavily on ordering. Sampling from an orderless NADE involves (randomly) choosing an ordering, and sampling variables one by one according to the chosen ordering. This process is called ancestral sampling, as the order of sampling follows the directed structure of the model. We have found that this produces poor results for the highly structured and complex domain of musical counterpoint.</p><p>Instead, we propose to use blocked-Gibbs sampling, a Markov Chain Monte Carlo method to sample from a joint probability distribution by repeatedly resampling subsets of variables using conditional distributions derived from the joint probability distribution. An instance of this was previously explored by <ref type="bibr" target="#b39">[40]</ref> who employed a NADE in the transition operator for a Markov Chain, yielding a Generative Stochastic Network (GSN). The transition consists of a corruption process that masks out a subset x ?C of variables, followed by a process that independently resamples variables x i (with i / ? C) according to the distribution p ? (x i | x C ) emitted by the model with parameters ?. Crucially, the effects of independent sampling are amortized by annealing the probability with which variables are masked out. Whereas <ref type="bibr" target="#b39">[40]</ref> treat their procedure as a cheap approximation to ancestral sampling, we find that it produces superior samples. Intuitively, the resampling process allows the model to iteratively rewrite the score, giving it the opportunity to correct its own mistakes. COCONET addresses the general task of completing partial scores; special cases of this task include "bridging" two musical fragments, and temporal upsampling and extrapolation. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of COCONET populating a partial piano roll using blocked-Gibbs sampling. Code and samples are publically available. <ref type="bibr" target="#b0">1</ref> Our samples on a variety of generative tasks such as rewriting, melodic harmonization and unconditioned polyphonic music generation show the versatility of our model. In this work we focus on Bach chorales, and assume four voices are active at all times. However, our model can be easily adapted to <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code:</head><p>https://github.com/czhuang/coconet Data:</p><p>https://github.com/czhuang/JSB-Chorales-dataset Samples: https://coconets.github.io/ the more general, arbitrarily polyphonic representation as used in <ref type="bibr" target="#b3">[4]</ref>. Section 2 discusses related work in modeling music composition, with a focus on counterpoint. The details of our model and training procedure are laid out in Section 3. We discuss evaluation under the model in Section 4, and sampling from the model in Section 5. Results of quantitative and qualitative evaluations are reported in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Computers have been used since their early days for experiments in music composition. A notable composition is Hiller and Issacson's string quartet Illiac Suite <ref type="bibr" target="#b17">[18]</ref>, which experiments with statistical sequence models such as Markov chains. One challenge in adapting such models is that music consists of multiple interdependent streams of events. Compare this to typical sequence domains such as speech and language, which involve modeling a single stream of events: a single speaker or a single stream of words. In music, extensive theories in counterpoint have been developed to address the challenge of composing multiple streams of notes that coordinate. One notable theory is due to Fux <ref type="bibr" target="#b8">[9]</ref> from the Baroque period, which introduces species counterpoint as a pedagogical scheme to gradually introduce students to the complexity of counterpoint. In first species counterpoint only one note is composed against every note in a given fixed melody (cantus firmus), with all notes bearing equal durations and the resulting vertical intervals consisting of only consonances.</p><p>Computer music researchers have taken inspiration from this pedagogical scheme by first teaching computers to write species counterpoint as opposed to full-fledged counterpoint. Farbood <ref type="bibr" target="#b6">[7]</ref> uses Markov chains to capture transition probabilities of different melodic and harmonic transitions rules. Herremans <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> takes an optimization approach by writing down an objective function that consists of existing rules of counterpoint and using a variable neighbourhood search (VNS) algorithm to optimize it. J.S. Bach chorales has been the main corpus in computer music that serves as a starting point to tackle fullfledged counterpoint. A wide range of approaches have been used to generate music in the style of Bach chorales, for example rule-based and instance-based approaches such as Cope's recombinancy method <ref type="bibr" target="#b5">[6]</ref>. This method involves first segmenting existing Bach chorales into smaller chunks based on music theory, analyzing their function and stylistic signatures and then re-concatenating the chunks into new coherent works. Other approaches range from constraint-based <ref type="bibr" target="#b29">[30]</ref> to statistical methods <ref type="bibr" target="#b4">[5]</ref>. In addition, <ref type="bibr" target="#b7">[8]</ref> gives a comprehensive survey of AI methods used not just for generating Bach chorales, but also algorithmic composition in general.</p><p>Sequence models such as HMMs and RNNs are natural choices for modeling music. Successful application of such models to polyphonic music often requires serializing or otherwise re-representing the music to fit the sequence paradigm. For instance, Liang in BachBot <ref type="bibr" target="#b26">[27]</ref> serializes four-part Bach chorales by interleaving the parts, while Allan and Williams <ref type="bibr" target="#b0">[1]</ref> construct a chord vocabulary. Boulanger et al. <ref type="bibr" target="#b3">[4]</ref> adopt a piano roll representation, a binary matrix x where x it = 1 iff some instrument is playing pitch i at time t. To model the joint probability distribution of the multi-hot pitch vector x t , they employ a Restricted Boltzmann Machine (RBM <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref> Hadjeres et al. <ref type="bibr" target="#b13">[14]</ref> instead employ an undirected Markov model to learn pairwise relationships between neighboring notes up to a specified number of steps away in a score. Sampling involves Markov Chain Monte Carlo (MCMC) using the model as a Metropolis-Hastings (MH) objective. The model permits constraints on the state space to support tasks such as melody harmonization. However, the Markov assumption can limit the expressivity of the model.</p><p>Hadjeres and Pachet in DeepBach <ref type="bibr" target="#b12">[13]</ref> model note predictions by breaking down its full context into three parts, with the past and the future modeled by stacked LSTMs going in the forward and backward directions respectively, and the present harmonic context modeled by a third neural network. The three are then combined by a fourth neural network and used in Gibbs sampling for generation.</p><p>Lattner et al. imposes higher-level structure by interleaving selective Gibbs sampling on a convolutional RBM <ref type="bibr" target="#b25">[26]</ref> and gradient descent that minimizes cost to template piece on features such as self-similarity. This procedure itself is wrapped in simulated annealing to ensure steps do not lower the solution quality too much.</p><p>We opt for an orderless NADE training procedure which enables us to train a mixture of all possible directed models simultaneously. Finally, an approximate blocked Gibbs sampling procedure <ref type="bibr" target="#b39">[40]</ref> allows fast generation from the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL</head><p>We employ machine learning techniques to obtain a generative model of musical counterpoint in the form of piano rolls. Given a dataset of observed musical pieces x <ref type="bibr" target="#b0">(1)</ref> . . . x (n) posited to come from some true distribution p(x), we introduce a model p ? (x) with parameters ?. In order to make p ? (x) close to p(x), we maximize the data log-likelihood i log p ? (x (i) ) (an approximation of E x?p(x) log p ? (x)) by stochastic gradient descent.</p><p>The joint distribution p(x) over D variables x 1 . . . x D is often difficult to model directly and hence we construct our model p ? (x) from simpler factors. In the NADE <ref type="bibr" target="#b24">[25]</ref> framework, the joint p ? (x) is factorized autoregressively, one variable at a time, according to some</p><formula xml:id="formula_0">ordering o = o 1 . . . o D , such that p ? (x) = d p ? (x o d | x o &lt;d ).<label>(1)</label></formula><p>For example, it can be factorized in chronological order:</p><formula xml:id="formula_1">p ? (x) = p ? (x 1 )p ? (x 2 |x 1 ) . . . p ? (x D |x D?1 . . . x 1 ) (2)</formula><p>In general, NADE permits any one fixed ordering, and although all orderings are equivalent from a theoretical perspective, they differ in practice due to effects of optimization and approximation. Instead, we can train NADE for all orderings o simultaneously using the orderless NADE <ref type="bibr" target="#b35">[36]</ref> training procedure. This procedure relies on the observation that, thanks to parameter sharing, computing</p><formula xml:id="formula_2">p ? (x o d | x o &lt;d ) for all d ? d</formula><p>is no more expensive than computing it only for d = d. <ref type="bibr" target="#b1">2</ref> Hence for a given o and d we can simultaneously obtain partial losses for all orderings that agree with o up to d:</p><formula xml:id="formula_3">L(x; o &lt;d , ?) = ? o d log p ? (x o d | x o &lt;d , o &lt;d , o d ) (3)</formula><p>An orderless NADE model offers direct access to all distributions of the form p ? (x i |x C ) conditioned on any set of contextual variables x C = x o &lt;d that might already be known. This gives us a very flexible generative model; in particular, we can use these conditional distributions to complete arbitrarily partial musical scores.</p><p>To train the model, we sample a training example x and context C such that |C| ? U (1, D), and update ? based on the gradient of the loss given by Equation <ref type="bibr" target="#b2">3</ref>. This loss consists of D ? d + 1 terms, each of which corresponds to one ordering. To ensure all orderings are trained evenly we must reweight the gradients by 1/(D ? d + 1). This correction, due to <ref type="bibr" target="#b35">[36]</ref>, ensures consistent estimation of the joint negative log-likelihood log p ? (x).</p><p>In this work, the model p ? (x) is implemented by a deep convolutional neural network <ref type="bibr" target="#b22">[23]</ref>. This choice is motivated by the locality of contrapuntal rules and their nearinvariance to translation, both in time and in pitch space.</p><p>We represent the music as a stack of piano rolls encoded in a binary three-dimensional tensor x ? {0, 1} I?T ?P . Here I denotes the number of instruments, T the number of time steps, P the number of pitches, and x i,t,p = 1 iff the ith instrument plays pitch p at time t. We will assume each instrument plays exactly one pitch at a time, that is, p x i,t,p = 1 for all i, t. Our focus is on four-part Bach chorales as used in prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. Hence we assume I = 4 throughout. We constrain ourselves to only the range that appears in our training data (MIDI pitches 36 through 88). Time is discretized at the level of 16th notes for similar reasons. To curb memory requirements, we enforce T = 128 by randomly cropping the training examples.</p><p>Given a training example x ? p(x), we present the model with values of only a strict subset of its elements x C = {x (i,t) | (i, t) ? C} and ask it to reconstruct its complement x ?C . The input h 0 ? {0, 1} 2I?T ?P is obtained by masking the piano rolls x to obtain the context x C and concatenating this with the corresponding mask:</p><formula xml:id="formula_4">h 0 i,t,p = 1 (i,t)?C x i,t,p<label>(4)</label></formula><p>h 0</p><p>I+i,t,p = 1 (i,t)?C <ref type="bibr" target="#b4">(5)</ref> where the time and pitch dimensions are treated as spatial dimensions to convolve over. Each instrument's piano roll h 0 i and mask h 0 I+i is treated as a separate channel and convolved independently.</p><p>With the exception of the first and final layers, all convolutions preserve the size of the hidden representation. That is, we use "same" padding throughout and all activations have the same number of channels H, such that h l ? R H?T ?P for all 1 &lt; l &lt; L. Throughout our experiments we used L = 64 layers and H = 128 channels. After each convolution we apply batch normalization <ref type="bibr" target="#b20">[21]</ref> (denoted by BN(?)) with statistics tied across time and pitch. Batch normalization rescales activations at each layer to have mean ? and standard deviation ?, which greatly improves optimization. After every second convolution, we introduce a skip connection from the hidden state two levels below to reap the benefits of residual learning <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_5">a l = BN(W l * h l?1 ; ? l , ? l )<label>(6)</label></formula><formula xml:id="formula_6">h l = ? ? ? ReLU(a l + h l?2 ) if 3 &lt; l &lt; L ? 1 and l mod 2 = 0 ReLU(a l ) otherwise h L = a L<label>(7)</label></formula><p>The final activations h L ? R I?T ?P are passed through the softmax function to obtain predictions for the pitch at each instrument/time pair:</p><formula xml:id="formula_7">p ? (x i,t,p | x C , C) = exp(h L i,t,p ) p exp(h L i,t,p )<label>(8)</label></formula><p>The loss function from Equation 3 is then given by</p><formula xml:id="formula_8">L(x; C, ?) = ? (i,t) / ?C log p ? (x i,t | x C , C) (9) = ? (i,t) / ?C p x i,t,p log p ? (x i,t,p | x C , C)</formula><p>where p ? denotes the probability under the model with parameters ? = W 1 , ? 1 , ? 1 , . . . , W L?1 , ? L?1 , ? L?1 . We train the model by minimizing</p><formula xml:id="formula_9">E x?p(x) E C?p(C) 1 |?C| L(x; C, ?)<label>(10)</label></formula><p>with respect to ? using stochastic gradient descent with step size determined by Adam <ref type="bibr" target="#b21">[22]</ref>. The expectations are estimated by sampling piano rolls x from the training set and drawing a single context C per sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>The log-likelihood of a given example is computed according to Algorithm 1 by repeated application of Equation 8. Evaluation occurs one frame at a time, within which the model conditions on its own predictions and does not see the ground truth. Unlike notewise teacher-forcing, where the ground truth is injected after each prediction, the framewise evaluation is thus sensitive to accumulation of error. This gives a more representative measure of quality of the generative model. For each example, we repeat the evaluation process a number of times to average over multiple orderings, and finally average across frames and examples. For chronological evaluation, we draw only orderings that have the t l s in increasing order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Framewise log-likelihood evaluation</head><p>Given a piano roll x L m,i,t ? 0 for all m, i, t for multiple orderings m = 0 . . . M do C ? ?, x ? x Sample an ordering t 1 , t 2 . . . t T over frames for l = 0 . . . T do Sample an ordering i 1 , i 2 . . . i I over instruments</p><formula xml:id="formula_10">for k = 0 . . . I do ? p ? p ? (x i k ,t l ,p | x C , C) for all p L m,i k ,t l ? p x i k ,t l ,p log ? p x i k ,t l ? Cat(P, ?) C ? C ? (i k , t l ) end for x C ? x C end for end for return ? 1 T t log 1 M m exp i L m,i,t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SAMPLING</head><p>We can sample from the model using the orderless NADE ancestral sampling procedure, in which we first sample an ordering and then sample variables one by one according to the ordering. However, we find that this yields poor samples, and we propose instead to use Gibbs sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Orderless NADE Sampling</head><p>Sampling according to orderless NADE involves first randomly choosing an ordering and then sampling variables one by one according to the chosen ordering. We use an equivalent procedure in which we arrive at a random ordering by at each step randomly choosing the next variable to sample. We start with an empty (zero everywhere) piano roll x 0 and empty context C 0 and populate them iteratively by the following process. We feed the piano roll x s and context C s into the model to obtain a set of categori-</p><formula xml:id="formula_11">cal distributions p ? (x i,t |x s C s , C s ) for (i, t) / ? C s .</formula><p>As the x i,t are not conditionally independent, we cannot simply sample from these distributions independently. However, if we sample from one of them, we can compute new conditional distributions for the others. Hence we randomly choose one (i, t) s+1 / ? C s to sample from, and let x s+1 i,t equal the one-hot realization. Augment the context with C s+1 = C s ? (i, t) s+1 and repeat until the piano roll is populated. This procedure is easily generalized to tasks such as melody harmonization and partial score completion by starting with a nonempty piano roll. Unfortunately, samples thus generated are of low quality, which we surmise is due to accumulation of errors. 5.03 ? 0.06 1.84 ? 0.02 0.57 ? 0.01 <ref type="table">Table 1</ref>. Framewise negative log-likelihoods (NLLs) on the Bach corpus. We compare against <ref type="bibr" target="#b3">[4]</ref>, who used quarter-note resolution. We also compare on higher temporal resolutions (eighth notes, sixteenth notes), against our own reimplementation of RNN-NADE. COCONET is an instance of orderless NADE, and as such we evaluate it on random orderings. However, the baselines support only chronological frame ordering, and hence we evaluate our model in this setting as well.</p><p>This is a well-known weakness of autoregressive models. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> While the model provides conditionals p ? (x i,t |x C , C) for all (i, t) / ? C, some of these conditionals may be better modeled than others. We suspect in particular those conditionals used early on in the procedure, for which the context C consists of very few variables. Moreover, although the model is trained to be order-agnostic, different orderings invoke different distributions, which is another indication that some conditionals are poorly learned. We test this hypothesis in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gibbs Sampling</head><p>To remedy this, we allow the model to revisit its choices: we repeatedly mask out some part of the piano roll and then repopulate it. This is a form of blocked Gibbs sampling <ref type="bibr" target="#b27">[28]</ref>. Blocked sampling is crucial for mixing, as the high temporal resolution of our representation causes strong correlations between consecutive notes. For instance, without blocked sampling, it would take many steps to snap out of a long-held note. Similar considerations hold for the Ising model from statistical mechanics, leading to the Swendsen-Wang algorithm <ref type="bibr" target="#b32">[33]</ref> in which large clusters of variables are resampled at once.</p><p>We consider two strategies for resampling a given block of variables: ancestral sampling and independent sampling. Ancestral sampling invokes the orderless NADE sampling procedure described in Section 5.1 on the masked-out portion of the piano roll. Independent sampling simply treats the masked-out variables x ?C as independent given the context x C .</p><p>Using independent blocked Gibbs to sample from a NADE model has been studied by <ref type="bibr" target="#b39">[40]</ref>, who propose to use an annealed masking probability ? n = max(? min , ? max ? n(? max ?? min )/(?N )) for some minimum and maximum probabilities ? min , ? max , total number of Gibbs steps N and fraction ? of time spent before settling onto the minimum probability ? min . Initially, when the masking probability is high, the chain mixes fast but samples are poor due to independent sampling. As ? n decreases, the blocked Gibbs process with independent resampling approaches standard Gibbs where one variable at a time is resampled, thus amortizing the effects of independent sampling. N is a hyperparameter which as a rule of thumb we set equal to IT ; it can be set lower than that to save computation at a slight loss of sample quality.</p><p>[40] treat independent blocked Gibbs as a cheap approximation to ancestral sampling. Whereas plain ancestral sampling (5.1) requires O(IT ) model evaluations, ancestral blocked Gibbs requires a prohibitive O(IT N ) model evaluations and independent Gibbs requires only O(N ), where N can be chosen to be less than IT . Moreover, we find that independent blocked Gibbs sampling in fact yields better samples than plain ancestral sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>We evaluate our approach on a popular corpus of four-part Bach chorales. While the literature features many variants of this dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, we report results on that used by <ref type="bibr" target="#b3">[4]</ref>. As the quarter-note temporal resolution used by <ref type="bibr" target="#b3">[4]</ref> is frankly too coarse to accurately convey counterpoint, we also evaluate on eighth-note and sixteenth-note quantizations of the same data.</p><p>It should be noted that quantitative evaluation of generative models is fundamentally hard <ref type="bibr" target="#b33">[34]</ref>. The gold standard for evaluation is qualitative comparison by humans, and we therefore report human evaluation results as well. <ref type="table">Table 4</ref> compares the framewise log-likelihood of the test data under variants of our model and those reported in <ref type="bibr" target="#b3">[4]</ref>. We find that the temporal resolution has a dramatic influence on the performance, which we suspect is an artifact of the performance metric. The log-likelihood is evaluated by teacher-forcing, that is, the prediction of a frame is conditioned on the ground truth of all previously predicted frames. As temporal resolution increases, chord changes become increasingly rare, and the model is increasingly rewarded for simply holding notes over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Log-likelihood</head><p>We evaluate COCONET on both chronological and random orderings, in both cases averaging likelihoods across an ensemble of M = 5 orderings. The chronological orderings differ only in the ordering of instruments within each frame. We see in <ref type="table">Table 4</ref> that fully random orderings lead to significantly better performance. We believe the members of the more diverse random ensemble are more mutually complementary. For example, a forward ordering is uncertain at the beginning of a piece and more certain toward the end, whereas a backward ordering is more certain at the beginning and less certain toward the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sample Quality</head><p>In Section 5 we conjectured that the low quality of NADE samples is due to poorly modeled conditionals p ? (x i,t | x C , C) where C is small. We test this hypothesis by evaluating the likelihood under the model of samples generated by the ancestral blocked Gibbs procedure with C chosen according to independent Bernoulli variables. When we set the inclusion probability ? to 0, we obtain NADE. Increasing ? increases the expected context size |C|, which should yield better samples if our hypothesis is true. The results shown in <ref type="table">Table 6</ref>.2 confirm that this is the case. For these experiments, we used sample length T = 32 time steps and number of Gibbs steps N = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling scheme</head><p>Framewise NLL Ancestral Gibbs, ? = 0.00 (NADE) 1.09 ? 0.06 Ancestral Gibbs, ? = 0.05</p><p>1.08 ? 0.06 Ancestral Gibbs, ? = 0.10 0.97 ? 0.05 Ancestral Gibbs, ? = 0.25 0.80 ? 0.04 Ancestral Gibbs, ? = 0.50 0.74 ? 0.04 Independent Gibbs <ref type="bibr" target="#b39">[40]</ref> 0.52 ? 0.01 <ref type="table">Table 2</ref>. Mean (? SEM) NLL under model of unconditioned samples generated from model by various schemes. <ref type="figure" target="#fig_1">Figure 2</ref> shows the convergence behavior of the various Gibbs procedures, averaged over 100 runs. We see that for low values of ? (small C), the chains hardly make progress beyond NADE in terms of likelihood. Higher values of ? (large C) enable the model to get off the ground and reach significantly better likelihood.   <ref type="figure">Figure 3</ref>. Human evaluations from MTurk on how many times a sampling procedure or Bach is perceived as more Bach-like. Error bars show the standard deviation of a binomial distribution fitted to each's binary win/loss counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Human Evaluations</head><p>To further compare the sample quality of different sampling procedures, we carried out a listening test on Amazon's Mechanical Turk (MTurk). The procedures include orderless NADE ancestral sampling and independent Gibbs <ref type="bibr" target="#b39">[40]</ref>, with each we generate four unconditioned samples of eight-measure lengths from empty piano rolls. To have an absolute reference for the quality of samples, we include first eight measures of four random Bach chorale pieces from the validation set. Each fragment lasts thirty-four seconds after synthesis. For each MTurk hit, participants are asked to rate on a Likert scale which of the two random samples they perceive as more Bach-like. A total of 96 ratings were collected, with each source involved in 64 (=96*2/3) pairwise comparisons. <ref type="figure">Figure 3</ref> shows the number of times each source was perceived as closer to Bach's style. We perform a Kruskal-Wallis H test on the ratings, ? 2 (2) = 12.23, p &lt; 0.001, showing there are statistically significant differences between models. A post-hoc analysis using the Wilcoxon signed-rank test with Bonferroni correction showed that participants perceived samples from independent Gibbs as more Bach-like than ancestral sampling (NADE), p &lt; 0.05/3. This confirms the loglikelihood comparisons on sample quality in 6.2 that independent Gibbs produces better samples. There was also a significance difference between Bach and ancestral samples but not between Bach and independent Gibbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We introduced a convolutional approach to modeling musical scores based on the orderless NADE <ref type="bibr" target="#b34">[35]</ref> framework. Our experiments show that the NADE ancestral sampling procedure yields poor samples, which we have argued is because some conditionals are not captured well by the model. We have shown that sample quality improves significantly when we use blocked Gibbs sampling to iteratively rewrite parts of the score. Moreover, annealed independent blocked Gibbs sampling as proposed by <ref type="bibr" target="#b39">[40]</ref> is not only faster but in fact produces better samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Blocked Gibbs inpainting of a corrupted Bach chorale by COCONET. At each step, a random subset of notes is removed, and the model is asked to infer their values. New values are sampled from the probability distribution put out by the model, and the process is repeated. Left: annealed masks show resampled variables. Colors distinguish the four voices. Middle: grayscale heatmaps show predictions p(x j | x C ) summed across instruments. Right: complete pianorolls after resampling the masked variables. Bottom: a sample from NADE (left) and the original Bach chorale fragment (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Likelihood under the model for ancestral Gibbs samples obtained with various context distributions p(C). NADE (Bernoulli(0.00)) is included for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) or Neural Autoregressive Distribution Estimator (NADE [25]) at each time step. Similarly Goel et al. [11] employ a Deep Belief Network [19] on top of an RNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? 0.09 4.21 ? 0.05 2.22 ? 0.03 COCONET (random)</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Temporal resolution</cell></row><row><cell></cell><cell>quarter</cell><cell>eighth</cell><cell>sixteenth</cell></row><row><cell>NADE [4]</cell><cell>7.19</cell><cell></cell></row><row><cell>RNN-RBM [4]</cell><cell>6.27</cell><cell></cell></row><row><cell>RNN-NADE [4]</cell><cell>5.56</cell><cell></cell></row><row><cell cols="2">RNN-NADE (our implementation) 5.03</cell><cell>3.78</cell><cell>2.05</cell></row><row><cell>COCONET (chronological)</cell><cell>7.79</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here xo &lt;d is used as shorthand for variables xo 1 . . . xo d?1 that occur earlier in the ordering.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kyle Kastner, Guillaume Alain, Gabriel Huang, Curtis (Fjord) Hawthorne, the Google Brain Magenta team, as well as Jason Freidenfelds for helpful feedback, discussions, suggestions and support. We also thank Calcul Qu?bec and Compute Canada for computational support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Harmonising chorales by probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moray</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Music generation from statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Conklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB 2003 Symposium on Artificial Intelligence and Creativity in the Arts and Sciences</title>
		<meeting>the AISB 2003 Symposium on Artificial Intelligence and Creativity in the Arts and Sciences</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computers and musical style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of palestrina-style counterpoint using markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Farbood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Sch?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMC</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ai methods in algorithmic composition: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="513" to="582" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The study of counterpoint from Johann Joseph Fux&apos;s Gradus ad Parnassum. Number 277</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann Joseph</forename><surname>Fux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Polyphonic music generation by modeling temporal dependencies using a rnn-dbn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raunaq</forename><surname>Vohra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-prediction deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deepbach: a steerable model for bach chorales generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?tan</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Pachet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01010</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Style imitation and chord invention in polyphonic music with exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?tan</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Pachet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05152</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Composing first species counterpoint with a variable neighbourhood search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorien</forename><surname>Herremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>S?rensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and the Arts</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="169" to="189" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Composing fifth species counterpoint music with a variable neighborhood search algorithm. Expert systems with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorien</forename><surname>Herremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>S?rensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="6427" to="6437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Musical composition with a high speed digital computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lejaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard M</forename><surname>Hiller</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isaacson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Engineering Society Convention 9</title>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How (not) to train your generative model: Scheduled sampling, likelihood, adversary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alias</forename><surname>Parth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imposing higher-level structure in polyphonic music generation using convolutional restricted boltzmann machines and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Grachten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04742</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bachbot: Automatic composition in the style of bach chorales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feynman</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Masters thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">427</biblScope>
			<biblScope unit="page" from="958" to="966" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Musical harmonization with constraints: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constraints</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Information processing in dynamical systems: Foundations of harmony theory. Technical report, DTIC Document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Nonuniversal critical dynamics in monte carlo simulations. Physical review letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Sheng</forename><surname>Swendsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">A?ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02226</idno>
		<title level="m">Neural autoregressive distribution estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving multi-step prediction of learned time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3024" to="3030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the equivalence between deep nade and generative stochastic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="322" to="336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
