<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DARTS-: ROBUSTLY STEPPING OUT OF PERFOR- MANCE COLLAPSE WITHOUT INDICATORS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>zhangbo97@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Lu</surname></persName>
							<email>lushun19@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
							<email>weixiaolin02@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meituan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DARTS-: ROBUSTLY STEPPING OUT OF PERFOR- MANCE COLLAPSE WITHOUT INDICATORS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the fast development of differentiable architecture search (DARTS), it suffers from long-standing performance instability, which extremely limits its application. Existing robustifying methods draw clues from the resulting deteriorated behavior instead of finding out its causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal to stop searching before the performance collapses. However, these indicator-based methods tend to easily reject good architectures if the thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections have a clear advantage over other candidate operations, where it can easily recover from a disadvantageous state and become dominant. We conjecture that this privilege is causing degenerated performance. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. We call this approach DARTS-. Extensive experiments on various datasets verify that it can substantially improve robustness. Our code is available at https://github.com/Meituan-AutoML/DARTS-.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent studies <ref type="bibr">(Zela et al., 2020;</ref><ref type="bibr" target="#b9">Chu et al., 2020b)</ref> have shown that one critical issue for differentiable architecture search <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref> regarding the performance collapse due to superfluous skip connections. Accordingly, some empirical indicators for detecting the occurrence of collapse have been produced. R-DARTS <ref type="bibr">(Zela et al., 2020)</ref> shows that the loss landscape has more curvatures (characterized by higher Hessian eigenvalues w.r.t. architectural weights) when the derived architecture generalizes poorly. By regularizing for a lower Hessian eigenvalue, <ref type="bibr">Zela et al. (2020)</ref>; <ref type="bibr" target="#b4">Chen &amp; Hsieh (2020)</ref> attempt to stabilize the search process. Meanwhile, by directly constraining the number of skip connections to a fixed number (typically 2), the collapse issue becomes less pronounced <ref type="bibr" target="#b5">(Chen et al., 2019b;</ref>. These indicator-based approaches have several main drawbacks. Firstly, robustness relies heavily on the quality of the indicator. An imprecise indicator either inevitably accepts poor models or mistakenly reject good ones. Secondly, indicators impose strong priors by directly manipulating the inferred model, which is somewhat suspicious, akin to touching the test set. Thirdly, extra computing cost <ref type="bibr">(Zela et al., 2020)</ref> or careful tuning of hyper-parameters <ref type="bibr" target="#b5">(Chen et al., 2019b;</ref> are required. Therefore, it's natural to ask the following questions: <ref type="figure">Figure 1</ref>: Schematic illustration about (a) DARTS and (b) the proposed DARTS-, featuring an auxiliary skip connection (thick red line) with a decay rate ? between every two nodes to remove the potential unfair advantage that leads to performance collapse.</p><p>To answer the above questions, we propose an effective and efficient approach to stabilize DARTS. Our contributions can be summarized as follows:</p><p>New Paradigm to Stabilize DARTS. While empirically observing that current indicators <ref type="bibr">(Zela et al., 2020;</ref><ref type="bibr" target="#b4">Chen &amp; Hsieh, 2020)</ref> can avoid performance collapse at a cost of reduced exploration coverage in the search space, we propose a novel indicator-free approach to stabilize DARTS, referred to as DARTS-1 , which involves an auxiliary skip connection (see <ref type="bibr">Figure 1)</ref> to remove the unfair advantage <ref type="bibr" target="#b9">(Chu et al., 2020b)</ref> in the searching phase.</p><p>Strong Robustness and Stabilization. We conduct thorough experiments across seven search spaces and three datasets to demonstrate the effectiveness of our method. Specifically, our approach robustly obtains state-of-the-art results on 4 search space with 3? fewer search cost than R-DARTS <ref type="bibr">(Zela et al., 2020)</ref>, which requires four independent runs to report the final performance.</p><p>Seamless Plug-in Combination with DARTS Variants. We conduct experiments to demonstrate that our approach can work together seamlessly with other orthogonal DARTS variants by removing their handcrafted indicators without extra overhead. In particular, our approach is able to improve 0.8% accuracy for P-DARTS, and 0.25% accuracy for PC-DARTS on CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Neural architecture search and DARTS variants. Over the years, researchers have sought to automatically discover neural architectures for various deep learning tasks to relieve human from the tedious effort, ranging from image classification , objection detection <ref type="bibr" target="#b14">(Ghiasi et al., 2019)</ref>, image segmentation  to machine translation <ref type="bibr" target="#b31">(So et al., 2019)</ref> etc. Among many proposed approaches, Differentiable Architecture Search <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref> features weight-sharing and resolves the searching problem via gradient descent, which is very efficient and easy to generalize. A short description of DARTS can be found in A.1. Since then, many subsequent works have been dedicated to accelerating the process <ref type="bibr" target="#b12">(Dong &amp; Yang, 2019b)</ref>, reducing memory cost <ref type="bibr" target="#b40">(Xu et al., 2020)</ref>, or fostering its ability such as hardware-awareness <ref type="bibr" target="#b2">(Cai et al., 2019;</ref><ref type="bibr" target="#b38">Wu et al., 2019)</ref>, finer granularity <ref type="bibr" target="#b26">(Mei et al., 2020)</ref> and so on. However, regardless of these endeavors, a fundamental issue of DARTS over its searching performance collapse remains not properly solved, which extremely prohibits its application.</p><p>Robustifying DARTS. As DARTS <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref> is known to be unstable as a result of performance collapse <ref type="bibr" target="#b9">(Chu et al., 2020b)</ref>, some recent works have devoted to resolving it by either designing indicators like Hessian eigenvalues for the collapse <ref type="bibr">(Zela et al., 2020)</ref> or adding perturbations to regularize such an indicator <ref type="bibr" target="#b4">(Chen &amp; Hsieh, 2020)</ref>. Both methods rely heavily on the indicator's accurateness, i.e., to what extent does the indicator correlate with the performance collapse? Other methods like Progressive DARTS <ref type="bibr" target="#b5">(Chen et al., 2019b)</ref> and DARTS+ ) employ a strong human prior, i.e., limiting the number of skip-connections to be a fixed value. Fair DARTS <ref type="bibr" target="#b9">(Chu et al., 2020b)</ref> argues that the collapse results from the unfair advantage in an exclusive competitive environment, from which skip connections overly benefit to cause an abundant aggregation. To suppress such an advantage from overshooting, they convert the competition into collaboration where each operation is independent of others. It is however an indirect approach. SGAS , instead, circumvents the problem with a greedy strategy where the unfair advantage can be prevented from taking effect. Nevertheless, potentially good operations might be pruned out too early because of greedy underestimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DARTS-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOTIVATION</head><p>We start from the detailed analysis of the role of skip connections. Skip connections were proposed to construct a residual block in ResNet <ref type="bibr" target="#b15">(He et al., 2016)</ref>, which significantly improves training stability. It is even possible to deepen the network up to hundreds of layers without accuracy degradation by simply stacking them up. In contrast, stacking the plain blocks of VGG has degenerated performance when the network gets deeper. <ref type="bibr">Besides, Ren et al. (2015)</ref>; <ref type="bibr" target="#b37">Wei et al. (2017)</ref>; <ref type="bibr" target="#b33">Tai et al. (2017)</ref>; <ref type="bibr" target="#b20">Li et al. (2018b)</ref> also empirically demonstrate that deep residual network can achieve better performance on various tasks.</p><p>From the gradient flow's perspective, skip connection is able to alleviate the gradient vanishing problem. Given a stack of n residual blocks, the output of the (i + 1) th residual block X i+1 can be computed as X i+1 = f i+1 (X i , W i+1 ) + X i , where f i+1 denotes the operations of the (i + 1) th residual block with weights W i+1 . Suppose the loss function of the model is L, and the gradient of X i can be obtained as follows (1 denotes a tensor whose items are all ones):</p><formula xml:id="formula_0">?L ?X i = ?L ?X i+1 ? ?f i+1 ?X i + 1 = ?L ?X i+j ? j k=1 ?f i+k ?X i+k?1 + 1<label>(1)</label></formula><p>We observe that the gradient of shallow layers always includes the gradient of deep layers, which mitigates the gradient vanishing of W i . Formally we have,</p><formula xml:id="formula_1">?L ?W i = ?L ?X i+j ? j k=1 ?f i+k ?X i+k?1 + 1 ? ?f i ?W i<label>(2)</label></formula><p>To analyze how skip connect affects the performance of residual networks, we introduce a trainable coefficient ? on all skip connections in ResNet. Therefore, the gradient of X i is converted to:</p><formula xml:id="formula_2">?L ?X i = ?L ?X i+1 ? ?f i+1 ?X i + ?<label>(3)</label></formula><p>Once ? &lt; 1, gradients of deep layers gradually vanish during the back-propagation (BP) towards shallow layers. Here ? controls the memory of gradients in BP to stabilize the training procedure. The residual structure is proved to learn a large ? to ease training in all three cases. All models are trained and tested on CIFAR-10.</p><p>We conduct a confirmatory experiment on ResNet50 and show the result in <ref type="figure" target="#fig_0">Fig 2.</ref> By initializing ? with {0, 0.5, 1.0}, we can visualize the tendency of ? along with training epochs. We observe that ? converges towards 1 after 40 epochs no matter the initialization, which demonstrates that the residual structure learns to push ? to a rather large value to alleviate gradient vanishing.</p><p>Similarly, DARTS <ref type="bibr" target="#b25">(Liu et al., 2019b</ref>) utilizes a trainable parameter ? skip to denote the importance of skip connection. However, In the search stage, ? skip can generally increase and dominate the architecture parameters, and finally leads to performance collapse. We analyze that a large ? skip in DARTS could result from two aspects: On the one hand, as the supernet automatically learns to alleviate gradient vanishing, it pushes ? skip to a proper large value; On the other hand, the skip connection is indeed an important connection for the target network, which should be selected in the discretization stage. As a consequence, the skip connection in DARTS plays two-fold roles: as an auxiliary connection to stabilize the supernet training, and as a candidate operation to build the final network. Inspired by the above observation and analysis, we propose to stabilize the search process by distinguishing the two roles of skip connection and handling the issue of gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STEPPING OUT OF THE PERFORMANCE COLLAPSE</head><p>To distinguish the two roles, we introduce an auxiliary skip connection between every two nodes in a cell, see <ref type="figure">Fig. 1 (b)</ref>. On the one hand, the fixed auxiliary skip connection carries the function of stabilizing the supernet training, even when ? skip is rather small. On the other hand, it also breaks the unfair advantage <ref type="bibr" target="#b9">(Chu et al., 2020b)</ref> as the advantageous contribution from the residual block is factored out. Consequently, the learned architectural parameter ? skip can be freed from the role of controlling the memory of gradients, and is more precisely aimed to represent the relative importance of skip connection as a candidate operation. In contrast to Eq. 7, the output feature map of edge e (i,j) can now be obtained by Eq. 4, where</p><formula xml:id="formula_3">? i,j o = exp(? (i,j) o ) o ?O exp(? (i,j) o )</formula><p>denotes the normalized importance, and ? is a coefficient independent from the architecture parameters.</p><p>Moreover, to eliminate the impact of auxiliary connection on the discretization procedure, we propose to decrease ? to 0 in the search phase, and our method can be degenerated to standard DARTS at the end of the search. Note that our method is not insensitive to the type of decay strategy, so we choose linear decay by default for simplicity.</p><formula xml:id="formula_4">o (i,j) (x) = ?x + o?O ? (i,j) o o(x) = ? + ? (i,j) skip x + o =skip ? (i,j) o o(x)<label>(4)</label></formula><p>We then analyze how the auxiliary skip connection handles the issue of gradient flow. Referring to the theorem of a recent work by <ref type="bibr" target="#b43">Zhou et al. (2020)</ref>, the convergence of network weight W in the supernet can heavily depend on ? skip . Specifically, suppose only three operations (none, skip connection, and convolution) are included in the search space and MSE loss is utilized as training loss, when architecture parameters ? o i,j are fixed to optimize W via gradient descent, training loss can decrease by ratio (1 ? ??/4) at one step with probability at least 1 ? ?, where ? is the learning rate that should be bounded by ?, and ? follows Eq. 5.</p><formula xml:id="formula_5">? ? h?2 i=0 ? (i,h?1) conv 2 i?1 t=0 ? (t,i) skip 2 (5)</formula><p>where h is the number of layers of the supernet. From Eq. 5, we observe that ? relies much on ? skip than ? conv , which indicates that the network weights W can converge faster with a large ? skip . However, by involving an auxiliary skip connection weighted as ?, Eq. 5 can be refined as follows: where ? ? skip making ? insensitive to ? skip , so that the convergence of network weights W depends more on ? conv . In the beginning of the search, the common value for ? skip is 0.15 while ? is 1.0. From the view of convergence theorem <ref type="bibr" target="#b43">(Zhou et al., 2020)</ref>, the auxiliary skip connection alleviates the privilege of ? skip and equalize the competition among architecture parameters. Even when ? gradually decays, the fair competition still holds since network weights W have been converged to an optimal point. Consequently, DARTS-is able to stabilize the search stage of DARTS.</p><formula xml:id="formula_6">? ? h?2 i=0 ? (i,h?1) conv 2 i?1 t=0 ? (t,i) skip + ? 2<label>(6</label></formula><p>Extensive experiments are performed to demonstrate the efficiency of the proposed auxiliary skip connection, and we emphasize that our method is flexible to combine with other methods to further improve the stabilization and searching performance. The overall algorithm is given in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RELATIONSHIP TO PRIOR WORK</head><p>Our method is aimed to address the performance collapse in differentiable neural architecture search. Most previous works <ref type="bibr">(Zela et al., 2020;</ref><ref type="bibr" target="#b4">Chen &amp; Hsieh, 2020;</ref> concentrate on developing various criteria or indicators characterizing the occurrence of collapse. Whereas, we don't study or rely on these indicators because they can mistakenly reject good models. Inspired by <ref type="bibr" target="#b9">Chu et al. (2020b)</ref>, our method focuses on calibrating the biased searching process. The underlying philosophy is simple: if the biased process is rectified, the searching result will be better. In summary, our method differs from others in two aspects: being process-oriented and indicator-free. Distinct from <ref type="bibr" target="#b9">Chu et al. (2020b)</ref> that tweaks the competitive environment, our method can be viewed as one that breaks the unfair advantage. Moreover, we don't introduce any handcrafted indicators to represent performance collapse, thus greatly reducing the burden of shifting to different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SEARCH SPACES AND TRAINING SETTINGS</head><p>For searching and evaluation in the standard DARTS space (we name it as S0 for simplicity), we keep the same settings as in DARTS <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref>. We follow R-DARTS <ref type="bibr">(Zela et al., 2020)</ref> for their proposed reduced spaces S1-S4 (harder than S0). However, the inferred models are trained with two different settings from R-DARTS <ref type="bibr">(Zela et al., 2020)</ref> and SDARTS <ref type="bibr" target="#b4">(Chen &amp; Hsieh, 2020)</ref>. The difference lies in the number of layers and initial channels for evaluation on CIFAR-100. R-DARTS sets 8 layers and 16 initial channels. Instead, SDARTS uses 20 and 36 respectively. For the proxyless searching on ImageNet, we instead search in MobileNetV2-like search space (we name it S5) proposed in FBNet <ref type="bibr" target="#b38">(Wu et al., 2019)</ref>. We use the SGD optimizer for weight and Adam (? 1 = 0.5 and ? 2 = 0.999, 0.001 learning rate) for architecture parameters with the batch-size of 768. The initial learning rate is 0.045 and decayed to 0 within 30 epochs following the cosine schedule. We also use L2 regularization with 1e-4. It takes about 4.5 GPU days on Tesla V100. More details are provided in the appendix. We also use NAS-Bench-201 (S6) since DARTS performs severely bad. In total, we use 7 different search spaces to conduct the experiments, which involves three datasets. CIFAR-10 and CIFAR-100. Following the settings of R-DARTS <ref type="bibr">(Zela et al., 2020)</ref>, we obtain an average top-1 accuracy of 97.36% on CIFAR-10, as shown in <ref type="table" target="#tab_2">Table 2</ref>. Moreover, our method is very robust since out of six independent runs the searching results are quite stable. The best cells found on CIFAR-10 (97.5%) are shown in <ref type="figure" target="#fig_6">Figure 9</ref> (B). Results on CIFAR-100 are presented in <ref type="table" target="#tab_1">Table 10</ref> (see A.2.1). Moreover, our method has a much lower searching cost (3? less) than R-DARTS <ref type="bibr">(Zela et al., 2020)</ref>, where four independent searches with different regularization settings are needed to generate the best architecture. In other words, its robustness comes from the cost of more CO 2 emissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEARCHING RESULTS</head><p>ImageNet. To further verify the efficiency of DARTS-, we directly search on ImageNet in S5 and compare our results with the state-of-the-art models under the mobile setting in <ref type="table" target="#tab_2">Table 2</ref>. The visualization of the architecture is given in <ref type="figure">Fig 10.</ref> DARTS-A obtains 76.2% top-1 accuracy on the ImageNet validation dataset. By contrast, direct applying DARTS on this search space only obtains 66.4% <ref type="bibr" target="#b9">(Chu et al., 2020b)</ref>. Moreover, it obtains 77.8% top-1 accuracy after being equipped with auto-augmentation (Cubuk et al., 2019) and squeeze-and-excitation <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>, which are also used in EfficientNet.</p><p>NAS-Bench-201. Apart from standard search spaces, benchmarking with known optimal in a limited setting is also recommended. NAS-Bench-201 <ref type="bibr" target="#b13">(Dong &amp; Yang, 2020)</ref> consists of 15,625 architectures in a reduced DARTS-like search space, where it has 4 internal nodes and 5 operations per node. We compare our method with prior work in <ref type="table" target="#tab_3">Table 3</ref>. We search on CIFAR-10 and look up the ground-truth performance with found genotypes on various test sets. Remarkably, we achieve a new state of the art, the best of which almost touches the optimal.  Transfer results on objection detection We further evaluate the transfer-ability of our models on down-stream object detection task by replacing the backbone of RetinaNet <ref type="bibr" target="#b22">(Lin et al., 2017)</ref> on MMDetection toolbox platform . Specifically, with the same training setting as <ref type="bibr" target="#b9">Chu et al. (2020b)</ref>, our model achieves 32.5% mAP on the COCO dataset, surpassing other similarsized models such as MobileNetV3, MixNet, and FairDARTS. The detailed results are shown in Appendix <ref type="table" target="#tab_1">(Table 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ORTHOGONAL COMBINATION WITH OTHER VARIANTS</head><p>Our method can be flexibly adapted to combine with prior work for further improvements. Here we investigate the joint outcome with two methods: P-DARTS and PC-DARTS. Progressive DARTS (P-DARTS). P-DARTS <ref type="bibr" target="#b5">(Chen et al., 2019b)</ref> proposes a progressive approach to search gradually with deeper depths while pruning out the uncompetitive paths. Additionally, it makes use of some handcrafted criteria to address the collapse (the progressive idea itself cannot deal with it), for instance, they impose two strong priors by regularizing the number of skip connections M as 2 as well as dropout. To be fair, we remove such a carefully handcrafted trick and run P-DARTS for several times. As a natural control group, we also combine DARTS-with P-DARTS. We run both experiments for 3 times on CIFAR-10 dataset in <ref type="table" target="#tab_4">Table 4</ref>. Without the strong priors, P-DARTS severely suffers from the collapse, where the inferred models contain an excessive number of skip connections. Specifically, it has a very high test error (3.42% on average), even worse than DARTS. However, P-DARTS can benefit greatly from the combination with DARTS-. The improved version (we call P-DARTS-) obtains much higher top-1 accuracy (+0.8%) on CIFAR-10 than its baseline. Memory Friendly DARTS (PC-DARTS). To alleviate the large memory overhead from the whole supernet, PC-DARTS <ref type="bibr" target="#b40">(Xu et al., 2020)</ref> selects the partial channel for searching. The proportion hyperparameter K needs careful calibration to achieve a good result for specific tasks. As a byproduct, the search time is also reduced to 0.1 GPU days (K=4). We use their released code and run repeated experiments across different seeds under the same settings.</p><p>To accurately evaluate the role of our method, we choose K=2 (a bad configuration in the original paper). We make comparisons between the original PC-DARTS and its combination with ours (named PC-DARTS-) in <ref type="table" target="#tab_6">Table 5</ref>. PC-DARTS-can marginally boost the CIFAR-10 top-1 accuracy (+0.26% on average). The result also confirms that our method can make PC-DARTS less sensitive to its hyper-parameter K while keeping its advantage of less memory cost and run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDY</head><p>Robustness to Decay Strategy. Our method is insensitive to the type of decay policy on ?. We design extra two strategies as comparisons: cosine and step decay. They both have the similar performance. Specifically, when ? is scheduled to zero by the cosine strategy, the average accuracy of four searched CIFAR-10 models in S3 is 97.33%?0.09, the best is 97.47%. The step decay at epoch 45 obtains 97.30% top-1 accuracy on average in the same search space. Robustness Comparison on C10 and C100 in S0-S4. To verify the robustness, it is required to search several times to report the average performance of derived models <ref type="bibr">(Yu et al., 2020;</ref><ref type="bibr" target="#b0">Antoine et al., 2020)</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, <ref type="table">Table 9</ref> and <ref type="table" target="#tab_5">Table 7</ref>, our method outperforms the recent SOTAs across several spaces and datasets. Note that SDARTS-ADV utilizes adversarial training and requires 3? more search times than ours. Especially, we find a good model in S 3 on CIFAR-100 with the lowest top-1 test error 15.86%. The architectures of these models can be found in the appendix.</p><p>Sensitivity Analysis of ? The power of the auxiliary skip connection branch can be discounted by setting a lower initial ?. We now evaluate how sensitive our approach is to the value of ?. It's trivial to see that our approach degrades to DARTS when ? = 0. We compare results when searching with ? ? {1, 0.7, 0.4, 0.1, 0} in <ref type="table" target="#tab_7">Table 6</ref>, which show that a bigger ? 0 is advantageous to obtain better networks.</p><p>The choice of auxiliary branch Apart from the default skip connection serving as the auxiliary branch, we show that it is also effective to replace it with a learnable 1?1 convolution projection, which is initialized with an identity tensor. The average accuracy of three searched CIFAR-10 models in S3 is 97.25%?0.09. Akin to the ablation in <ref type="bibr" target="#b15">He et al. (2016)</ref>, the projection convolution here works in a similar way as the proposed skip connection. This proves the necessity of the auxiliary branch.</p><p>Performance with longer epochs. It is claimed by <ref type="bibr" target="#b1">Bi et al. (2019)</ref> that much longer epochs lead to better convergence of the supernet, supposedly beneficial for inferring the final models. However, many of DARTS variants fail since their final models are full of skip connections. We thus evaluate how our method behaves in such a situation. Specifically, we extend the standard 50 epochs to 150, 200 and we search 3 independent times each for S0, S2 and S3. Due to the longer epochs, we slightly change our decay strategy, we keep ? = 1 all the way until for last 50 epochs we decay ? to 0. Other hyper-parameters are kept unchanged. The results are shown in <ref type="table">Table 8</ref> and the found genotypes are listed in <ref type="figure">Figure 17</ref>, 18, 19, 20 and 21. It indicates that DARTS-doesn't suffer from longer epochs since it has reasonable values for #P compared with those (#P = 0) investigated by <ref type="bibr" target="#b1">Bi et al. (2019)</ref>. Notice S2 and S3 are harder cases where DARTS suffers more severely from the collapse than S0. As a result, DARTS-can successfully survive longer epochs even in challenging search spaces. Noticeably, it is still unclear whether longer epochs can truly boost searching performance. Although we achieve a new state-of-the-art result in S2 where the best model has 2.50% error rate (previously 2.63%), it still has worse average performance (2.71?0.11%) in S0 than the models searched with 50 epochs (2.59?0.08%), and the best model in S3 (2.53%) is also weaker than before (2.42%).  Besides, compared with first-order DARTS with a cost of 0.4 GPU days in S0, Amended-DARTS <ref type="bibr" target="#b1">(Bi et al., 2019)</ref>, particularly designed to survive longer epochs, reports 1.7 GPU days even with pruned edges in S0. Our approach has the same cost as first-order DARTS, which is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS AND DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FAILURE OF HESSIAN EIGENVALUE</head><p>The maximal Hessian eigenvalue calculated from the validation loss w.r.t ? is regarded as an indicator of performance collapse <ref type="bibr">(Zela et al., 2020;</ref><ref type="bibr" target="#b4">Chen &amp; Hsieh, 2020)</ref>. Surprisingly, our method develops a growing eigenvalue in the majority of configurations, which conflicts with the previous observations. We visualize these statistics across different search space and datasets in <ref type="figure">Figure 4</ref> (A.2.2). Although eigenvalues increase almost monotonously and reach a relatively large value in the end, the final models still have good performance that matches with state-of-the-art ones (see <ref type="table">Table 9</ref>). These models can be mistakenly deemed as bad ones or never visited according to the eigenvalue criteria. Our observations disclose one fatal drawback of these indicator-based approaches: they are prone to rejecting good models. Further analysis can be found in A.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VALIDATION ACCURACY LANDSCAPE</head><p>Recent works, R-DARTS <ref type="bibr">(Zela et al., 2020)</ref> and SDARTS <ref type="bibr" target="#b4">(Chen &amp; Hsieh, 2020)</ref> point that the architectural weights are expected to converge to an optimal point where accuracy is insensitive to perturbations to obtain a stable architecture after the discretization process, i.e., the convergence point should have a smooth landscape. SDARTS proposes a perturbation-based regularization, which further stabilizes the searching process of DARTS. However, the perturbation regularization disturbs the training procedure and thus misleads the update of architectural weights. Different from SDARTS that explicitly smooths landscape by perturbation, DARTS-can implicitly do the same without directly perturbing architectural weights. To analyze the efficacy of DARTS-, we plot the validation accuracy landscape w.r.t architectural weights ?, and find that auxiliary connection smooths the landscape and thus stabilizes the searching stage. Specifically, we choose two random directions and apply normalized perturbation on ? (following <ref type="bibr" target="#b19">Li et al. 2018a)</ref>. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, DARTS-is less sensitive to the perturbation than DARTS, and the contour map of DARTS-descends more gently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a simple and effective approach named DARTS-to address the performance collapse in differentiable architecture search. Its core idea is to make use of an auxiliary skip connection branch to take over the gradient advantage role from the candidate skip connection operation. This can create a fair competition where the bi-level optimization process can easily differentiate good operations from the bad. As a result, the search process is more stable and the collapse seldom happens across various search spaces and different datasets. Under strictly controlled settings, it steadily outperforms recent state-of-the-art RobustDARTS <ref type="bibr">(Zela et al., 2020)</ref>  Then what's the solution? In principle, it's difficult to find a perfect indicator of the collapse. Our approach shows the potential to control the search process and doesn't impose limitations or priors on the final model. We hope more attention be paid in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>This research was supported by Meituan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 PRELIMINARY ABOUT DARTS</head><p>In differentiable architecture search <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref>, a cell-based search space in the form of Directed Acyclic Graph (DAG) is constructed. The DAG has two input nodes from the previous layers, four intermediate nodes, and one output node. There are several paralleling operators (denoted as O) between each two nodes (say i, j), whose output? <ref type="bibr">(i,j)</ref> given an input x is defined as,</p><formula xml:id="formula_7">o (i,j) (x) = o?O exp(? (i,j) o ) o ?O exp(? (i,j) o ) o(x)<label>(7)</label></formula><p>It is essentially applying softmax over all operators where each operator is assigned with an architectural weight ?. A supernet is built on two kinds of such cells, so-called normal cells and reduction cells (for down-sampling). The architectural search is then characterized as a bi-level optimization:</p><formula xml:id="formula_8">min ? L val (w * (?), ?) (8) s.t. w * (?) = arg min w L train (w, ?)<label>(9)</label></formula><p>This indicates that the training of such a cell-based supernet should be interleaved, where at each step the network weights and architectural weights are updated iteratively. The final model is determined by simply choosing operations with the largest architectural weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 TRAINING DETAILS</head><p>CIFAR-10 and CIFAR-100. <ref type="table">Table 9</ref> gives the averaged performance in reduced search spaces S1-S4, as well with maximum eigenvalues. <ref type="table" target="#tab_1">Table 10</ref> reports the CIFAR-100 results in S0. <ref type="table">Table 9</ref>: Comparison of searched CNN architectures in four reduced search spaces S1-S4 <ref type="bibr">(Zela et al., 2020)</ref> on CIFAR-10 and CIFAR-100. We report the mean?std of test error over 3 found architectures retrained from scratch, alongside with eigenvalue (EV) that corresponds to the best validation accuracy. We follow the same settings as <ref type="bibr">Zela et al. (2020)</ref>. ImageNet classification. For training on ImageNet, we use the same setting as MnasNet . To be comparable with EfficientNet , we also use squeeze-and-excitation <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>. Furthermore, we don't include methods trained using large model distillation, because it can boost final validation accuracy marginally. To be fair, we don't use the efficient head in MobileNetV3 <ref type="bibr" target="#b16">(Howard et al., 2019)</ref> although it can reduce FLOPs marginally.</p><p>COCO object detection. All models are trained and evaluated on MS COCO dataset for 12 epochs with a batch size of 16. The initial learning rate is 0.01 and reduced by 0.1 at epoch 8 and 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 FURTHER DISCUSSIONS ON FAILURE OF EIGENVALUE</head><p>To further explore the relationship between the searching performance and Hessian Eigenvalue, we plot the performance trajectory of the searched models in <ref type="figure">Figure 4 (b)</ref>. Specifically, we sample  models every 10 epochs and train these models from scratch using the same setting as above <ref type="figure" target="#fig_3">(Figure 5)</ref>. The performance of the inferred models continues growing, where the accuracy is boosted from 96.5% to 97.4%. This affirms the validity of searching using our method. In contrast, the early-stopping strategies based on eigenvalues <ref type="bibr">(Zela et al., 2020)</ref> would fail in this setting. We argue that the proposed auxiliary skip branch can regularize the overfitting of the supernet, leaving the architectural weights to represent the ability of candidate operations. This experiment poses as a counterexample to R-DARTS, where good models can appear albeit Hessian eigenvalues change fast. It again denies the need for a costly indicator.  <ref type="figure">See  Fig 4 (b)</ref> for the corresponding eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 MORE ABLATION STUDIES</head><p>To supplement the sensitivity analysis in Section 4.4, <ref type="figure" target="#fig_4">Figure 6</ref> shows the training loss curve when initializing ? with different values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 LOSS LANDSCAPE</head><p>To show the consistent smoothing capacity of DARTS-, we draw more loss landscapes in S0 and S3 (on several seeds) w.r.t architectural parameters and their contours in <ref type="figure">Figure 7</ref> and 8. Generally, DARTS-'s slopes are more inflated if we consider them as camping tents, which suggest better convergence of the over-parameterized network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 LIST OF EXPERIMENTS</head><p>We summarize all the conducted experiments with their related figures and tables in <ref type="table" target="#tab_1">Table 12.</ref> B FIGURES OF GENOTYPES    <ref type="table" target="#tab_1">Stem  MBE6_K3  MBE6_K5  MBE6_K3  MBE6_K5  MBE6_K3  MBE6_K7  MBE6_K3  MBE6_K7  MBE6_K3  MBE6_K7  MBE6_K7   224?224?3  112?112?32  112?112?16  56?56?32  56?56?32  28?28?40  28?28?40  28?28?40  14?14?80  14?14?80  14?14?96  14?14?96  7?7?192  7?7?192</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MBE1_K3 MBE6_K3</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Tendency of trainable coefficient ? (initialized with {0, 0.5, 1}) of the skip connection in ResNet50 and test accuracy (inset figure) vs. epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>8: Searching performance on CIFAR-10 in S0, S2 and S3 using longer epochs. Following Bi et al. (2019), #P means the number of parametric operators in the normal cell. Averaged on 3 runs of search. Epoch S0 S2 S3 #P Params (M) Error (%) #P Params (M) Error (%) #P Params (M) Error (%) 150 6.6?1.1 3.3?0.3 2.74?0.06 6.0?0.0 3.9?0.3 2.58?0.11 6.0?1.0 3.6?0.3 2.55?0.03 200 7.3?0.6 3.2?0.3 2.71?0.11 8.0?0.0 4.3?0.1 2.65?0.21 7.6?0.5 4.3?0.2 2.66?0.09</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the validation accuracy landscape of (a) DARTS and (b) DARTS-w.r.t. ? on CIFAR-10 in S3. Their contour maps are shown respectively in (c) and (d), where we set the step of contour map as 0.1. The accuracy of the derived models are 94.84% (a,c) and 97.58% (b,d), while the maximum Hessian eigenvalues are similarly high (0.52 and 0.42)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Trajectory of eigenvalues in S0-S4 (left to right) on CIFAR-100Figure 4: The evolution of maximal eigenvalues of DARTS-when searching in different search spaces S0-S4 on CIFAR-10 (a) and CIFAR-100 (c). We run each experiment 3 times on different seeds. (b) DARTS-'s growing Hessian eigenvalues don't induce poor performance. Among the sampled five models, the one corresponding to the highest eigenvalue has the best performance. This example is done in S0 on CIFAR-10. Training five models sampled every 10 epochs during DARTS-searching process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The training loss curve of the over-parameterized network on CIFAR-10 in S3 with different initial ? 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>More visualization of validation accuracy landscapes of DARTS (a,b) and DARTS-(e,f) w.r.t. the architectural weights ? on CIFAR-10 in S0. Their contour maps are shown respectively in (c,d) and(g,h). The step of contour map is 0.1. The inferred models by DARTS-have higher accuracies (97.50%, 97.49%) than DARTS (97.19%, 97.20%). More visualization of validation accuracy landscapes of DARTS (a,b) and DARTS-(e,f) w.r.t. the architectural weights ? on CIFAR-10 in S3. Their contour maps are shown respectively in (c,d) and(g,h). The step of contour map is 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>The best found normal cell and reduction cell in search spaces S0-S4 on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :Figure 12 :</head><label>101112</label><figDesc>Architecture of DARTS-A searched on ImageNet dataset. The best found normal cell and reduction cell in search spaces S0-S4 on CIFAR-100 dataset. Found normal cells and reduction cells by P-DARTS (Chen et al., 2019b) without prior (M=2) in the DARTS' standard search space on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :Figure 21 :</head><label>131415161718192021</label><figDesc>Found normal cells and reduction cells by P-DARTS<ref type="bibr" target="#b5">(Chen et al., 2019b)</ref> with the proposed auxiliary skip connections in the DARTS' standard search space on CIFAR-10 dataset. Found normal cells and reduction cells by PC-DARTS<ref type="bibr" target="#b40">(Xu et al., 2020)</ref> without channel shuffling in the DARTS' standard search space on CIFAR-10 dataset. Found normal cells and reduction cells by PC-DARTS<ref type="bibr" target="#b40">(Xu et al., 2020)</ref> with the proposed auxiliary skip connections in the DARTS' standard search space on CIFAR-10 dataset. Keep ? skip = 1 throughout the DARTS-searching in the DARTS' standard search space on CIFAR-10 dataset. Best cells found when decaying ? skip in the last 50 epochs during the DARTS-searching for 150 and 200 epochs respectively in the DARTS search space on CIFAR-10. Decaying ? skip in the last 50 epochs during the DARTS-searching for 150 epochs in S2 on CIFAR-10 dataset. Decaying ? skip in the last 50 epochs during the DARTS-searching for 200 epochs in S2 on CIFAR-10 dataset. Decaying ? skip in the last 50 epochs during the DARTS-searching for 150 epochs in S3 on CIFAR-10 dataset. Decaying ? skip in the last 50 epochs during the DARTS-searching for 200 epochs in S3 on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of searched CNN in the DARTS search space on two different datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">DARTS R-DARTS (L2)</cell><cell>Ours</cell></row><row><cell>C10 (S0)</cell><cell>2.91?0.25</cell><cell>2.95?0.21</cell><cell>2.63?0.07</cell></row><row><cell cols="2">C100 (S0) 20.58?0.44</cell><cell cols="2">18.01?0.26 17.51?0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the state-of-the-art models on CIFAR-10 (left) and ImageNet (right). On CIFAR-10 dataset, our average result is obtained on 5 independently searched models to assure the robustness. For ImageNet, networks in the top block are directly searched on ImageNet; the middle indicates that architectures are searched on CIFAR-10 and then transferred to ImageNet; the bottom indicates models have SE and Swish. We search in S0 for CIFAR-10 and S5 for ImageNet.</figDesc><table><row><cell cols="5">Models NASNet-A (2018) ENAS (2018) DARTS (2019b) SNAS (2019) GDAS (2019b) P-DARTS (2019b) PC-DARTS (2020) DARTS-(best) P-DARTS (2019b)  ? 3.3?0.21 540?34 97.19?0.14 0.3 Params FLOPs Acc Cost (M) (M) (%) GPU Days 3.3 608  ? 97.35 2000 4.6 626  ? 97.11 0.5 3.3 528  ? 97.00?0.14 0.4 2.8 422  ? 97.15?0.02 1.5 3.4 519  ? 97.07 0.2 3.4 532  ? 97.5 0.3 3.6 558  ? 97.43 0.1 3.5 568 97.5 0.4 R-DARTS (2020) --97.05?0.21 1.6 SDARTS-ADV (2020) 3.3 -97.39?0.02 1.3 DARTS-(avg.) 3.5?0.13 583?22 97.41?0.08 0.4  Models AmoebaNet-A (2019) 555 FLOPs Params Top-1 Top-5 (M) (M) (%) (%) (GPU days) Cost 5.1 74.5 92.0 3150 MnasNet-92 (2019) 388 3.9 74.79 92.1 3791  ? FBNet-C (2019) 375 5.5 74.9 92.3 9 FairNAS-A (2019b) 388 4.6 75.3 92.4 12 SCARLET-C (2019a) 365 6.7 76.9 93.4 10 FairDARTS-D (2020b) 440 4.3 75.6 92.6 3 PC-DARTS (2020) 597 5.3 75.8 92.7 3.8 DARTS-(ours) 467 4.9 76.2 93.0 4.5 NASNet-A (2018) 564 5.3 74.0 91.6 2000 DARTS (2019b) 574 4.7 73.3 91.3 0.4 SNAS (2019) 522 4.3 72.7 90.8 1.5 PC-DARTS (2020) 586 5.3 74.9 92.2 0.1 FairDARTS-B (2020b) 541 4.8 75.1 92.5 0.4 MobileNetV3 (2019) 219 5.4 75.2 92.2 ?3000 MoGA-A (2020a) 304 5.1 75.9 92.8 12</cell></row><row><cell>MixNet-M (2019)</cell><cell>360</cell><cell>5.0</cell><cell>77.0 93.3</cell><cell>?3000</cell></row><row><cell cols="2">EfficientNet B0 (2019) 390</cell><cell>5.3</cell><cell>76.3 93.2</cell><cell>?3000</cell></row><row><cell>NoisyDARTS-A</cell><cell>449</cell><cell>5.5</cell><cell>77.9 94.0</cell><cell>12</cell></row><row><cell>DARTS-(ours)</cell><cell>470</cell><cell>5.5</cell><cell>77.8 93.9</cell><cell>4.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Based on the provided genotypes.? 5 independent searches using their released code. Training the best searched model for several times (whose average doesn't indicate the stability of the method)? Estimated by Wu et al. (2019). SE modules and Swish enabled.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Searching performance on NAS-Bench-201<ref type="bibr" target="#b13">(Dong &amp; Yang, 2020</ref>). Our method robustly obtains new SOTA. Averaged on 4 runs of searching. 1st : first-order, 2nd : second-order 77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 DARTS 2nd (2019b) 10.2 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 GDAS (2019b) 8.7 89.89?0.08 93.61?0.09 71.34?0.04 70.70?0.30 41.59?1.33 41.71?0.98 SETN (2019a) 9.5 84.04?0.28 87.64?0.00 58.86?0.06 59.05?0.24 33.06?0.02 32.52?0.21 DARTS-3.2 91.03?0.44 93.80?0.40 71.36?1.51 71.53?1.51 44.87?1.46 45.12?0.82</figDesc><table><row><cell>Method</cell><cell>Cost</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet16-120</cell></row><row><cell></cell><cell>(hours)</cell><cell>valid</cell><cell>test</cell><cell>valid</cell><cell>test</cell><cell>valid</cell><cell>test</cell></row><row><cell cols="3">DARTS 1st (2019b) 3.2 39.DARTS-(best) 3.2 91.55</cell><cell>94.36</cell><cell>73.49</cell><cell>73.51</cell><cell>46.37</cell><cell>46.34</cell></row><row><cell>optimal</cell><cell>n/a</cell><cell>91.61</cell><cell>94.37</cell><cell>73.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>We remove the strong constraints on the number of skip connections as 2 and dropout (priors) for P-DARTS and compare its performance w/ and w/o DARTS-.</figDesc><table><row><cell>Method Setting Acc (%)</cell></row><row><cell>P-DARTS w/o priors 96.48?0.55</cell></row><row><cell>P-DARTS-w/o priors 97.28?0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Comparison in various search spaces. We report the lowest error rate of 3 found architectures. ? : under Chen &amp; Hsieh (2020)'s training settings where all models have 20 layers and 36 initial channels (the best is shown in boldface). ? : underZela et al. (2020)'s settings where CIFAR-100 models have 8 layers and 16 initial channels (The best is in boldface and underlined).</figDesc><table><row><cell cols="5">Benchmark DARTS  ? R-DARTS  ?</cell><cell>DARTS  ?</cell><cell>Ours  ? PC-DARTS  ? SDARTS  ?</cell><cell>Ours  ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DP</cell><cell>L2</cell><cell>ES ADA</cell><cell>RS ADV</cell></row><row><cell></cell><cell>S1</cell><cell>3.84</cell><cell cols="4">3.11 2.78 3.01 3.10 2.68</cell><cell>3.11</cell><cell>2.78 2.73 2.68</cell></row><row><cell>C10</cell><cell>S2 S3</cell><cell>4.85 3.34</cell><cell cols="4">3.48 3.31 3.26 3.35 2.63 2.93 2.51 2.74 2.59 2.42</cell><cell>3.02 2.51</cell><cell>2.75 2.65 2.63 2.53 2.49 2.42</cell></row><row><cell></cell><cell>S4</cell><cell>7.20</cell><cell cols="4">3.58 3.56 3.71 4.84 2.86</cell><cell>3.02</cell><cell>2.93 2.87 2.86</cell></row><row><cell></cell><cell>S1</cell><cell cols="5">29.46 25.93 24.25 28.37 24.03 22.41</cell><cell>18.87</cell><cell>17.02 16.88 16.92</cell></row><row><cell>C100</cell><cell>S2 S3</cell><cell cols="5">26.05 22.30 22.24 23.25 23.52 21.61 28.90 22.36 23.99 23.73 23.37 21.13</cell><cell>18.23 18.05</cell><cell>17.56 17.24 16.14 17.73 17.12 15.86</cell></row><row><cell></cell><cell>S4</cell><cell cols="5">22.85 22.18 21.94 21.26 23.20 21.55</cell><cell>17.16</cell><cell>17.17 15.46 17.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of PC-DARTS removing the strong prior (i.e. channel shuffle) and combining DARTS-. The results are from 3 independent runs on CIFAR-10. The GPU memory cost is on a batch size of 256.</figDesc><table><row><cell>Method</cell><cell>Setting Acc (%) Memory Cost</cell></row><row><cell cols="2">PC-DARTS K = 2 97.09?0.14 19.9G 3.75h</cell></row><row><cell cols="2">PC-DARTS-K = 2 97.35?0.02 20.8G 3.41h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Searching performance on CIFAR-10 in S3 w.r.t the initial linear decay rate ? 0 . Each setting is run for three times.</figDesc><table><row><cell>? 0</cell><cell>Error (%)</cell></row><row><cell>1</cell><cell>2.65?0.04</cell></row><row><cell cols="2">0.7 2.76?0.16</cell></row><row><cell cols="2">0.4 3.04?0.19</cell></row><row><cell cols="2">0.1 3.11?0.16</cell></row><row><cell>0</cell><cell>4.58?1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>with 3? fewer search cost. Moreover, our method disapproves of various handcrafted regularization tricks. Last but not least, it can be used stand-alone or in cooperation with various orthogonal improvements if necessary. This paper conveys two important messages for future research. On the one hand, the Hessian eigenvalue indicator for performance collapse(Zela et al., 2020;<ref type="bibr" target="#b4">Chen &amp; Hsieh, 2020)</ref> is not ideal because it has a risk of rejecting good models. On the other hand, handcraft regularization tricks<ref type="bibr" target="#b5">(Chen et al., 2019b)</ref> seem more critical to search a good model instead of the proposed methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of searched models on CIFAR-100. : Reported by Dong &amp; Yang (2019b), : Reported by Zela et al. (2020), ? :Rerun their code.</figDesc><table><row><cell>Models</cell><cell cols="2">Params Error</cell><cell>Cost</cell></row><row><cell></cell><cell>(M)</cell><cell>(%)</cell><cell>GPU Days</cell></row><row><cell>ResNet (2016)</cell><cell>1.7</cell><cell>22.10</cell><cell>-</cell></row><row><cell cols="2">AmoebaNet (2019) 3.1</cell><cell>18.93</cell><cell>3150</cell></row><row><cell>PNAS (2018)</cell><cell>3.2</cell><cell>19.53</cell><cell>150</cell></row><row><cell>ENAS (2018)</cell><cell>4.6</cell><cell>19.43</cell><cell>0.45</cell></row><row><cell cols="2">DARTS (2019b) -</cell><cell cols="2">20.58?0.44 0.4</cell></row><row><cell>GDAS (2019b)</cell><cell>3.4</cell><cell>18.38</cell><cell>0.2</cell></row><row><cell cols="2">P-DARTS (2019b) 3.6</cell><cell>17.49  ?</cell><cell>0.3</cell></row><row><cell cols="2">R-DARTS (2020) -</cell><cell cols="2">18.01?0.26 1.6</cell></row><row><cell>DARTS-(avg.)</cell><cell>3.3</cell><cell cols="2">17.51?0.25 0.4</cell></row><row><cell>DARTS-(best)</cell><cell>3.4</cell><cell>17.16</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Transfer results on COCO datasets of various drop-in backbones.</figDesc><table><row><cell>Backbones</cell><cell cols="2">Params (M) Acc AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>MobileNetV2 (2018)</cell><cell>3.4</cell><cell cols="2">72.0 28.3 46.7</cell><cell>29.3</cell><cell>14.8</cell><cell>30.7</cell><cell>38.1</cell></row><row><cell>SingPath NAS (2019)</cell><cell>4.3</cell><cell cols="2">75.0 30.7 49.8</cell><cell>32.2</cell><cell>15.4</cell><cell>33.9</cell><cell>41.6</cell></row><row><cell>MnasNet-A2 (2019)</cell><cell>4.8</cell><cell cols="2">75.6 30.5 50.2</cell><cell>32.0</cell><cell>16.6</cell><cell>34.1</cell><cell>41.1</cell></row><row><cell>MobileNetV3 (2019)</cell><cell>5.4</cell><cell cols="2">75.2 29.9 49.3</cell><cell>30.8</cell><cell>14.9</cell><cell>33.3</cell><cell>41.1</cell></row><row><cell>MixNet-M (2019)</cell><cell>5.0</cell><cell cols="2">77.0 31.3 51.7</cell><cell>32.4</cell><cell>17.0</cell><cell>35.0</cell><cell>41.9</cell></row><row><cell cols="2">FairDARTS-C (2020b) 5.3</cell><cell cols="2">77.2 31.9 51.9</cell><cell>33.0</cell><cell>17.4</cell><cell>35.3</cell><cell>43.0</cell></row><row><cell>DARTS-A (Ours)</cell><cell>5.5</cell><cell cols="2">77.8 32.5 52.8</cell><cell>34.1</cell><cell>18.0</cell><cell>36.1</cell><cell>43.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>List of experiments conducted in this paper</figDesc><table><row><cell>Method</cell><cell cols="2">Search Space Dataset</cell><cell cols="2">Figures</cell><cell cols="2">Tables</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Main text</cell><cell>Supp.</cell><cell>Main text</cell><cell>Supp.</cell></row><row><cell>DARTS</cell><cell>S0</cell><cell>CIFAR-10</cell><cell></cell><cell>7</cell><cell></cell></row><row><cell>DARTS</cell><cell>S3</cell><cell>CIFAR-10</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>DARTS-</cell><cell>S0</cell><cell>CIFAR-10</cell><cell></cell><cell cols="2">4,5,9,7 1,2</cell></row><row><cell>DARTS-</cell><cell>S1-S4</cell><cell>CIFAR-10</cell><cell>3</cell><cell cols="2">4,6,9,8 7,9</cell><cell>6</cell></row><row><cell>DARTS-</cell><cell>S5</cell><cell>ImageNet</cell><cell></cell><cell>5,10</cell><cell>2</cell></row><row><cell>DARTS-</cell><cell>S5</cell><cell>MS COCO</cell><cell></cell><cell></cell><cell>11</cell></row><row><cell>DARTS-</cell><cell>S6</cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>3</cell></row><row><cell>DARTS-</cell><cell>S1-S4</cell><cell>CIFAR-100</cell><cell></cell><cell>4,11</cell><cell>7</cell><cell>9</cell></row><row><cell>DARTS-</cell><cell>S0</cell><cell>CIFAR-100</cell><cell></cell><cell>4,11</cell><cell>1,10</cell></row><row><cell>P-DARTS w/o M=2</cell><cell>S0</cell><cell>CIFAR-10</cell><cell></cell><cell>12</cell><cell>4</cell></row><row><cell>P-DARTS w/ auxiliary skip</cell><cell>S0</cell><cell>CIFAR-10</cell><cell></cell><cell>13</cell><cell>4</cell></row><row><cell cols="2">PC-DARTS w/o channel shuffling S0</cell><cell>CIFAR-10</cell><cell></cell><cell>14</cell><cell>5</cell></row><row><cell>PC-DARTS w/ auxiliary skip</cell><cell>S0</cell><cell>CIFAR-10</cell><cell></cell><cell>15</cell><cell>5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We name it so as we undertake an inward way, as opposed to those outward ones who design new indicators, add extra cost and introduce new hyper-parameters.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esperan?a</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlucci</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAS Evaluation is Frustratingly Hard. In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stabilizing darts with amended gradient estimation on architectural parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11831</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbationbased regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06022</idno>
		<title level="m">Scarletnas: Bridging the gap between scalability and fairness in neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MoGA: Searching Beyond MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4042" to="4046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning Augmentation Policies from Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxyZkBKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6389" to="6399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">DARTS+: Improved Differentiable Architecture Search with Early Stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search. In ECCV</title>
		<imprint>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">AtomNAS: Fine-Grained End-to-End Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jianchao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mo-bileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixed Depthwise Convolutional Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting the accuracy of multispectral image pansharpening by learning a deep residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangqiang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanfeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1795" to="1799" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SNAS: Stochastic Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlS634tPr" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1loF2NFwr" />
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gDNyrKDS" />
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16537</idno>
		<title level="m">Theory-inspired path-regularized differential network architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
