<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-Encoding Score Distribution Regression for Action Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
						</author>
						<title level="a" type="main">Auto-Encoding Score Distribution Regression for Action Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Quality Asessment</term>
					<term>Uncertainty Learn- ing</term>
					<term>Video Captioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The action quality assessment (AQA) of videos is a challenging vision task since the relation between videos and action scores is difficult to model. Thus, AQA has been widely studied in the literature. Traditionally, AQA is treated as a regression problem to learn the underlying mappings between videos and action scores. But previous methods ignored data uncertainty in AQA dataset. To address aleatoric uncertainty, we further develop a plug-and-play module Distribution Auto-Encoder (DAE). Specifically, it encodes videos into distributions and uses the reparameterization trick in variational autoencoders (VAE) to sample scores, which establishes a more accurate mapping between videos and scores. Meanwhile, a likelihood loss is used to learn the uncertainty parameters. We plug our DAE approach into MUSDL and CoRe. Experimental results on public datasets demonstrate that our method achieves state-of-the-art on AQA-7, MTL-AQA, and JIGSAWS datasets. Our code is available at https://github.com/InfoX-SEU/DAE-AQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action quality assessment (AQA) refers to the automatic scoring of behaviors in the video, such as scoring diving/gymnastics movements, and comparing which doctor has a higher surgical level. It is gaining increasing attention for its wide applications like the judgment of accuracy of an operation <ref type="bibr" target="#b0">[1]</ref> or score estimation of an athlete's performance (at the Olympic Games) <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. AQA analyzes how well an action is carried out, so it is more challenging than the traditional video action recognition (VAR) problem since identifying the difference in the same action category is imperceptible.</p><p>Finding a solid link between the action score and videos is essential for AQA. In the past years, many AQA approaches <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref> tried to treat AQA as a regression problem and learned the direct mapping between videos and action scores. These works use the 3D convolutional neural network or LSTM to extract video features, and then regression methods are applied to get the prediction score.</p><p>In fact, most existing AQA methods ignore the inherent aleatoric uncertainty in datasets. AQA datasets are constructed <ref type="bibr">Boyu</ref>   subjectively by judges, which means there is observational noise that corrupts the target values. Thus, there does not exist an accurate mapping y ? F(x) between label y and data x. Revised mapping form can be interpreted as</p><formula xml:id="formula_0">y = F(x) + noise(x)<label>(1)</label></formula><p>Given the above consideration, it is necessary to model the observational noise(x) from a statistical point of view. To this end, we introduce uncertainty learning into AQA and propose a new regression model, named Distribution Auto-Encoder (DAE). By using DAE, the video features are synthesized into a score distribution. The final predicted score is then sampled from this distribution by reparameterization trick <ref type="bibr" target="#b7">[8]</ref>. Compared with the traditional regression method, our DAE method can automatically generate the inherent target distribution of videos. In this way, our approach can achieve a better prediction performance. Taking the Gaussian distribution as an example, which is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the action score predicted by DAE is continuously varying, and the variance of score distribution is adaptively learned from the dataset.</p><p>To illustrate the efficacy of our proposed method, We first construct our DAE model based on multi-layer perceptron (MLP). DAE-MLP requires score and video one-to-one information. The structure of DAE-MLP is composed of a features extractor and an encoder. Firstly, action videos are fed into an Inflated 3D ConvNets I3D <ref type="bibr" target="#b8">[9]</ref> to extract feature vectors. Then, the feature vector is coded as a Gaussian distribution through the encoder, and the final predicted scores are sampled from this distribution.</p><p>Then we plug DAE into MUSDL <ref type="bibr" target="#b9">[10]</ref> and CoRe <ref type="bibr" target="#b6">[7]</ref> to show our approach is pluggable and effective. DAE-MT applies to multi-task datasets. It is proposed to utilize multiple tasks better. The feature extractor of DAE-MT is the same as DAE-MLP. In encoder part, DAE-MT predicts seven scores from seven judges rather than the final scores. The final score is the multiplication of difficulty degree(DD) and the raw score. Raw score is the average of seven judges' scores. DAE-CoRe adds DAE module to the last layer of the regression tree in CoRe. This method uses the regression tree to make predictions at smaller intervals.</p><p>The main contributions of this work can be summarised as follows: </p><formula xml:id="formula_1">? We</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work is closely related to three topics: action quality assessment, uncertainty learning, and auto-encoder. In this section, we briefly review existing methods related to these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Quality Assessment</head><p>Action Quality Assessment(AQA) automatically scores the quality of actions by analyzing features extracted from videos and images. It's different from conventional action recognition problems <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In the past few years, much work has been devoted to different AQA tasks, such as healthcare <ref type="bibr" target="#b10">[11]</ref>, sports video analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and many others <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. The earliest model based on deep learning was proposed by Parmar et al. <ref type="bibr" target="#b2">[3]</ref>, who use C3D-SVR and C3D-LSTM to predict Olympic scores. Based on the assumption that the final score is a set of continuous sub-action scores, the incremental label training method is introduced to train the LSTM model. Xiang et al. <ref type="bibr" target="#b15">[16]</ref> choose to decompose video clips into action-specific clips and fuse the average features of clips to replace over full videos. More recently, Parmar et al. <ref type="bibr" target="#b3">[4]</ref> propose a C3D-AVG-MTL approach to learn Spatiotemporal features that explain three related tasks-fine-grained action recognition, commentary generation, and estimating the AQA score. Meanwhile, they collect a new multi-task AQA (MTL-AQA) dataset on a larger scale. Tang et al. notice the underlying ambiguity of action scores. To address this problem, they propose an improved approach: uncertaintyaware score distribution learning (USDL) <ref type="bibr" target="#b9">[10]</ref> based on label distribution learning (LDL) <ref type="bibr" target="#b16">[17]</ref>. Multi uncertainty-aware score distribution learning (MUSDL) <ref type="bibr" target="#b9">[10]</ref> is designed to fit the multi-task dataset. It uses judges' information in the dataset and treated every judge as a scoring model. Contrastive Regression (CoRe) <ref type="bibr" target="#b6">[7]</ref> uses a pairwise strategy to regress the relative scores with reference to another video. Although the above models more or less take into account the interference of data uncertainty, they do not measure data uncertainty and thereby reduce the impact of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Uncertainty Learning</head><p>Uncertainty study focuses on how to measure the implicit noise in a model or dataset. Two uncertainties are of major concern, epistemic uncertainty, and heteroscedastic aleatoric uncertainty. Epistemic uncertainty comes from the noise in model parameters or model outputs. Aleatoric uncertainty exists in dataset itself. Many people try to introduce uncertainty in modeling to get better results. <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Others also consider forming a generic learning paradigm to study uncertainty. Geng et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref> try to represent an instance by a specific distribution rather than one label or multiple labels. Pate et al. <ref type="bibr" target="#b20">[21]</ref> use a risk level framework to measure uncertainty. Recently, uncertainty study on neural networks has been extensively studied <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. And there are many tasks applied uncertainty analysis, e.g.. semantic segmentation <ref type="bibr" target="#b24">[25]</ref>, face recognition <ref type="bibr" target="#b25">[26]</ref> and object detection <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> for the improvement of model robustness and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auto-Encoder</head><p>The Auto-Encoder (AE) is first proposed by Hinton et al. <ref type="bibr" target="#b29">[30]</ref>, which uses a multi-layer neural network to obtain low-dimensional expression of high-dimensional data. It uses the classical bottleneck network architecture and reconstructs the low-dimensional information back to the high-dimensional representation in the decoder. With the development of deep learning, many variant models based on AE have been created, such as Denoising Auto-Encoder for image denoising <ref type="bibr" target="#b30">[31]</ref>, and Convolutional Auto-Encoder for image compression and feature extraction <ref type="bibr" target="#b31">[32]</ref>. According to the variational Bayes inference, the Variational Auto-Encoder (VAE) <ref type="bibr" target="#b7">[8]</ref> is proposed by Diederik P.Kingm and Max Welling based on the conventional auto-encoders. VAE used a unique reparameterization trick for sampling the hidden variable distribution <ref type="bibr" target="#b7">[8]</ref>. The method we proposed also refers to the Gaussian distribution encoding and sampling techniques in VAE. Specifically, our model maps video features to low-dimensional distributions and does not leverage the neural network for decoding, but directly outputs the final label through reparameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DISTRIBUTION AUTO-ENCODER BASED ON</head><p>MULTI-LAYER PERCEPTRON (DAE-MLP) The distribution auto-encoder (DAE) is a plug-and-play regression module. In this section, We first constructed it based on multi-layer perceptron (MLP). DAE-MLP maps video clips to action score distribution via a deep neural network. The architecture of DAE-MLP is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In particular, it consists of two parts, a video feature extractor to obtain video features (Section III-A) and an auto-encoder for distribution learning (Section III-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Feature Extraction</head><p>Given an action video with n frames, features need to be extracted first. As shown in the left half part of <ref type="figure" target="#fig_1">Figure 2</ref>, the complete action video is down-sampled and divided into n video clips, {c 1 , ..., c n }. Each video clip contains the same number of frames, representing a consecutive action snapshot. Parmar et al. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref> pre-processed the video sequence in this way as well. The spatial resolution is improved by downsampling since the number of network parameters can be reduced significantly. This technique was first proposed by Nibali et al. <ref type="bibr" target="#b32">[33]</ref> and widely used in the later works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the feature extraction of collected video clips, we use Inflated 3D ConvNets (I3D) <ref type="bibr" target="#b8">[9]</ref>. I3D is a network model expanded from 2D to 3D convolution kernel, which has better performance in video analysis. I3D is followed by three fully connected layers, resulting in an m-dimensional feature. Different clips share the exact weights of fully connected layers. After getting all the feature vectors of n video clips, we take the average as the final feature vector of the action video to guarantee that each video clip's information is considered equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DAE-MLP</head><p>Compared with traditional regression methods, our approach captures aleatoric uncertainty. The action features are encoded into score distribution, and the final result is sampled from the auto-encoder output. This architecture enables learning a continuous distribution without loss in training procedure and quantifies the uncertainty of action score with high accuracy.</p><p>The encoder uses a neural network to encode mean and variance simultaneously. The input 1024-dimensional feature vector x is encoded into the parameters ?(x) and ? 2 (x) via a neural network.</p><p>Treating the action score as a random variable, we need to learn its score distribution and then sample the predicted score from the obtained distribution. For the input features, the first half of the structure of VAE is applied to encode the 1024dimensional video feature x into a random variable y through a probabilistic encoder p(y; ?(x)). The encoded random variable is assumed to be subject to Gaussian. distribution 1 p(y; ?(x)) = 1</p><formula xml:id="formula_2">2?? 2 (x) exp ? (y ? ?(x)) 2 2? 2 (x) .<label>(2)</label></formula><p>The parameters ?(x) and variance ? 2 (x) are used to quantify the quality and uncertainty of the action score. Reparameterization Trick: To generate a sample from Gaussian distributed y as the predicted score and make full use of the two parameters in the score distribution at the same time, we invoke the reparameterization trick. According to reparameterization trick in VAE <ref type="bibr" target="#b7">[8]</ref>, assume that z is a random variable, and z ? q(z; ?), ? is its parameter. We can express z as a deterministic variable, z = g( ; ?), is an auxiliary variable with independent marginal p( ), and g(?; ?) is a deterministic function parameterized by ?.</p><p>As illustrated in the right half part of <ref type="figure" target="#fig_1">Figure 2</ref>, we do not directly sample from the score distribution, but firstly sample from , which is distributed in N (0, 1). Then, y is calculated according to the sampling random variable , mean parameter ?(x) and variance parameter ? 2 (x) of the auto-encoder output</p><formula xml:id="formula_3">y = ?(x) + * ?(x).<label>(3)</label></formula><p>By applying the reparameterization trick, the score distribution sampling process is differentiable to ensure that the encoder training is feasible <ref type="bibr" target="#b7">[8]</ref>. Loss Function: Considering the optimization of parameters in neural networks from the perspective of likelihood, we obtain a form for loss function by expressing our goal as maximizing the log-likelihood of the target distribution <ref type="formula" target="#formula_2">(2)</ref>,</p><formula xml:id="formula_4">ln l(y; x) = ? 1 2 ln(2?) ? 1 2 ln ? 2 (x) ? (y ? ?(x)) 2 2? 2 (x)<label>(4)</label></formula><p>The first term in RHS is a constant and can be ignored in maximization. Since maximizing a value is the same as minimizing the negative of that value, we interpret our overall loss function as</p><formula xml:id="formula_5">L = 1 N N i=1 ? ? (x i ) 2 y i ? ? (x i ) 2 + ? log ? (x i ) 2 1 N N i=1 ?L rec + ?L sup<label>(5)</label></formula><p>?, ? are the weights of two different parts of the reconstruction loss L rec and the support loss L sup . The larger ? represents the more attention paid to uncertain information ? 2 (x). On the contrary, the larger ? represents DAE tends to become a traditional neural network regression model and pays more attention to regression precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Uncertainty Regression via DAE</head><p>We can compare DAE with traditional regression method to show the improvement by introducing uncertainty. Let X = (X 1 , X 2 , ..., X N ), X i = (x 1 , x 2 , ..., x k ) T represent the input data, and Y = (y 1 , y 2 , ..., y N ) T represents the label. The factors affecting y are often multi-dimensional. Assume that there are k factors ? = (? 0 , ? 1 , ? 2 , ? ? ? , ? k ) T . The linear equation can be written as</p><formula xml:id="formula_6">y = ? 0 + ? 1 x 1 + ? 2 x 2 + ? ? ? + ? k x k + ?<label>(6)</label></formula><p>After N independent observations on y and x, we can get N groups of observations (x i1 , x i2 , ..., x ik ), i = 1, 2, ..., N. 2 It satisfy the following equations y t = ? 0 +? 1 x i1 +? 2 x i2 +? ? ?+? k x ik +? i , i = 1, . . . , n, <ref type="bibr" target="#b6">(7)</ref> where ? 1 . . . ? N are independent and with the same distribution as ?. For regression problem Y = X T ? + ?, we can solve it with traditional regression methods such as the least square method. Assuming that the error belongs to normal distribution</p><formula xml:id="formula_7">2 N ? k + 1.</formula><p>? ? N 0,? 2 , the variance of estimation error <ref type="bibr" target="#b34">[34]</ref> can be calculated by?</p><formula xml:id="formula_8">2 = 1 ? r 2 N ? 2 N i=1 (y i ?? i ) 2 ,<label>(8)</label></formula><p>where y 1 ,? 1 , . . . , y N ,? N are the true and predicted labels, r is correlation coefficient. Such estimation error is based on the assumption that the regression data X is independent of the observational noise ?. However, due to the highly non-linearity of the prediction task at hand, the observational noise may be statistically coupled with the regression data. Traditional regression methods cannot estimate the noise variance while doing regression. Nevertheless, within neural uncertainty regressor (e.g., DAE), we can fit with data and estimate error specifically at the same time. The uncertainty should be related to X. We can write it as</p><formula xml:id="formula_9">? ? N 0, ? 2 (X)<label>(9)</label></formula><p>DAE allows us to learn underlying ? 2 (X) while regressing. And the training process of (3) can be regarded as an estimation of error. The prediction label of DAE is</p><formula xml:id="formula_10">Y ? N X T ?, ? 2 (X)<label>(10)</label></formula><p>Uncertainty regression introduced by DAE provides data augment and can be seen as a better fitting of underlying distribution in datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PLUG-AND-PLAY APPLICATIONS OF DAE</head><p>DAE is a generalization method and it can easily plug in any regression model. Plugging DAE in regression allows the baseline to capture aleatoric uncertainty. Under this consideration, we extend the plug-in application of DAE, we designed DAE-MT and DAE-CoRe on previous work MUSDL <ref type="bibr" target="#b9">[10]</ref> and CoRe <ref type="bibr" target="#b6">[7]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DAE-MT</head><p>MUSDL [10] applies to multi-task datasets MTL-AQA. MUSDL shows that the scores from multiple judges and difficulty degree (DD) in MTL-AQA dataset are two essential pieces of information. Using them to calculate the final score of the player can be more reasonable. We adopt the same international diving scoring rules are adopted as MUSDL: Seven judges score, then remove the two lowest points and two highest points respectively. The remaining three scores are summed to get the raw score. The final score is obtained by multiplying the raw score and difficulty degree as follows,</p><formula xml:id="formula_11">S f inal = (s max1 + s max2 + s max3 ) * DD.<label>(11)</label></formula><p>We plug DAE into MUSDL and proposed DAE-MT. <ref type="figure" target="#fig_2">Figure  3</ref> shows the mapping of DAE-MT. We applied DAE-MT to predict the judges scores. It predicts scores without difficult degree (DD). Uncertainty only occurs when the judges score. So constructing the relationship between video and raw score directly is a more practical choice for learning uncertainty. A small spray (a) will lead to a higher evaluation from judges, while a large spray (d) will get unsatisfactory scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DAE-CoRe</head><p>CoRe <ref type="bibr" target="#b6">[7]</ref> used a contrastive strategy to regress the relative scores between an input video and several exemplar videos as references. It divides the range of the scores into several non-overlapping intervals and regresses in these small intervals with a binary tree. The final regression result can be written as y = R (I right ? I left ) + I left <ref type="bibr" target="#b11">(12)</ref> where R represents CoRe regressor, I left and I left represents the left and the right interval boundary respectively. DAE is easy to plug in the regressor of CoRe. We modify the last regression layer of the binary tree to capture aleatoric uncertainty. Regressor R can be modified from a neural network layer to DAE framework. This can be interpreted as  JIGSAWS <ref type="bibr" target="#b36">[36]</ref>: JIGSAWS is a surgical activity dataset for human motion modeling, consisting of 3 main tasks as "Suturing (S)," "Needle Passing (NP)," and "Knot Typing (KT)." Each task is divided into four folders. The video data of JIGSAWS are captured from a left camera and a right camera. Since these two videos are similar at a high level, we utilize the left to do our experiment. The label of JIGSAWS is surgical skill annotation, which is a global rating score (GRS) using modified objective structured assessments of technical skills approach. Evaluation Metrics: AQA uses Spearman's rank correlation <ref type="bibr" target="#b37">[37]</ref> to measure the performance of our methods between the ground-truth and predicted score series. Spearman's correlation is defined as</p><formula xml:id="formula_12">y = ? ? (I right ? I left ) + ? ? (I right ? I left ) + I left<label>(13)</label></formula><formula xml:id="formula_13">? = i (p i ?p) (q i ?q) i (p i ?p) 2 i (q i ?q) 2<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We implemented our DAE approach using the PyTorch framework. Two NVIDIA RTX 3090 GPUs are used to accelerate training. Moreover, We use 16 threads of Intel(R) Core(TM) i9-9900KF CPU @ 3.60GHz to accelerate data loading. Auto-encoder has three network layers. We choose ReLU as activation function. The input layer's size is <ref type="figure" target="#fig_0">(1024,512)</ref>, and the size of the hidden layers are (512,256) and (256,128). The final output layers of mean and variance are both (128,1). When evaluating DAE-MLP, the learning rate is 1e ?4 on AQA-7, MTL-AQA and 5e ?4 , 2e ?4 and 1e ?4 on JIGSAWS-KT, -NP and -S, respectively. The hyper-parameters of DAE-MT and DAE-CoRe are set to be the same as the original baseline paper. According to our preliminary experiment, the weights of loss are selected as ? = 0.6 and ? = 0.4. The optimizer is Adam <ref type="bibr" target="#b40">[40]</ref> on all datasets. On AQA-7 and MTL-AQA datasets, we selected the highest score as our method final performance during training. On JIGSAWS data, the final performance is averaged for the best ten consecutive scores to compare with previous methods, which is different from others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AQA-7:</head><p>We choose SOTA methods in the last five years to compare with our DAE, The final results are shown in <ref type="table" target="#tab_3">Table  I</ref>. Our DAE-MLP method achieves 3.96%, 2.43%, 4.56%, 1.70%, 0.77% and 0.83% performance improvement for each sports class compared with USDL <ref type="bibr" target="#b9">[10]</ref> under Spearman's correlation. The Average correlation rank of DAE-MLP improves 1.93%. And the results of pluggable DAE also have achieved better performance in Diving (0.8923), Gym Vault (0.7786), Snowboard (0.6842), Sync.3m (0.9606), Sync.10m (0.9129) than CoRe <ref type="bibr" target="#b6">[7]</ref>. MTL-AQA: We further applied DAE to MTL-AQA dataset to verify our approach's efficiency and show the comparison of differences between DAE and previous methods in more detail. Since multi judges information exists in MTL-AQA, we conduct an experiment using both DAE-MT and DAE-CoRe on MTL-AQA. The first block in <ref type="table" target="#tab_3">Table II</ref> shows single-task training mode results. The prediction correlation coefficient of DAE-MLP reaches 0.9231, which is significantly improved in comparing the previous method. The second block shows that our method still has a good performance in multi-task mode, and the correlation coefficient of the final result is 0.9452, which exceeds the baseline model MUSDL <ref type="bibr" target="#b9">[10]</ref>. And DAE-CoRe (0.9589) also achieves a better result than CoRe (0.9512). JIGSAWS: The experiments results on JIGSAWS are shown in <ref type="table" target="#tab_3">Table III</ref>. In all three surgical videos, our DAE-MT achieves </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sp. Corr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C3D-SVR [3]</head><p>0.7716 C3D-LSTM <ref type="bibr" target="#b2">[3]</ref> 0.8489 MSCADC-STL <ref type="bibr" target="#b3">[4]</ref> 0.8472 MSCADC-MTL <ref type="bibr" target="#b3">[4]</ref> 0.8612 C3D-AVG-STL <ref type="bibr" target="#b3">[4]</ref> 0.8960 C3D-AVG-MTL <ref type="bibr" target="#b3">[4]</ref> 0.9044 USDL <ref type="bibr" target="#b9">[10]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head><p>Study on Different Distributions: When our method encodes features into distributions, the form of distribution is not limited to normal distribution. For the reparameterization trick for any "location-scale" (Eq. (3)) family of distributions, we can choose the standard distribution (with location = 0, scale   <ref type="table" target="#tab_3">Table IV</ref>. We find that Gaussian distribution performs best on this dataset. Although this result is largely in line with our expectations, it does not mean that Gaussian distribution is the most appropriate in other datasets or other video evaluation tasks. For specific datasets and applications, we need to select the best distribution through further experiments. Case Study: We applied a case study to compare the prediction distribution of DAE for different videos on both MTL-AQA and JIGSAWS datasets, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Each line shows four videos and their distributions predicted by our DAE. Different action quality generates different mean and variance distribution, which corresponds to the actual performance. Taking diving as an example, a small spray <ref type="figure" target="#fig_3">Figure 4</ref>(a) will lead to a higher evaluation of judges, while a large spray <ref type="figure" target="#fig_3">Figure 4(d)</ref> will get bad scores. This proves the effectiveness of our method. DAE can make an effective prediction according to different video content. The predicted distribution parameters are adaptive according to the video itself. <ref type="bibr" target="#b2">3</ref> Regression Analysis: We compare our method and regression baseline detailedly with plotting to scatter diagrams. In <ref type="figure" target="#fig_5">Figure  6</ref> Regression target is the ideal regression result. The closer scatters are to this line, the better regression is. From <ref type="figure" target="#fig_5">Figure 6</ref>, we can see that DAE-MLP and DAE-MT both perform well. Scatters of our method are all closer to the target line than regression baseline. And the prediction of our method is more concentrated.</p><p>We further compare the training of DAE with previous models USDL <ref type="bibr" target="#b9">[10]</ref> , MUSDL <ref type="bibr" target="#b9">[10]</ref> ,C3D-AVG-STL and C3D-AVG-MTL <ref type="bibr" target="#b3">[4]</ref>. The comparison results on MTL-AQA dataset are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. It can be seen that the final stable correlation coefficient of DAE is higher than that of other methods, and DAE converges faster during training and there are minor fluctuations.</p><p>Besides, a parallel experiment is carried out to find out the variation range of variance. We trained DAE-MLP twenty times to observe the variance of the best-performing model in each training round. Box chart <ref type="figure" target="#fig_7">Figure 8</ref> shows the results of the parallel comparison. The variation range of variance is ?0.1 in MTL-AQA training rounds. And range in JIGSAWS training rounds is ?0.2. From quartiles, it can be seen that variances in five observations are stable relatively. The upper and lower quartile lines are roughly the same distance from the median. There is only one outlier in rounds JIGSAWS-NP, it goes beyond the maximum observation and exceeds the upper edge. This may be caused by sampling. Loss Study: A two parts likelihood loss function is adopted in this paper. We evaluated the effectiveness of our loss function in this section. We used common MSE loss  <ref type="table" target="#tab_7">Table V</ref> shows ablation results on MTL-AQA and JIG-SAWS. We first used the single-task training mode for the loss experiment on MTL-AQA. When common MSE loss is used, the mean of predicted distribution will be more likely to close the final predicted score. At this point, the learning of variance is reduced. Our regression result is similar to the baseline of USDL. Their baseline scores reached 0.8905, and our scores reached 0.9091. When we used our likelihood loss, we find that the training performance has greatly improved. At the same time, the stability of variance has also been improved after many observations. The performance of DAE-MT in multi-task training mode has similar trends to DAE. The performance reached 0.9415 when MSE loss is used. When training with our loss, DAE-MT achieved the best performance with reaching a score of 0.9452. Compared with baseline, the performance of our best method increases by 6.1%. The second block in <ref type="table" target="#tab_7">Table V</ref> shows ablation results on JIGSAWS. It can be seen that The effect of combined loss is better than that of a single loss and baseline also. Inference Time: At inference time, all methods are applied to ten video samples at a time, <ref type="table" target="#tab_3">Table VI</ref> shows the average inference time on MTL-AQA. We tested all the models using a GPU, NVIDIA RTX 3090. It shows that only a slight time increment is needed to plug DAE into other regression methods. DAE-MT and DAE-CoRe involve 7% and 9.7% inferencing time increments, respectively.</p><formula xml:id="formula_14">L M SE = 1 N N i=1 y i ? (?(x i ) + ? 2 (x i )) 2 for comparison.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a new method for action quality assessment. Referring to the architecture of variational autoencoders, we propose a new regression model, Distribution auto-encoders (DAE). Furthermore, DAE is pluggable and can be extended on any regression method. We have tested our approach on AQA-7,MTL-AQA, and JIGSAWS to show that our method outperforms better than the state-of-the-art. The increment time cost of inferencing is also acceptable. Although this paper is explicitly geared toward action quality 9 <ref type="figure">Fig. 9</ref>. Case study on AQA-7 (Gym Vault) dataset. Each row indicates four frames of a video corresponding to its prediction distribution. <ref type="figure" target="#fig_0">Fig. 10</ref>. Case study on MTL-AQA dataset. Each row indicates four frames of a video corresponding to its prediction distribution. assessment, DAE provides a general solution paradigm for uncertainty learning. A specific distribution is used to represent an instance (or features), the parameters of the distribution are obtained by the encoder to quantify the value of the label, and the uncertainty is quantified by sampling from the distribution. This method captures the uncertainty on the basis of the traditional regression method and learns the inherent characteristics of uncertainty through the multi-layer neural networks, which is more explainable in theory. In the future, we plan to apply this method to other video analysis problems, such as age estimation <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> and facial beauty prediction <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A CASE STUDY</head><p>We conduct more case studies here. <ref type="figure">Figure 9</ref>-12 illustrate that DAE captures the uncertainty in video quality assessment datasets well. Uncertainty in each case can be learned adaptively by the case itself. Specifically, for an excellent movement, the variance of the evaluation is minor, and the judges' scores will be close to stable, while for low-quality  movements, the range of changes in the score will increase, and the reliability will decrease accordingly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Different actions have dissimilar aleatoric uncertainty when scored by the judges. Aleatoric uncertainty in DAE can be modeled as specific target distribution. e.g. Gaussian distribution. Our DAE achieves better performance since it can learn the adaptive variance from the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The pipeline of DAE architecture contains two segments: video features extraction network and label distribution encoding network. The input video is divided into n small clips by down-sampling. Then the clips are sent into I3D ConvNets for extracting features. The final features are synthesized by three fully-connected layers. By Using an encoder, the video features are encoded into a Gaussian distribution, and the reparameterization trick is applied to generate samples from the distribution of the final predicted score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The mapping rule of DAE-MT. DAE-MT uses the difficulty degree (DD) as prediction information, so it directly predicts the judges score distribution. DAE-MT outputs distributions for each judge. The final score is obtained by raw score multiplied by difficulty degree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of different distributions of different videos on MTL-AQA and JIGSAWS. Different operations indicate different prediction distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of training procedure on MTL-AQA. The correlation coefficients are shown in exponential form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparisons of our methods and regression baseline. (a) and (b) show the results of DAE and DAE-MT on MTL-AQA. (c), (d) and (e) show the results on JIGSAWS-KT, JIGSAWS-NP and JIGSAWS-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Parallel experiments on different distributions. We tested five distributions of auxiliary variable p( ) on MTL-AQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Variation range of variance in parallel training on MTL-AQA and JIGSAWS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a)-(b), we show the results of methods DAE-MLP and DAE-MT on MTL-AQA. In Figure 6(c)-(e), we show the results on JIGSAWS-KT, JIGSAWS-NP and JIGSAWS-S respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Case study on JIGSAWS (KT) dataset. Each row indicates four frames of a video corresponding to its prediction distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Case study on JIGSAWS (NP) dataset. Each row indicates four frames of a video corresponding to its prediction distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zhang and Jiayuan Chen contribute equally. Corresponding authors: Yinfei Xu; Hui Zhang. Boyu Zhang and Jiayuan Chen are with the School of Computer Science and Engineering, Southeast University, Nanjing 211189, China (email: byz@seu.edu.cn; jiaychen@seu.edu.cn). Yinfei Xu is with the School of Information Science and Engineering, Southeast University, Nanjing 210096, China (email: yinfeixu@seu.edu.cn). Hui Zhang is with the Inspur Acadaemy of Science and Technology, Jinan 250000, China (email: zhanghui@inspur.com). Xu Yang and Xin Geng are with the School of Computer Science and Engineering, and the Key Lab of Computer Network and Information Integration (Ministry of Education), Southeast University, Nanjing 211189, China (email: s170018@e.ntu.edu.sg; xgeng@seu.edu.cn).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF CORRELATION COEFFICIENTS ON AQA-7. THE BEST RESULTS ARE SHOWN IN BOLD IN EACH BLOCK, RESPECTIVELY.</figDesc><table><row><cell>Method</cell><cell>Diving</cell><cell>Gym Vault</cell><cell>Skiing</cell><cell>Snowboard</cell><cell>Sync.3m</cell><cell>Sync.10m</cell><cell>Ave</cell></row><row><cell>Pose+DCT [38]</cell><cell>0.5300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-GCN [37]</cell><cell>0.3286</cell><cell>0.5770</cell><cell>0.1681</cell><cell>0.1234</cell><cell>0.6600</cell><cell>0.6483</cell><cell>0.4433</cell></row><row><cell>C3D-LSTM [3]</cell><cell>0.6047</cell><cell>0.5636</cell><cell>0.4593</cell><cell>0.5029</cell><cell>0.7912</cell><cell>0.6927</cell><cell>0.6165</cell></row><row><cell>C3D-SVR [3]</cell><cell>0.7902</cell><cell>0.6824</cell><cell>0.5209</cell><cell>0.4006</cell><cell>0.5937</cell><cell>0.9120</cell><cell>0.6937</cell></row><row><cell>JRG [39]</cell><cell>0.7630</cell><cell>0.7358</cell><cell>0.6006</cell><cell>0.5405</cell><cell>0.9013</cell><cell>0.9254</cell><cell>0.7849</cell></row><row><cell>USDL [10]</cell><cell>0.8099</cell><cell>0.7570</cell><cell>0.6538</cell><cell>0.7109</cell><cell>0.9166</cell><cell>0.8878</cell><cell>0.8102</cell></row><row><cell>DAE-MLP</cell><cell>0.8420</cell><cell>0.7754</cell><cell>0.6836</cell><cell>0.7230</cell><cell>0.9237</cell><cell>0.8902</cell><cell>0.8258</cell></row><row><cell>CoRe [7]</cell><cell>0.8824</cell><cell>0.7746</cell><cell>0.7115</cell><cell>0.6624</cell><cell>0.9442</cell><cell>0.9078</cell><cell>0.8401</cell></row><row><cell>DAE-CoRe</cell><cell>0.8923</cell><cell>0.7786</cell><cell>0.7102</cell><cell>0.6842</cell><cell>0.9506</cell><cell>0.9129</cell><cell>0.8520</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF CORRELATION COEFFICIENTS ON MTL-AQA. THE BEST RESULTS ARE SHOWN IN BOLD IN EACH BLOCK, RESPECTIVELY.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF DIFFERENT SAMPLING DISTRIBUTIONS.</figDesc><table><row><cell>Method</cell><cell>Sp. Corr.</cell></row><row><cell>Student't Distributions</cell><cell>0.9399</cell></row><row><cell>Laplace Distributions</cell><cell>0.9290</cell></row><row><cell>Triangular Distributions</cell><cell>0.9207</cell></row><row><cell>LogisticNormal Distributions</cell><cell>0.9262</cell></row><row><cell>Gaussian Distributions</cell><cell>0.9452</cell></row><row><cell cols="2">= 1) as the auxiliary variable ? p( ), and let</cell></row><row><cell cols="2">g(?; ?) = location + scale  *</cell></row><row><cell cols="2">We do parallel experiments on Laplace, Elliptical, Student's</cell></row><row><cell cols="2">t, Logistic, Uniform, Triangular and Gaussian distributions</cell></row><row><cell cols="2">on MTL-AQA dataset. A case is shown in Fig7 and the</cell></row><row><cell>evaluation is summarized in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF TRAINING RESULTS WITH DIFFERENT LOSSES ON MTL-AQA AND JIGSAWS. WE USED THE REGRESSION METHOD AS OUR BASELINE. WE USED DAE-MLP AND DAE-MT TRAINING,RESPECTIVELY.</figDesc><table><row><cell cols="2">Loss</cell><cell cols="3">Sp. Corr. Sp. Corr.(MT)</cell><cell></cell></row><row><cell cols="2">Regression baseline</cell><cell>0.8905</cell><cell></cell><cell>0.8905</cell><cell></cell></row><row><cell cols="2">MSE loss</cell><cell>0.9180</cell><cell></cell><cell>0.9415</cell><cell></cell></row><row><cell cols="2">Sup+Rec loss</cell><cell>0.9231</cell><cell></cell><cell>0.9452</cell><cell></cell></row><row><cell cols="2">Loss</cell><cell cols="3">Sp. Corr. Sp. Corr.(MT)</cell><cell></cell></row><row><cell cols="2">Regression baseline</cell><cell>0.68</cell><cell></cell><cell>0.68</cell><cell></cell></row><row><cell cols="2">MSE loss</cell><cell>0.71</cell><cell></cell><cell>0.72</cell><cell></cell></row><row><cell cols="2">Sup+Rec loss</cell><cell>0.72</cell><cell></cell><cell>0.76</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">COMPARISON RESULTS OF INFERENCE TIME.</cell><cell></cell></row><row><cell>Method</cell><cell cols="5">DAE-MLP MUSDL DAE-MT CoRe DAE-CoRe</cell></row><row><cell>Time(ms)</cell><cell>0.84</cell><cell>1.42</cell><cell>1.52</cell><cell>2.16</cell><cell>2.37</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the Gaussian distribution is just one choice, and not a limitation in our method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Whether the variance is positive does not affect the reparameterization trick. Here variance shows in absolute value.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pros and cons: Rank-aware temporal attention for skill determination in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7854" to="7863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Am i a baller? basketball performance assessment from first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What and how well you performed? a multitask learning approach to action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trajectory based assessment of coordinated human activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dezman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kova?i?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Syst. (ICVS)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="534" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On temporal order invariance for view-invariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murshed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Group-aware contrastive regression for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7919" to="7928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uncertainty-aware score distribution learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="9836" to="9845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relative hidden markov models for video-based evaluation of motion skills in surgical training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1224" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated video assessment of human performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>As</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AI-ED</title>
		<meeting>AI-ED</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Who&apos;s better? who&apos;s best? pairwise deep ranking for skill determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6057" to="6066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long activity video understanding using functional object-oriented network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Jelodar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1813" to="1824" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning composite latent structures for 3d human action representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2195" to="2208" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">S3d: Stacking segmental p3d for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE Int. Conf. Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aleatory or epistemic? does it matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ditlevsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural safety</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the treatment of uncertainties and probabilities in engineering decision analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Faber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilabel ranking with inconsistent rankers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3742" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainties in risk analysis: Six levels of treatment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Pat?-Cornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="95" to="111" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5710" to="5719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust person re-identification by modelling feature uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Antoine Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extraction and classification of diving clips from continuous video footage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Cvf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Pattern Recognit. Workshops</title>
		<imprint>
			<biblScope unit="page" from="94" to="104" />
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Applied multiple regression/correlation analysis for the behavioral sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action quality assessment across multiple actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. App. Comput. Vis. (WACV)</title>
		<meeting>IEEE Winter Conf. App. Comput. Vis. (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lingling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop: M2CAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action assessment by joint relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6330" to="6339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action assessment by joint relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6330" to="6339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">C3ae: Exploring the limits of compact model for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Finegrained age estimation in the wild with attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3140" to="3152" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for facial beauty prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2m beautynet: Facial beauty prediction based on multi-task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaoyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-C</forename><surname>Piuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="issue">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
