<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reference-based Video Super-Resolution Using Multi-Camera Video Triplets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
							<email>junyonglee@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghee</forename><surname>Lee</surname></persName>
							<email>myeonghee@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
							<email>s.cho@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Postech</forename></persName>
						</author>
						<title level="a" type="main">Reference-based Video Super-Resolution Using Multi-Camera Video Triplets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the first reference-based video superresolution (RefVSR) approach that utilizes reference videos for high-fidelity results. We focus on RefVSR in a triplecamera setting, where we aim at super-resolving a lowresolution ultra-wide video utilizing wide-angle and telephoto videos. We introduce the first RefVSR network that recurrently aligns and propagates temporal reference features fused with features extracted from low-resolution frames. To facilitate the fusion and propagation of temporal reference features, we propose a propagative temporal fusion module. For learning and evaluation of our network, we present the first RefVSR dataset consisting of triplets of ultra-wide, wide-angle, and telephoto videos concurrently taken from triple cameras of a smartphone. We also propose a twostage training strategy fully utilizing video triplets in the proposed dataset for real-world 4? video super-resolution. We extensively evaluate our method, and the result shows the state-of-the-art performance in 4? super-resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent mobile devices such as Apple iPhone or Samsung Galaxy series are manufactured with at least two or three asymmetric multi-cameras typically having different but fixed focal lengths. In a triple camera setting, each ultrawide, wide-angle, and telephoto camera has a different field of view (FoV) and optical zoom factor. One advantage of such configuration is that, compared to an ultra-wide camera, a wide-angle camera captures a subject with more details and higher resolution, and the advantage escalates even further with a telephoto camera. A question naturally follows is why not leverage higher-resolution frames of a camera with a longer focal length to improve the resolution of frames of a camera with a short focal length.</p><p>Utilizing a reference (Ref) image to reconstruct a highresolution (HR) image from a low-resolution (LR) image has been widely studied in previous reference-based image Code and dataset: https://github.com/codeslake/RefVSR  <ref type="figure">Figure 1</ref>. Comparison on 8K 4?SR video results from a real HD video between state-of-the-art (SOTA) RefSR approach <ref type="bibr" target="#b25">[26]</ref> and the proposed RefVSR approach. Our method learns to superresolve an LR video by utilizing relevant high-quality patches of reference frames and robustly recovers sharp textures of both inside and outside the overlapped FoV between the input ultra-wide and reference wide-angle frames (white dashed box).</p><p>super-resolution (RefSR) approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. However, it has not been explored yet to utilize a Ref video for video super-resolution (VSR). In this paper, we expand the RefSR to the VSR task and introduce referencebased video super-resolution (RefVSR) that can be applied for videos captured in an asymmetric multi-camera setting. RefVSR inherits objectives of both RefSR and VSR tasks and utilizes a Ref video for reconstructing an HR video from an LR video. Applying RefVSR for a video captured in an asymmetric multi-camera setting requires consideration of the unique relationship between LR and Ref frames in multi-camera videos. In the setting, a pair of LR and Ref frames at each time step shares almost the same content in their overlapped FoV (top and middle rows of the leftmost column in <ref type="figure">Fig. 1)</ref>. Moreover, as a video exhibits a motion, neighboring Ref frames might contain high-quality contents useful for recovering the outside the overlapped FoV (the bottom row of the leftmost column in <ref type="figure">Fig. 1</ref>).</p><p>For successful RefVSR in an asymmetric multi-camera setting, we take advantage of temporal Ref frames in reconstructing regions both inside and outside the overlapped FoV. In previous RefSR approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, global matching has been a common choice for establishing nonlocal correspondence between a pair of LR and Ref images. However, given a pair of LR and Ref video sequences, it is not straightforward to directly apply global matching between an LR frame and multiple Ref frames. To utilize as many frames as possible in the global matching for large real-world videos (e.g., HD videos), we need a framework capable of managing Ref frames in a memory-efficient way.</p><p>We propose the first end-to-end learning-based RefVSR network that can generally be applied for super-resolving an LR video using a Ref video. Our network adopts a bidirectional recurrent pipeline <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>  To train and validate our model, we present the first Re-fVSR dataset consisting of 161 video triplets of ultra-wide, wide-angle, and telephoto videos simultaneously captured with triple cameras of a smartphone. Wide-angle and telephoto videos have the same size as ultra-wide videos but their resolutions are 2? and 4? the resolution of ultra-wide videos, respectively. With the RefVSR dataset, we train our network to super-resolve an ultra-wide video 4? to produce an 8K video with the same resolution as a telephoto video. To this end, we propose a two-stage training strategy that fully utilizes video triplets in the proposed dataset. We show that, with our training strategy, our network can successfully learn super-resolution of a real-world HD video and produce a high-fidelity 8K video.</p><p>To summarize, our contributions include: ? the first RefVSR framework with the focus on videos recorded in an asymmetric multi-camera setting, ? the propagative temporal fusion module that effectively fuses and propagates temporal Ref features,</p><p>? the RealMCVSR dataset, which is the first dataset for the RefVSR task, and</p><p>? the two-stage training strategy fully utilizing video triplets for real-world 4?VSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Reference-based Super-Resolution (RefSR) Previous RefSR approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> have focused on establishing non-local correspondence between LR and Ref features. For establishing correspondence, either offsetbased matching (optical flow <ref type="bibr" target="#b34">[35]</ref> and deformable convolution <ref type="bibr" target="#b22">[23]</ref>) or patch-based matching (patch-match <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, learnable patch-match <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, learnable patch-match with affine correction <ref type="bibr" target="#b25">[26]</ref>) are employed.</p><p>Video Super-Resolution (VSR) Previous VSR methods have focused on how to effectively utilize highly related but unaligned LR frames in a video sequence. With respect to how LR frames in video sequences are handled by a model, previous VSR approaches can be categorized into either sliding window-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> or recurrent framework-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref> approaches. For handling unaligned LR frames, warping using optical flow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>, patch-based correlation <ref type="bibr" target="#b14">[15]</ref>, and deformable convolution <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> have been employed. The aforementioned previous studies in RefSR and VSR have developed various components. In this paper, to match and align Ref features to an LR frame, we adopt the learnable patch-match-based reference alignment module <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. To handle video sequences, we adopt a bidirectional recurrent framework <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. However, for RefVSR, we modify the components to handle both LR and Ref videos. We also equip our network with the propagative temporal fusion module, designed to effectively and efficiently exploit temporal Ref features in reconstructing HR frames.</p><p>Recently, a RefVSR method <ref type="bibr" target="#b32">[33]</ref>, in which only the first frame of an HR video is used as a reference to super-resolve an LR video downsampled from the HR video, has been concurrently proposed alongside our work. However, to the best of our knowledge, ours is the first RefVSR framework that utilizes multiple frames in a Ref video for superresolving a real-world LR video. <ref type="figure" target="#fig_0">Fig. 2</ref> shows an overview of the proposed network, which can generally be applied to a RefVSR task for superresolving an LR video utilizing a Ref video. Our network follows a typical bidirectional propagation scheme <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>, consisting of bidirectional recurrent cells F f and F b , where the subscripts f and b indicate forward and backward propagation branches, respectively ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Our network is dis- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Camera Video Super-Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><formula xml:id="formula_0">?1 ?1 +1 +1 ?1 +1</formula><p>For reconstructing an SR result I SR t , the upsampling module U first takes the intermediate features h {f,b} t and accumulated matching confidences c {f,b} t of both forward and backward branches. Then, the features are aggregated and upsampled with multiple convolution and pixel-shuffle <ref type="bibr" target="#b21">[22]</ref> layers to produce I SR t . Mathematically, we have:</p><formula xml:id="formula_2">I SR t = U (h f t , h b t , c f t , c b t ).<label>(2)</label></formula><p>For the upsampling module U to accurately reconstruct </p><formula xml:id="formula_3">I SR t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bidirectional Recurrent Cells</head><p>In each recurrent cell F f and F b <ref type="figure" target="#fig_1">(Fig. 3</ref>  into the feature space by a shared encoder ? <ref type="bibr" target="#b23">[24]</ref>, where ? denotes the downsampling operator. Then, we extract 3 ? 3 patches from the LR and Ref feature maps with stride 1 and compute a cosine similarity matrix C between them, such that C i,j is a similarity between the i-th patch of the LR feature map and the j-th patch of the Ref feature map. The matching index map p and confidence map c is then computed as:</p><formula xml:id="formula_4">p t,i = argmax j C i,j , c t,i = max j C i,j ,<label>(4)</label></formula><formula xml:id="formula_5">where p t,i is the patch index of Ref features ?(I Ref t? ) that is the most relevant to the i-th patch of LR features ?(I LR t )</formula><p>, and c t,i is their matching confidence, respectively. Reference Alignment Module We use the reference alignment module proposed in <ref type="bibr" target="#b25">[26]</ref>   <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Finally, the module compensates for possible inter-patch misalignment (e.g., scale and rotation) in the coarsely aligned Ref features using the patch-wise affine spatial transformer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training Strategy for Real-World 4?VSR</head><p>To train our network, we propose the RealMCVSR dataset, which consists of triplets of ultra-wide, wide-angle, and telephoto videos, where wide-angle and telephoto videos have the same size as ultra-wide videos, but their resolutions are 2? and 4? that of ultra-wide videos. The detail of the dataset is given in Sec. 5. Given video triplets, we train our network to perform 4? super-resolution of an ultra-wide HD video with a wide-angle video as a Ref video for obtaining an 8K video. The resulting 8K video has the same resolution as a telephoto video, but 16? larger in size.</p><p>It is worth noting that we use only a wide-angle video as a Ref video. While it may look reasonable to use a telephoto video as an additional Ref video to achieve the resolution of a telephoto video, we found that it does not improve the super-resolution quality much because a telephoto video covers only 1/16 the area of an ultra-wide video. A detailed discussion and experiments are provided in the supplement.</p><p>Training our network to produce 8K videos is not trivial as there are no ground-truth 8K videos. While we have wide-angle and telephoto videos, they neither cover the entire area nor perfectly align with an ultra-wide video. To overcome this, we propose a novel training strategy that fully exploits wide-angle and telephoto videos.</p><p>Our training strategy consists of pre-training and adaptation stages. In the pre-training stage, we downsample ultrawide and wide-angle videos 4?. We then train the network to 4? super-resolve a downsampled ultra-wide video using a downsampled wide-angle video as a reference. The training is done in a supervised manner using the original ultra-wide video as the ground-truth. Finally, in the adaptation stage, we fine-tune the network to adapt it to realworld videos of the original sizes. This stage uses a telephoto video as supervision to train the network to recover high-frequency details of a telephoto video. The following subsections describe each stage in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Stage</head><p>In this stage, we train our network using two loss functions: a reconstruction loss motivated by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> and a multi-Ref fidelity loss. The reconstruction loss minimizes the low-and high-frequency differences between a superresolved ultra-wide frame I SR t and the ground-truth ultrawide frame I HR t . The reconstruction loss rec is defined as:</p><formula xml:id="formula_6">rec = I SR t,blur ? I HR t,blur + ? rec i ? i (I SR t , I HR t ),<label>(8)</label></formula><p>where the subscript blur indicates a filtering operation with a 3?3 Gaussian kernel with ?=1.0 and ? rec is a weight for the second term. ? i (X, Y )= min j D(x i , y j ) is the contextual loss <ref type="bibr" target="#b18">[19]</ref> that measures the distance between the pixel x i in X and its most similar pixel y j in Y under some feature distance measure D, e.g., a perceptual distance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In the first term on the right-hand side in Eq. (8), filtering frames with Gaussian kernels imposes results to follow low-frequency structures of a ground-truth ultra-wide frame I HR t . The second term enforces the network to follow the high-frequency details of I HR t . Note that in the second term, we use the contextual loss even for aligned pairs I SR t and I HR t , as the loss is verified to be better in boosting the perceptual quality than the perceptual loss <ref type="bibr" target="#b10">[11]</ref> designed for aligned pairs <ref type="bibr" target="#b17">[18]</ref>.</p><p>To guide the network to take advantage of multiple Ref frames </p><formula xml:id="formula_7">Mfid = t ?? i ? i (I SR t , I Ref HR t ) ? c t ,i t ?? i c t ,i ,<label>(9)</label></formula><p>where ?=[t-k?1 2 , . . . , t+ k?1 2 ] is set of frame indices in a temporal window of size k. We use k=7 in practice. Here, c t ,i is the matching confidence used for weighting the distance where ? pre is a weight for the multi-Ref fidelity loss for the pre-training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptation Stage for Real-world 4?VSR</head><p>For adaptation, our network takes real-world ultra-wide I U W t and wide-angle I W ide t HD frames as LR and Ref frames, respectively. As in the pre-training stage, the adaptation stage separately handles low-and high-frequency of a super-resolved ultra-wide frame I SR t . However, as there is no ground-truth frame available for I SR t , we downsample I SR t and use the input ultra-wide frame I U W t as the supervision for recovering low-frequency structures. For recovering high-frequency details, we directly utilize telephoto frames I T ele t?? as the supervision for the proposed multi-Ref fidelity loss Mfid . The adaptation loss is defined as:</p><formula xml:id="formula_8">8K = ||I SR t?,blur ? I U W t,blur || + ? 8K Mfid (I SR t , I T ele t?? ),<label>(11)</label></formula><p>where ? 8K is a weight for the multi-Ref fidelity loss for the adaptation stage. The first term imposes our network to reconstruct low-frequency structures of input ultra-wide frames, and the second term trains our network to transfer the finest high-frequency details of telephoto frames. Implementation The network is trained using rectified-Adam <ref type="bibr" target="#b15">[16]</ref> with an initial learning rate 2.0?10 -4 , which is steadily decreased to 1.0?10 -6 using the cosine annealing strategy <ref type="bibr" target="#b16">[17]</ref>. The network is trained for 300k and 50k iterations for the pre-training and adaptation stages, respectively, with ? rec = 0.01, ? pre = 0.05, and ? 8K = 0.1. For each iteration, we randomly sample batches of frame triplets from the RealMCVSR training set. For the pre-training stage, we downsample ultra-wide LR and wide-angle Ref frames 4? using bicubic downsampling provided by MATLAB function imresize. We crop patches from each frame in a triplet to have overlapped contents and apply random translation on each crop window. Then, ultra-wide LR frames are cropped to 64 ? 64 and 128 ? 128 patches for the pretraining and adaptation stages, respectively. Wide-angle and telephoto Ref frames are cropped into patches of 2? and 4? the patch size of LR patches, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>To analyze the effect of each component of our model, we conduct ablation studies. First, we validate the effects of the propagative temporal fusion module (Eq. 6) and multi-Ref fidelity loss Mfid (Eq. 9). To this end, we compare the stripped-out baseline model with its two variants. The baseline model is trained with rec and Mfid , but we set the temporal window size k=1 for Mfid , indicating only a sin-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR? / Ref? Bicubic Baseline</head><p>Mfid Mfid +PTF <ref type="figure">Figure 6</ref>. Qualitative ablation study. </p><formula xml:id="formula_9">h {f,b} t = {conv(c t ) ? conv([ h Ref t , h {f,b} t ])} + h {f,b} t .</formula><p>For the other variants, we recover the key components one by one from the baseline model. For the variant with Mfid , we train the baseline model with rec and Mfid with window size k=7. For the last variant, we attach the propagative temporal fusion module. For quantitative and qualitative comparison, we compare pre-trained models (Sec. 4.1) and their fine-tuned models (Sec. 4.2) on the proposed RealM-CVSR test set, respectively. <ref type="table" target="#tab_5">Table 1</ref> shows quantitative results. The table indicates that compared to the baseline model (the first row in the table), the model trained with Mfid (the second row) shows much better VSR performance. The model additionally equipped with the propagative temporal fusion module (the third row) achieves the best results in every measure. We also validate the effects of the proposed training strategy. Specifically, we qualitatively compare between the model pre-trained with the pre-training loss pre (Eq. 10) and the model fine-tuned with the adaptation loss 8K (Eq. 11). For comparison, we show 8K VSR results given real-world HD videos. Note that in the real-world scenario, there is no ground-truth available for a quantitative comparison. <ref type="figure">Fig. 7</ref> shows the results. The pre-trained model does not improve details of a real-world input, due to the domain gap between real-world inputs and downsampled inputs (the third column). However, the fine-tuned model shows much higher fidelity results compared to the pre-trained model (the last column), thanks to the adaptation stage that trains the network to well adapt to real-world videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison on RealMCVSR Dataset</head><p>In this section, we compare our method with previous state-of-the-art approaches: SRCNN <ref type="bibr" target="#b5">[6]</ref>, RCAN <ref type="bibr" target="#b30">[31]</ref>, TTSR <ref type="bibr" target="#b28">[29]</ref>, DCSR <ref type="bibr" target="#b25">[26]</ref>, EDVR <ref type="bibr" target="#b26">[27]</ref>, BasicVSR <ref type="bibr" target="#b3">[4]</ref>, and IconVSR <ref type="bibr" target="#b3">[4]</ref>. SRCNN and RCAN are SISR models that take only a single LR frame. TTSR and DCSR are RefSR models fed with a pair of LR and Ref frames. EDVR is a sliding window-based VSR model that takes multiple frames in a local temporal window. BasicVSR and IconVSR are VSR models with a bidirectional recurrent framework, where each video frame is fed to a recurrent cell for each time step. We train each model with the proposed RealM-CVSR dataset and the code provided by the authors. Quantitative Comparison <ref type="table" target="#tab_7">Table 2</ref> shows a quantitative comparison, where ultra-wide HD frames and their 4? downsampled ones are used as ground-truths and inputs, respectively. For comparison, we use our model pre-trained with Eq. 10 (Ours). Moreover, to consider the trade-off between the model size and SR quality, we show the results of a smaller model with fewer parameters (Ours-small) and a larger model (Ours-IR) attached with information-refill and coupled propagation modules proposed in <ref type="bibr" target="#b3">[4]</ref>. We also compare our models trained only with the 1 loss function (models indicated with -1 ), for a fair comparison with the previ- ous models trained with pixel-based losses, such as 1 , 2 , and ch (Charbonnier loss <ref type="bibr" target="#b11">[12]</ref>), which are known for having an advantage in PSNR over perceptual-based loss <ref type="bibr" target="#b10">[11]</ref>.</p><p>In <ref type="table" target="#tab_7">Table 2</ref>, while RefSR methods show a better performance than SISR methods, our methods outperform all previous ones. Interestingly, VSR methods outperform RefSR methods that are additionally fed with Ref frames. However, this is not particularly true if we measure the performance on the regions of the SR frame corresponding to different FoV ranges. <ref type="table">Table 3</ref> shows the results. For comparison, we measure the SR quality for the region inside the overlapped FoV (0%-50%) between an ultra-wide SR and a wide-angle Ref frames. For outside the overlapped FoV, we measure SR performance for the banded regions at different FoV ranges from the overlapped FoV (50%) to full FoV (100%). In the table, DCSR <ref type="bibr" target="#b25">[26]</ref> outperforms IconVSR <ref type="bibr" target="#b3">[4]</ref> for the overlapped FoV (0%-50%) between an input and Ref frames, while IconVSR outruns DCSR for the rest of the regions. Our models exceed all models for all regions.</p><p>Note that in <ref type="table">Table 3</ref>, our models show a performance gap between regions inside (0%-50%) and outside (50%-100%) the overlapped FoV. However, compared to the PSNR/SSIM gap of DCSR <ref type="bibr">(</ref>  <ref type="table">Table 3</ref>. Quantitative results measured with varying FoV range. The center 50% of FoV in an ultra-wide SR frame is overlapped with the FoV of a wide-angle reference frame. Here, 0%-50% indicates the region inside the overlapped FoV, and 50%-100% is the region outside the overlapped FoV. 50%-60% means the banded region between the center 50% and 60% of an ultra-wide SR frame.</p><p>LR? / Ref? (a) Bicubic (c) RCAN <ref type="bibr" target="#b30">[31]</ref> (d) DCSR <ref type="bibr" target="#b25">[26]</ref> (e) IconVSR <ref type="bibr" target="#b3">[4]</ref> (f) Ours <ref type="figure">Figure 8</ref>. Qualitative comparison on 8K 4?SR video results from real-world HD videos. <ref type="figure">Fig. 8</ref> shows a qualitative comparison for 8K 4?SR results from real-world HD videos. The results show that nonreference-based SR methods, RCAN and IconVSR, tend to over-exaggerate textures, while non-textured regions tend to be overly smoothed out. The RefSR method, DCSR, shows better fidelity than RCAN and IconVSR in the overlapped FoV (red box). However, DCSR tends to smooth out regions outside the overlapped FoV (green box). Our method shows the best result compared to the previous ones. Compared to DCSR, our model robustly reconstructs finer details with balanced fidelity between regions inside and outside the overlapped FoV. Moreover, the details and textures reconstructed outside the FoV are more photo-realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed the first RefVSR framework with the practical focus on videos captured in an asymmetric multicamera setting. To efficiently utilize a Ref video sequence, we adopted a bidirectional recurrent framework and pro-posed the propagative temporal fusion module to fuse and propagate Ref features well-matched to LR features. To train and validate the network, we provided the RealM-CVSR dataset consisting of real-world HD video triplets. An adaptation training strategy is proposed to fully utilize video triplets in the dataset. In the experiments, we verified the effects of key components in our model, and our model achieves the state-of-the-art 4?VSR performance.</p><p>Limitation As previous RefSR methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, our network consumes quite an amount of memory for applying global matching between real-world HD frames. We plan to develop a memory-efficient RefVSR framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our RefVSR framework. tinguished from previous ones in additional inputs, intermediate features, and modules to utilize a Ref video sequence. Specifically, for a time step t, each recurrent cell F f or F b takes not only low-resolution LR frames I LR t?1 at the previous time step and I LR t at the current time step, but also a Ref frame I Ref t at the current time step. Each cell is also recurrently fed with aggregated LR and Ref features h {f,b} t?1 and accumulated confidence maps c {f,b} t?1 propagated from the previous time step. Here, the accumulated confidence maps are utilized for fusing well-matched Ref features later in each recurrent cell. Finally, each recurrent cell propagates the resulting features h {f,b} , h b t+1 , c b t+1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>the intermediate features h {f,b} t should contain details integrated from both LR and Ref frames in a video sequence. To this end, each recurrent cell F f and F b performs inter-frame alignment between the previous and current LR input frames, then aggregates and propagates the features (Sec. 3.2). To exploit multiple Ref frames, each recurrent cell aligns the current Ref features to the current LR frame and fuses the aligned Ref features to the aggregated features of the previous Ref, LR, and current LR frames using a reference alignment and propagation module (Sec. 3.3). In this way, features of temporally distant LR input and Ref frames can be recurrently integrated and propagated. Forward (top) and backward (bottom) recurrent cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The reference alignment and propagation module. Cosine Similarity Module To compute an index map p t and a confidence map c t , we first embed I LR t and I Ref t?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The propagative temporal fusion module. However, a na?ve fusion of the Ref features h Ref tis errorprone, as matching is not necessarily accurate. Inspired by<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, we thus perform feature fusion guided by the matching confidences c t , which guides the fusion module to select only well-matched features in h Ref t . The fusion module also needs a guidance for propagated Ref features aggregated in h {f,b} t . The guidance should accommodate temporal information that coincides with propagated Ref features maintained in the propagation pipeline. To this end, we accumulate matching confidences throughout the propagation pipeline and use the accumulated confidence as the guidance for the temporally aggregated features h {f,b}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we encourage Ref features to keep propagating from one to the next cells. Motivated by [26], we propose a multi-Ref fidelity loss. Given a super-resolved ultra-wide frame I SR t and ground-truth wide-angle frames I Ref HR t?? , the multi-Ref fidelity loss is defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>? i (I SR t , I Ref HR t ). Specifically, during training, pixels of I SR t with higher matching confidence c t ,i are assigned with larger weights for optimization. Eq. 9 enables our network to effectively utilize multiple Ref frames I Ref t?? and keep the details of multiple Ref frames to flow through the propagation pipeline. Our loss for the pre-training stage is defined as: pre = rec (I SR t , I HR t ) + ? pre Mfid (I SR t , I Ref HR t?? ). (10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 Figure 7 .</head><label>67</label><figDesc>shows a qualitative comparison. As shown in the figure, the model trained with Mfid (the fourth column of the figure) enhances details inside (red box) and outside (green box) the overlapped FoV much better compared to the results of the baseline model (the third column). The result confirms that Mfid enforces temporal Ref features to keep streaming through the propagation pipeline to be utilized in reconstructing high-fidelity results. The model attached with the propagative temporal fusion module shows accurately recovered structures and enhanced details for both inside and outside the overlapped FoV (the last column). This demonstrates the propagative temporal fusion module promotes well-matched Ref features to be fused and to flow through the propagation pipeline. Ablation study on the two-stage training strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to recurrently align and propagate Ref features that are fused with the features of LR frames. Our network is efficient in terms of computation and memory consumption because the global matching needed for aligning Ref features is performed only between a pair of LR and corresponding Ref frames at each time step. Still, our network is capable of utilizing temporal Ref frames, as the aligned Ref features are continuously fused and propagated in the pipeline. As a key component for managing Ref features in the pipeline, we propose a propagative temporal fusion module that fuses and propagates only well-matched Ref features. The module leverages the matching confidence computed during the global matching between LR and Ref features as the guidance to determine well-matched Ref features to be fused and propagated. The module also accumulates the matching confidence throughout the pipeline and uses the accumulated value as the guidance when fusing the propagated temporal Ref features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>), we first use a flow estimation network S [20] to estimate the optical flow between the LR frame I LR</figDesc><table><row><cell></cell><cell></cell><cell>Reference Alignment and Propagation (</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell>cosine</cell><cell>(matching confidence)</cell></row><row><cell></cell><cell></cell><cell>similarity</cell><cell>4 ? 5</cell></row><row><cell></cell><cell></cell><cell>matrix</cell><cell>? ? ? ? 9 2 (matching index)</cell></row><row><cell cols="2">{ , }</cell><cell>? ? Reference feature aligned to LR ? { , } { , } Propagative Temporal Fusion Accumulated matching confidence Reference Alignment</cell></row><row><cell>? ?</cell><cell>{ , }</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>to obtain Ref features aligned to I LR t , which will be used for the fusion later. The module first takes I Ref t and extracts Ref features h Ref t . Then, using the matching index map p t (Eq. 4), we warp patches of Ref features h Ref t to coarsely align the features to the current LR frame I LR t</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.</head><label></label><figDesc>We denote the final aligned Ref features as h Ref t . Propagative Temporal Fusion Module Finally, we propose the propagative temporal fusion module that fuses the aligned Ref features h Ref t with the temporally aggregated features h {f,b}</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Propagative Temporal Fusion</cell></row><row><cell>temporally aggregated features</cell><cell>? ?</cell><cell>{ , }</cell><cell></cell></row><row><cell></cell><cell></cell><cell>concat</cell><cell>conv</cell></row><row><cell>aligned Ref features</cell><cell>? ?</cell><cell></cell><cell></cell><cell>?</cell><cell>{ , }</cell></row><row><cell></cell><cell></cell><cell>concat</cell><cell></cell></row><row><cell>Guidance for the fusion</cell><cell></cell><cell></cell><cell>conv</cell></row><row><cell>matching</cell><cell></cell><cell></cell><cell></cell></row><row><cell>confidence</cell><cell></cell><cell></cell><cell></cell></row><row><cell>accumulated matching confidence</cell><cell cols="2">{ , }</cell><cell>max</cell><cell>{ , }</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Quantitative ablation study. The first row corresponds to the baseline model. Mfid and PTF indicate the models trained with Eq. 9 and propagative temporal fusion module, respectively.</figDesc><table><row><cell>Mfid</cell><cell>PTF</cell><cell>PSNR?</cell><cell cols="2">SSIM? Params (M)</cell></row><row><cell></cell><cell></cell><cell>30.71</cell><cell>0.894</cell><cell>4.2768</cell></row><row><cell></cell><cell></cell><cell>31.31</cell><cell>0.913</cell><cell>4.2768</cell></row><row><cell></cell><cell></cell><cell>31.68</cell><cell>0.914</cell><cell>4.2772</cell></row><row><cell cols="2">5. Experiments</cell><cell></cell><cell></cell></row><row><cell cols="5">RealMCVSR Dataset Our RealMCVSR dataset provides</cell></row><row><cell cols="5">real-world HD video triplets concurrently recorded by Ap-</cell></row><row><cell cols="5">ple iPhone 12 Pro Max equipped with triple cameras hav-</cell></row><row><cell cols="5">ing fixed focal lengths: ultra-wide (30mm), wide-angle</cell></row><row><cell cols="5">(59mm), and telephoto (147mm). To concurrently record</cell></row><row><cell cols="5">video triplets, we built an iOS app that provides full con-</cell></row><row><cell cols="5">trol over exposure parameters (i.e., shutter speed and ISO)</cell></row><row><cell cols="5">of the cameras. For recording each scene, we set the cam-</cell></row><row><cell cols="5">eras in the auto-exposure mode, where the shutter speeds</cell></row><row><cell cols="5">of the three cameras are synced to avoid varying motion</cell></row><row><cell cols="5">blur across a video triplet. ISOs are adjusted accordingly</cell></row><row><cell cols="5">for each camera to pick up the same exposure. Each video</cell></row><row><cell cols="5">is saved in the MOV format using HEVC/H.265 encoding</cell></row><row><cell cols="5">with the HD resolution (1080?1920). The dataset contains</cell></row><row><cell cols="5">triplets of 161 video clips with 23,107 frames in total. The</cell></row><row><cell cols="5">video triplets are split into training, validation, and testing</cell></row><row><cell cols="5">sets, each of which has 137, 8, and 16 triplets with 19,426,</cell></row><row><cell cols="3">1,141, and 2,540 frames, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation on the RealMCVSR test set.</figDesc><table><row><cell></cell><cell>Model</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>Params (M)</cell></row><row><cell>SISR</cell><cell>Bicubic SRGAN [13] RCAN-1 [31]</cell><cell>26.65 29.38 31.07</cell><cell>0.800 0.877 0.915</cell><cell>-0.734 15.89</cell></row><row><cell>RefSR</cell><cell>TTSR [29] TTSR-1 [29] DCSR [26]</cell><cell>30.31 30.83 30.63</cell><cell>0.905 0.911 0.895</cell><cell>6.730 6.730 5.419</cell></row><row><cell></cell><cell>DCSR-1 [26]</cell><cell>32.43</cell><cell>0.933</cell><cell>5.419</cell></row><row><cell></cell><cell>EDVR-M-ch [27]</cell><cell>33.26</cell><cell>0.946</cell><cell>3.317</cell></row><row><cell>VSR</cell><cell>EDVR-ch [27] BasicVSR-ch [4]</cell><cell>33.47 33.66</cell><cell>0.948 0.951</cell><cell>20.63 4.851</cell></row><row><cell></cell><cell>IconVSR-ch [4]</cell><cell>33.80</cell><cell>0.951</cell><cell>7.255</cell></row><row><cell></cell><cell>Ours-small</cell><cell>31.63</cell><cell>0.912</cell><cell>1.052</cell></row><row><cell>RefVSR</cell><cell>Ours-small-1 Ours Ours-1 Ours-IR</cell><cell>33.88 31.68 34.74 31.73</cell><cell>0.951 0.914 0.958 0.916</cell><cell>1.052 4.277 4.277 4.774</cell></row><row><cell></cell><cell>Ours-IR-1</cell><cell>34.86</cell><cell>0.959</cell><cell>4.774</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>8.5% / 4.2%), our models show much smaller gap (Ours-1 : 4.2% / 1.8% and Ours-IR-.757 26.30 / 0.785 26.42 / 0.789 26.71 / 0.798 26.99 / 0.801 27.29 / 0.815 -RCAN-1 [31] 29.77 / 0.895 30.69 / 0.908 30.86 / 0.910 31.17 / 0.914 31.50 / 0.918 31.80 / 0.921 15.89 RefSR DCSR-1 [26] 34.90 / 0.963 31.96 / 0.927 31.61 / 0.921 31.58 / 0.919 31.81 / 0.921 31.93 / 0.923 5.419 VSR IconVSRch [4] 32.79 / 0.946 33.43 / 0.949 33.60 / 0.950 33.89 / 0.951 34.19 / 0.953 34.40 / 0.953 7.255 RefVSR Ours-1 36.02 / 0.971 34.59 / 0.958 34.31 / 0.956 34.23 / 0.954 34.40 / 0.955 34.50 / 0.954 4.277 Ours-IR-1 36.14 / 0.971 34.66 / 0.959 34.40 / 0.956 34.34 / 0.955 34.52 / 0.955 34.63 / 0.955 4.774</figDesc><table><row><cell></cell><cell>Model</cell><cell>0%-50%</cell><cell>PSNR / SSIM measured for regions in the indicated FoV range 50%-60% 50%-70% 50%-80% 50%-90%</cell><cell>50%-100%</cell><cell>Params (M)</cell></row><row><cell>SISR</cell><cell>Bicubic</cell><cell>25.38 / 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t and the accumulated matching confidences c {f,b} t to the next cell. Formally, we have:{h f t , c f t } = F f (I LR t?1 , I LR t , I Ref t , h f t?1 , c f t?1 ), {h b t , c b t } = F b (I LR t+1 , I LR t , I Ref t</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t at the current time step and I LR t?1 at the previous time step to align propagated features h {f,b} t?1 to I LR t . Then, using a residual block R, we aggregate an LR frame I LR t into the aligned features to obtain temporally aggregated features h {f,b} t . Specifically, we have:w {f,b} t = S(I LR t , I LR t?1 ), h {f,b} t = warp(h {f,b} t?1 , w {f,b} t ), h {f,b} t = R {f,b} (I LR t , h {f,b} t ),(3)where warp(, ) denotes warping operation, and w {f,b} t is the optical flow estimated by the flow estimation network S. Note that the temporally aggregated features h {f,b}t contains details aggregated from multiple LR features, as well as temporal Ref features propagated from neighboring cells. Now we propose the reference alignment and propagation module for each cell F f and F b to fuse the current Ref frame I Ref t into temporally aggregated features h {f,b} t .3.3. Reference Alignment and PropagationOur reference alignment and propagation module (Fig. 4)consists of three sub-modules: cosine similarity, reference alignment, and propagative temporal fusion modules. The cosine similarity module computes a cosine similarity matrix between the Ref frameI Ref t and target LR frames I LR t and computes an index map p t and a confidence map c t needed for the other two sub-modules. The reference alignment module extracts a feature map from the current Ref frame I Ref t and warps the feature map to I LR t using the index map p t . Then, the propagative temporal fusion module fuses the aligned Ref features with the temporally aggregated features h {f,b} t .In the following, we describe each module in more detail.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t and propagates the fused features h {f,b} t to the next cell (Fig. 5). Note that aligned Ref features h Ref t contain Ref features at the current time step, while the temporally aggregated features h {f,b} t contain aggregated temporal Ref features propagated from neighboring recurrent cells. For the successful fusion, the propagative temporal fusion module has to fuse h {f,b} t and h Ref t in the way of selecting the Ref features better aligned to the target frame so that well-matched Ref features can keep propagating to the next cell. Otherwise, erroneous Ref features can be accumulated in the pipeline, leading to blurry results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t during the fusion. Formally, we have: c {f,b} t = warp(c {f,b} t?1 , w {f,b} t ),(5)where c {f,b} t?1 is the accumulated matching confidence propagated from neighboring cells. We align the confidence to obtain c {f,b} t using the optical flow pre-computed in Eq. 3. For the fusion, we provide matching confidence c t computed between the current target and reference frames, and we also provide aligned matching confidence c {f,b} t propagated from neighboring recurrent cells as guidance. The matching confidences are embedded with a convolution layer to consider matching scores of neighboring patches for providing more accurate guidance during the fusion<ref type="bibr" target="#b25">[26]</ref>. Formally, the fusion process is defined as:h {f,b} t = {conv([c t , c {f,b} t ]) ? conv([ h Ref t , h {f,b} t ])} + h {f,b} t ,(6)where[, ]  and ? indicate concatenation operation and element-wise multiplication, respectively.For the next cell, we use max(, ) operation to accumulate c t into c {f,b} t and pick up a larger confidence score. The accumulation process is defined as:c {f,b} t = max(c t , c {f,b} t ). (7) Max operation between c t and c {f,b} t indirectly imposes the propagative temporal fusion module to selectively fuse and propagate the better matched features between corresponding features, h Ref t and h {f,b} t , respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">: 4.2% / 1.6%). The result implies the proposed architecture effectively utilizes neighboring Ref features for recovering regions both inside and outside of the overlapped FoV.Qualitative Comparison For the qualitative comparison, we show 8K (4320?7280) 4?SR video results given realworld HD (1080 ? 1920) videos. For the comparison, we select the best models from each SISR, RefSR, and VSR approaches: RCAN<ref type="bibr" target="#b30">[31]</ref>, DCSR<ref type="bibr" target="#b25">[26]</ref>, and IconVSR<ref type="bibr" target="#b3">[4]</ref>, respectively, according to their quantitative performance shown with the RealMCVSR test set. We train each model with the proposed training strategy (Sec. 4).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Hyeongseok Son for helpful discussions and Jihye Kim and Anna Choi for their help in collecting the RealMCVSR dataset. This work was supported by the Ministry of Science and ICT, Korea, through IITP grants (SW Star Lab, 2015-0-00174; AI Innovation Hub, 2021-0-02068; Artificial Intelligence Graduate School Program (POSTECH), 2019-0-01906) and NRF grants (2018R1A5A1060031; 2020R1C1C1014863).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving resolution and depth-of-field of light field cameras using a hybrid imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computational Photography (ICCP)</title>
		<meeting>the IEEE International Conference on Computational Photography (ICCP)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video superresolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast spatio-temporal residual network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maintaining natural image statistics with the contextual loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Shama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust reference-based super-resolution with similarity-aware deformable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyumin</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual-camera super-resolution with aligned attention modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature representation matters: End-to-end learning for reference-based image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuaner</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efenet: Reference-based video superresolution with enhanced flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning cross-scale correspondence and patch-based synthesis for reference-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC</title>
		<meeting>the British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crossnet: An end-to-end reference-based super resolution network using cross-scale warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Springer European Conference on Computer Vision (ECCV)</title>
		<meeting>the Springer European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
