<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection Recovery in Online Multi-Object Tracking with Sparse Graph Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseok</forename><surname>Hyun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggu</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Wee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detection Recovery in Online Multi-Object Tracking with Sparse Graph Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In existing joint detection and tracking methods, pairwise relational features are used to match previous tracklets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only highscored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the online setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the online setting, missed detection problem is far more critical than in the offline setting; tracklets are disconnected once the corresponding detections are missed, while tracklet interpolation is infeasible to fill the past missed detections. As illustrated in <ref type="figure">Figure 1</ref>, occlusion leads to low-confident detections, and if they are included in the association step, the complexity of tracking increases with too many spurious detections. Pairwise relational features (e.g., position or visual similarity) may not be discriminative enough to distinguish targets in such case and result * The work was partially done during an intern at Clova AI. <ref type="figure">Figure 1</ref>. Motivation of detection recovery by tracking. Tracking of 1 cannot be performed if the high-scored detection ( ? ) is selected as the tracking candidate. Meanwhile, adding low-scored detections ( ? , ? , ? ) to the candidate pool results in wrong matching of 1 with ? in (a) since pairwise relations are ambiguous to be used for discriminating 1 among them. In contrast, our SGT (b) exploits multi-hop relations updated via a GNN to contain visual features of neighboring detections and their relations. Despite many tracking candidates selected by top-K, SGT successfully tracks 1 , and its missed detection is consequently recovered. in wrong matching. Thus, existing works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref>, which use the pairwise relations for tracking, exploit only high-scored detections as tracking candidates.</p><p>A graph is an effective way to represent relations between objects in a video, and a graph neural network (GNN) is effective in modeling the relationship. Bearing this in mind, we model the spatio-temporal relationship in video data using a GNN to extract higher-order relational features (i.e., multi-hop relational features) which consider the relations between neighboring objects or background patches. These features are powerful and can perform association correctly even if a large number of detections (e.g., 300) is selected as tracking candidates. Thus, we propose Sparse Graph Tracker (SGT), a novel online graph tracker that adopts joint detection and tracking (JDT) framework <ref type="bibr" target="#b39">[40]</ref> where object detector and tracker share a backbone network to achieve fast inference speed.</p><p>In the association step, existing online JDT methods uti-lize pairwise relational features between two detections, such as similarity of appearance features <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b38">39]</ref>, center point distance <ref type="bibr" target="#b52">[53]</ref>, and Intersection over Union (IoU) score <ref type="bibr" target="#b36">[37]</ref>. Although <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b38">39]</ref> fuse appearance information and motion information by weighted sum, these features reflect only the relations between two objects and are not discriminative for accurate matching in a crowded scene. Motion predictor (e.g., Kalman filter <ref type="bibr" target="#b2">[3]</ref>) is commonly employed to improve tracking performance.</p><p>On the contrary, we utilize higher-order relational features by aggregating the features of neighboring nodes and edges through iterations of GNNs. Even without a motion predictor, higher-order relational features are still powerful to correctly match the previous tracklets with current frame's (I t2 ) top-K scored detections which contain a large number of spurious detections due to a large K value. The main contributions of this work are as follows: 1. We propose a novel online graph-based tracker that is jointly trained with object detector and performs long-term association without any motion model. Our SGT shows superior performance on the MOT16/17/20 and HiEve benchmarks with real-time inference speed. 2. We propose training and inference techniques for SGT effectively achieving detection recovery by tracking. Their effectiveness is demonstrated through extensive ablation experiments and a large improvement on MOT20 where severe crowdedness results in low confident detections for the occluded objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">JDT Methods</head><p>Recently, many JDT methods are proposed due to fast inference speed and single-stage training based on the shared backbone. They extend object detectors to MOT models with the extra tracking branch which is jointly trained with the detector. They fall into two categories: (1) reidentification (reID) branch outputs discriminative appearance features for tracking, and (2) motion prediction branch outputs the updated location of tracklets for tracking. JDT by reID. RetinaTrack <ref type="bibr" target="#b25">[26]</ref>, JDE <ref type="bibr" target="#b39">[40]</ref>, FairMOT <ref type="bibr" target="#b49">[50]</ref> append reID branch to RetinaNet <ref type="bibr" target="#b22">[23]</ref>, YOLOv3 <ref type="bibr" target="#b31">[32]</ref>, and CenterNet <ref type="bibr" target="#b54">[54]</ref>, respectively. Liang et al. <ref type="bibr" target="#b21">[22]</ref> points out that the objectives of detection and ReID are conflicting, and proposes a cross-correlation network that learns taskspecific features. On the other hand, GSDT <ref type="bibr" target="#b38">[39]</ref> and CorrTracker <ref type="bibr" target="#b37">[38]</ref> enhance the current features by spatiotemporal relational modeling that exploits previous frames. CorrTracker <ref type="bibr" target="#b37">[38]</ref>, the current state-of-ther-art model, fuses the correlation in both spatial and temporal dimensions to the image features at multiple pyramid levels. All these methods associate the tracklets and detections using the similarity of reID features. Also, Kalman filter <ref type="bibr" target="#b2">[3]</ref> is commonly employed and motion information is fused to the similarity. JDT by motion prediction. D&amp;T <ref type="bibr" target="#b12">[13]</ref> and Center-Track <ref type="bibr" target="#b52">[53]</ref> append the learnable motion predictor into R-FCN <ref type="bibr" target="#b7">[8]</ref> and CenterNet <ref type="bibr" target="#b54">[54]</ref>, respectively. CenterTrack associates the tracklets and detections using the center point distance of the detections and the tracklets updated by predicted motion. TraDeS <ref type="bibr" target="#b42">[43]</ref> predicts the center offset of objects between two consecutive frames based on a cost volume which is computed by a similarity of reID features of the frames. TransTrack <ref type="bibr" target="#b36">[37]</ref> is a transformer-based tracker that propagates the previous frame's tracklets to the current frame and matches with the current detections by IoU score. Comparison. Our SGT extends CenterNet <ref type="bibr" target="#b54">[54]</ref> with a graph tracker. Compared with others using pairwise relational features (e.g., IoU, cosine similarity or fusion of them), SGT exploits edge features updated through GNN which are higher-order relational features, and solves association as edge classification as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph-based Multi-object Tracking</head><p>A graph is an effective way to represent relational information, and GNN can learn higher-order relational information through a message passing process that propagates node or edge features to the connected nodes or edges and aggregates neighboring features. STRN <ref type="bibr" target="#b44">[45]</ref> is an online MOT method with a spatio-temporal relation network that consists of a spatial relation module and a temporal relation module. The features from these two modules are fused to predict the affinity score for association. MPNTrack <ref type="bibr" target="#b4">[5]</ref> adopts a message passing network <ref type="bibr" target="#b13">[14]</ref> with time-aware node update module that aggregates past and future features separately and solves MOT problem as edge classification. LPCMOT <ref type="bibr" target="#b8">[9]</ref> generates and scores tracklet proposals based on a set of frames and detections with Graph Convolution Network (GCN) <ref type="bibr" target="#b16">[17]</ref>. GSDT <ref type="bibr" target="#b38">[39]</ref> is the first work that applies a GNN in an online JDT method, but its use of GNN is limited to enhancing the current feature map and tracking is still performed using pairwise relational features. In contrast, SGT is the first JDT method using higher-order relational features for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Online Detection Recovery</head><p>In the TBD framework, detections to be tracked are decided based on certain threshold. Two detection threshold values are commonly used in online MOT methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b21">22]</ref>: ? init and ? D for initializing unmatched detections as new tracklets and choosing tracking candidates, respectively. Due to a high value of ? D (e.g., 0.4), some lowscored true detections are not included in the tracking candidates. In ByteTrack <ref type="bibr" target="#b48">[49]</ref>, an extra association stage is deployed that the unmatched tracklets are matched with low- </p><formula xml:id="formula_0">(S2) A sparse graph is built, where a node, n i T ? NT (i ? [1, K])</formula><p>, is a detection of frame T = {t1, t2} and an edge (ei,j) is a connection between n i t1 and n j t2 . The green nodes are the tracklets that are missed until t1 and they are appended to Nt1. The red nodes indicate the positive detection that has its assigned ID. Two nodes at Nt2 are red since their detection scores are above ?init, so new ID can be assigned to them. (S3) GNN updates the features of nodes and edges to become higher-order by aggregating neighboring features. (S4) The edge score of the red line (a positive edge) is above the edge threshold (?E) while the green line represents a negative edge. The yellow node (n 3 t2 ) is an example of detection recovery. It was previously negative detection due to its low score, but it becomes a positive detection with the help of a positive edge. (S5) The recovered detection (n 3 t2 ) in S4 is verified by the node score. If the node score is below the node threshold (?N ), it is regarded as a false positive and is filtered out. Otherwise, the node is recovered and hence can be successfully detected which is shown by the yellow node becoming red. scored detections using IoU score. However, a new detection threshold, ? D low (e.g., 0.2), is introduced for selecting the low-scored detections as the candidates, and this is a critical value deciding the trade-off between FP and FN. OMC <ref type="bibr" target="#b20">[21]</ref> introduces an extra stage before the association step to complement missed detections which may not be detected due to the low confidence score. <ref type="figure" target="#fig_0">Figure 2</ref> shows the architecture of SGT. While various image backbones and object detectors can be flexibly adopted to SGT, our main experiment is based on Center-Net <ref type="bibr" target="#b54">[54]</ref> with a variant of DLA-34 backbone <ref type="bibr" target="#b46">[47]</ref> as same as our baseline, FairMOT <ref type="bibr" target="#b49">[50]</ref>. Following <ref type="bibr" target="#b49">[50]</ref>, we modify CenterNet that the box size predictor outputs left, right, top, and bottom sizes (s l , s r , s t , s b ) from a center point of an object instead of the width and height. CenterNet is a point-based detector that predicts object at every pixel of a feature map. The score head's output is denoted as B score ? R H h ?Hw?1 , where H h and H w are height and width of the feature map. The output from the size head is denoted as B size ? R H h ?Hw?4 . The offset head adjusts the center coordinates of objects using B of f ? R H h ?Hw?2 . At frame T , CenterNet outputs detections D T = (S T , B T ), where S T is the detection score (B score ) and B T ? R H h ?Hw?4 is top-left and bottom-right coordinates. Sparse graph builder takes top-K scored detections from each frame (I t1 and I t2 ) and sets them as the nodes of a graph (N t1 and N t2 ). In the inference phase, the previous timestep's N t2 will be the current timestep's N t1 . We sparsely connect N t1 and N t2 only if they are close in either Euclidean or feature space. Specifically, n i t1 ? N t1 is connected to N t2 with three criteria: 1) small distance between their center coordinates; 2) high cosine similarity between their features; 3) high IoU score. For each criterion, the given number of N t2 (e.g., 10) are selected to be connected to n i t1 without duplicates. The connection is bidirectional so that both N t1 and N t2 update their features. The visual features of the detections and relational features are used as the features of nodes (V ) and edges (E), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Graph Tracker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>To include low-scored detections for tracking, a low threshold value can be an alternative to top-K. Although it can also achieve good performance if it is small enough as shown in the supplementary material, such detection threshold value is sensitive to the detector's score distribution. As a result, careful calibration is required for different detectors and datasets. In contrast, top-K method is robust to such issues as it is not affected by the score distribution. Since K is the maximum number of objects that the model can track, we set K to be sufficiently larger than the maximum number of people in the dataset (e.g., 100 in MOT16/17; 300 in MOT20). In <ref type="table">Table 7</ref>, we experimentally show the robustness of K values.</p><p>Some tracklets are failed to track for a while when they are invisible due to full occlusion. These missing tracklets are stored for a period of age max and appended to N t1 . Al-though existing MOT works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b37">38]</ref> apply a motion predictor (e.g., Kalman filter <ref type="bibr" target="#b2">[3]</ref>) for predicting the possible location of missing tracklets, SGT can perform longterm association without a motion predictor. Here, we store the tracklets whose length is longer than age min to prevent false positive cases. Graph neural network updates features of nodes (V ) and edges (E) in a graph through the message passing process, as described in <ref type="figure" target="#fig_1">Figure 3</ref>, that propagates the features to the neighboring nodes and edges and then aggregates them. By iterating this process, V now contains the features of both the neighboring nodes and edges, and E indirectly aggregates the features of other edges which are connected to the same node. While the initial edge features represent the pairwise relation of two detections, the iteration of the process allows the updated edge features to represent the higher-order (multi-hop) relation that also considers neighboring detections. Section 3.2 presents more details. Edge classifier is a FC layer that predicts the edge score (ES) from the updated edge features. The edge score is the probability that the connected detections at t1 and t2 refer to the same object. Since n i t1 is connected to many nodes at t2, we use the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref> for optimal matching based on the edge score matrix. As a result, n i t1 has only one edge score which is optimally assigned. Then, the edge threshold (? E ) is used for deciding a positive or negative edge. The yellow box shown in <ref type="figure" target="#fig_0">Figure 2</ref> is the recovered detection that n 3 t2 is negative due to its low detection score, but its connected node, n 1 t1 , and edge (e 1,3 ) are positive. Node classifier is a FC layer that prevents incorrect detection recovery by predicting the node score (N S) from the updated node features. If the recovered detection's node score is below the node threshold (? N ), we decide not to recover it, thus the node stays negative. Otherwise, we confirm recovery of the missed detection and the node becomes positive as shown by n 3 t2 in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Construction and Update</head><p>This section explains design of the node and edge features in SGT. Note that a FC block refers to a stack of FC layer, layer normalization <ref type="bibr" target="#b0">[1]</ref>, and ReLU function Initial node features. Contrary to the graph-based MOT works using reID features of detected objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41]</ref>, SGT exploits the image backbone's visual features (H) which are shared for detection and jointly trained. Initial edge features. Edge feature are denoted as e l i,j , where i and j are the starting and ending node indices respectively, and l indicates iteration. Inspired by MPN-Track <ref type="bibr" target="#b4">[5]</ref>, SGT initializes high-dimensional edge features as Eq. 1.</p><formula xml:id="formula_1">e 0 i, j = f enc x i ? x j , y i ? y j , log( wi wj ), log( hi hj ), IoU i, j , Sim i, j ,<label>(1)</label></formula><p>where [?] is concatenation operator, x and y are the center coordinates, h and w are the height and width of a bounding box, Sim is cosine similarity, and f enc refers to two FC blocks. As the initialized edge features are directionaware, two edges connecting the same nodes but reversely will have different features considering different relations (e.g., t1 ? t2 and t2 ? t1). V t1 and V t2 are updated on two different MLPs with these different edge features. After updating in GNN, these bidirectional edge features are averaged to predict a single edge score. Initial graph, shown in the left of <ref type="figure" target="#fig_1">Figure 3</ref>, is denoted by</p><formula xml:id="formula_2">G 0 = {V 0 , E 0 }, where E 0 = {e 0 i,j | 1 ? i, j ? 2K + |V miss |} is a set of initial edge features and V 0 = V 0 t1 V 0 t2 V 0</formula><p>miss is a set of initial node features at t1, t2, and missing tracklets. Update in node and edge features. <ref type="figure" target="#fig_1">Figure 3</ref> describes two steps to update the features of nodes and edges during the message passing process in GNN. The initial edge features e 0 i, j , shown in the left side of the graph, are pairwise relational features considering only the two connected nodes at t1 and t2 (direction i ? j).</p><p>In Step 1 of <ref type="figure" target="#fig_1">Figure 3</ref>, the edge features are updated as Eq. 2.</p><formula xml:id="formula_3">e l i, j = f e v l?1 i , v l?1 j , e 0 i, j , e l?1 i, j ,<label>(2)</label></formula><p>where f e refers to two FC blocks, l is the number of itera-</p><formula xml:id="formula_4">tions (l ? [1, N iter ]), v i is the features of node i, and v l?1 i</formula><p>indicates the node features of the previous iteration. Therefore, the current state of the two connected nodes, initial and current edge features are concatenated and passed to f e to update edge features as e l i, j . Initial edge features (e 0 i,j ) are concatenated every iteration to prevent the over-smoothing issue in GNN <ref type="bibr" target="#b28">[29]</ref>. Although we use the shared MLPs (f e ) for the edges of two different directions, the edge features of the opposite direction may not be the same since their edge features are encoded in a direction-aware manner.</p><p>In Step 2 of <ref type="figure" target="#fig_1">Figure 3</ref>, node j aggregates the features of the connected nodes and edges as Eq</p><formula xml:id="formula_5">. 3. v l j = f vout 1 |E l :,j | i f venc v l?1 i , e l i, j ,<label>(3)</label></formula><p>where f vout is an FC block, |E l :,j | is the number of edges connected to the node j, f venc refers to two FC blocks, e l i j is the updated edge features in Step 1 (Eq. 2) and v l?1 i is the features of starting node. We suppose the index of N t2 is from 1 to K and N t1 is from K+1 to 2K+|V miss |. When i &gt; j, e i j is the edge features with direction of t1 ? t2. Thus, message passing is from t1 to t2 and V t2 are updated. As our edge features are direction-aware, we use different f venc for message passing t1 ? t2 and t2 ? t1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference Techniques</head><p>SGT is trained by the sum of the detection loss (L D ) and the association loss (L A ). Detection loss. Since we adopt CenterNet <ref type="bibr" target="#b54">[54]</ref> as a detector, we follow <ref type="bibr" target="#b54">[54]</ref> to compute the detection loss which is the weighted sum of losses from three heads as Eq. 4.</p><formula xml:id="formula_6">L D = L score + w size L size + w of f L of f (4)</formula><p>The size head outputs B size composed of (s l , s r , s t , s b ).</p><p>The offset head outputs B of f which is the quantization error of the center coordinates caused by the stride of feature map (e.g., 4). For each ground-truth (GT)</p><formula xml:id="formula_7">objectb i = (x i l ,? i t ,x i r ,? i b ), GT sizeb i size = (? i l ,? i r ,? i t ,? i b ) is computed by the difference between center coordinates (? i x ,? i y ) = (x i l +x i r 2 ,? i t +? i b 2 ) andb i . Each GT sizeb i size is assigned to the prediction b xy size ? B size , where (x, y) = (?? i x 4 ?, ?? i y 4 ?). Each GT offset (? x ,? y ) = (? i x 4 ? ?? i x 4 ?,? i y 4 ? ?? i y 4 ?)</formula><p>is assigned to the prediction b xy of f . Then, l 1 loss is used to compute L size and L of f . For training the score head, GT heatmap M xy ? R H h ?Hw?1 is generated by the Gaussian kernel as Eq. 5.</p><formula xml:id="formula_8">M xy = ND i=1 exp(? (x??? i x 4 ?) 2 )+(y??? i y 4 ?) 2 ) 2? 2 d ),<label>(5)</label></formula><p>where N D is the number of GT object and ? d is computed by width and height of each object <ref type="bibr" target="#b18">[19]</ref>. L score is computed as the pixel-wise logistic regression with the penaltyreduced focal loss <ref type="bibr" target="#b22">[23]</ref>. Association loss. Our association loss is the weighted sum of the edge and node classification losses as Eq. 6.</p><formula xml:id="formula_9">L A = w edge L edge + w node L node<label>(6)</label></formula><p>In SGT, the edge and node classifiers output the edge and node scores (ES and N S), respectively. L edge and L node are computed on these scores with the focal loss <ref type="bibr" target="#b22">[23]</ref>. Since it is difficult to assign GT labels to the edges connecting the background patches, we exclude them in L edge as Eq. 7.</p><formula xml:id="formula_10">L edge = 1 N E + ei,j ?E FL(ES i,j , ey i,j ), if ny i = 1 or ny j = 1; 0 otherwise,<label>(7)</label></formula><p>where N E + is the number of GT edges which at least one of the endpoints is positive, E is a set of edges in G, FL is the focal loss, edge in direction of t1 ? t2, ey i,j is the GT label of edge connecting the nodes n i and n j , and ny i is the GT label of n i . We compute L node only on the node scores at t2 as Eq. 8.</p><formula xml:id="formula_11">L node = 1 N N + t2 nj ?Nt2 FL(NS j , ny j ),<label>(8)</label></formula><formula xml:id="formula_12">where N N + t2</formula><p>is the number of GT positive nodes at t2. We output zero when N E + = 0 or N N + t2 = 0. Node and edge label assignment is an essential step for computing the association loss. While existing GNN-based tracker <ref type="bibr" target="#b4">[5]</ref> trains its matching network using GT objects, we introduce a novel training technique using pseudo labels to effectively train the edge and node classifiers in a single step with a detector and a shared backbone network. The top-K detections are optimally matched with the GT objects based on their IoU score matrix and Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>, and IDs of the GTs are assigned to the matched detections. To prevent the misallocation of GT ID, the assigned IDs are filtered out if IoU of their matching is lower than the threshold (e.g., 0.5). This step is repeated for N t1 and N t2 to assign (ny i and ny j ). Finally, the GT edge label (ey i,j ) is assigned to the edges by matching the IDs of nodes. An edge is labeled as 1 if the two connected nodes have the same GT ID, and 0 otherwise. Adaptive feature smoothing (AdapFS) is a novel inference technique for the proposed detection recovery framework. Following JDE <ref type="bibr" target="#b39">[40]</ref>, recent online TBD models update appearance features of tracklets in an exponential moving average manner as emb trk t2 = ? ? emb trk t1 + (1 ? ?) ? emb det t2 . The features of tracklets are updated by adding the features of new detections with the fixed weight, ?. However, lowscored recovered objects have unreliable appearance features since they may suffer from occlusion or blur. Thus, we incorporate adaptive weight computed by the object scores (S T ) of matched tracklets and detections as Eq. 9.</p><formula xml:id="formula_13">emb trk t2 = emb trk t1 ? St1 St1+St2 + emb det t2 ? St2 St1+St2<label>(9)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>Datasets. We train and evaluate the proposed method using MOT16/17/20 and HiEve Challenge datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref> which target pedestrian tracking. MOT20 and HiEve are complex datasets composed of crowded scenes. On each frame, MOT20 has 170 people on average compared to MOT17 containing 30 people. Due to the small size of the MOT datasets, JDE <ref type="bibr" target="#b39">[40]</ref> introduces the pedestrian detection and reID datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> for training.</p><p>FairMOT <ref type="bibr" target="#b49">[50]</ref> further exploits extra pedestrian detection dataset, CrowdHuman <ref type="bibr" target="#b34">[35]</ref>. We only use CrowdHuman as an extra training dataset to achieve competitive performance. Since CrowdHuman does not have ID labels and is not a video dataset, we assign a unique ID to every object and randomly warp an image to generate a pair of consecutive frames (I t1 -I t2 ). Implementation details. We use CenterNet <ref type="bibr" target="#b54">[54]</ref> pretrained on COCO object detection dataset <ref type="bibr" target="#b23">[24]</ref> to initialize SGT's detector. For fair comparison with <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b38">39]</ref>, we use the image size of 1088 ? 608 and the feature map size (H w ? H h ) of 272 ? 152. Two consecutive frames are randomly sampled in the interval of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>. Following <ref type="bibr" target="#b49">[50]</ref>, random flip, warping and color jittering are selected as data augmentation. The same augmentation is applied to a pair of images. We use Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with a batch size of 12 and initial learning rate (lr) of 2e ?4 which drops to 2e ?5 . There are 60 training epochs and lr is dropped at 50. For training, we use 1 for w of f , 0.1 for w size , w edge , and 10 for w node . For inference, we use 0.5, 0.4 and 0.4 as ? init , ? E and ? N , respectively. These values are chosen empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MOT Challenge Evaluation Results</head><p>We submit our result to the MOT16/17/20 Challenge test server and compare it with the recent online MOT models as shown in <ref type="table" target="#tab_0">Table 1</ref>. Note that the methods using tracklet interpolation as post-processing (e.g., ByteTrack <ref type="bibr" target="#b48">[49]</ref>) are excluded to satisfy online setting. Visualization results are provided in the supplementary material. Evaluation metrics. We use the standard evaluation metrics for 2D MOT <ref type="bibr" target="#b1">[2]</ref>: Multi-object Tracking Accuracy (MOTA), ID F1 Score (IDF1), False Negative (FN), False Positive (FP), and Identity Switch (IDS) <ref type="bibr" target="#b19">[20]</ref>. While MOTA is computed by FP, FN, and IDS, and thus focuses on the detection performance, IDF1 <ref type="bibr" target="#b33">[34]</ref> is a metric focused on tracking performance. Also, mostly tracked targets (MT) and mostly lost targets (ML) represent the ratio of GT trajectories covered by a track hypothesis for at least 80% and at most 20% of their respective life span, respectively. Evaluation results of MOT16/17. Without extra training datasets, MOTA of SGT is higher than CSTrack <ref type="bibr" target="#b21">[22]</ref> and FairMOT <ref type="bibr" target="#b49">[50]</ref>  attributed to the proposed detection recovery mechanism.</p><p>Compared with the models based on the same detector <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b37">38]</ref>, SGT outperforms all of them, except CorrTracker <ref type="bibr" target="#b37">[38]</ref> which shows marginally higher MOTA by 0.1% on MOT17. When we compare SGT with OMC-F <ref type="bibr" target="#b20">[21]</ref> which is also specially designed to track low-scored detections on top of FairMOT <ref type="bibr" target="#b49">[50]</ref>, SGT shows lower FN with lower FP, demonstrating the superiority of our detection recovery mechanism over OMC <ref type="bibr" target="#b20">[21]</ref>. SGT achieves excellent tracking performance on MOT20, while it shows high IDS on MOT17. This is caused by non-human occluders (e.g., vehicles) frequently appearing in MOT17.</p><p>Since pedestrian is the only target class for training the detector, other non-human objects are not included in top-K detections and for relational modeling as well. This leads to inferior tracking performance in MOT17.</p><p>Evaluation results of MOT20. In MOT20, scenes are severely crowded and partial occlusion is dominant compared with MOT16/17. When the same detection threshold (? D ) is employed, existing methods suffer from missed detections caused by less confident detection output. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref> use lower detection threshold for MOT20, yet this results in high FP, IDS and low IDF1 since their pairwise relational features are not strong enough for correctly matching with a large number of tracking candidates. On the other hand, our higher-order relational features allows SGT to effectively address this problem and achieve state-of-theart in MOTA as shown in <ref type="table" target="#tab_0">Table 1</ref>. SGT surpasses Cor-rTracker <ref type="bibr" target="#b37">[38]</ref> in MOTA by 7.6% while it shows higher MOTA than SGT in MOT17. SGT achieves better tradeoff between FN and FP, and higher MOTA and IDF1 than OMC <ref type="bibr" target="#b20">[21]</ref> whose detection recovery method is applied to CSTrack <ref type="bibr" target="#b21">[22]</ref>. Although OMC exploits past frame as temporal cues to carefully select the low-scored detections, its matching is still limited by the pairwise relational features. In contrast, SGT performs matching using the higher-order relational features updated by GNNs, and thus, SGT outperforms OMC in spite of the low-scored detections simply selected by top-K sampling.   <ref type="bibr" target="#b49">[50]</ref> and Cen-terTrack <ref type="bibr" target="#b52">[53]</ref>. SGT can even achieve comparable MOTA and higher IDF1 than CSTrack <ref type="bibr" target="#b21">[22]</ref> which uses extra training datasets following the MOT benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">HiEve Challenge Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>Ablation experiments are conducted by training model on the first half of the MOT17 train dataset and evaluating it on the rest. More analysis on detection recovery and ablation experiments can be found in the supplementary material. Detection recovery. We conduct analysis of our architecture and comparison with another detection recovery  <ref type="figure">Figure 4</ref>. Illustration of long-term association in SGT without Kalman filter <ref type="bibr" target="#b2">[3]</ref>. Each color indicates the unique ID.     Effectiveness of AdapFS. With the fixed weight, IDF1 is slightly decreased as shown in <ref type="table" target="#tab_6">Table 5</ref>. On the other hand, IDF1 increases and tracking performance is improved based on our proposed adaptive feature smoothing in SGT. Long-term association. As shown in <ref type="table">Table 6</ref>, introducing long-term association into SGT significantly increases IDF1. We use age min of 10 frames so that only the stable tracklets are stored and false positive recovery cases are avoided as much as possible. When the objects are fully occluded as shown in <ref type="figure">Figure 4</ref>, SGT fails to track and recover them. However, SGT can match them without a motion model when they reappear. Robustness of K value. We validate the robustness of K in SGT by training with different K values (e.g., 100, 300, 500) and inference with unseen K values (e.g., 50, 300, 500) using the model trained with K = 100. As shown in <ref type="table">Table 7</ref>, consistent tracking performance is observed across different K values for training and unseen K values. Memory and time consumption for training and inference speed (FPS) are only slightly affected by increasing K. Number of GNN iterations. As shown in <ref type="table" target="#tab_1">Table 8</ref>, FN is much higher when GNN is not yet used for updating edge and node features. Also, more GNN iteration improves MOTA and IDF1. This trend proves that the higher-order relational features are more effective in learning spatiotemporal consistency to perform detection recovery than the pairwise relational features (N iter = 0). Design of edge feature. As stated in Eq. 1 of the main paper, we use difference of center coordinates, ratio of width and height, IoU, and cosine similarity to initialize the edge features. Here, both position and appearance relational features are included in the edge features. As shown in <ref type="table" target="#tab_9">Table 9</ref>, SGT achieves the best by utilizing all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>Partial occlusion in a video leads to low-confident detection output. Existing online MOT models suffer from missed detections since they use only high-scored detections for tracking. This paper presents SGT, a novel online graph tracker that is jointly trained with a detector and recovers the missed detections by tracking top-K scored detections. We also show that pseudo labeling is critical to training SGT and adaptive feature smoothing is a simple but effective inference technique. SGT captures the object-level spatio-temporal consistency in video data by exploiting higher-order relational features of objects and background patches from the current and past frames. SGT outperforms recent online MOT models in MOTA on MOT16/17/20, but particularly shows a large improvement in MOTA on MOT20 which is vulnerable to missed detections due to occlusion caused by severe crowdedness. Our effective detection recovery method contributes to the outstanding performance of SGT as demonstrated in the extensive ablation experiments. Future works will exploit longer temporal cues and model the spatio-temporal relationship of non-human objects (e.g., vehicles). We hope SGT inspires MOT community to further explore graph tracker and detection recovery by tracking framework for online setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>We use shorter epochs when CrowdHuman dataset is not included but only MOT or HiEve datasets are used for training. The models are trained for 30 epochs and the learning rate is dropped from 2e ?4 to 2e ?5 at 20 epoch. The same loss weights are adopted, but we use w edge as 1, instead of 0.1. For inference, the same threshold values are used regardless of whether CrowdHuman <ref type="bibr" target="#b34">[35]</ref> dataset is used or not. In the ablation experiments, we deploy the detection threshold values of (? init , ? D , ? D low ) as (0.5, 0.4, 0.2) for FairMOT <ref type="bibr" target="#b49">[50]</ref> and BYTE <ref type="bibr" target="#b48">[49]</ref> following the official code of ByteTrack * .</p><p>Our implementation is based on detectron2 framework ? . Regarding training time of the ablation experiments, with two NVIDIA V100 GPUs, 3 hours are spent on training SGT without CrowdHuman dataset, while 2 days are taken if it is included. We observe that the inference speed is affected by the version of NVIDIA driver and the number of CPUs. The reported inference speed is measured on NVIDIA driver version of 460.73.01 with CUDA version of 10.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. MOT Detection Challenge Evaluation Results</head><p>For evaluation metrics of MOT17/20 Detection benchmarks, we choose precision, recall, F1, and average precision (AP) <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="table" target="#tab_0">Table 10</ref>, SGT achieves the best in every metric on MOT17/20 Detection benchmarks. SGT outperforms GSDT <ref type="bibr" target="#b38">[39]</ref> which is also based on Cen-terNet <ref type="bibr" target="#b54">[54]</ref> and GNNs. As stated in Section 2.2 of the main paper, GNNs in GSDT aggregate the current and past features to enhance the current image features. However, relational features used for association in GSDT are still limited to pairwise features while relational features in SGT are updated by GNNs to become multi-hop features. Consequently, low-scored detections are not tracked in GSDT. The high recall supports the effectiveness of detection recovery in SGT. On the other hand, the high precision indicates that detection recovery is achieved without introducing extra false positives. <ref type="figure">Figure 5</ref> shows qualitative comparison between SGT and others <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> using the same detector. In MOT17, * https://github.com/ifzhang/ByteTrack ? https://github.com/facebookresearch/detectron2 the partially occluded people have low detection confidence score and they are not used as tracking candidates in Fair-MOT, GSDT, and CorrTracker since they only use highscored detections for tracking. As a result, these occluded people are missed. In MOT20, existing methods use lower detection threshold (? D ) than MOT17 due to frequent occlusions in MOT20. However, they suffer from FPs while SGT does not have such FPs without missed detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Visualization Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional Ablation Experiments</head><p>Top-K vs Detection threshold. In <ref type="table">Table 6</ref> of the main paper, the robustness of top-K sampling in SGT is shown by the consistent performance with a wide range of K values and different K values for training and inference. Here, we experiment about using a low value of detection threshold, ? D , that is an alternative option for choosing tracking candidates instead of top-K sampling. According to <ref type="table" target="#tab_0">Table 11</ref>, when SGT is trained with tracking candidates sampled by top-K whose K is 100 or 300, it shows the consistent performance with both K = {50, 300} and ? D = {0.01, 0.1} while marginal degradation is ob- <ref type="figure">Figure 5</ref>. Qualitative comparison of CenterNet <ref type="bibr" target="#b54">[54]</ref> based online JDT models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51]</ref> on the MOT17/20 test datasets. served with ? D = 0.3. On the other hand, large drop in MOTA and IDF1 is shown when SGT is trained with ? D = 0.01 and evaluated with ? D = 0.3. In contrast, SGT trained with ? D = 0.1 shows the consistent performance when ? D = 0.3 is used for selecting tracking candidates for inference. Based on the results, detection threshold can be viewed as the hyperparameter that should be carefully tuned. Although K is also the hyperparameter, K is more intuitive value representing the maximum number of objects to be tracked and is easier to be decided than ? D which is affected by many factors (e.g., model architecture, training method). For this reason, we adopt top-K sampling method to include low-scored detections in SGT. Different backbone networks. <ref type="table" target="#tab_0">Table 12</ref> shows the performance comparison of SGT and FairMOT <ref type="bibr" target="#b49">[50]</ref> with different backbone networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b27">28]</ref> used in Center-Net <ref type="bibr" target="#b54">[54]</ref>. For hourglass backbone, we use the image size of (H, W ) = (640, 1152), instead of (608, 1088) since only a multiple of 128 is allowed. SGT achieves lower FN and ML, and higher MT and MOTA than FairMOT across all backbone networks. In other words, SGT has less missed detections and more long-lasting tracklets than FairMOT.  This result is corresponding to our motivation of detection recovery by tracking in SGT. Especially, SGT shows larger improvement in MOTA with the small backbone networks (e.g., resnet18 and dla34). When MOT models are deployed with the limited resource of hardware, SGT can be served as an effective solution.</p><p>The number of edges. In SGT, nodes across frames are sparsely connected if only they are close in either Euclidean or feature space. Specifically, n i t1 is connected to the nodes of N t2 using three criteria: center distance, IoU, and cosine similarity. We choose nodes for each criterion and remove the duplicates. <ref type="table" target="#tab_0">Table 13</ref> shows the result of experimenting with different number of nodes for each criterion. Although the best performance is achieved with 10, there is only marginal decrease in MOTA and IDF1 with 5 and 20. Thus, this is also robust hyperparameter. Sensitivity of ? init and ? E . The robustness of K, which is the number of tracking candidates, has been shown through the extensive ablation experiments. Here, we measure the sensitivity of ? init and ? E as well. As shown by <ref type="figure" target="#fig_2">Figure 6</ref>, ? init is a sensitive threshold value since it decides initialization of new tracklets. On the other hand, ? E is the minimum edge score used for matching. It is robust within the range between 0.2 and 0.4 since correct matching may have high edge score and the node classifier prevents false positive matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Detection Recovery</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Ratio of Recovered Detections</head><p>According to <ref type="table" target="#tab_0">Table 14</ref>, MOT17-08 and MOT20-08 are the sequences that SGT outputs the highest ratio of recovered detections whose confidence score is lower than ? init = 0.5. <ref type="table" target="#tab_0">Table 15</ref> shows that SGT surpasses Cor-rTracker in MOTA by 2.7% in MOT17-08 while SGT shows lower MOTA than CorrTracker in MOT17-06 whose recovery ratio is low. In MOT20, similar trend is observed that SGT achieves larger improvement of MOTA in MOT20-08 than MOT20-07.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Recall per Visibility Level</head><p>We measure the recall ratio for different visibility levels of objects and compare them of different models as shown in <ref type="figure">Figure 7</ref>. Both BYTE <ref type="bibr" target="#b48">[49]</ref> and SGT show higher recall value for low visibility levels than FairMOT <ref type="bibr" target="#b49">[50]</ref> since they perform association of low-scored detections. When objects are almost invisible with the visibility in the range of (0.0, 0.3), SGT outperforms BYTE in terms of the recall. These results indicate that SGT successfully tracks the low-scored detections caused by occlusion, and SGT is robust against partial occlusion. Also, the effectiveness of node classifier preventing false positive recovery is demonstrated through higher precision value of SGT than that of BYTE. <ref type="figure">Figure 8</ref> shows the cases of detection recovery in the MOT20 test dataset. In the first row, people indicated by the blue and brown bounding boxes occlude each other. In the frame #33, their detection scores are higher than ? init which is detection threshold value to initialize new tracklet. However, from frame #34 to #37, their detection scores are between 0.3 or 0.4 which are lower than ? init , nevertheless, SGT successfully tracks them. If only high-scored detections are used for association, they are failed to track, leading to missed detections and disconnected tracklets. The second row of <ref type="figure">Figure 8</ref> is another example of detection recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Visualization of Recovered Detections</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>High IDS in MOT16/17. In Section 4.2 of the main paper, we stated that non-human occluders in MOT16/17 result in high IDS in MOT16/17 compared to low IDS in MOT20. <ref type="figure" target="#fig_3">Figure 9</ref> shows the example whose video is taken from a department store, where non-human occluders commonly exist. <ref type="figure">Figure 7</ref>. Recall ratio comparison of FairMOT <ref type="bibr" target="#b49">[50]</ref>, BYTE <ref type="bibr" target="#b48">[49]</ref> on top of FairMOT <ref type="bibr" target="#b49">[50]</ref>, and SGT for different visibility levels of objects in MOT17 validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOT20-08-#33</head><p>MOT20-08-#34 MOT20-08-#35 MOT20-08-#37 MOT20-08-#36 MOT20-08-#82 MOT20-08-#95 MOT20-08-#96 MOT20-08-#108 MOT20-08-#104 <ref type="figure">Figure 8</ref>. Detection recovery cases in MOT20 test dataset <ref type="bibr" target="#b9">[10]</ref>. We show the annotation of each bounding box in the format of "{id}-{detection score}". The tracklets in the red circles are recovered in the next frames. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of detection recovery in the inference pipeline of SGT. (S1) Top-K scored detections and their features are extracted from It1 and It2. The red boxes indicate positive detection that contains ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of message passing in a GNN. Initial edge features, E 0 , are updated to E 1 containing the features of two connected nodes, V 0 t1 and V 0 t2 . Initial node features, V 0 , are then updated to V 1 containing the features of connected nodes, V 0 , and the updated edges features, E 1 . For simplicity, we omit bidirectional connections and show only few edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Sensitivity analysis of ?E and ?init.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Example of ID switch caused by non-human occluder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>by about 3%, and it is comparable with FairMOT trained with extra training datasets. With CrowdHuman as an extra training dataset, SGT achieves the highest MOTA on MOT16/17 based on the best tradeoff between FP and FN. The highest MT indicates that SGT generates stable and long-lasting tracklets which is Evaluation results of ours and recent online JDT models on the MOT16/17/20 benchmarks (private detection). OMC-F<ref type="bibr" target="#b20">[21]</ref> applies its method on FairMOT<ref type="bibr" target="#b49">[50]</ref>. For each metric, the best is bolded and the second best is underlined. The values not provided are filled by "-". ? indicates no extra training dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">MOTA? IDF1? MT? ML?</cell><cell>FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell></cell><cell></cell><cell cols="2">MOT16 [27]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QDTrack [30]  ?</cell><cell>69.8</cell><cell>67.1</cell><cell cols="2">41.6 19.8</cell><cell>9861</cell><cell>44050</cell><cell>1097</cell></row><row><cell>TraDes [43]</cell><cell>70.1</cell><cell>64.7</cell><cell cols="2">37.3 20.0</cell><cell>8091</cell><cell>45210</cell><cell>1144</cell></row><row><cell>CSTrack [22]  ?</cell><cell>71.3</cell><cell>68.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1356</cell></row><row><cell>SGT (Ours)  ?</cell><cell>74.1</cell><cell>71.0</cell><cell cols="2">43.6 15.8</cell><cell>9784</cell><cell>35946</cell><cell>1528</cell></row><row><cell>GSDT [39]</cell><cell>74.5</cell><cell>68.1</cell><cell cols="2">41.2 17.3</cell><cell>8913</cell><cell>36428</cell><cell>1229</cell></row><row><cell>FairMOT [50]</cell><cell>74.9</cell><cell>72.8</cell><cell cols="2">44.7 15.9</cell><cell>-</cell><cell>-</cell><cell>1074</cell></row><row><cell>CSTrack [22]</cell><cell>75.6</cell><cell>73.3</cell><cell cols="2">42.8 16.5</cell><cell>9646</cell><cell>33777</cell><cell>1121</cell></row><row><cell>RelationTrack [46]</cell><cell>75.6</cell><cell>75.8</cell><cell cols="2">43.1 21.5</cell><cell>9786</cell><cell>34214</cell><cell>448</cell></row><row><cell>OMC [21]</cell><cell>76.4</cell><cell>74.1</cell><cell cols="2">46.1 13.3</cell><cell>10821</cell><cell>31044</cell><cell>-</cell></row><row><cell>CorrTracker [38]</cell><cell>76.6</cell><cell>74.3</cell><cell cols="2">47.8 13.3</cell><cell>10860</cell><cell>30756</cell><cell>979</cell></row><row><cell>SGT (Ours)</cell><cell>76.8</cell><cell>73.5</cell><cell cols="2">49.3 10.5</cell><cell>10695</cell><cell>30394</cell><cell>1276</cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17 [27]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTracker [31]  ?</cell><cell>66.6</cell><cell>57.4</cell><cell cols="2">32.2 24.2</cell><cell cols="3">22284 160491 5529</cell></row><row><cell>CenterTrack [53]</cell><cell>67.8</cell><cell>64.7</cell><cell cols="2">34.6 24.6</cell><cell cols="3">18489 160332 3039</cell></row><row><cell>QDTrack [30]  ?</cell><cell>68.7</cell><cell>66.3</cell><cell cols="2">40.6 21.9</cell><cell cols="3">26598 146643 3378</cell></row><row><cell>TraDes [43]</cell><cell>69.1</cell><cell>63.9</cell><cell cols="2">36.4 21.5</cell><cell cols="3">20892 150060 3555</cell></row><row><cell>FairMOT [50]  ?</cell><cell>69.8</cell><cell>69.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3996</cell></row><row><cell>SOTMOT [51]</cell><cell>71.0</cell><cell>71.9</cell><cell cols="2">42.7 15.3</cell><cell cols="3">39537 118983 5184</cell></row><row><cell>GSDT [39]</cell><cell>73.2</cell><cell>66.5</cell><cell cols="2">41.7 17.5</cell><cell cols="3">26397 120666 3891</cell></row><row><cell>SGT (Ours)  ?</cell><cell>73.2</cell><cell>70.2</cell><cell cols="2">42.0 17.7</cell><cell cols="3">25332 121155 4809</cell></row><row><cell>FairMOT [50]</cell><cell>73.7</cell><cell>72.3</cell><cell cols="2">43.2 17.3</cell><cell cols="3">27507 117477 3303</cell></row><row><cell>RelationTrack [46]</cell><cell>73.8</cell><cell>74.7</cell><cell cols="2">41.7 23.2</cell><cell cols="3">27999 118623 1374</cell></row><row><cell>TransTrack [37]</cell><cell>74.5</cell><cell>63.9</cell><cell cols="2">46.8 11.3</cell><cell cols="3">28323 112137 3663</cell></row><row><cell>OMC-F [21]</cell><cell>74.7</cell><cell>73.8</cell><cell cols="2">44.3 15.4</cell><cell cols="2">30162 108556</cell><cell>-</cell></row><row><cell>CSTrack [22]</cell><cell>74.9</cell><cell>72.3</cell><cell cols="2">41.5 17.5</cell><cell cols="3">23847 114303 3567</cell></row><row><cell>OMC [21]</cell><cell>76.3</cell><cell>73.8</cell><cell cols="2">44.7 13.6</cell><cell cols="2">28894 101022</cell><cell>-</cell></row><row><cell>SGT (Ours)</cell><cell>76.4</cell><cell>72.8</cell><cell cols="2">48.0 11.7</cell><cell cols="3">25974 102885 4101</cell></row><row><cell>CorrTracker [38]</cell><cell>76.5</cell><cell>73.6</cell><cell cols="2">47.6 12.7</cell><cell>29808</cell><cell>99510</cell><cell>3369</cell></row><row><cell></cell><cell></cell><cell cols="2">MOT20 [10]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FairMOT [50]</cell><cell>61.8</cell><cell>67.3</cell><cell>68.8</cell><cell>7.6</cell><cell cols="2">103440 88901</cell><cell>5243</cell></row><row><cell>TransTrack [37]</cell><cell>64.5</cell><cell>59.2</cell><cell cols="2">49.1 13.6</cell><cell cols="3">28566 151377 3565</cell></row><row><cell>SGT (Ours)  ?</cell><cell>64.5</cell><cell>62.7</cell><cell cols="2">62.7 10.2</cell><cell cols="3">67352 111201 4909</cell></row><row><cell>CorrTracker [38]</cell><cell>65.2</cell><cell>69.1</cell><cell>66.4</cell><cell>8.9</cell><cell>79429</cell><cell>95855</cell><cell>5183</cell></row><row><cell>CSTrack [22]</cell><cell>66.6</cell><cell>68.6</cell><cell cols="2">50.4 15.5</cell><cell cols="3">25404 144358 3196</cell></row><row><cell>GSDT [39]</cell><cell>67.1</cell><cell>67.5</cell><cell cols="2">53.1 13.2</cell><cell cols="3">31913 135409 3131</cell></row><row><cell>RelationTrack [46]</cell><cell>67.2</cell><cell>70.5</cell><cell>62.2</cell><cell>8.9</cell><cell cols="3">61134 104597 4243</cell></row><row><cell>SOTMOT [51]</cell><cell>68.6</cell><cell>71.4</cell><cell>64.9</cell><cell>9,7</cell><cell cols="3">57064 101154 4209</cell></row><row><cell>OMC [21]</cell><cell>70.7</cell><cell>67.8</cell><cell cols="2">56.6 13.3</cell><cell cols="2">22689 125039</cell><cell>-</cell></row><row><cell>SGT (Ours)</cell><cell>72.8</cell><cell>70.6</cell><cell cols="2">64.3 12.7</cell><cell cols="3">25161 112963 2474</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>38] and</cell></row></table><note>further supports the importance of higher-order relational features. Inference speed. We measure the inference speed in terms of frames-per-second (FPS) using a single V100 GPU. SGT runs at 23.0/23.0/19.9 FPS on MOT16/17/20, respec- tively. For fair comparison, we select the methods report- ing FPS measured on the same GPU. CorrTracker [</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>compares ours and online JDT models on the HiEve Challenge. Without extra training datasets, SGT achieves 47.2 MOTA and 53.7 IDF1. Under the condition without extra datasets, SGT shows a large improvement in both MOTA and IDF1 compared to FairMOT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results of ours and recent online JDT models on the HiEve benchmark (private detection). For each metric, the best is bolded and the second best is underlined. ? indicates no extra training dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">MOTA? IDF1? MT? ML? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>DeepSORT [42]  ?</cell><cell>27.1</cell><cell>28.5</cell><cell>8.4</cell><cell cols="2">41.4 5894 42668 3122</cell></row><row><cell>JDE [40]  ?</cell><cell>33.1</cell><cell>36.0</cell><cell cols="3">15.1 24.1 6318 43577 3747</cell></row><row><cell>FairMOT [50]  ?</cell><cell>35.0</cell><cell>46.7</cell><cell cols="3">16.3 44.2 6523 37750</cell><cell>995</cell></row><row><cell>CenterTrack [53]  ?</cell><cell>40.9</cell><cell>45.1</cell><cell cols="3">10.8 32.2 3208 36414 1568</cell></row><row><cell>NewTracker [36]</cell><cell>46.4</cell><cell>43.2</cell><cell cols="3">26.3 30.8 4667 30489 2133</cell></row><row><cell>SGT (Ours)  ?</cell><cell>47.2</cell><cell>53.7</cell><cell cols="3">24.0 28.8 4699 30727 1361</cell></row><row><cell>CSTrack [22]</cell><cell>48.6</cell><cell>51.4</cell><cell cols="3">20.4 33.5 2366 31933 1475</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of the detection recovery (DR) method. DR-GNN denotes DR by tracking using the edge features updated by GNNs. NC denotes the node classifier that filtering out recovered detections using the node classifier. BG denotes that top-K scored detections of It1 are used as Nt1.</figDesc><table><row><cell>Model</cell><cell cols="6">DR-GNN NC BG MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>70.7</cell><cell>73.3</cell><cell cols="2">49.9 3400 11794</cell><cell>619</cell></row><row><cell>SGT (Ours)</cell><cell>?</cell><cell>?</cell><cell></cell><cell>70.8</cell><cell>73.3</cell><cell cols="2">45.4 1952 13074</cell><cell>750</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>71.3</cell><cell>73.8</cell><cell cols="2">46.6 2190 12742</cell><cell>588</cell></row><row><cell>FairMOT [50]</cell><cell></cell><cell></cell><cell></cell><cell>69.6</cell><cell>72.5</cell><cell cols="2">44.0 2681 13341</cell><cell>414</cell></row><row><cell>BYTE [49]</cell><cell></cell><cell></cell><cell></cell><cell>69.7</cell><cell>73.3</cell><cell cols="2">47.5 3638 12347</cell><cell>400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of training techniques. J and P denote joint training and pseudo labeling, respectively. BYTE<ref type="bibr" target="#b48">[49]</ref>, and the results are shown inTable 3. Spatio-temporal relational modeling with top-K detections of the previous frame benefits SGT to extract discriminative edge features. Otherwise, tracking performance is degraded with high IDS. In SGT, low-scored detections are also used for tracking and can be recovered, but FP recovery could be occurred. The node classifier verifies recovery cases with the updated node features and reduces FP. When we compare DR-GNN with BYTE [49] applied on FairMOT<ref type="bibr" target="#b49">[50]</ref>, DR-GNN achieves better trade-off between FP and FN, and consequently, higher MOTA. This demonstrates the effectiveness of edge features updated in the GNN.</figDesc><table><row><cell cols="4">P J MOTA? IDF1? MT?</cell><cell>FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell></cell><cell>38.5</cell><cell>55.8</cell><cell cols="4">54.3 21678 10136 1418</cell></row><row><cell>?</cell><cell>69.1</cell><cell>69.7</cell><cell>47.2</cell><cell cols="2">3478 12360</cell><cell>847</cell></row><row><cell>? ?</cell><cell>71.3</cell><cell>73.8</cell><cell>46.6</cell><cell cols="2">2190 12742</cell><cell>588</cell></row><row><cell>method,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Training strategy. According to Table 4, pseudo labeling based on top-K detections is an important training tech- nique for SGT. Both object-object and object-background pairs are included in edge labels by employing top-K de-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Effectiveness of adaptive feature smoothing (AdapFS).</figDesc><table><row><cell cols="5">Feature Soothing Adaptive Weight MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell></cell><cell></cell><cell>71.3</cell><cell>73.1</cell><cell cols="2">46.9 2178 12724</cell><cell>590</cell></row><row><cell>?</cell><cell></cell><cell>71.4</cell><cell>72.9</cell><cell cols="2">46.9 2170 12713</cell><cell>589</cell></row><row><cell>?</cell><cell>?</cell><cell>71.3</cell><cell>73.8</cell><cell cols="2">46.6 2190 12742</cell><cell>588</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Ablation study of the long-term association in SGT. The unit of age max is second while that of age min is frame. Robustness of K. Memory and time taken for training, MOT performance and speed are measured on different K values.</figDesc><table><row><cell cols="5">age max age min MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>0</cell><cell>0</cell><cell>70.5</cell><cell>63.1</cell><cell cols="2">45.7 2043 13048</cell><cell>833</cell></row><row><cell>1</cell><cell>0</cell><cell>71.2</cell><cell>72.9</cell><cell cols="2">47.2 2374 12599</cell><cell>627</cell></row><row><cell>1</cell><cell>1</cell><cell>71.2</cell><cell>73.8</cell><cell cols="2">47.2 2341 12620</cell><cell>587</cell></row><row><cell>1</cell><cell>10</cell><cell>71.3</cell><cell>73.8</cell><cell cols="2">46.6 2190 12742</cell><cell>588</cell></row><row><cell cols="7">K (Train) K (Test) MOTA? IDF1? FPS? Memory (GB) Time (hr)</cell></row><row><cell>100</cell><cell>50</cell><cell>71.3</cell><cell>73.8</cell><cell>23.6</cell><cell>13.5</cell><cell>3</cell></row><row><cell>100</cell><cell>100</cell><cell>71.3</cell><cell>73.8</cell><cell>23.5</cell><cell>13.5</cell><cell>3</cell></row><row><cell>100</cell><cell>300</cell><cell>71.5</cell><cell>72.8</cell><cell>22.2</cell><cell>13.5</cell><cell>3</cell></row><row><cell>100</cell><cell>500</cell><cell>71.4</cell><cell>72.9</cell><cell>21.8</cell><cell>13.5</cell><cell>3</cell></row><row><cell>300</cell><cell>300</cell><cell>71.2</cell><cell>75.2</cell><cell>22.0</cell><cell>14.4</cell><cell>3.4</cell></row><row><cell>500</cell><cell>500</cell><cell>71.8</cell><cell>73.4</cell><cell>21.9</cell><cell>15.6</cell><cell>3.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Effect of the number of GNN iterations.</figDesc><table><row><cell cols="4">N iter MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>0</cell><cell>67.3</cell><cell>71.2</cell><cell cols="2">42.2 2538 14547</cell><cell>578</cell></row><row><cell>1</cell><cell>70.9</cell><cell>73.2</cell><cell cols="2">46.0 2572 12571</cell><cell>604</cell></row><row><cell>2</cell><cell>71.0</cell><cell>73.4</cell><cell cols="2">48.4 2578 12516</cell><cell>583</cell></row><row><cell>3</cell><cell>71.3</cell><cell>73.8</cell><cell cols="2">46.6 2190 12742</cell><cell>588</cell></row><row><cell cols="6">tections as pseudo labels. Training with object-background</cell></row><row><cell cols="6">pairs as extra negative examples effectively supervises SGT</cell></row><row><cell cols="6">not to false positively match. Jointly training detector and</cell></row><row><cell cols="6">tracker leads to better performance, instead of using frozen</cell></row><row><cell cols="5">backbone which is pretrained with detector.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Effect of choosing different relational features for initializing edge feature.</figDesc><table><row><cell cols="3">Combination of features MOTA? IDF1? FP? FN? IDS?</cell></row><row><cell>x, y, w, h, IoU, Sim</cell><cell>71.3</cell><cell>73.8 2190 12742 588</cell></row><row><cell>x, y, w, h, IoU</cell><cell>69.9</cell><cell>71.5 1953 13508 821</cell></row><row><cell>x, y, w, h</cell><cell>70.4</cell><cell>70.6 2113 13205 704</cell></row><row><cell>x, y</cell><cell>69.6</cell><cell>70.3 1691 13899 837</cell></row><row><cell>IoU, Sim</cell><cell>69.6</cell><cell>72.1 2066 13272 640</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Evaluation results of the MOT17/20 Detection benchmarks.</figDesc><table><row><cell>Benchmark</cell><cell cols="2">Method</cell><cell cols="4">AP? Recall? Precision? F1?</cell></row><row><cell></cell><cell cols="3">FRCNN [33] 0.72</cell><cell>77.3</cell><cell>89.8</cell><cell>83.1</cell></row><row><cell>MOT</cell><cell cols="2">GSDT [39]</cell><cell>0.89</cell><cell>90.7</cell><cell>87.8</cell><cell>89.2</cell></row><row><cell>17Det</cell><cell cols="2">YTLAB [6]</cell><cell>0.89</cell><cell>91.3</cell><cell>86.2</cell><cell>88.7</cell></row><row><cell></cell><cell cols="3">SGT (ours) 0.90</cell><cell>93.1</cell><cell>92.5</cell><cell>92.8</cell></row><row><cell>MOT 20Det</cell><cell cols="3">ViPeD20 [7] 0.80 GSDT [39] 0.81 SGT (ours) 0.90</cell><cell>86.5 88.6 91.6</cell><cell>68.1 90.6 92.6</cell><cell>76.2 89.6 92.1</cell></row><row><cell cols="7">Table 11. Ablation study of threshold and top-K for choosing de-</cell></row><row><cell cols="7">tections for tracking candidates. CrowdHuman dataset [35] is ad-</cell></row><row><cell cols="3">ditionally used for training.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>train</cell><cell>inference</cell><cell cols="4">MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>K = 300</cell><cell>K = 300</cell><cell>73.8</cell><cell>74.7</cell><cell cols="3">52.5 2620 11047</cell><cell>476</cell></row><row><cell>K = 300</cell><cell>K = 50</cell><cell>73.8</cell><cell>74.0</cell><cell cols="3">52.5 2531 11159</cell><cell>474</cell></row><row><cell cols="2">K = 300 ? D = 0.01</cell><cell>74.1</cell><cell>74.9</cell><cell cols="3">53.4 2492 11050</cell><cell>459</cell></row><row><cell>K = 300</cell><cell>? D = 0.1</cell><cell>73.5</cell><cell>73.4</cell><cell cols="3">51.6 2331 11398</cell><cell>604</cell></row><row><cell>K = 300</cell><cell>? D = 0.3</cell><cell>72.8</cell><cell>72.4</cell><cell cols="3">50.7 2256 11605</cell><cell>843</cell></row><row><cell>K = 100</cell><cell>K = 300</cell><cell>74.1</cell><cell>76.5</cell><cell cols="3">53.1 2544 10971</cell><cell>460</cell></row><row><cell>K = 100</cell><cell>K = 50</cell><cell>74.2</cell><cell>75.9</cell><cell cols="3">53.1 2505 11000</cell><cell>458</cell></row><row><cell cols="2">K = 100 ? D = 0.01</cell><cell>74.2</cell><cell>76.2</cell><cell cols="3">53.1 2530 10961</cell><cell>451</cell></row><row><cell>K = 100</cell><cell>? D = 0.1</cell><cell>74.0</cell><cell>75.9</cell><cell cols="3">52.2 2391 11146</cell><cell>538</cell></row><row><cell>K = 100</cell><cell>? D = 0.3</cell><cell>73.3</cell><cell>73.6</cell><cell cols="3">51.0 2348 11313</cell><cell>795</cell></row><row><cell cols="2">? D = 0.01 K = 300</cell><cell>74.4</cell><cell>76.0</cell><cell cols="3">53.4 2542 10774</cell><cell>498</cell></row><row><cell>? D = 0.01</cell><cell>K = 50</cell><cell>74.3</cell><cell>77.2</cell><cell cols="3">53.1 2475 10920</cell><cell>485</cell></row><row><cell cols="2">? D = 0.01 ? D = 0.01</cell><cell>74.4</cell><cell>76.2</cell><cell cols="3">53.4 2543 10780</cell><cell>500</cell></row><row><cell cols="2">? D = 0.01 ? D = 0.1</cell><cell>73.2</cell><cell>73.1</cell><cell cols="3">52.2 2438 11034 1002</cell></row><row><cell cols="2">? D = 0.01 ? D = 0.3</cell><cell>71.9</cell><cell>71.4</cell><cell cols="3">51.0 2348 11344 1480</cell></row><row><cell>? D = 0.1</cell><cell>K = 300</cell><cell>73.9</cell><cell>76.0</cell><cell cols="3">52.5 2851 10764</cell><cell>470</cell></row><row><cell>? D = 0.1</cell><cell>K = 50</cell><cell>74.0</cell><cell>76.0</cell><cell cols="3">53.1 2807 10745</cell><cell>484</cell></row><row><cell cols="2">? D = 0.1 ? D = 0.01</cell><cell>74.0</cell><cell>75.7</cell><cell cols="3">52.5 2842 10722</cell><cell>496</cell></row><row><cell>? D = 0.1</cell><cell>? D = 0.1</cell><cell>73.9</cell><cell>75.5</cell><cell cols="3">52.8 2849 10789</cell><cell>486</cell></row><row><cell>? D = 0.1</cell><cell>? D = 0.3</cell><cell>73.7</cell><cell>74.5</cell><cell cols="3">52.5 2885 10804</cell><cell>541</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Performance comparison of FairMOT<ref type="bibr" target="#b49">[50]</ref> and SGT with different backbone networks: ResNet-18/101<ref type="bibr" target="#b14">[15]</ref>, DLA-34<ref type="bibr" target="#b46">[47]</ref>, and hourglass-104<ref type="bibr" target="#b27">[28]</ref>. The models are trained using the extra CrowdHuman<ref type="bibr" target="#b34">[35]</ref> dataset.</figDesc><table><row><cell>Model</cell><cell cols="4">Backbone MOTA? IDF1? MT? ML? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>FairMOT [50]</cell><cell>Res-18</cell><cell>66.1</cell><cell>69.9</cell><cell cols="2">40.1 20.4 2036 16029</cell><cell>265</cell></row><row><cell>SGT (ours)</cell><cell>Res-18</cell><cell>68.4</cell><cell>69.3</cell><cell cols="2">47.2 15.6 2359 14086</cell><cell>659</cell></row><row><cell>FairMOT [50]</cell><cell>Res-101</cell><cell>70.2</cell><cell>72.2</cell><cell cols="2">47.8 14.5 2545 13178</cell><cell>364</cell></row><row><cell>SGT (ours)</cell><cell>Res-101</cell><cell>71.1</cell><cell>72.4</cell><cell cols="2">53.4 12.4 3197 11678</cell><cell>720</cell></row><row><cell>FairMOT [50]</cell><cell>DLA-34</cell><cell>72.2</cell><cell>74.7</cell><cell cols="2">47.8 18.0 2660 12025</cell><cell>336</cell></row><row><cell>SGT (ours)</cell><cell>DLA-34</cell><cell>74.2</cell><cell>76.3</cell><cell cols="2">53.1 13.3 2514 10978</cell><cell>451</cell></row><row><cell>FairMOT [50]</cell><cell>HG-104</cell><cell>74.4</cell><cell>77.1</cell><cell cols="2">54.0 12.1 2636 10844</cell><cell>344</cell></row><row><cell>SGT (ours)</cell><cell>HG-104</cell><cell>74.8</cell><cell>77.1</cell><cell cols="2">56.0 10.9 2713 10454</cell><cell>428</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Effect of the number of the edges for each criterion. Center distance, IoU, and cosine similarity are three criteria used in SGT.</figDesc><table><row><cell cols="4">#edges per criterion MOTA? IDF1? MT? FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell>5</cell><cell>71.0</cell><cell>72.8</cell><cell cols="2">47.2 2450 12590</cell><cell>624</cell></row><row><cell>10</cell><cell>71.3</cell><cell>73.8</cell><cell cols="2">46.6 2190 12742</cell><cell>588</cell></row><row><cell>20</cell><cell>71.1</cell><cell>73.6</cell><cell cols="2">47.8 2308 12535</cell><cell>755</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .Table 15 .</head><label>1415</label><figDesc>Ratio of recovered detections over all detections in each sequence of MOT17/20 test datasets. Evaluation result per sequence of MOT17/20 test dataset. We choose one with high recovery ratio and the other one with low recovery ratio.</figDesc><table><row><cell cols="2">Benchmark</cell><cell>Sequence</cell><cell></cell><cell cols="3">Recovery Ratio (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-01</cell><cell cols="2">12.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-03</cell><cell></cell><cell>1.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-06</cell><cell></cell><cell>6.8</cell><cell></cell><cell></cell></row><row><cell cols="2">MOT17</cell><cell cols="2">MOT17-07</cell><cell cols="2">10.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-08</cell><cell cols="2">14.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-12</cell><cell cols="2">10.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT17-14</cell><cell cols="2">12.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT20-04</cell><cell></cell><cell>3.8</cell><cell></cell><cell></cell></row><row><cell cols="2">MOT20</cell><cell cols="2">MOT20-06 MOT20-07</cell><cell cols="2">29.0 4.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MOT20-08</cell><cell cols="2">35.4</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MOTA? IDF1? MT? ML?</cell><cell>FP?</cell><cell>FN?</cell><cell>IDS?</cell></row><row><cell></cell><cell cols="4">MOT17-06 (6.8% recovery)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGT (Ours)</cell><cell>65.5</cell><cell>63.2</cell><cell cols="2">48.2 12.2</cell><cell>942</cell><cell>2917</cell><cell>210</cell></row><row><cell>FairMOT [50]</cell><cell>64.1</cell><cell>65.9</cell><cell cols="2">40.1 18.5</cell><cell>526</cell><cell>3533</cell><cell>176</cell></row><row><cell>GSDT [39]</cell><cell>63.0</cell><cell>62.0</cell><cell cols="2">40.1 21.2</cell><cell>681</cell><cell>3500</cell><cell>180</cell></row><row><cell>CorrTracker [38]</cell><cell>66.2</cell><cell>68.2</cell><cell cols="2">41.0 17.1</cell><cell>465</cell><cell>3346</cell><cell>171</cell></row><row><cell></cell><cell cols="4">MOT17-08 (14.6% recovery)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGT (Ours)</cell><cell>52.6</cell><cell>44.0</cell><cell cols="2">32.9 14.5</cell><cell>1076</cell><cell>8546</cell><cell>347</cell></row><row><cell>FairMOT [50]</cell><cell>42.2</cell><cell>42.0</cell><cell cols="2">22.4 28.9</cell><cell>776</cell><cell>11191</cell><cell>237</cell></row><row><cell>GSDT [39]</cell><cell>44.0</cell><cell>40.5</cell><cell cols="2">26.3 22.4</cell><cell>991</cell><cell>10523</cell><cell>323</cell></row><row><cell>CorrTracker [38]</cell><cell>49.9</cell><cell>46.7</cell><cell cols="2">25.0 17.1</cell><cell>1137</cell><cell>9201</cell><cell>250</cell></row><row><cell></cell><cell cols="4">MOT20-07 (4.9% recovery)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGT (Ours)</cell><cell>77.9</cell><cell>71.4</cell><cell>76.6</cell><cell>2.7</cell><cell>2277</cell><cell>4774</cell><cell>254</cell></row><row><cell>FairMOT [50]</cell><cell>75.6</cell><cell>70.0</cell><cell>76.6</cell><cell>0.9</cell><cell>2988</cell><cell>4770</cell><cell>333</cell></row><row><cell>GSDT [39]</cell><cell>75.0</cell><cell>68.1</cell><cell>64.0</cell><cell>1.8</cell><cell>1870</cell><cell>6115</cell><cell>282</cell></row><row><cell>SOTMOT [51]</cell><cell>72.6</cell><cell>71.2</cell><cell>76.6</cell><cell>2.7</cell><cell>3675</cell><cell>5066</cell><cell>317</cell></row><row><cell></cell><cell cols="4">MOT20-08 (35.4% recovery)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGT (Ours)</cell><cell>54.1</cell><cell>54.5</cell><cell cols="2">26.7 26.2</cell><cell cols="2">2434 32625</cell><cell>468</cell></row><row><cell>FairMOT [50]</cell><cell>27.0</cell><cell>49.5</cell><cell cols="4">41.9 14.7 32104 23447</cell><cell>981</cell></row><row><cell>GSDT [39]</cell><cell>39.4</cell><cell>48.9</cell><cell cols="2">22.5 32.5</cell><cell cols="2">9916 36420</cell><cell>608</cell></row><row><cell>SOTMOT [51]</cell><cell>43.1</cell><cell>55.1</cell><cell cols="4">35.6 19.9 16025 27216</cell><cell>863</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported by a Research Impact Fund project (R6003-21) and a Theme-based Research Scheme project (T41-603/20R) funded by the Hong Kong Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we provide additional experiment results, visualization results, and detailed analysis of detection recovery which are not included in the main paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to the kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH, Course</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Highspeed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual to real adaptation of pedestrian detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">5250</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a proposal classifier for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangping</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09003</idno>
		<title level="m">Mot20: A benchmark for multi object tracking in crowded scenes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2953" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One more check: Making &quot;fake background&quot; be tracked again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1546" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the competition between detection and reid in multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3182" to="3196" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Human in events: A large-scale benchmark for humancentric video analysis in complex events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04490</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14668" to="14678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Application of multi-object tracking with siamese track-rcnn to the human in events dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4625" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple object tracking with correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3876" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint object detection and multi-object tracking with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13708" to="13715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="107" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gnn3dmot: Graph neural network for 3d multi-object tracking with 2d-3d multi-feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6499" to="6508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Track to detect and segment: An online multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12352" to="12361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatialtemporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Relationtrack: Relation-aware multiple object tracking with decoupled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04322</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bytetrack: Multi-object tracking by associating every detection box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06864</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3069" to="3087" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving multiple object tracking with single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2453" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Global tracking transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8771" to="8780" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
