<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Subsampling of Realistic Images From GANs Conditional on a Class or a Continuous Variable</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-20">20 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Wang</surname></persName>
							<email>yongweiw@ece.ubc.cayongweiwang</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Subsampling of Realistic Images From GANs Conditional on a Class or a Continuous Variable</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-20">20 Apr 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to XXX</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>conditional generative adversarial networks</term>
					<term>subsampling</term>
					<term>conditional density ratio estimation * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, subsampling or refining images generated from unconditional generative adversarial networks (GANs) has been actively studied to improve the overall image quality. Unfortunately, these methods are often observed to be less effective or inefficient in handling conditional GANs (cGANs) -conditioning on a class (a.k.a class-conditional GANs) or a continuous variable (a.k.a continuous cGANs or CcGANs). In this work, we introduce an effective and efficient subsampling scheme, named conditional Density Ratio-guided Rejection Sampling (cDR-RS), to sample high-quality images from cGANs. Specifically, we first develop a novel conditional density ratio estimation method, termed cDRE-F-cSP, by proposing an improved feature (F) extraction mechanism and a conditional Softplus (cSP) loss. We then derive the error bound of a density ratio model trained with the cSP loss. Finally, we accept or reject a fake image in terms of its estimated conditional density ratio. A filtering scheme is also developed to increase fake images' label consistency without losing diversity when sampling from CcGANs. We extensively test the effectiveness and efficiency of cDR-RS in sampling from both class-conditional GANs and CcGANs on five benchmark datasets. This comparison includes state-of-the-art subsampling or refining methods (e.g., DRS, Collab, DDLS, DRE-F-SP+RS). When sampling from class-conditional GANs, cDR-RS outperforms all state-of-the-art methods (except DRE-F-SP+RS) by a large margin in terms of effectiveness (i.e., Intra-FID, FID, and IS scores). Although the effectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is substantially more efficient. For example, cDR-RS only requires 1.19% of the storage usage and 77% of the implementation time spent by DRE-F-SP+RS on ImageNet-100. When sampling from CcGANs, the superiority of cDR-RS is even more noticeable in terms of both effectiveness and efficiency. Notably, with the consumption of reasonable computational resources, cDR-RS can substantially reduce the Label Score metric without decreasing the diversity of CcGAN-generated images, while other methods often need to trade much diversity for slightly improved Label Score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head> <ref type="figure">Fig. 1</ref><p>: The efficiency and effectiveness comparisons between the proposed cDR-RS and four baseline methods in sampling 100,000 and 60,000 fake images from BigGAN <ref type="bibr" target="#b0">[1]</ref> (a) and CcGAN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> (b), respectively. The dashed red lines denote Intra-FID <ref type="bibr" target="#b3">[4]</ref> and Label Score <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> of sampling from cGANs without subsampling or refining in (a) and <ref type="bibr">(b)</ref>, respectively. We prefer lower Intra-FID and Label Score. Label Score is a metric to evaluate the discrepancy between the actual and conditioning labels of fake images. cDR-RS achieves state-of-the-art performances in sampling from cGANs while requiring only reasonable computational resources.</p><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> are a popular type of generative model for image synthesis, aiming to estimate the marginal distribution of images. As an extension and an essential family of GANs, conditional GANs (cGANs) <ref type="bibr" target="#b8">[9]</ref> intend to estimate the image distribution given some conditions. In this paper, we focus on two main types of cGANs, i.e., class-conditional GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> and continuous conditional GANs (CcGANs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Class-conditional GANs such as BigGAN <ref type="bibr" target="#b0">[1]</ref> take class labels (i.e., categorical variables) as conditions. CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> are a new type of cGANs, which take continuous, scalar variables (termed regression labels) as conditions. Unconditional GANs and cGANs have many applications, such as single image super-resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, image translation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, text-to-image synthesis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, and 3D shape generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recent advances in unconditional GANs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and class-conditional GANs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> generally enable these models to generate high-quality images. Nevertheless, low-quality images still appear frequently even with such advanced GAN models during image generation. We would like to emphasize that the image quality discussed in this paper are three-fold <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>: (1) visual quality, (2) diversity, and (3) label consistency. Label consistency applies to cGANs only and is defined as the consistency of generated images with respect to the conditioning label. Notably, CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> often suffer from low label consistency because they are designed to sacrifice label consistency for better visual quality and higher diversity. Ding et al. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> proposes Label Score to measure label consistency for CcGANs, where a lower Label Score implies higher label consistency and vice versa.</p><p>To enhance the image quality of unconditional GANs, increasing attention has been paid to improve the sampling strategy of pre-trained unconditional GANs via subsampling or refining. Refining methods (e.g., Collab <ref type="bibr" target="#b25">[26]</ref>) aim to refine visually unrealistic fake images to improve visual quality. For example, Collab <ref type="bibr" target="#b25">[26]</ref> refines an intermediate hidden map of a generator to remove artifacts in generated images by using information from a trained discriminator. Subsampling methods (e.g., DRS <ref type="bibr" target="#b26">[27]</ref>, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, DDLS <ref type="bibr" target="#b28">[29]</ref>) remove visually unrealistic images to improve visual quality and adjust the likelihood of generated images to increase diversity. To accomplish subsampling, discriminator rejection sampling (DRS) <ref type="bibr" target="#b26">[27]</ref> accepts or rejects a fake image by rejection sampling (RS). DRS requires an accurate density ratio estimation (DRE), however, because the DRE step in DRS relies on the assumption of optimality of the discriminator. Thus DRS may not perform well if the discriminator is far from optimal. DRS is also inapplicable to some GANs such as MMD-GAN <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr">Ding et al.</ref> improves DRS by proposing density ratio estimation in the feature space with Softplus loss (DRE-F-SP) <ref type="bibr" target="#b27">[28]</ref>, which does not require an optimal discriminator and is applicable to various GANs. Then, with RS as the sampler, <ref type="bibr" target="#b27">[28]</ref> introduces DRE-F-SP+RS to subsample GANs. Besides these density ratio-based methods, discriminator driven latent sampling (DDLS) <ref type="bibr" target="#b28">[29]</ref> proposes to accept or reject samples via an energy-based model defined in the latent space of the generator in a GAN. The methods discussed above, however, are not designed for class-conditional GANs or CcGANs.</p><p>An intuitive approach to better sample from class-conditional GANs or CcGANs is applying unconditional methods described above for each distinct class or regression label. Unfortunately, this approach may be inefficient or even impractical, especially when many distinct classes or regression labels exist. For example, if we apply DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref> to sample from BigGAN <ref type="bibr" target="#b0">[1]</ref> trained on ImageNet-100 <ref type="bibr" target="#b30">[31]</ref> (a subset of ImageNet <ref type="bibr" target="#b31">[32]</ref> with 100 classes), we need to fit 100 density ratio models separately. As visualized in <ref type="figure">Fig. 1(a)</ref>, this is often time-consuming (84.4 hours) and it requires a large storage space <ref type="bibr">(39.97 GB)</ref>. Another noticeable example is DDLS <ref type="bibr" target="#b28">[29]</ref>, which spends 218 hours subsampling BigGAN trained on ImageNet-100. Furthermore, as shown in <ref type="figure">Fig. 1</ref> <ref type="bibr">(b)</ref> and Section 4.2, the unconditional approach is usually ineffective in sampling from CcGANs because it is not designed to solve the label inconsistency problem suffered by CcGANs. Moreover, although DRE-F-SP+RS may perform well in subsampling class-conditional GANs (e.g., <ref type="figure">Fig. 1(a)</ref>), it is not suitable for subsampling CcGANs for two reasons: it lacks a suitable feature extraction method for regression datasets, and it cannot sample from CcGANs conditional on labels that are unseen in the training phase. The ineffectiveness or inefficiency of applying unconditional sampling methods to class-conditional GANs or CcGANs is demonstrated in our empirical study in Section 4.</p><p>Recently, <ref type="bibr" target="#b32">[33]</ref> proposes a rejection sampling scheme, called GOLD, to subsample the auxiliary classifier GAN (ACGAN) <ref type="bibr" target="#b9">[10]</ref>, a special class-conditional GAN. GOLD is based on the gap in logdensities that measures the discrepancy between the actual image distribution and the fake image distribution of given samples. <ref type="bibr" target="#b32">[33]</ref> shows that, empirically, GOLD can improve the performance of ACGAN in the class-conditional image synthesis. However, this method does not apply to the most recent cGANs (e.g., BigGAN <ref type="bibr" target="#b0">[1]</ref> and CcGAN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>) for three reasons. First, its purpose, algorithm, and official codes are only designed for ACGAN. Second, GOLD relies on a strong assumption that the optimal discriminator is D * (x) = p r (x)/(p r (x) + p g (x)) (Sec. 3.1 of <ref type="bibr" target="#b32">[33]</ref>). Unfortunately, this assumption holds only if the discriminator is trained with the vanilla loss (Eq. (1) in <ref type="bibr" target="#b4">[5]</ref>) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref>. It is invalid with GANs trained with other losses, e.g., hinge loss, Wasserstein loss, MMD loss, etc. Third, GOLD is inappropriate for subsampling CcGANs, because CcGANs do not have an ACGAN architecture, and GOLD does not provide a mechanism to control the label inconsistency problem suffered by CcGANs.</p><p>To improve the overall image quality of class-conditional GANs and CcGANs effectively and efficiently, we propose the conditional density ratio-guided rejection sampling (cDR-RS). Our contributions can be summarized as follows:</p><p>? In Section 3.1, we propose a DRE scheme to estimate an image's density ratio conditional on a class or regression label. We first introduce a new feature extraction method for images with regression labels, when the feature extraction mechanism in DRE-F-SP <ref type="bibr" target="#b27">[28]</ref> is not applicable. Then, we propose a novel conditional Softplus (cSP) loss, which enables us to estimate density ratios conditional on different class/regression labels by fitting only one density ratio model. This density ratio model takes as input both the high-level features and the class/regression label of an image and outputs the density ratio conditional on the given label.</p><p>? To analyze the proposed cSP loss theoretically, we derive in Section 3.2 the error bound of a density ratio model trained with the proposed cSP loss.</p><p>? In Section 3.3, we propose a novel rejection sampling scheme to subsample class-conditional GANs and CcGANs. A filtering scheme is also proposed in Section 3.4 for CcGANs to increase fake images' label consistency without sacrificing diversity. By only tuning one hyper-parameter of this filtering scheme, we can easily control the trade-off between label consistency and diversity according to users' needs.</p><p>? In Section 4, extensive experiments on five benchmark datasets and different GAN architectures convincingly demonstrate the state-of-the-art performances of the proposed subsampling scheme over baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditional generative adversarial networks</head><p>Conditional GANs (cGANs), first proposed in <ref type="bibr" target="#b8">[9]</ref>, extend GANs <ref type="bibr" target="#b4">[5]</ref> to the conditional image synthesis setting, where a condition y is fed into both the generator and discriminator networks. Mathematically, cGANs aim to estimate the density function p r (x|y) of the actual conditional image distribution. The estimated density function p g (x|y) is the density of the fake conditional image distribution induced by the generator network. The conditional y is often a categorical variable such as a class label, and cGANs with class labels as conditions are also known as class-conditional GANs. Class-conditional GANs have been widely studied in the literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref>. Stateof-the-art class conditional GANs (e.g., BigGAN <ref type="bibr" target="#b0">[1]</ref>) can generate photo-realistic images for a given class. However, GANs conditional on regression labels have been rarely studied due to two problems. First, very few (even zero) real images exist for some regression labels. Second, since regression labels are continuous and infinitely many, they cannot be embedded by one-hot encoding like class labels. To solve these two problems, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> propose the CcGAN framework, which introduces novel empirical cGAN losses and label input mechanisms. The novel empirical cGAN losses, consisting of the hard vicinal discriminator loss (HVDL), the soft vicinal discriminator loss (SVDL), and a new generator loss, are developed to solve the first problem. The second problem is solved by a naive label input (NLI) mechanism and an improved label input (ILI) mechanism. The effectiveness of CcGAN has been demonstrated on diverse datasets. Class-conditional GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref> and CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, as two main types of cGANs, are our focus in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Subsampling GANs</head><p>Among existing sampling methods for unconditional GANs, DRE-F-SP+RS proposed by <ref type="bibr" target="#b27">[28]</ref> can achieve state-of-the-art sampling performance. This subsampling framework consists of two components: a density ratio estimation method, termed DRE-F-SP, and a rejection sampling scheme. DRE-F-SP aims to estimate the density ratio function r * (x) := p r (x)/p g (x) based on N r real images x r 1 , x r 2 , . . . , x r N r ? p r (x) and N g fake images x g 1 , x g 2 , . . . , x g N g ? p g (x), where p r (x) and p g (x) are the density functions of the actual and fake conditional image distributions, respectively. Based on the estimated density ratios, to push p g towards p r , rejection sampling (RS) is used to sample from the trained GAN model. Empirical studies in <ref type="bibr" target="#b27">[28]</ref> show that DRE-F-SP+RS substantially outperforms existing sampling methods (e.g., DRS <ref type="bibr" target="#b26">[27]</ref>) in subsampling different types of unconditional GANs.</p><p>As the key component of DRE-F-SP+RS, DRE-F-SP first trains a specially designed feature extractor ? on a set of real images with class labels under the cross-entropy loss. As visualized in <ref type="figure" target="#fig_1">Fig. 2</ref>), ? is a convolutional neural network (CNN) adapted from ResNet-34 <ref type="bibr" target="#b33">[34]</ref>. The network architecture of this feature extractor is adjusted so that the dimension of one hidden map h equals that of the input image x, thus ensuring that the Jacobian determinant det(?h/?x) exists. The feature extractor defines a mapping of an image x to a high-level feature h, i.e., h = ?(x), where ? is assumed invertible and the absolute value of the Jacobian determinant | det(?h/?x)| is assumed positive. Then, DRE-F-SP estimates the density ratio of an image in the feature space defined by ? rather than in the pixel space. Specifically, DRE-F-SP models the actual density ratio function in the feature space by a 5-layer multilayer perceptron (MLP-5). <ref type="bibr" target="#b27">[28]</ref> also proposes a novel loss function called Softplus (SP) loss to train this MLP-5. Finally, compositing ?(x) and MLP-5 leads to an estimate of r * (x).</p><p>Though DRE-F-SP+RS has been demonstrated both effective and efficient in subsampling unconditional GANs <ref type="bibr" target="#b27">[28]</ref>; as discussed in Section 1, however, several crucial challenges prevent us from applying it effectively to class-conditional GANs and CcGANs:</p><p>(C1) DRE-F-SP+RS becomes very inefficient if many distinct class/regression labels exist; (C2) The feature extractor in DRE-F-SP+RS is inapplicable to images with regression labels only; (C3) DRE-F-SP+RS does not have a mechanism to control the label inconsistency problem suffered by CcGANs; (C4) DRE-F-SP+RS cannot sample from CcGANs conditional on regression labels that are unseen in the training set.</p><p>These challenges are addressed by the proposed method, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>As discussed in Section 1, due to their ineffectiveness or inefficiency, the unconditional subsampling or refining methods (e.g., DRS <ref type="bibr" target="#b26">[27]</ref>, Collab <ref type="bibr" target="#b25">[26]</ref>, DDLS <ref type="bibr" target="#b28">[29]</ref> and DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>) may be impractical for sampling from class-conditional GANs and CcGANs. Moreover, the only existing conditional subsampling method <ref type="bibr" target="#b32">[33]</ref> is designed for ACGAN <ref type="bibr" target="#b9">[10]</ref> and cannot be applied to other cGANs. Motivated by these limitations, in this section, we propose an effective and efficient dual-functional subsampling method, which is suitable for both class-conditional GANs and CcGANs regardless of the GAN architecture and the number of distinct class or regression labels.</p><p>To overcome the four crucial challenges listed at the end of Section 2.2, the proposed method has to go well beyond straightforward extension of <ref type="bibr" target="#b27">[28]</ref> to class-conditional GANs and CcGANs. In this article we propose: 1) a novel loss for conditional DRE (for solving (C1) and (C4)); 2) a new feature extractor for CcGANs (for solving (C2)); and 3) a novel filtering scheme to control label inconsistency (for solving (C3)). Consequently, compared with <ref type="bibr" target="#b27">[28]</ref>, the proposed novel methodology is substantially more efficient and is able to subsample CcGANs.</p><p>To aid the reader, we summarize in <ref type="table" target="#tab_0">Table 1</ref> the essential notation used in this paper along with definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional density ratio estimation in feature space with conditional Softplus loss</head><p>In this section, we introduce cDRE-F-cSP, a novel conditional density ratio estimation (cDRE) method. Assume we have N r real image-label pairs (x r 1 , y r 1 ), . . . , (x r N r , y r N r ) and N g fake image-label pairs (x g 1 , y g 1 ), . . . , (x g N g , y g N g ), where x r i and x g i can be seen as samples drawn from p r (x|y r i ) and p g (x|y g i ), respectively. Based on these observed samples, we aim to estimate r * (x|y) := p r (x|y) p g (x|y)</p><p>.</p><p>Like DRE-F-SP <ref type="bibr" target="#b27">[28]</ref>, we conduct cDRE in a feature space learned by a pre-trained, invertible CNN ?. DRE-F-SP <ref type="bibr" target="#b27">[28]</ref> trains a classification CNN adapted from ResNet-34 on some real images with class labels to extract high-level features for DRE (refer to Section 2.2 and <ref type="figure" target="#fig_1">Fig. 2</ref>). This mechanism also applies to class-conditional GANs; however, it is inapplicable to CcGANs since regression datasets may not have class labels. Therefore, when subsampling CcGANs, we develop a specially designed sparse autoencoder (SAE) to extract features whose architecture is visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>. The encoder with ReLU <ref type="bibr" target="#b34">[35]</ref> as the final layer is treated as ? to extract sparse high-level features from images. The bottleneck dimension of the sparse autoencoder equals the dimension of the flattened input image. The decoding process is trained to reconstruct the input image and predict the regression label of the input image. The training loss of SAE is the summation of three loss components:</p><p>(1) The mean square error (MSE) between the input image x and the reconstructed imagex, i.e., 1 C?H?W x ?x 2 2 , where ? 2 is the Euclidean norm; (2) The squared error between the actual regression label y and the predicted regression label?, i.e., (y ??) 2 ; (3) The product of a positive constant ? and the L 1 norm of h, i.e., ? ? 1 C?H?W h 1 , where ? 1 is the L 1 norm. The constant ? controls the sparsity and ? = 10 ?3 often empirically works well. the density function of the fake images conditional on y q r (h|y) the density function of the real features conditional on y q g (h|y)</p><p>the density function of the fake features conditional on y p(y) the density function of real/fake labels q r (h, y)</p><p>the joint density function of the real features and labels q g (h, y)</p><p>the joint density function of the fake features and labels det (?h/?x) the determinant of the Jacobian matrix ?h/?x ?(t) Sigmoid function, i.e., ?(t) = e t /1 + e t ?(t)</p><p>Softplus function, i. Specifically, we minimize the following training loss L S AE in practice defined based on N r real image-label pairs:</p><formula xml:id="formula_1">L S AE = 1 N r ? C ? H ? W N r i=1 x r i ?x r i 2 2 + 1 N r N r i=1 (y r i ?? r i ) 2 + ? ? 1 N r ? C ? H ? W N r i=1 h r i 1 ,<label>(2)</label></formula><p>where x r i is the i-th real image,x r i is the i-th reconstructed real image, y r i is the i-th real label,? r i is the predicted label of x r i , and h r i is the extracted feature vector of x r i . Usually, the bottleneck dimension of an autoencoder is much smaller than the input image, but DRE in feature space <ref type="bibr" target="#b27">[28]</ref> requires they have equal dimensions. To prevent the proposed SAE from overfitting the training data, we design an extra branch to predict regression labels and an L 1 regularizer to encourage the sparsity of extracted features. The additional predictor is also applied in the filtering scheme proposed in Section 3.4.  <ref type="formula" target="#formula_2">3)</ref> to hold, and the vanilla ResNet-34 architecture is not invertible, <ref type="bibr" target="#b27">[28]</ref> shows that ?(x) adapted from ResNet-34 still works very well as the feature extractor in subsampling unconditional GANs. One possible reason is that the adjustment of the ResNet-34 architecture to ensure equal dimensionality improves the invertibility of ?. Therefore, we also employ such a modified ResNet-34 in subsampling class-conditional GANs.</p><p>Next, we propose a formulation of the actual conditional density ratio function in the feature space: Given that (1) h and the flattened x have equal dimension, (2) the feature extractor ? is invertible, and (3) the Jacobian determinant | det(?h/?x)| is nonzero, then</p><formula xml:id="formula_2">? * (h|y) := q r (h|y) q g (h|y) = q r (h|y) ? det ?h ?x q g (h|y) ? det ?h ?x = p r (x|y) p g (x|y) = r * (x|y),<label>(3)</label></formula><p>where q r (h|y) and q g (h|y) are respectively the density functions of the real and fake conditional distributions of high-level features. Based on Eq. (3) and the pre-trained neural network ?(x), we model the conditional density ratio function ? * (h|y) in the feature space by a 5-layer MLP (MLP-5) denoted by ?(h|y) with both h and its label y as input. To train ?(h|y), we propose the conditional Softplus (cSP) loss:</p><formula xml:id="formula_3">L c (?) = E (h,y)?q g (h,y) ?(?(h|y))?(h|y) ? ?(?(h|y)) ? E (h,y)?q r (h,y) ?(?(h|y)) ,<label>(4)</label></formula><p>where ? and ? are Sigmoid and Softplus functions respectively, q g (h, y) = q g (h|y)p(y), q r (h, y) = q r (h|y)p(y), and p(y) is the distribution of labels. This new loss extends the SP loss <ref type="bibr" target="#b27">[28]</ref> to the conditional setting. The empirical approximation to Eq. <ref type="formula" target="#formula_3">(4)</ref> is</p><formula xml:id="formula_4">L c (?) = 1 N g N g i=1 ?(?(h g i |y g i ))?(h g i |y g i ) ? ?(?(h g i |y g i )) ? 1 N r N r i=1 ?(?(h r i |y r i )).<label>(5)</label></formula><p>Similar to DRE-F-SP, to prevent ?(h|y) from overfitting the training data, a natural constraint applied to ?(h|y) is </p><formula xml:id="formula_5">E h?q g (h|y) [?(h|y)] = 1.<label>(6)</label></formula><formula xml:id="formula_6">h i )/m is ( m i |h i |)/m, i.e.</formula><p>, the L 1 penalty on h commonly used to encourage sparsity. Additionally, this SAE also has an extra branch to predict the regression label of x. Both the sparsity penalty and the extra prediction branch are used to avoid overfitting. Note that, ?(x) is trained to be invertible and Decoder can be seen as its inverse function.</p><formula xml:id="formula_7">If Eq. (6) holds, then E (h,y)?q g (h,y) [?(h|y)] = 1.<label>(7)</label></formula><p>An empirical approximation to Eq. (7) is</p><formula xml:id="formula_8">1 N g N g i=1 ?(h g i |y g i ) = 1.<label>(8)</label></formula><p>Therefore, in practice, we minimize the penalized version of Eq. (5) as follows:</p><formula xml:id="formula_9">min ? L c (?) + ? Q c (?) ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">Q c (?) = ? ? ? ? ? ? ? 1 N g N g i=1 ?(h g i |y g i ) ? 1 ? ? ? ? ? ? ? 2 .<label>(10)</label></formula><p>An algorithm shown in Alg. 1 is used to implement cDRE-F-cSP in practice. Both <ref type="bibr" target="#b27">[28]</ref> and our empirical study show that the fitted (conditional) density ratio model often performs well in subsampling if ? ? [0, 0.1]. In our experiments, ? is set as 10 ?2 or 10 ?3 empirically. Remark 1. Although the invertibility of the feature extractor ? is a necessary assumption for Eq.</p><p>(3) to hold, an empirical study in <ref type="bibr" target="#b27">[28]</ref> shows that the modified ResNet-34 (refer to Section 2.2 and <ref type="figure" target="#fig_1">Fig. 2</ref>) still works very well as the feature extractor in subsampling unconditional GANs. One possible reason is that the adjustment of the ResNet-34 architecture to ensure equal dimensionality improves the invertibility of the feature extractor. Therefore, we also employ such a modified ResNet-34 in subsampling class-conditional GANs. For CcGANs, we propose an SAE to extract Algorithm 1: Optimization algorithm for conditional density ratio model training in cDRE-F-cSP.</p><p>Data: N r real image-label pairs {(x r 1 , y r 1 ), ? ? ? , (x r N r , y r N r )}, a generator G, a pre-trained feature extractor ?(x), a 5-layer MLP ?(h|y) and a preset hyperparameter ?.</p><formula xml:id="formula_11">Result: A trained conditional density ratio model r(x|y) = ?(?(x)|y) = ?(h|y). 1 Initialize ?; 2 for k = 1 to K do 3 Sample a mini-batch of m real image-label pairs {(x r (1) , y r (1) ), ? ? ? , (x r (m) , y r (m) )} from {(x r 1 , y r 1 ), ? ? ? , (x r N r , y r N r )}; 4 Sample a mini-batch of m fake image-label pairs {(x g (1) , y g (1) ), ? ? ? , (x g (m) , y g (m) )} from G; 5</formula><p>Update ? via mini-batch SGD or a variant with the gradient of Eq. (9), i.e.,</p><formula xml:id="formula_12">L c (?) + ? Q c (?) . 6 end</formula><p>features, where the encoder is trained to be invertible (i.e., the decoder can be seen as the inverse function of the encoder). A similar SAE may also be used in subsampling class-conditional GANs, but it seems unnecessary in practice and is not used in the experiments of Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.</head><p>A condition y can be incorporated into the conditional density ratio model ?(h|y) through concatenation. Specifically, we first convert the class/regression label y into an embedding vector. In the class-conditional scenario, y is encoded as a one-hot vector. In the CcGAN scenario, the condition y is a continuous scalar, so it cannot be one-hot encoded. In this case, we adopt the improved label input (ILI) method in <ref type="bibr" target="#b2">[3]</ref> which projects the scalar y into a high-dimensional embedding space. After the embedding, to input the condition into ?(h|y), the encoded class/regression label is then concatenated with the input (i.e., the extracted feature vector h) or a hidden map of ?(h|y). Please refer to <ref type="table" target="#tab_6">Table S</ref>.3.8 in Appendix for an example of how to input y into ?(h|y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Error bound</head><p>In this section, we derive the error bound of a density ratio model ?(h|y) trained with the empirical cSP loss L c (?). For simplicity, we ignore the penalty term in this analysis.</p><p>Firstly, we introduce some notation. Let ? = {? : h ? ?(h|y)} denote the hypothesis space of the density ratio model ?(h|y). We also define? and? as follows:? = arg min ??? L c (?) and ? = arg min ??? L c (?). Please note that the hypothesis space ? may not cover the actual density ratio function ? * . Therefore, L c (?) ? L c (? * ) ? 0. Denote by ? all learnable parameters of ? and assume ? is in a parameter space A. Denote ?(?(h|y))?(h|y) ? ?(?(h|y)) by g(h|y; ?). Let R q r (h,y),N r (?) denote the empirical Rademacher complexity <ref type="bibr" target="#b35">[36]</ref> of ?, which is defined based on independent feature-label pairs {(h r 1 , y r 1 ), . . . , (h r N r , y r N r )} from q r (h, y). Then, we derive the error bound of the conditional density ratio estimate? under Eq. (4) as follows:</p><formula xml:id="formula_13">Theorem 1. If (i) N g is large enough, (ii) A is compact, (iii) ?g(h|y; ?)</formula><p>is continuous at ?, (iv) ?g(h|y; ?), ? a function g u (h|y) that does not depend on ?, s.t. |g(h|y; ?)| ? g u (h|y), and (iv) E (h,y)?q g (h,y) g u (h|y) &lt; ?, then ?? ? (0, 1) and ?? ? (0, ?] with probability at least 1 ? ?,</p><formula xml:id="formula_14">L c (?) ? L c (? * ) ? 1 N g +R q r (h,y),N r (?) + 2 4 N r log 2 ? + L c (?) ? L c (? * ).<label>(11)</label></formula><p>Proof. The proof is in Supp. S.1.</p><p>Remark 3. Theorem 1 provides an error bound of a conditional density ratio model trained with the cSP loss.R q r (h,y),N r (?) on the right of Eq. (11) implies that, a more complicated density ratio model has a more significant error bound, i.e., a worse generalization performance. Thus, we should not use an overly complicated density ratio model. It supports our proposed cDRE-F-cSP because we just need a small neural network (e.g., a shallow MLP) to model the density ratio function in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">cDR-RS: Conditional density ratio-guided rejection sampling for both class-conditional GANs and CcGANs</head><p>Based on the cDRE method proposed in Section 3.1, we develop a rejection sampling scheme, termed cDR-RS, to subsample class-conditional GANs and CcGANs. The workflow can be summarized in <ref type="figure" target="#fig_6">Fig. 4</ref> and Alg. 2. This rejection sampling scheme is conducted based on ?(h|y) for each distinct label y of interest. For example, in the ImageNet-100 <ref type="bibr" target="#b30">[31]</ref> experiment, after fitting ?(h|y) on real and fake image datasets, we conduct rejection sampling from the trained BigGAN within each class. Since the ImageNet-100 dataset consists of 100 classes, in total we will do 100 rejection sampling operations to get fake images, i.e., conducting one separate rejection sampling for each individual class. <ref type="figure" target="#fig_6">Fig. 4</ref>: The workflow of cDR-RS has two sequential modules: cDRE-F-cSP and rejection sampling. M in the acceptance probability p equals to max h {q r (h)/q g (h)}, which can be estimated by evaluating ?(h|y) on some burn-in samples before subsampling. We input the condition y into ?(h|y) via concatenation (please refer to Remark 2 and <ref type="table" target="#tab_6">Table S</ref>.3.8). The rejection sampling scheme is conducted based on ?(h|y) for each distinct label y of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A filtering scheme to increase label consistency in subsampling CcGANs</head><p>To solve the problem of insufficient data, CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> use real images with labels in a vicinity of a conditioning regression label y to estimate p r (x|y). Consequently, the actual labels of some Algorithm 2: Subsampling fake images with label y by cDR-RS.</p><p>Data: A generator G, a trained feature extractor ?(x), a trained conditional density ratio model ?(h|y). Result: images = {N filtered fake images with label y} 1 Generate N burn-in fake images from G conditional on label y.; 2 Estimate the density ratios of these N fake images conditional on y by evaluating ?(?(x)|y); 3 M ? max{N estimated density ratios}; 4 images ? ?; 5 while |images| &lt; N do <ref type="bibr" target="#b5">6</ref> x ? get a fake image with label y from G; fake images sampled from p g (x|y) may be far from y (a.k.a label inconsistency). Unfortunately, the current subsampling scheme in <ref type="figure" target="#fig_6">Fig. 4</ref> may still accept these fake images if they have good visual quality or can contribute to the diversity increase (see <ref type="table">Table 5</ref>). An intuitive solution to this issue is to filter out these fake images with actual labels different from y, but this filtering may substantially decrease the diversity of fake images.</p><p>Please note that the actual labels of fake images are unknown to us, therefore we need to estimate them in practice. Specifically, we first predict the labels of fake images via a pre-trained regression CNN (i.e., the composition of the encoder and predictor of SAE in <ref type="figure" target="#fig_2">Fig. 3</ref>), and the predicted labels are assumed to be close to the actual labels. Then, we can estimate fake images' actual labels by their predicted labels.</p><p>To increase label consistency without losing diversity, we propose a filtering scheme for cDR-RS, which is based on an assumption that we may sample fake images with actual labels equal to y from both p g (x|y) and p g (x|y ) in the CcGAN sampling, where y is close to y. To illustrate the proposed filtering scheme consider subsampling at y. We first replace p g (x|y) with p y,? g (x) as the density of the proposal distribution, where p y,? g (x) stands for the density function of the distribution of fake images with predicted labels in Y ? y := [y ? ?, y + ?] and ? is a hyper-parameter. Afterwards, the density ratio model r(x|y) is used to model p r (x|y)/p y,? g (x) and it is trained on fake images with predicted labels in Y ? y . In the sampling phase, before conducting the rejection sampling process ( <ref type="figure" target="#fig_6">Fig. 4</ref> and Alg. 2), we filter out fake images with predicted labels outside of Y ? y . Please note that ? controls the trade-off between label consistency and diversity. As shown in <ref type="figure" target="#fig_5">Fig. 8</ref>, a smaller ? often leads to higher label consistency but lower diversity, while a larger ? often leads to lower label consistency but higher diversity. We can adjust ? according to our needs, but a good ? should improve label consistency without decreasing diversity. In our experiment, ? is set based on a vicinity parameter m ? of CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Please refer to Remark 4 for a rule of thumb to select ?.</p><p>Remark 4 (A rule of thumb to select ?). Let's first review the rule of thumb for selecting the vicin-ity hyper-parameter in CcGANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. CcGANs estimate p r (x|y) based on real images with labels in a vicinity of y. Let ? denote the hyper-parameter to control the width of the hard vicinity of CcGANs. Denote by N r uy the number of unique labels in the training set, and denote by y r [l] the l-th smallest normalized distinct real label. Define ? base as ? base = max y r</p><formula xml:id="formula_15">[2] ? y r [1] , y r [3] ? y r [2] , . . . , y r [N r uy ] ? y r [N r uy ?1]</formula><p>. Then, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> set ? as ? = m ? ? base , where m ? stands for 50% of the minimum number of neighboring labels used for estimating p r (x|y) given a label y.</p><p>As we discussed in Section 3.4, we estimate fake images' actual labels by their predicted labels. The predicted labels may deviate from the actual labels, but such deviation is assumed not significant. Although the prediction error may be small, it still exists and won't be zero. Therefore, when designing the proposal distribution in the rejection sampling, instead of using fake images with predicted labels exactly equal to y (i.e., set ? = 0), we consider fake images with predicted labels in a vicinity of y, i.e., Y ? y . We expect that the actual labels of fake images in Y ? y should be equal or close to y. Furthermore, if we let ? = 0, the training and sampling time may be very long. On UTKFace, we conduct an ablation study to show the effect of ? on the training and sampling. Relevant results are summarized in <ref type="figure" target="#fig_4">Fig. 5</ref>. We can see a smaller ? substantially increases the training and sampling time, which may make cDR-RS less efficient. Therefore, in practice, we usually use a relatively large ?, e.g., 3 ? m ? ? base . In other words, we let the vicinity Y ? y = [y ? ?, y + ?] to be three times wider than the hard vicinity in CcGANs. Our experiments in Section 4.2 show that this rule of thumb can let cDR-RS effectively increase label consistency without losing diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will empirically evaluate the efficiency and the effectiveness of the proposed cDR-RS scheme in subsampling class-conditional GANs and CcGANs. We compare cDR-RS with six state-of-the-art sampling methods: Baseline (no subsampling or refining), GOLD <ref type="bibr" target="#b32">[33]</ref>, Collab <ref type="bibr" target="#b25">[26]</ref>, DRS <ref type="bibr" target="#b26">[27]</ref>, DDLS <ref type="bibr" target="#b28">[29]</ref>, and DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>. For sampling methods proposed for unconditional GANs, we implement them for each distinct class or regression label. For a fair comparison, extensive experiments are conducted on multiple benchmark datasets and network architectures, and diverse evaluation metrics are utilized to demonstrate the robust superiority of our cDR-RS method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sampling from class-conditional GANs</head><p>Experimental setup: For class-conditional GANs, we conduct experiments on three image datasets: (i) CIFAR-10 <ref type="bibr" target="#b36">[37]</ref>, (ii) CIFAR-100 <ref type="bibr" target="#b36">[37]</ref>, and (iii) ImageNet-100 <ref type="bibr" target="#b30">[31]</ref>, respectively. (i) CIFAR-10 consists of 60,000 (32 ? 32) RGB images uniformly from 10 classes. (ii) CIFAR-100 also includes 60,000 (32 ? 32) RGB images, but uniformly from 100 classes. On both datasets, the overall number of training samples is 50,000, i.e., 5000 images per class on CIFAR-10 or 500 images per class for CIFAR-100. The remaining 10,000 samples are for test on CIFAR-10 or CIFAR-100. (iii) ImageNet-100 <ref type="bibr" target="#b30">[31]</ref>, as a subset of ImageNet <ref type="bibr" target="#b31">[32]</ref>, has 128,503 RGB images at 128 ? 128 resolution from 100 classes. In our experiment, we randomly split ImageNet-100 into a training set and a test set, where 10,000 images are for testing (on average 100 images per class) and the rest images are for training.</p><p>In <ref type="table">Table 2</ref>, we show the investigated combinations of network architectures and sampling methods for the three datasets in this experiment. On CIFAR-10, we train three class-conditional GANs: ACGAN <ref type="bibr" target="#b9">[10]</ref>, SNGAN <ref type="bibr" target="#b37">[38]</ref>, and BigGAN <ref type="bibr" target="#b0">[1]</ref>. On CIFAR-100 and ImageNet-100, we only implement BigGAN because the training of ACGAN and SNGAN is unstable. When sampling from ACGAN on CIFAR-10, we test four candidate methods: Baseline, GOLD <ref type="bibr" target="#b32">[33]</ref>, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and cDR-RS. When sampling from SNGAN on CIFAR-10 or BigGAN on all three datasets, we test six candidates: Baseline, Collab <ref type="bibr" target="#b25">[26]</ref>, DRS <ref type="bibr" target="#b26">[27]</ref>, DDLS <ref type="bibr" target="#b28">[29]</ref>, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and cDR-RS. Please note that Collab, DRS, and DDLS are inapplicable to ACGAN, and Baseline refers to sampling without subsampling or refining. Please refer to Supp. S.3, S.4, and S.5 for details. <ref type="table">Table 2</ref>: The investigated combinations of datasets, network architectures, and sampling methods in the classconditional GANs experiment. We evaluate seven sampling methods in this experiment: Baseline (no subsampling or refining), GOLD <ref type="bibr" target="#b32">[33]</ref>, Collab <ref type="bibr" target="#b25">[26]</ref>, DRS <ref type="bibr" target="#b26">[27]</ref>, DDLS <ref type="bibr" target="#b28">[29]</ref>, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and the proposed cDR-RS. However, when sampling from ACGAN <ref type="bibr" target="#b9">[10]</ref>, three candidate methods, i.e., Collab, DRS, and DDLS, are not used because of their inapplicability. When sampling from SNGAN <ref type="bibr" target="#b37">[38]</ref> and BigGAN <ref type="bibr" target="#b0">[1]</ref>, GOLD is excluded because it is designed for ACGAN only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Architectures Sampling methods </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics:</head><p>We evaluate the quality of fake images by Fr?chet Inception Distance (FID) <ref type="bibr" target="#b38">[39]</ref>, Intra-FID <ref type="bibr" target="#b3">[4]</ref>, and Inception Score (IS) <ref type="bibr" target="#b39">[40]</ref>. Intra-FID is an overall image quality metric, which computes FID separately for each class and reports the average FID score. A lower Intra-FID or FID score indicates better image quality or vice versa. Conversely, a larger Inception Score implies better image quality. <ref type="figure">Fig. 6</ref>: Some example images for the "indigo bunting" class at 128 ? 128 resolution in the ImageNet-100 experiment. The first row includes six real images. From the second row to the bottom, we show some example fake images generated from Baseline, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and the proposed cDR-RS, respectively. We often observe some fake images sampled by Baseline without recognizable birds (e.g., two images with red rectangles), while both DRE-F-SP+RS and cDR-RS can effectively remove such images. Please note that, as shown in <ref type="figure">Fig. 1</ref> and <ref type="table">Table 4</ref>, although DRE-F-SP+RS and cDR-RS have similar effectiveness (similar Intra-FID, FID, and IS scores), cDR-RS requires much less storage or implementation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results:</head><p>We quantitatively compare the image quality of fake images sampled from class-conditional GANs with different candidate methods. For the CIFAR-10 experiment, we draw 10,000 fake images for each class by each sampling method. For the CIFAR-100 and ImageNet-100 experiments, we sample 1000 fake images for each class. Quantitative results are summarized in <ref type="table">Table 3</ref>. We can see, in all settings, the proposed cDR-RS and DRE-F-SP+RS perform comparably, and they substantially outperform other candidate methods by a large margin in terms of all three metrics. We also show in <ref type="figure">Fig. 6</ref> some example fake images for the "indigo bunting" class sampled by Baseline, DRE-F-SP+RS, and cDR-RS with real images for reference. Both DRE-F-SP+RS and cDR-RS can effectively remove some fake images with unrecognizable birds (marked by red rectangles). Although cDR-RS and DRE-F-SP+RS have similar effectiveness, their efficiency differs significantly. Our efficiency analysis on ImageNet-100, visualized in <ref type="figure">Fig. 1(a)</ref> and summarized in <ref type="table">Table 4</ref>, shows that cDR-RS only requires 1.19% of the storage usage and 77% of the implementation time consumed by DRE-F-SP+RS. Furthermore, since the cDRE-F-cSP is sample-based and does not rely on any properties of cGANs, cDR-RS is applicable to various cGAN architectures, making it more flexible than GOLD, Collab, DRS and DDLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sampling from CcGANs</head><p>Experimental setup: Besides class-conditional GANs, we evaluate the performances of candidate methods in sampling from CcGANs. We experiment on the UTKFace <ref type="bibr" target="#b40">[41]</ref> and RC-49 datasets <ref type="table">Table 3</ref>: Effectiveness of different methods in sampling various class-conditional GANs in terms of Intra-FID, FID, and IS. We evaluate the quality of 100,000 fake images (10,000 per class for CIFAR-10 or 1000 per class for CIFAR-100 and ImageNet-100) sampled from each method. The numbers in the parentheses are the standard deviations of the FIDs computed within each distinct class. The quality of real images from each datasets is also provided as references, where the Intra-FID and FID are computed between training and test samples while IS is computed in terms of test samples only. Please note that the test samples of ImageNet-100 are insufficient for computing a reliable Intra-FID. In all settings, cDR-RS is either better than or comparable to DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and these two methods substantially outperform the others. Compared with DRE-F-SP+RS, the advantage of cDR-RS in subsampling class-conditional GANs is its significantly improved efficiency (see <ref type="figure">Fig. 1</ref> for efficiency comparisons).  <ref type="table">Table 4</ref>: Efficiency of different sampling methods on ImageNet-100 based on Two NVIDA V100. For DRE-F-SP+RS and cDR-RS, the training time includes the time spent on the feature extractor training and the MLP-5 network training. Note that, with comparable effectiveness, cDR-RS only requires 1.19% of the storage usage and 77% of the implementation time spent by DRE-F-SP+RS. With much better performance, cDR-RS requires only 30.7% of the implementation time spent by DDLS. Collab and DRS require less storage space and implementation time, but they they are much less effective in sampling from cGANs (see <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Total storage usage (GB) We follow the official implementation of CcGAN (SVDL+ILI) in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. GOLD is not taken as a baseline method because of its incompatibility in subsampling CcGANs. DRE-F-SP+RS is tested on UTKFace but excluded from the RC-49 experiment because it cannot subsample images that are generated conditional on unseen labels. DDLS is also not tested in the RC-49 experiment due to its extremely long sampling time. When implementing DRE-F-SP+RS, we use the SAE proposed in Section 3.1 to extract features. Please refer to Supp. S.6 and S.7 for details. Evaluation metrics: Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we adopt four evaluation metrics: Intra-FID <ref type="bibr" target="#b3">[4]</ref>, Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b41">[42]</ref>, Diversity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and Label Score <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Intra-FID is taken as the overall image quality metric, which computes FID separately at each evaluation angle and report the average score (along with the standard deviation). Diversity measures the diversity of fake images generated conditional on a given label in terms of some categorical properties of fake images (i.e., races for UTKFace and chair types for RC-49). Label Score evaluates label consistency, measuring the discrepancy between actual and conditioning labels of fake images. Quantitatively, we prefer smaller Intra-FID, NIQE and Label Score indices but larger Diversity values. Please see Supp. S.6 and S.7 for detailed definitions. Experimental results: We quantitatively compare the performance of different candidate methods in sampling from CcGANs. For the UTKFace experiment, we sample 1000 fake images for each age by each method. For the RC-49 experiment, we sample 200 fake images for each of 899 distinct angles by each method (449 angles are unseen in the training set).</p><p>We compare the effectiveness of candidate methods based on multiple evaluation metrics in <ref type="table">Table 5</ref>. We can see Collab has little effect when sampling from CcGANs. DRS can improve NIQE in both experiments, but it fails to simultaneously improve Diversity and Label Score. DDLS improves visual quality in the UTKFace experiment, but it sacrifices Diversity for a slightly lower Label Score. The sampling time (11.24 hours) spent by DDLS is also much longer than that for the other methods, making DDLS infeasible for the RC-49 experiment. DRE-F-SP+RS performs worst among all methods, and it does not apply to RC-49. The proposed cDR-RS with the filtering scheme, denoted by cDR-RS (Filter), outperforms all other candidate methods on both datasets regarding Intra-FID and Label Score. We also show in <ref type="figure">Fig. 7</ref> some example fake images for age 24 sampled by Baseline, DRE-F-SP+RS, and cDR-RS (Filter) with real images as reference. <ref type="figure">Fig. 7</ref> demonstrates the effectiveness of cDR-RS (Filter) and the failure of DRE-F-SP+RS. In <ref type="table">Table 5</ref>, we also show the performance of cDR-RS without the filtering scheme, i.e., cDR-RS (No Filter).</p><p>We can see, cDR-RS (No filter) cannot effectively solve the label inconsistency problem and even makes it worse. Thus, this observation validates the effectiveness of the proposed filtering scheme. In addition, we conduct an ablation study on UTKFace to analyze the effects of ? of the filtering scheme, and this analysis is visualized in <ref type="figure" target="#fig_5">Fig. 8</ref>. The efficiency analysis of candidate methods on UTKFace is shown in <ref type="table">Table 6</ref>. Collab and DRS request low storage space and very little implementation time. Oppositely, DDLS spends a very long implementation time, and DRE-F-SP+RS requests a lot of storage space. cDR-RS requires more storage space and implementation time than Collab and DRS do, but it is much more efficient than DDLS and DRE-F-SP+RS.</p><p>In terms of the effectiveness and efficiency analysis, we can conclude that, although Collab and DRS are very efficient, they fail to improve CcGANs effectively. DRE-F-SP+RS is inefficient and ineffective in sampling from CcGANs. DDLS is not a practical sampling method for CcGANs because its implementation takes too long. Unlike other candidate methods, cDR-RS requires only reasonable computational resources but leads to substantial performance improvements. Why the Diversity score is improved? It may be counter-intuitive that the Diversity score is increased when cDR-RS may reject some generated images. <ref type="figure">Fig. 9</ref> provides an explanation by visualizing the distributions of 1000 fake images sampled from Baseline and cDR-RS, respectively, for age 36 over 5 races in the UTKFace experiment. To make the illustration clearer, we increase ? to 0.183, so that the Label Scores of Baseline and cDR-RS are comparable, and the improvement caused by cDR-RS focuses on Diversity. Please note that, in the evaluation of the UTKFace experiment, Diversity is defined as the average entropy of the races of fake images that are predicted by a pre-trained classification CNN (races are taken as class labels). Therefore, a more balanced race distribution implies higher Diversity. From <ref type="figure">Fig. 9</ref>, we can see that, after applying cDR-RS, the frequency of Race 1 decreases while the frequencies for other races increase. Therefore, the Diversity score is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have presented a novel conditional subsampling scheme to improve the image quality of fake images from conditional GANs, with conditioning on a class or a continuous variable. First, we propose novel conditional extensions of density ratio estimation (cDRE) in the feature space and the Softplus loss function (cSP). Then, we learn the conditional density ratio model <ref type="table">Table 5</ref>: Effectiveness analysis of different sampling methods with CcGANs (SVDL+ILI) and two regression datasets in terms of Intra-FID, NIQE, Diversity, and Label Score. Values in the parentheses represent the standard deviation of evaluation scores reported at each distinct regression label. The proposed cDR-RS substantially outperforms other candidate methods on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Intra  <ref type="figure">Fig. 7</ref>: Some example images for age 24 at 64 ? 64 resolution in the UTKFace experiment. The first row includes ten real images. From the second row to the bottom, we show some example fake images generated from Baseline, DRE-F-SP+RS <ref type="bibr" target="#b27">[28]</ref>, and the proposed cDR-RS, respectively. By comparing Row 2 and 4, we can see cDR-RS can effectively improve the visual quality. By contrast, DRE-F-SP+RS worsens the visual quality. <ref type="table">Table 6</ref>: Efficiency analysis of different sampling methods on UTKFace based on One NVIDA V100. For DRE-F-SP+RS and cDR-RS, the training time includes the time spent on the SAE training and the MLP-5 network training. cDR-RS is more efficient than DRE-F-SP+RS and DDLS in terms of either storage usage or implementation time. Collab and DRS requires less storage space and implementation time, but they do not have significant enough effectiveness in sampling from CcGANs (see <ref type="table">Table 5</ref>).   <ref type="table">Table 5</ref>, respectively. The vertical grey line specifies the ? used in <ref type="table">Table 5</ref>. We can see both Label Score and Diversity increase as ? increases. If we prefer higher label consistency, we can decrease ?. Oppositely, if we prefer higher image diversity, we can increase ?. No matter what the preference is, when choosing ?, we should ensure that the corresponding Label Score is below the red dotted line while Diversity is above the black dotted line. In that case, both label consistency and image diversity can be improved. A rule of thumb for the parameter selection is provided in Remark 4. <ref type="figure">Fig. 9</ref>: The distributions of fake images sampled from Baseline and cDR-RS, respectively, at age 36 over 5 races in the UTKFace experiment. After subsampling by cDR-RS, the distribution of fake images for 5 races is more balanced, resulting in higher diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>through an MLP network. Also, we derive the error bound of a conditional density ratio model trained with the proposed cSP loss. A novel filtering scheme is also proposed in subsampling CcGANs to improve the label consistency. Finally, we validate the superiority of the proposed subsampling framework with extensive experiments in sampling multiple conditional GAN models on four benchmark datasets with diverse evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1. The Proof of Theorems 1</head><p>Proof. Following <ref type="bibr" target="#b27">[28]</ref>, we decompose L c (?) ? L c (? * ) as follows:</p><formula xml:id="formula_16">L c (?) ? L c (? * ) =L c (?) ? L c (?) + L c (?) ? L c (?) + L c (?) ? L c (?) + L c (?) ? L c (? * ) (Since L c (?) ? L c (?) ? 0) ?L c (?) ? L c (?) + L c (?) ? L c (?) + L c (?) ? L c (? * ) ?2 sup ??? L c (?) ? L c (?) + L c (?) ? L c (? * ). (S.1)</formula><p>The second term in Eq. (S.1) is a constant which implies an inevitable error. The first term can be bounded as follows:</p><formula xml:id="formula_17">sup ??? L c (?) ? L c (?) ? sup ??? E (h,y)?q g (h,y) ?(?(h|y))?(h|y) ? ?(?(h|y)) ? 1 N g N g i=1 ?(?(h g i |y g i ))?(h g i |y g i ) ? ?(?(h g i |y g i )) + E (h,y)?q r (h,y) ?(?(h|y)) ? 1 N r N r i=1 ?(?(h r i |y r i )) . (S.2)</formula><p>Because of assumptions (i)-(iv), based on the uniform law of large number <ref type="bibr" target="#b42">[43]</ref>, for ? &gt; 0, lim N g ?? P sup ??? E (h,y)?q g (h,y) ?(?(h|y))?(h|y) ? ?(?(h|y))</p><formula xml:id="formula_18">? 1 N g N g i=1 ?(?(h g i |y g i ))?(h g i |y g i ) ? ?(?(h g i |y g i )) ? ? ? ? ? ? ? = 0.</formula><p>Based on this limit, we can derive an upper bound of the first term of Eq. (S.2) as follows. Since we can generate infinite fake images from a trained cGAN, N g is large enough. Let = 1/2N g , ?? 1 ? (0, 1) with probability at least 1 ? ? 1 , sup ??? E (h,y)?q g (h,y) ?(?(h|y))?(h|y) ? ?(?(h|y))</p><formula xml:id="formula_19">? 1 N g N g i=1 ?(?(h g i |y g i ))?(h g i |y g i ) ? ?(?(h g i |y g i )) ? 1 2N g . (S.3)</formula><p>The second term of Eq. (S.2) can be bounded based on Lemma 1 and Theorem 2 (the Rademacher bound <ref type="bibr" target="#b43">[44]</ref>) in <ref type="bibr" target="#b27">[28]</ref> as follows: ?? 2 ? (0, 1) with probability at least 1 ? ? 2 , </p><formula xml:id="formula_20">E (h,y)?q r (h,y) ?(?(h|y)) ? 1 N r N r i=1 ?(?(h r i |y r i )) ?2R q r (h,y),N r (? ? ?) + 4 N r log 2 ? 2 ? 1 2R q r (h,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2. Resources for Implementing cGANs and Sampling Methods</head><p>To implement ACGAN, we refer to https://github.com/sangwoomo/GOLD. To implement SNGAN, we refer to https://github.com/christiancosgrove/pytorchspectral-normalization-gan and https://github.com/pfnet-research/sngan projection.</p><p>To implement BigGAN, we refer to https://github.com/ajbrock/BigGAN-PyTorch.</p><p>To implement CcGANs, we refer to https://github.com/UBCDingXin/improved CcGAN.</p><p>To implement GOLD, we refer to https://github.com/sangwoomo/GOLD. To implement Collab, we refer to https://github.com/YuejiangLIU/pytorch-collaborativegan-sampling.</p><p>To implement DRS and DRE-F-SP+RS, we refer to https://github.com/UBCDingXin/ DDRE Sampling GANs.</p><p>To implement DDLS, we refer to https://github.com/JHpark1677/CGAN-DDLS and https: //github.com/Daniil-Selikhanovych/ebm-wgan/blob/master/notebook/EBM GAN.ipynb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3. More Details of Experiments on CIFAR-10</head><p>In the CIFAR-10 experiment, we first train ACGAN, SNGAN, and BigGAN with setups shown as follows. We train ACGAN for 100,000 iterations with batch size 512. We train SNGAN for 50,000 iterations with batch size 512. BigGAN is trained for 39,000 with batch size 512. We use the architecture described in <ref type="figure" target="#fig_4">Figure 15</ref> of <ref type="bibr" target="#b0">[1]</ref> for BigGAN.</p><p>To implement Collab, following <ref type="bibr" target="#b25">[26]</ref>, we conduct discriminator shaping for 5000 and 2500 iterations for SNGAN and BigGAN, respectively. We do the refinement 30 times in a middle layer of SNGAN and 20 times in a middle layer of BigGAN. The step size of refinement is set as 0.1 for both SNGAN and BigGAN.</p><p>To implement DDLS, we run the Langevin dynamics procedure for SNGAN and BigGAN within each class with step size 10 ?4 up to 1000 iterations.</p><p>To implement DRE-F-SP+RS, we first train the specially designed ResNet-34 on the training set for 350 epochs with the SGD optimizer, initial learning rate 0.1 (decayed at epoch 150 and 250 with factor 0.1), weight decay 10 ?4 , and batch size 256. Ten MLP-5 models for modeling the density ratio function within each class are trained on the training set with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 100 and 250), batch size 256, 400 epochs, and ? = 10 ?2 . The network architecture of MLP-5 is shown in <ref type="table" target="#tab_6">Table S</ref>.3.7.</p><p>To implement cDR-RS, we use the specially designed ResNet-34 in the implementation of DRE-F-SP+RS to extract features from images. The MLP-5 to model the conditional density ratio function is shown in <ref type="table" target="#tab_6">Table S</ref>.3.8. It is trained with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 80 and 150), batch size 256, 200 epochs, and ? = 10 ?2 .</p><p>For a more accurate evaluation, we do not use Inception-V3 <ref type="bibr" target="#b45">[46]</ref> that was pre-trained on ImageNet <ref type="bibr" target="#b31">[32]</ref> to compute Intra-FID, FID, and IS. Instead, following <ref type="bibr" target="#b27">[28]</ref>, we train Inception-V3 from scratch on CIFAR-10 to evaluate fake images.</p><p>Please refer to our codes for more detailed setups such as network architectures and hyperparameter settings. Extracted feature h ? R 3072 fc? 2048, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 1024, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 512, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 256, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 128, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 1, ReLU fc? 2048, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 1024, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 512, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 256, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 128, GN (8 groups), ReLU, Dropout(p = 0.5) fc? 1, ReLU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4. More Details of Experiments on CIFAR-100</head><p>In the CIFAR-100 experiment, we only test candidate sampling methods on BigGAN because both ACGAN and SNGAN are unstable on CIFAR-100 and somehow suffer from the model collapse problem <ref type="bibr" target="#b46">[47]</ref>. BigGAN is trained for 38,000 iterations with batch size 512 on the training set of CIFAR-100. We use the architecture described in <ref type="figure" target="#fig_4">Figure 15</ref> of <ref type="bibr" target="#b0">[1]</ref> for BigGAN. We also adopt DiffAugment <ref type="bibr" target="#b47">[48]</ref> (a data augmentation method for the GAN training with limited data) to improve the performance of BigGAN. The strongest data augmentation policy, "color,translation,cutout", is used for DiffAugment.</p><p>To implement Collab, following <ref type="bibr" target="#b25">[26]</ref>, we conduct discriminator shaping for 2500 iterations for BigGAN. We do the refinement 30 times in a middle layer of the generator network of BigGAN. The step size of refinement is set as 0.1.</p><p>To implement DDLS, we run the Langevin dynamics procedure for BigGAN within each class with step size 10 ?4 up to 1000 iterations.</p><p>To implement DRE-F-SP+RS, we first train the specially designed ResNet-34 on the training set for 350 epochs with the SGD optimizer, initial learning rate 0.1 (decayed at epoch 150 and 250 with factor 0.1), weight decay 10 ?4 , and batch size 256. Ten MLP-5 models for modeling the density ratio function within each class are trained on the training set with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 100 and 250), batch size 256, 400 epochs, and ? = 10 ?2 . The network architecture of MLP-5 is shown in <ref type="table" target="#tab_6">Table S</ref>.3.7.</p><p>To implement cDR-RS, we use the specially designed ResNet-34 in the implementation of DRE-F-SP+RS to extract features from images. The MLP-5 to model the conditional density ratio function is shown in <ref type="table" target="#tab_6">Table S</ref>.3.8. It is trained with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 80 and 150), batch size 256, 200 epochs, and ? = 10 ?2 .</p><p>For a more accurate evaluation, we do not use Inception-V3 <ref type="bibr" target="#b45">[46]</ref> that was pre-trained on ImageNet <ref type="bibr" target="#b31">[32]</ref> to compute Intra-FID, FID, and IS. Instead, following <ref type="bibr" target="#b27">[28]</ref>, we train Inception-V3 from scratch on CIFAR-100 to evaluate fake images.</p><p>Please refer to our codes for more detailed setups such as network architectures and hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5. More Details of Experiments on ImageNet-100</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.1. Setups of training, sampling, and evaluation</head><p>In the ImageNet-100 dataset, we implement BigGAN with the BigGAN-deep architecture described in <ref type="figure">Figure 16</ref> of <ref type="bibr" target="#b0">[1]</ref>. BigGAN is trained for 96,000 iterations with batch size 1024. We also adopt DiffAugment <ref type="bibr" target="#b47">[48]</ref> (a data augmentation method for the GAN training with limited data) to improve the performance of BigGAN. The strongest data augmentation policy, "color,translation,cutout", is used for DiffAugment.</p><p>To implement Collab, following <ref type="bibr" target="#b25">[26]</ref>, we conduct discriminator shaping for 3000 iterations for BigGAN. We do the refinement 16 times in a middle layer of the generator network of BigGAN. The step size of refinement is set as 0.5.</p><p>To implement DRS, we fine-tune the discriminator of BigGAN for 5 epochs with batch size 128.</p><p>To implement DDLS, we run the Langevin dynamics procedure for BigGAN within each class with step size 10 ?4 up to 1000 iterations.</p><p>To implement DRE-F-SP+RS, we first train the specially designed ResNet-34 on the training set for 350 epochs with the SGD optimizer, initial learning rate 0.1 (decayed at epoch 150 and 250 with factor 0.1), weight decay 10 ?4 , and batch size 128. Ten MLP-5 models for modeling the density ratio function within each class are trained on the training set with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 100 and 250), batch size 256, 400 epochs, and ? = 10 ?2 . The network architecture of MLP-5 is similar to Table S.3.7. Please note that, when implementing DRE-F-SP+RS, we use more epochs than cDR-RS does (400 epochs vs 200 epochs) to ensure that all density ratio models are well-trained.</p><p>To implement cDR-RS, we use the specially designed ResNet-34 in the implementation of DRE-F-SP+RS to extract features from images. The MLP-5 to model the conditional density ratio function is similar to Table S.3.8. It is trained with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 80 and 150), batch size 128, 200 epochs, and ? = 10 ?2 .</p><p>For a more accurate evaluation, we fine-tune on the training set of ImageNet-100 <ref type="bibr" target="#b30">[31]</ref> an Inception-V3 network that was pre-trained on ImageNet <ref type="bibr" target="#b31">[32]</ref>. We fine-tune the Inception-V3 with the SGD optimizer, 50 epochs, batch size 128, and initial learning rate 10 ?4 . The fine-tuned Inception-V3 is then used to compute Intra-FID, FID, and IS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.2. More details of the efficiency analysis</head><p>In order to compare the efficiency of candidate sampling methods, we summarize their storage usage and training and sampling time in <ref type="table" target="#tab_6">Table S</ref>.5.9, based on which we plot <ref type="figure">Fig. 1</ref>. Some pie charts are also plotted to show more detailed storage usage and training time for DRE-F-SP+RS and cDR-RS in <ref type="figure" target="#fig_4">Fig. S.5.10</ref>. Please note that, the storage usage here, is the overall storage usage consumed by all models (e.g., the discriminator network in DRS and density ratio models in cDR-RS) in each sampling method except the generator network of cGANs. For DRE-F-SP+RS, the 100 MLP-5 models take a lot of disk space (almost 40 GB), making it less efficient in subsampling class-conditional GANs with many classes. We use two NVIDIA V100 GPUs (32GB) for this analysis.  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> to implement CcGANs (SVDL+ILI) with the SNGAN architecture. The detailed setups can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or our codes. Please note that, due to randomness in training, our reported evaluation results of CcGANs, such as Intra-FID, are slightly different from those in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To implement Collab, we conduct discriminator shaping for 3000 iterations for CcGAN. We do the refinement 16 times in a middle layer of the generator network of CcGAN. The step size of refinement is set as 0.5.</p><p>To implement DRS, we fine-tune the discriminator of CcGAN for 2000 iterations with batch size 128.</p><p>To implement DDLS, we run the Langevin dynamics procedure for CcGAN for each age with step size 10 ?4 up to 400 iterations. More iterations will make the sampling process too time-consuming.</p><p>To implement DRE-F-SP+RS, we first train the specially designed SAE on the training set for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decayed every 50 epochs with factor 0.1), weight decay 10 ?4 , batch size 256, and sparsity parameter ? = 10 ?3 . The network architecture of this SAE is shown in <ref type="table" target="#tab_6">Table S</ref>  <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 100 and 250), batch size 256, 400 epochs, and ? = 10 ?2 . The network architecture of MLP-5 is similar to Table S.3.7. Similar to above experiments, when implementing DRE-F-SP+RS, we use more epochs than cDR-RS does to let all density ratio models converge. Nevertheless, in this experiment, there are still many density ratio models do not converge, which may be one reason of the failure of DRE-F-SP+RS in subsampling CcGANs.</p><p>To implement cDR-RS, we use the specially designed SAE in the implementation of DRE-F-SP+RS to extract features from images. The MLP-5 to model the conditional density ratio function is similar to Table S.3.8. It is trained with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 80 and 150), batch size 256, 200 epochs, and ? = 10 ?2 . The ? in the filtering scheme of cDR-RS is set to be 0.1.</p><p>The models for the computation of Intra-FID, NIQE, Diversity, and Label Score are consistent with those used by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. We quote and rephrase the definitions of these metrics in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> as follows.</p><p>? Intra-FID <ref type="bibr" target="#b3">[4]</ref>: "We take Intra-FID as the overall score to evaluate the quality of fake images and we prefer the small Intra-FID score. At each evaluation angle, we compute the FID <ref type="bibr" target="#b38">[39]</ref> between real images and 1000 fake images in terms of the bottleneck feature of the pre-trained AE."</p><p>? NIQE <ref type="bibr" target="#b41">[42]</ref>: "NIQE is used to evaluate the visual quality of fake images with the real images as the reference and we prefer the small NIQE score."</p><p>? Diversity: "Diversity is used to evaluate the intra-label diversity and the larger the better." In UTKFace, there are 6 races. At each age, we ask a pre-trained classification-oriented ResNet-34 to predict the races of the 1000 fake images and an entropy is computed based on these predicted races. The Diversity score is the average of the entropies computed on all ages.</p><p>? Label Score: "Label Score is used to evaluate the label consistency and the smaller the better." We ask the pre-trained regression-oriented ResNet-34 to predict the ages of all fake images and the predicted ages are then compared with the conditioning ages. The Label To implement cDR-RS, first train the specially designed SAE on the training set for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decayed every 50 epochs with factor 0.1), weight decay 10 ?4 , batch size 256, and sparsity parameter ? = 10 ?3 . The MLP-5 to model the conditional density ratio function is similar to Table S.3.8. It is trained with the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, initial learning rate 10 ?4 (decayed at epoch 80 and 150), batch size 256, 200 epochs, and ? = 10 ?3 . The ? in the filtering scheme of cDR-RS is set to be 0.13.</p><p>In the testing phase, we generate 179,800 fake images (200 per angle) from each candidate method over 899 distinct labels. This experiment adopts four evaluation metrics-(i) Intra-FID <ref type="bibr" target="#b3">[4]</ref> is an overall image quality metric; (ii) Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b41">[42]</ref> evaluates the visual quality of fake images. Please note again that the visual quality is only one aspect of image quality; (iii) Diversity measures the diversity of fake images; and (iv) Label Score (LS) evaluates label consistency. Specifically, the four metrics are computed as follows. (i) For the Intra-FID index, at each of the 899 angles (0.1 ? ? 89.9 ? ), we compute the FID <ref type="bibr" target="#b38">[39]</ref> value between 49 real images and 200 fake images in terms of the bottleneck feature of the pre-trained autoencoder. The Intra-FID score is the average FID over all 899 evaluation angles. (ii) For the NIQE index, firstly we fit an NIQE model with the 49 real rendered chair images at each of the 899 angles which gives 899 NIQE models. We then compute an average NIQE score for each evaluated angle using the NIQE model at that angle. Finally, we report the average of the 899 average NIQE scores over the 899 yaw angles. The block size and the sharpness threshold are set to 8 and 0.1 respectively in this experiments. We employ the built-in NIQE library in MATLAB. (iii) For the Diversity index, at each evaluation angle, firstly we use a pretrained classification-oriented ResNet-34 to predict the chair types (49 types in total) of these 200 fake images. Then, an entropy value can be computed based on the chair type predictions at this angle. Finally, the Diversity index is defined as the average of the entropies at all 899 angles. (iv) For the Label Score index, at each evaluation angle, firstly we ask a pretrained regression-oriented ResNet-34 to predict the yaw angles of all fake image samples and the predicted angles are then compared with the assigned angles. The Label Score value is defined as the average absolute distance between the predicted angles and assigned angles over all fake images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>e., ?(t) = ln (1 + e t ) r * (x|y) the actual conditional density ratio function of images ? * (h|y) the actual conditional density ratio function of extracted features ?(h|y) conditional density ratio model (neural network) L c (?) the proposed conditional Softplus (cSP) loss L c (?) the empirical version of the cSP loss ? a set of functions that can be represented by ?(h|y) ?? = arg min ??? L c (?) ?? = arg min ??? L c (?) Q c (?) the penalty term used when training ? ? a positive hyper-parameter to control the penalty strength of Q c (?) ? a positive hyper-parameter to control the sparsity of extracted features Y ? y Y ? y := [y ? ?, y + ?], i.e., the neighborhood of a regression label y with width ? ? a positive hyper-parameter to control the width of y's neighborhood Y ? y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The network architecture of the feature extractor ?(x) for subsampling class-conditional GANs. The feature extractor ?(x) consists of a convolutional block (Conv.) and a fully-connected block (fc 1), where Conv. is the convolutional block of ResNet-34 and fc 1 increases the dimension of features to (C ? H ? W) ? 1 for equal dimensionality. An extra fully-connected block (fc 2) predicts the class label y of the input image x in terms of the extracted feature h. The whole model (? and fc 2) is trained under the cross entropy loss. Although the invertibility of ? is a necessary assumption for Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The network architecture of the sparse autoencoder (SAE) for feature extraction in subsampling Cc-GANs. Usually, the bottleneck dimension of SAE is much smaller than the input image, but the validity of Eq.(3) requires they have equal dimensions. To prevent SAE from overfitting training data, we penalize the mean of the features to encourage sparsity. With ReLU as the last layer, the features h = [h 1 , ...h m ] extracted by the encoder are non-negative. Thus, their mean ( m i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 ratio 8 M 9 p</head><label>789</label><figDesc>? ?(?(x)|y); ? max{M, ratio}; ? ratio/M (i.e., the acceptance probability in RS); 10 u ? Uniform[0, 1]; 11 if u ? p then 12Append(x, images);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The effect of ? in the filtering scheme on the implementation time of cDR-RS on UTKFace. Larger ? implies shorter run time, i.e., the the training time of MLP-5 plus the sampling time. The training time of SAE for feature extraction is not influenced by ? and is excluded from this analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>The effects of ? in the filtering scheme of cDR-RS on the relation between Diversity and Label Score of CcGAN-generated samples in the UTKFace experiment. The dotted black and red lines stand for Diversity and Label Score of Baseline in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 )</head><label>4</label><figDesc>y),N r (?) Let ? = max{? 1 , ? 2 } and ? = ?, based on Eq. (S.3) and (S.4), we can derive Eq. (11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation and definitions.</figDesc><table><row><cell>Notation</cell><cell>Definitions</cell></row><row><cell>x x</cell><cell>an image at C ? H ? W resolution, which may have a subscript or a superscript, e.g., x r i for real image ? a reconstructed image at C ? H ? W resolution, which may have a subscript or a superscript, e.g., x r i</cell></row><row><cell>?(x)</cell><cell>the feature extractor, i.e., a pre-trained, invertible neural network</cell></row><row><cell>h y y</cell><cell>the high-level feature (a C ? H ? W by 1 vector) extracted by ?(x) from an image, which may have a subscript or a superscript, e.g., h r i a class/regression label, which may have a subscript or a superscript, e.g., y r ? a predicted class/regression label, which may have a subscript or a superscript, e.g.,? r i</cell></row><row><cell>p r (x|y)</cell><cell>the density function of the real images conditional on y</cell></row><row><cell>p g (x|y)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S .</head><label>S</label><figDesc>3.7: The 5-layer MLP for DRE in feature space for CIFAR-10 and CIFAR-100.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S .</head><label>S</label><figDesc>3.8: The 5-layer MLP for cDRE in feature space for CIFAR-10 and CIFAR-100. The embedded class label is appended to the extracted feature h.</figDesc><table><row><cell>Input: extracted feature h ? R 3072 and embedded class label y ? R C ,</cell></row><row><cell>where C = 10 for CIFAR-10 and C = 100 for CIFAR-100 Concatenate [h, y] ? R 3082</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S .</head><label>S</label><figDesc>5.9: Efficiency analysis of different sampling methods on ImageNet-100 based on Two NVIDA V100. For DRE-F-SP+RS and cDR-RS, the training time includes the time spent on the ResNet-34 training and the MLP-5 network training.</figDesc><table><row><cell>Methods</cell><cell>Total storage usage (GB)</cell><cell>Total training time (hours)</cell><cell>Total sampling time (hours)</cell><cell>Total implementation time (hours)</cell></row><row><cell>Collab</cell><cell>0.13</cell><cell>5.58</cell><cell>5.23</cell><cell>10.81</cell></row><row><cell>DRS</cell><cell>0.13</cell><cell>1.47</cell><cell>0.75</cell><cell>2.22</cell></row><row><cell>DDLS</cell><cell>0.13</cell><cell>0</cell><cell>218.05</cell><cell>218.05</cell></row><row><cell>DRE-F-SP+RS</cell><cell>38.94</cell><cell>84.41</cell><cell>2.43</cell><cell>86.84</cell></row><row><cell>cRS-RS</cell><cell>0.75</cell><cell>65.69</cell><cell>1.18</cell><cell>66.87</cell></row><row><cell cols="3">S.6. More Details of Experiments on UTKFace</cell><cell></cell><cell></cell></row><row><cell cols="3">S.6.1. Setups of training, sampling, and evaluation</cell><cell></cell><cell></cell></row><row><cell>We follow</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>.6.10, Table S.6.11, and Table S.6.12. Sixty MLP-5 models for modeling the density ratio function at each age are trained on the training set with the Adam optimizer</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S</head><label>S</label><figDesc>.6.11: The architecture of the decoder in the sparse autoencoder for reconstructing 64 ? 64 input images from extracted features. In transposed-convolutional (ConvT) operations, ch denotes the number of channels, k/s/p denote kernel size, stride and number of padding, respectively. Input: extracted sparse features h ? R 12288 from Table S.6.10. fc? 4096, BN, ReLU Reshape: 4096 ? 64 ? 4 ? 4 ? 4 ConvT (ch ? 256, k4/s2/p1), BN, ReLU ConvT (ch ? 128, k3/s1/p1), BN, ReLU ConvT (ch ? 128, k4/s2/p1), BN, ReLU ConvT (ch ? 64, k3/s1/p1), BN, ReLU ConvT (ch ? 64, k4/s2/p1), BN, ReLU ConvT (ch ? 64, k3/s1/p1), BN, ReLU ConvT (ch ? 64, k4/s2/p1), BN, ReLU ConvT (ch ? 64, k3/s1/p1), BN, ReLU ConvT (ch ? 3, k1/s1/p0), Tanh Output: a reconstructed image x ? R 64?64?3 Table S.6.12: The architecture of the label prediction branch in the sparse autoencoder for 64 ? 64 images. Input: extracted sparse features h ? R 12288 from Table S.6.10. SP+RS is not applicable to this scenario, where we need to generate images conditional on labels that are unseen in the training phase.</figDesc><table><row><cell>fc? 1024, BN, ReLU</cell></row><row><cell>fc? 512, BN, ReLU</cell></row><row><cell>fc? 256, BN, ReLU</cell></row><row><cell>fc? 1, ReLU</cell></row><row><cell>Output: the predicted label?</cell></row><row><cell>DRE-F-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S .</head><label>S</label><figDesc>6.13: Efficiency analysis of different sampling methods on UTKFace based on One NVIDA V100. For DRE-F-SP+RS and cDR-RS, the training time includes the time spent on the SAE training and the MLP-5 network training.</figDesc><table><row><cell>Methods</cell><cell>Total storage usage (MB)</cell><cell>Total training time (hours)</cell><cell>Total sampling time (hours)</cell><cell>Total implementation time (hours)</cell></row><row><cell>Collab</cell><cell>82.8</cell><cell>1.05</cell><cell>0.27</cell><cell>1.32</cell></row><row><cell>DRS</cell><cell>82.8</cell><cell>0.16</cell><cell>0.13</cell><cell>0.29</cell></row><row><cell>DDLS</cell><cell>82.8</cell><cell>0</cell><cell>11.24</cell><cell>11.24</cell></row><row><cell>DRE-F-SP+RS</cell><cell>6,671</cell><cell>1.92</cell><cell>5.69</cell><cell>7.61</cell></row><row><cell>cRS-RS</cell><cell>303</cell><cell>1.99</cell><cell>0.49</cell><cell>2.48</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CcGAN: Continuous conditional generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07466</idno>
		<title level="m">Continuous conditional generative adversarial networks: Novel empirical losses and label input mechanisms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cGANs with projection discriminator</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The theoretical research of generative adversarial networks: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial-learning-based image-to-image transformation: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="468" to="486" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Degan: Decentralized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Faezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bijani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dolati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mfc-gan: class-imbalanced dataset classification using multiple fake class generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali-Gombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="page" from="212" to="221" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional image generation with one-vs-all classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="page" from="261" to="267" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Twist-gan: Towards wavelet transform and transferred gan for spatio-temporal single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Dharejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deeba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jatoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zawish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised generative adversarial networks with cross-model weight transfer mechanism for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1814" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic multi-label image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dae-gan: Dynamic aspect-aware gan for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13960" to="13969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on generative adversarial network-based text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">451</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Range-constrained generative adversarial network: Design synthesis under constraints using conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Nobari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mechanical Design</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Triess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>B?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Z?llner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08526</idno>
		<title level="m">Point cloud generation with continuous conditioning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08175</idno>
		<title level="m">On the evaluation of conditional GANs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative sampling in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminator rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Subsampling generative adversarial networks: Density ratio estimation in feature space with Softplus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1910" to="1922" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12275" to="12287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmd</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hashnet: Deep learning to hash by continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5608" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining GOLD samples for conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6170" to="6181" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
	<note>Improved techniques for training GANs</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Some useful asymptotic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Link</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<title level="m">Concentration of measure</title>
		<imprint/>
	</monogr>
	<note>Link</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
	<note>Improved training of Wasserstein GANs</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7559" to="7570" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Score is defined as the average absolute distance between the predicted ages and conditioning ages over all fake images</title>
		<imprint/>
	</monogr>
	<note>which is equivalent to the Mean Absolute Error (MAE</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Please refer to the official implementation of CcGANs at</title>
		<ptr target="https://github.com/UBCDingXin/improvedCcGANformoredetails" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">10: The architecture of the encoder in the sparse autoencoder for extracting features from 64 ? 64 RGB images. In convolutional (Conv) operations, ch denotes the number of channels, k/s/p denote kernel size, stride and number of padding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Table</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">respectively. Input: an RGB image x ? R 64?64?3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">BN, ReLU Conv (ch ? 64, k4/s2/p1), BN, ReLU Conv (ch ? 128, k3/s1/p1), BN, ReLU Conv (ch ? 128, k4/s2/p1), BN, ReLU Conv (ch ? 256, k3/s1/p1), BN, ReLU Conv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<idno>k3/s1/p1</idno>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>ch ? 64, k4/s2/p1. ch ? 256, k4/s2/p1), BN, ReLU Conv (ch ? 256, k3/s1/p1), BN, ReLU Flatten: 64 ? 4 ? 4 ? 4 ? 4096 fc? 12288, ReLU Output: extracted sparse features h ? R 12288</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">More details of the efficiency analysis Similar to the ImageNet-100 experiment, we analyze the efficiency of all candidate methods, and summarize the results in Table S.6.13 and Fig. S.6.11. Please note that, the storage usage here, is the overall storage usage consumed by all models (e.g., the discriminator network in DRS and density ratio models in cDR-RS) in each sampling method except the generator network of cGANs. The 60 MLP-5 models take a lot of disk space for DRE-F-SP+RS</title>
		<idno>S.6.2</idno>
		<imprint/>
	</monogr>
	<note>We use one NVIDIA V100 GPUs (16GB) for this analysis</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">More Details of Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno>on RC-49</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Similar to the UTKFace experiment, we follow [2, 3] to implement CcGANs (SVDL+ILI) with the SNGAN architecture. The detailed setups can be found in [2, 3] or our codes. Please note that, due to randomness in training, our reported evaluation results of CcGAN (SVDL+ILI), such as Intra-FID</title>
		<imprint/>
	</monogr>
	<note>are slightly different from those in [2, 3</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">We do the refinement 16 times in a middle layer of the generator network of CcGAN. The step size of refinement is set as 0.5. To implement DRS, we fine-tune the discriminator of CcGAN for 1000 iterations with batch size 128</title>
	</analytic>
	<monogr>
		<title level="m">To implement Collab, we conduct discriminator shaping for 3000 iterations for CcGAN</title>
		<imprint/>
	</monogr>
	<note>49 due to a too long sampling time. (a) Storage Usage (GB) for DRE-F-SP+RS</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Storage Usage (GB) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Training Time (hours) for DRE-F-SP+RS (d) Training Time (hours) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Total Implementation Time (hours) for DRE-F-SP+RS (f) Total Implementation Time (hours) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<idno>Fig. S.5.10</idno>
		<title level="m">Pie charts for the efficiency analysis of DRE-F-SP+RS and cDR-RS on ImageNet-100. (a) Storage Usage (MB) for DRE-F-SP+RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Storage Usage (MB) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Training Time (hours) for DRE-F-SP+RS (d) Training Time (hours) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Total Implementation Time (hours) for DRE-F-SP+RS (f) Total Implementation Time (hours) for cDR-RS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">11: Pie charts for the efficiency analysis of DRE-F-SP+RS and cDR-RS on UTKFace</title>
	</analytic>
	<monogr>
		<title level="j">Fig. S</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
