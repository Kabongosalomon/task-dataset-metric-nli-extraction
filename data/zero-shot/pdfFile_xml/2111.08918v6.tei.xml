<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Texture Estimator for Implicit Representation Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyong</forename><surname>Hwan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Texture Estimator for Implicit Representation Function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works with an implicit neural function shed light on representing images in arbitrary resolution. However, a standalone multi-layer perceptron shows limited performance in learning high-frequency components. In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, enabling an implicit function to capture fine details while reconstructing images in a continuous manner. When jointly trained with a deep super-resolution (SR) architecture, LTE is capable of characterizing image textures in 2D Fourier space. We show that an LTE-based neural function achieves favorable performance against existing deep SR methods within an arbitrary-scale factor. Furthermore, we demonstrate that our implementation takes the shortest running time compared to previous works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) is one of the most fundamental problems in computer vision and graphics. SISR aims to reconstruct high-resolution (HR) images from its degraded low-resolution (LR) counterpart. Dominant approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> are to extract feature maps using a deep vision architecture and then upsample to HR images at the end of a network. However, we need to train and store several models for each scale factor when an upsampler is implemented by sub-pixel convolution <ref type="bibr" target="#b24">[24]</ref>. In contrast, arbitrary-scale SR methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> are promising since such ideas pave the way to restore images in a continuous manner with only a single network.</p><p>Recently, implicit neural functions parameterized by a multi-layer perceptron (MLP) achieved remarkable performance in representing continuous-domain signals, such as images <ref type="bibr" target="#b3">[4]</ref>, occupancy <ref type="bibr" target="#b18">[19]</ref>, signed distance <ref type="bibr" target="#b20">[21]</ref>, shape representation <ref type="bibr" target="#b10">[11]</ref>, and view synthesis <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">26]</ref>. Such MLPs take coordinates as inputs and are trained in a framework of gradient descent optimization and machine learning. In- <ref type="figure">Figure 1</ref>. Overview of Local Texture Estimator (LTE). Our LTE estimates dominant frequencies and corresponding Fourier coefficients for natural images. Then, an MLP makes use of the estimated essential Fourier information to reconstruct an HR image in arbitrary resolution. We provide low-frequency information by adding an upscaled LR image to the output of MLP. spired by recent progress in implicit representation, LIIF <ref type="bibr" target="#b3">[4]</ref> replaced sub-pixel convolution with MLPs to accomplish arbitrary-scale SR, even at substantial scale factors.</p><p>One limitation of implicit neural representations is that a standalone MLP is biased towards learning low-frequency components <ref type="bibr" target="#b23">[23]</ref> and fails to capture fine details <ref type="bibr" target="#b29">[29]</ref>. Such phenomenon is referred to as spectral bias, and recent lines of research in resolving this problem are projecting input coordinates into a high-dimensional Fourier feature space <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">29]</ref> or substituting a ReLU with a sinusoidal activation <ref type="bibr" target="#b25">[25]</ref>. Motivated from previous works, we study arbitraryscale SISR problems through the lens of Fourier analysis.</p><p>In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, to allow an implicit function to learn fine details while restoring images in arbitrary resolution. We assume that an implicit function prioritizes learning image textures when utilizing dominant frequencies of images, as described in <ref type="figure">Fig. 1</ref>. Let us take an example from an image with vertical textures. Intuitively, the dominant frequencies of such an image are located on an x-axis in 2D Fourier space. We observe that LTE is capable of extracting such dominant frequencies, characterizing image textures in 2D Fourier space, when jointly trained with a deep SR architecture, such as EDSR <ref type="bibr" target="#b14">[15]</ref>, RDN <ref type="bibr" target="#b35">[35]</ref>, and SwinIR <ref type="bibr" target="#b13">[14]</ref>. In addition to extracting dominant frequencies, we show that estimating Fourier coefficients is also essential in improving a representational power of an implicit function in Sec. 5.3.</p><p>Short computation time is essential in SR applications. In addition, restoring larger than 2K-sized images is a memory-consuming task. Hence, we study the computation time of our LTE at various memory conditions and demonstrate that our approach is faster compared to previous works regardless of memory setting in Sec. 6.</p><p>In summary, our main contributions are as follows:</p><p>? We point out that a deep SR network followed by LTE is capable of estimating dominant frequencies and corresponding Fourier coefficients for natural images.</p><p>? We show that an implicit representation function for arbitrary resolution prioritizes learning high-frequency details when essential Fourier information is estimated by LTE.</p><p>? We investigate the computation time of our LTE at several memory settings and demonstrate that our approach is faster compared to previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Implicit neural representation Based on the fact that neural networks are universal function approximators <ref type="bibr" target="#b7">[8]</ref>, coordinate-based MLP is widely applied to represent continuous-domain signals in computer vision and graphics. Such MLPs are a memory-efficient framework for HR data since the amount of memory to store MLPs is independent of data resolution. However, pre-trained MLPs show limited performance in representing unseen data, requiring training per each signal. To overcome this generalization issue, <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref> trained representation with a hypernetwork, which maps latent variables to weights of an MLP as in <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>, an MLP takes not only coordinates but also latent variables as inputs to enable representation to be a function of data. Recently, meta-learned initialization <ref type="bibr" target="#b28">[28]</ref> has been proposed, which provides a strong prior for representing signals, leading to fast convergence. Our work is mainly related to local implicit neural representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Such approaches assume that latent variables are evenly distributed over space, allowing an implicit function to focus on learning local features. Inspired by this, we hypothesize that the local region in HR representation shares the same image textures.</p><p>Spectral bias Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29]</ref> have shown that a standard MLP with ReLUs shows limited performance in representing high-frequency textures. Such a phenomenon is referred to as spectral bias, and various methods have been proposed to alleviate this problem. Recently, SIREN <ref type="bibr" target="#b25">[25]</ref> used the sine layer as a non-linear activation instead of a ReLU, resulting in fast convergence and high data fidelity. Other approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">29]</ref> are to map input coordinates into high dimensional Fourier space by using position encoding or Fourier feature mapping before passing an MLP. Frequencies are fixed to the power of two <ref type="bibr" target="#b19">[20]</ref> or randomly sampled from Gaussian distribution <ref type="bibr" target="#b29">[29]</ref>. Unlike previous works, dominant frequencies from our LTE are data-driven and characterize high-frequency textures in 2D Fourier space.</p><p>Deep SR architecture After ESPCN <ref type="bibr" target="#b24">[24]</ref> has proposed an efficient learnable upsampling module using pixelshuffling, numerous CNN-based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> have been studied to improve the representational power of models. Such approaches have exploited more complicated neural network architecture designs, such as residual block <ref type="bibr" target="#b14">[15]</ref>, densely connected residual block <ref type="bibr" target="#b35">[35]</ref>, channel attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">34]</ref>, or non-local neural networks <ref type="bibr" target="#b17">[18]</ref>. Inspired by the success of the self-attention mechanism in high-level vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, general-purpose image processing transformers, such as IPT <ref type="bibr" target="#b2">[3]</ref> or SwinIR <ref type="bibr" target="#b13">[14]</ref>, have been proposed. Even though transformer-based SR architectures using a large dataset surpass CNN-based architectures in performance, these approaches nevertheless need to utilize a specific upscale module for each upsampling rate.</p><p>Arbitrary-scale SR Our work is highly related to SR tasks within an arbitrary-scale factor <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31]</ref>, which is convenient and efficient for practical benefits. Training and storing models for each specific scale factor <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> is unfeasible when considering limited memory resources. Since MetaSR <ref type="bibr" target="#b8">[9]</ref> first proposed an arbitraryscale SR method with a single model, various approaches have been explored. Recently, ArbSR <ref type="bibr" target="#b31">[31]</ref> has been proposed as a general plug-in module using conditional convolutions. ArbSR conducts an SR with different scales along horizontal and vertical axes, respectively. More recently, SRWarp <ref type="bibr" target="#b27">[27]</ref> successfully transformed an LR image into any shape in HR representation using a differentiable adaptive warping layer. Most related to ours is the model of <ref type="bibr" target="#b3">[4]</ref>. Inspired by advancements in implicit neural representation, LIIF replaced sub-pixel convolution with an MLP, taking continuous coordinates and latent variables as inputs. Even though such a method outperformed previous works at large upsampling rates, structural distortion occurs at extreme scales. Instead of concatenating coordinates and latent variables, our LTE-based architecture transforms input coordinates into the Fourier domain using dominant frequencies extracted from latent variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>In this section, we aim to represent I HR ? R ryH?rxW ?3 from I LR ? R H?W ?3 given any fraction number r x , r y . We first review a continuous representation of an RGB image with a local implicit neural representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Even though such approaches showed outstanding performance in representing continuous-domain signals, a standalone MLP fails to capture high-frequency details <ref type="bibr" target="#b23">[23]</ref>. To overcome this spectral bias problem, we formulate a Local Texture Estimator (LTE), a dominant frequency estimator for natural images. Unlike prior arts <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">29]</ref>, estimated frequencies are data-driven and strongly correlated to image textures. Additionally, we introduce scale-dependent phase estimation and LR skip connection, which aid LTE in learning high-frequency textures.</p><p>Local implicit neural representation In local implicit neural representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>, a decoding function f ? is shared by all images and is parameterized by an MLP with trainable weights ?. A decoder f ? maps both latent tensors and local coordinates into RGB values; f ? (z, x) : (Z, X ) ? S. z ? Z is a latent tensor from an encoder E ? , x ? X is a 2D coordinate in the continuous image domain, S is a space of predicted values from f ? . For simplicity, we assume that a latent tensor z ? R H?W ?C has the same width and height as I LR . Then, predicted RGB values (s ? R 3 ) at a coordinate x ? R 2 are estimated as</p><formula xml:id="formula_0">s(x, I LR ; ?) = j?J w j f ? (z j , x ? x j ),<label>(1)</label></formula><p>where z = E ? (I LR ).</p><p>(2) ? = [ ?; ?], J ? Z 4 is a set of indices for four nearest (Euclidean distance) latent codes around x, w j is the bilinear interpolation weight corresponding to the latent code j (referred to as the local ensemble weight <ref type="bibr" target="#b3">[4]</ref>, j w j = 1), z j ? R C is the j?th nearest latent feature vector from x, and x j ? R 2 is the coordinate of the latent code j. Given a series of M data points from N images such as (x m , I HR n (x m )), m = 1, . . . , M and n = 1, . . . , N , the learning problem is defined as follows:</p><formula xml:id="formula_1">? = arg min ? M,N m,n I HR n (x m ) ? s(x m , I LR n ; ?) 1 . (3)</formula><p>In practice, X spans [?H, H] and [?W, W ] for two dimensions. Note that a step size (Cell in <ref type="figure" target="#fig_0">Fig. 2</ref>) of an output grid (R ryH?rxW ) and an input grid (R H?W ) is different. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>, their decoding function (f ? ) predicts continuous representation with a relative coordinate: x ? x j (|x ? x j | ? 1) known as Local grid. The same coordinate (Local grid) with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> is used for our work in order to represent a r x ? r y local area in HR representation.</p><p>Learning dominant frequency component Recent works have shown that an MLP with ReLUs is biased towards learning low-frequency content <ref type="bibr" target="#b23">[23]</ref>. To resolve this spectral bias problem of an implicit neural function, we propose a Local Texture Estimator (LTE), an essential Fourier information estimator for natural images. Inspired by position encoding <ref type="bibr" target="#b19">[20]</ref> and Fourier feature mapping <ref type="bibr" target="#b29">[29]</ref>, LTE transforms input coordinates into the Fourier domain before passing an MLP. However, unlike <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">29]</ref>, estimated Fourier information is data-driven and reflects image textures in 2D Fourier space. The local implicit neural representation in Eq. (1) can be modified as follows:</p><formula xml:id="formula_2">s(x, I LR ; ?, ?) = j?J w j f ? (h ? (z j , x ? x j )) (4)</formula><p>where h ? (?, ?) denotes the LTE, which is shift-invariant. LTE (h ? (?, ?)) consists of three elements;(1) an amplitude</p><formula xml:id="formula_3">estimator (h a (?) : R C ? R 2K ), (2) a frequency estimator (h f (?) : R C ? R K?2 ), (3) a phase estimator (h p (?) : R 2 ? R K ). Thus, given a local-grid coordinate ?(= x ? x j ) ? R 2 , the estimating function h ? (?, ?) : (R C , R 2 ) ? R 2K is defined as h ? (z j , ?) = A j cos(?F j ?) sin(?F j ?) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_4">A j = h a (z j ), F j = h f (z j ).<label>(6)</label></formula><p>A j ? R 2K is an amplitude vector for a latent code j, F j ? R K?2 denotes a frequency matrix for a latent code j, and represents element-wise multiplication. We believe that the amplitude vector and the frequency matrix are extracted from the latent code j so as to represent s(x) as close as possible to original signals I HR (x). In this perspective, we understand that by observing pixels inside a receptive field (RF), LTE with the encoder (h ? ? E ? ) estimates dominant frequencies and corresponding Fourier coefficients accurately. Here, the size of RF is decided by the encoder (E ? ). We visually demonstrate estimated frequencies and corresponding Fourier coefficients in Sec. 5.4.</p><p>In practice, to enrich the information in outputs of LTE, we apply the unfolding technique to z j leading to a concatenation of the 3 ? 3 nearest latent variables in? j <ref type="bibr" target="#b3">[4]</ref>. This is implemented with trainable convolutional filters (h a (?) :</p><formula xml:id="formula_5">R 9C ? R 2K , h f (?) : R 9C ? R K?2 ).</formula><p>Scale-dependent phase estimation Phase in Eq. <ref type="formula" target="#formula_6">(7)</ref> contains information about edge locations of features. For SR tasks, the location of edge changes within a small neighborhood in its HR domain when the scale factor changes. To address this issue, we redefine the estimating function as:</p><formula xml:id="formula_6">h ? (? j , ?, c) = A j cos(?(F j ? + h p (?))) sin(?(F j ? + h p (?)))<label>(7)</label></formula><p>where c denotes the cell size. Inspired by the observation that MLPs with ReLUs are incapable of extrapolating unseen non-linear space <ref type="bibr" target="#b32">[32]</ref>, we use? = max(c, c tr ), where c tr denotes the minimum cell size during training. LR skip connection A long skip connection in local implicit representation enriches high-frequency components in residuals and stabilizes convergence <ref type="bibr" target="#b11">[12]</ref>. In the Fourier domain, LTE tends to predict frequencies located near a lowfrequency region (DC). To prevent LTE from learning the DC only, we add upscaled LR. Thus, local implicit neural representation with the proposed LTE can be formulated as follows:?</p><formula xml:id="formula_7">(x) = s(x, I LR ; ?, ?) + I LR ? (x)<label>(8)</label></formula><p>4. Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Detail</head><p>Our LTE-based arbitrary-scale SR network includes an encoder (E ? ), the LTE (a pink shaded area in <ref type="figure" target="#fig_0">Fig. 2)</ref>, a decoder (f ? ), and an LR skip connection. This section first describes a backbone structure (including encoder, decoder, and LR skip connection) and our LTE.</p><p>Backbone We use EDSR-baseline <ref type="bibr" target="#b14">[15]</ref>, RDN <ref type="bibr" target="#b35">[35]</ref>, and SwinIR <ref type="bibr" target="#b13">[14]</ref> without their upsampling layers as an encoder (E ? ). Thus, an output of the encoder has the same width and height as an input LR image. We hypothesize that deep SISR networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">35]</ref> aid LTE in estimating crucial Fourier information by extracting features of natural images inside an RF. Our decoder (f ? ), shared by all images as <ref type="bibr" target="#b3">[4]</ref>, is a 4-layer MLP with a ReLU activation, and its hidden dimensions are 256. Lastly, we add a bilinear upscaled LR image to the decoder output as in Eq. <ref type="bibr" target="#b7">(8)</ref>. We expect that such a long skip connection provides DC offsets; thus, LTE is biased toward learning dominant frequencies and corresponding essential Fourier coefficients.</p><p>LTE Our LTE contains an amplitude estimator (h a ), a frequency estimator (h f ), a phase estimator (h p ), and sinusoidal activations. An amplitude and a frequency estimator are designed with 3x3 convolutional layers having 256 (= 2K) output channels, respectively, identical to a fully connected layer when feature maps are unfolded. The phase estimator is a single fully connected layer with hidden dimensions of 128. Note that an amplitude and a frequency estimator take the same feature map while the phase estimator takes cell as an input. Let us assume that a r x ? r y local region in an HR domain shares amplitude and frequency information extracted from our LTE (h ? ) as in Eq. (4). According to this, we upscale the extracted Fourier information using nearest-neighborhood interpolation. Then, a predicted phase is added to an inner product between the predicted frequency and local grid before passing it through the sinusoidal activation layer, as in Eq. <ref type="bibr" target="#b6">(7)</ref>. Finally, we multiply the predicted amplitude and sinusoidal activation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Strategy</head><p>We construct a minibatch with uniformly sampled scales from ?1 ? ?4, dubbed in-scale, to teach the nature of bicubic degradation at various scales. Note that we evaluate our LTE for both in-scale and out-of-scale, which is an unseen scale (specifically ?6 ? ?30), to verify the generalization ability of our network.</p><p>Let r (= r x = r y ) be a scale factor randomly sampled from ?1 ? ?4 and H, W be a height, a width of training patch, respectively. We first crop rH ? rW patches from an HR image. When preparing training pairs, we randomly sample HW pixels from an HR patch for ground truth (GT) and downsample an HR patch by the scale factor r for an LR counterpart. When computing loss during training, we pick HW pixels from interpolation outputs to match the dimensions of prediction with GT.</p><formula xml:id="formula_8">Urban100(?8) DIV2K(?30)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR Image</head><p>Bicubic MetaSR <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> LIIF <ref type="bibr" target="#b3">[4]</ref> LTE (ours) GT <ref type="figure">Figure 3</ref>. Qualitative comparison to other arbitrary-scale SR. RDN <ref type="bibr" target="#b35">[35]</ref> is used as an encoder for all methods.  EDSR <ref type="bibr" target="#b14">[15]</ref> RDN <ref type="bibr" target="#b35">[35]</ref> SwinIR <ref type="bibr" target="#b13">[14]</ref> GT <ref type="figure">Figure 5</ref>. Qualitative comparison between various encoders with our LTE for ?6 SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training</head><p>Dataset We use a DIV2K dataset <ref type="bibr" target="#b0">[1]</ref> of an NTIRE 2017 Challenge <ref type="bibr" target="#b30">[30]</ref> for network training. For evaluation, we report peak signal-to-noise ratio (PSNR) results on the DIV2K validation set <ref type="bibr" target="#b0">[1]</ref>, Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b33">[33]</ref>, B100 <ref type="bibr" target="#b16">[17]</ref>, and Urban100 <ref type="bibr" target="#b9">[10]</ref>.</p><p>Implementation detail We follow a prior implementation <ref type="bibr" target="#b14">[15]</ref> and use 48 ? 48 patches for inputs of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Bicubic SwinIR-LTE <ref type="figure">Figure 6</ref>. Visual demonstration of SwinIR-LTE for ?32 SR.</p><p>For arbitrary-scale down-sampling during training time, we follow <ref type="bibr" target="#b3">[4]</ref> and use bicubic resizing in Pytorch <ref type="bibr" target="#b21">[22]</ref>. We use an L1 loss <ref type="bibr" target="#b14">[15]</ref> and an Adam <ref type="bibr" target="#b12">[13]</ref> method for optimization. When we train LTE with CNN-based encoders, such as EDSR-baseline <ref type="bibr" target="#b14">[15]</ref>   <ref type="table">Table 2</ref>. Quantitative comparison with state-of-the-art methods for arbitrary-scale SR on benchmark datasets (PSNR (dB)). Red and blue colors indicate the best and the second-best performance, respectively. ? indicates our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>Quantitative result Tab. 1 demonstrates a quantitative comparison between our LTE and existing arbitrary-scale SR methods, MetaSR <ref type="bibr" target="#b8">[9]</ref>, LIIF <ref type="bibr" target="#b3">[4]</ref>, on the DIV2K validation set. The top, middle, and bottom rows show results when EDSR-baseline <ref type="bibr" target="#b14">[15]</ref>, RDN <ref type="bibr" target="#b35">[35]</ref>, and SwinIR <ref type="bibr" target="#b13">[14]</ref> are used as encoders. We notice that, regardless of a choice of an encoder, LTE achieves the best performance for all the scale factors, which indicates the effectiveness of the local texture.</p><p>In Tab. 2, we compare our LTE and RDN <ref type="bibr" target="#b35">[35]</ref>, SwinIR <ref type="bibr" target="#b13">[14]</ref>, MetaSR <ref type="bibr" target="#b8">[9]</ref>, LIIF <ref type="bibr" target="#b3">[4]</ref> on benchmark datasets. Note that RDN and SwinIR <ref type="bibr" target="#b13">[14]</ref> are trained with a specific scale; thus, it has significant benefits for in-scale <ref type="bibr" target="#b3">[4]</ref>. However, including RDN and SwinIR, our LTE shows remarkable performance compared to other methods. The maximum PSNR gain is 0.15dB on Urban100 for ?3 within RDN.</p><p>Qualitative result Qualitative comparisons to other arbitrary-scale SR methods are provided in <ref type="figure">Fig. 3</ref>. For a fair comparison, MetaSR <ref type="bibr" target="#b8">[9]</ref>, LIIF <ref type="bibr" target="#b3">[4]</ref>, and our LTE are trained with RDN <ref type="bibr" target="#b35">[35]</ref>. Note that MetaSR <ref type="bibr" target="#b8">[9]</ref> follows <ref type="bibr" target="#b3">[4]</ref>'s implementation to reconstruct an HR image for large-scale factors (&gt; ?4). We see that MetaSR suffers from blocky artifacts, and LIIF shows structural distortion. In contrast, our LTE captures high-frequency details without any discontinuities. <ref type="figure" target="#fig_1">Fig. 4</ref> compares LIIF <ref type="bibr" target="#b3">[4]</ref> and our LTE for text images with non-integer scale factors. We observe that our LTE is capable of restoring more clear edges of printed texts for all scale factors (particularly, 'n', 't', 's' in the first row and 'u', 'i', 'n' in the second row).</p><p>In <ref type="figure">Fig. 5</ref>, we show a qualitative comparison for ?6 SR. We remark that SwinIR <ref type="bibr" target="#b13">[14]</ref> followed by LTE reconstructs the most visually pleasing image, faithful to the GT. It implies that LTE precisely extracts dominant frequencies and corresponding essential Fourier coefficients when jointly trained with a robust encoder. An empirical explanation using Fourier analysis is provided in <ref type="figure">Fig. 8</ref> and Sec. 5.4.</p><p>As shown in <ref type="figure">Fig. 6</ref>, we visually demonstrate our LTE at an extremely large scale factor, specifically ?32. For the demonstration, we trained our LTE with SwinIR <ref type="bibr" target="#b13">[14]</ref>, and the width of an input image is 64px. We note that our LTE interpolates images with more sharp and natural edges compared to the bicubic method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this section, we demonstrate the effect of each component in LTE. Our LTE consists of an amplitude estimator, a frequency estimator, a phase estimator, and an LR skip connection. To support the significance of each component, we retrain the following models with EDSR-baseline <ref type="bibr" target="#b14">[15]</ref>. phase difference causes a significant performance drop. We see that an LR skip connection consistently enhances the quality of LTE when comparing LTE with LTE (-L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Fourier Space</head><p>In this section (Figs. 7 to 9), we visualize extracted dominant frequencies with various textures. Furthermore, we investigate the contributions of each LTE component (specifically amplitude, frequency, phase, LR skip connection) through the lens of Fourier space.</p><p>Setup For visualization, we observe outputs of an amplitude estimator (h a ) and a frequency estimator (h f ). We first scatter dominant frequencies on 2D space and set color for each point with a magnitude. All scatter maps are defined on [?1.5, 1.5] 2 , and the value range of each map is different from the other. In addition, 16-tap discrete Fourier transform (DFT) of GT images are provided to compare dominant frequencies of LTE and those of GT.</p><p>Image texture and Fourier space We choose three different textures: horizontal, vertical, diagonal textures, as illustrated in <ref type="figure">Fig. 7</ref>. Frequency maps from LTE in the bottom row are obtained from two-fold downsampled images. By comparing the middle row and the bottom row in <ref type="figure">Fig. 7</ref>, we observe that estimated dominant frequencies fol-LTE LTE (-A) LTE (-F) LTE (-P) LTE (-L) <ref type="figure">Figure 9</ref>. Ablation study and corresponding Fourier space. -F refers to reducing the number of estimated frequencies, and -A, -P, -L refers to removing amplitude estimator, phase estimator, LR skip connection, respectively. The diagonal texture in <ref type="figure">Fig. 7</ref> is chosen for an ablation study, and EDSR-baseline <ref type="bibr" target="#b14">[15]</ref> is used as an encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT LTE LTE(-A) LTE(-F) LTE(-P)</head><p>LTE(-L) <ref type="figure">Figure 10</ref>. Qualitative ablation study of LTE on Urban100.</p><p>Definitions of -A, -F, -P, -L are shown in a caption of <ref type="figure">Fig. 9</ref>. EDSR-baseline <ref type="bibr" target="#b14">[15]</ref> is used as an encoder.  <ref type="table">Table 3</ref>. Quantitative ablation study of LTE on Set14. Definitions of -A, -F, -P, -L are shown in a caption of <ref type="figure">Fig. 9</ref>. EDSRbaseline <ref type="bibr" target="#b14">[15]</ref> is used as an encoder. low the dominant frequencies of GT. It indicates that LTE obtains dominant frequencies and corresponding Fourier coefficients by observing pixels inside an RF. Note that the size of RF is determined by deep SR encoders (E ? ), such as EDSR-baseline <ref type="bibr" target="#b14">[15]</ref>, RDN <ref type="bibr" target="#b35">[35]</ref>, and SwinIR <ref type="bibr" target="#b13">[14]</ref>.</p><p>Encoder and Fourier space Tab. 2 and <ref type="figure">Fig. 5</ref> demonstrate that LTE accomplishes better performance when SwinIR <ref type="bibr" target="#b13">[14]</ref> is used as an encoder. <ref type="figure">Fig. 8</ref> supports the observation by visualizing Fourier space. We remark that SwinIR-LTE captures dominant frequencies on a diagonal axis while EDSR-baseline-LTE estimates only lowfrequency components. From this observation, LTE with a powerful encoder extracts precise dominant frequencies.</p><p>Ablation study on Fourier space <ref type="figure">Fig. 9</ref> shows Fourier spaces of LTE when each component is missing. We choose a diagonal texture in <ref type="figure">Fig. 7</ref> for an ablation study. Please see a caption of <ref type="figure">Fig. 9</ref> for definitions of -A, -F, -P, -L. LTE (-A) considers coefficients of all frequencies as equal since Fourier coefficients are not given from LTE. Therefore, LTE (-A) focuses on learning low-frequency content. LTE (-P) is incapable of estimating frequencies positioned on a diagonal axis. We suppose that without a scale-dependent phase estimation, LTE (-P) detects only scale-independent information: Image signal is compactly supported by lowfrequency regions. We validate that the deficiency of dominant frequencies fails to learn high-frequency details by comparing LTE and LTE (-F). On comparing LTE and LTE (-L), LTE (-L) is poor in capturing dominant frequencies.   <ref type="figure">Fig. 8</ref> shows that SwinIR-LTE is capable of estimating dominant frequencies of natural images. In addition, the middle row in <ref type="figure" target="#fig_0">Fig. 12</ref> demonstrate that SwinIR-LTE extracts essential Fourier information under mild aliasing. However, such capability of SwinIR-LTE is limited when an LR image has severe aliasing. From <ref type="figure" target="#fig_0">Fig. 12</ref>, we observe that dominant frequencies (bottom right) are inconsistent with a GT spectrum (top right) when harsh aliasing artifacts occur in an LR image (bottom left). We can resolve such limitations by extending the size of an encoder's RF, followed by increased computation and memory costs. Costeffective architectures achieving robust performance even under severe aliasing will be investigated in future work. Gibbs phenomenon When representing continuous signals with a finite sum of Fourier basis, function overshoots at discontinuities. Such observation is referred to in the literature as the Gibbs phenomenon or ringing artifacts in 2D images. From <ref type="figure" target="#fig_5">Fig. 13</ref>, we notice that LTE might cause overshoot at large scale factors, e.g., ?12. Further investigation of smoothing algorithms to alleviate such an issue is a promising direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of aliasing</head><p>Computation time In practice, SR applications require short computation time. Moreover, reconstructing highquality images, such as DIV2K, consumes extensive memory during evaluation. Tab. 4 compares the computa-  <ref type="table">Table 4</ref>. Memory consumption (GB) and computation time (ms) comparison to other arbitrary-scale SR methods for an ?2 SR task. We use a 624 ? 624 sized input and EDSR-baseline <ref type="bibr" target="#b14">[15]</ref> as an encoder. OOM denotes an out-of-memory, and our GPU memory was deficient in evaluating MetaSR <ref type="bibr" target="#b8">[9]</ref>.</p><p>tion time of our LTE to other arbitrary-scale SR methods for both cases: memory-limited (top rows) and memoryconsuming (bottom rows) on NVIDIA RTX 3090 24GB.</p><p>To evaluate an HR image under a memory-limited condition, we compute 96 ? 96 output pixels per query <ref type="bibr" target="#b3">[4]</ref>. From the top rows of Tab. 4, we observe that our LTE takes the shortest computation time while increasing memory usage. LTE has 4-layer MLP for querying, two convolution layers for estimators, and LIIF has 5-layer MLP only for querying. When querying evaluation points only once, estimators become more dominant than querying, resulting in more computation time, as in the bottom rows of Tab. 4. To overcome such a limitation, we design an LTE+, which utilizes 1 ? 1 convolution instead of a shared MLP for decoder implementation. Since 1 ? 1 convolution has a GPU-friendly data structure, our LTE+ takes a shorter computation time and consumes less memory compared to previous works when all output pixels are queried at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we proposed the Local Texture Estimator (LTE) to overcome the spectral bias problem of an implicit neural function. Our LTE-based arbitrary-scale SR method consists of three components: (1) Deep SR encoder (2) LTE (3) Implicit representation function. Firstly, a deep SR encoder extracts feature maps whose height and width are the same as an LR image. Then, LTE takes feature maps from the encoder and estimates dominant frequencies with corresponding Fourier coefficients for natural images. Scaledependent phase and LR skip connection are further provided to allow LTE to be biased in learning high-frequency textures. Finally, the implicit function reconstructs an image in arbitrary resolution using estimated Fourier information. We showed that our LTE-based neural function outperforms other arbitrary-scale SR methods in performance and visual quality with the shortest computation time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Arbitrary-scale SR with our proposed Local Texture Estimator (LTE). LTE-based arbitrary-scale SR architecture consists of an encoder (E?), LTE (a pink shaded region), a decoder (f ? ), and an LR skip connection. Inputs of LTE are as follows: feature map from the encoder, local grid, and cell. LTE transforms input coordinates into the Fourier domain using extracted amplitude, frequency, phase information. We add a bilinear upscaled LR image to the output of a decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison with non-integer scale factors by LIIF [4] (top) and our LTE (bottom). RDN [35] is used as an encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>(-A): LTE without an amplitude estimator. (-F): LTE with a frequency estimator that estimates only 128 frequencies (not 256). (-P): LTE without a phase estimator. (-L): LTE without an LR skip connection. Fig. 10 and Tab. 3 show the contributions of each LTE component on visual quality and performance. To verify the importance of each estimated frequency, we compare LTE to LTE (-F). We find that an amplitude estimator emphasizes dominant frequencies compared between LTE and LTE (-A). By comparing LTE and LTE (-P), disregarding a Horizontal Vertical Diagonal GT image (top), GT image spectrum (middle), and corresponding estimated Fourier space from LTE (bottom) of various textures. EDSR-baseline [15] is used as an encoder. Estimated Fourier space with various encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 11 .</head><label>11</label><figDesc>DFT and LTE Advantage of LTE over DFT In DFT, Fourier information is represented with a linear combination of image intensity. However, the DFT of an LR image with aliasing (middle) is limited in capturing dominant frequencies. In contrast, LTE with a deep neural encoder (bottom), which is a multi-chain of linear combinations and non-linear activations, is capable of estimating accurate Fourier information for an HR image (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 .</head><label>12</label><figDesc>Aliasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13</head><label>13</label><figDesc>Figure 13. Gibbs phenomenon (?12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MetaSR<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> 34.64 30.93 28.92 26.61 23.55 22.03 21.06 20.37 EDSR-baseline-LIIF [4] 34.67 30.96 29.00 26.75 23.71 22.17 21.18 20.48 EDSR-baseline-LTE (ours) 34.72 31.02 29.04 26.81 23.78 22.23 21.24 20.53 RDN-MetaSR [4, 9] 35.00 31.27 29.25 26.88 23.73 22.18 21.17 20.47 RDN-LIIF [4] 34.99 31.26 29.27 26.99 23.89 22.34 21.31 20.59 RDN-LTE (ours) 35.04 31.32 29.33 27.04 23.95 22.40 21.36 20.64 SwinIR-MetaSR ? [4, 9] 35.15 31.40 29.33 26.94 23.80 22.26 21.26 20.54 SwinIR-LIIF ? [4] 35.17 31.46 29.46 27.15 24.02 22.43 21.40 20.67 Swinir-LTE (ours) 35.24 31.50 29.51 27.20 24.09 22.50 21.47 20.73 Table 1. Quantitative comparison with state-of-the-art methods for arbitrary-scale SR on DIV2K validation set (PSNR (dB)). Red and blue colors indicate the best and the second-best performance, respectively. ? indicates our implementation. ] 38.22 34.63 32.38 29.04 26.96 33.98 30.54 28.78 26.51 24.97 32.33 29.26 27.71 25.90 24.83 32.92 28.82 26.55 23.99 22.59 RDN-LIIF [4] 38.17 34.68 32.50 29.15 27.14 33.97 30.53 28.80 26.64 25.15 32.32 29.26 27.74 25.98 24.91 32.87 28.82 26.68 24.20 22.79 RDN-LTE (ours) 38.23 34.72 32.61 29.32 27.26 34.09 30.58 28.88 26.71 25.16 32.36 29.30 27.77 26.01 24.95 33.04 28.97 26.81 24.28 22.88 MetaSR ? [4, 9] 38.26 34.77 32.47 29.09 27.02 34.14 30.66 28.85 26.58 25.09 32.39 29.31 27.75 25.94 24.87 33.29 29.12 26.76 24.16 22.75 SwinIR-LIIF ? [4] 38.28 34.87 32.73 29.46 27.36 34.14 30.75 28.98 26.82 25.34 32.39 29.34 27.84 26.07 25.01 33.36 29.33 27.15 24.59 23.14 SwinIR-LTE (ours) 38.33 34.89 32.81 29.50 27.35 34.25 30.80 29.06 26.86 25.42 32.44 29.39 27.86 26.09 25.03 33.50 29.41 27.24 24.62 23.17</figDesc><table><row><cell>or RDN [35], networks are trained for 1000 epochs with batch size 16. The learning rate is initialized as 1e-4 and decayed by factor 0.5 at [200, 400, Out-of-scale ?4 ?6 ?12 ?18 ?24 ?30 31.01 28.22 26.66 24.82 22.27 21.00 20.19 19.59 In-scale ?2 ?3 34.55 30.90 28.94 -----Set14 B100 Urban100 In-scale Out-of-scale In-scale Out-of-scale In-scale Out-of-scale ?3 ?4 ?6 ?8 ?2 ?3 ?4 ?6 ?8 ?2 ?3 ?4 ?6 ?8 34.01 30.57 28.81 EDSR-baseline [15] Bicubic [15] EDSR-baseline-Method Set5 In-scale Out-of-scale ?2 ?3 ?4 ?6 ?8 ?2 RDN [35] 38.24 34.71 32.47 ----32.34 29.26 27.72 --32.89 28.80 26.61 --RDN-MetaSR [4, 9SwinIR [14] 38.35 34.89 32.72 --34.14 30.77 28.94 --32.44 29.37 27.83 --33.40 29.29 27.07 --600, 800Method SwinIR-</cell></row></table><note>]. For a transformer-based encoder, specifically SwinIR [14], a model is trained for 1000 epochs with batch size 32. The learning rate is initialized as 2e-4 and decayed by factor 0.5 at [500, 800, 900, 950].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>LTE 33.72 30.37 28.65 26.50 24.99 LTE (-A) 33.66 30.07 28.66 26.49 24.93 LTE (-F) 33.66 30.35 28.64 26.50 24.98 LTE (-P) 33.58 30.26 28.58 26.40 24.90 LTE (-L) 33.62 30.36 28.64 26.48 24.98</figDesc><table><row><cell></cell><cell>In-scale</cell><cell></cell><cell cols="2">Out-of-scale</cell></row><row><cell>?2</cell><cell>?3</cell><cell>?4</cell><cell>?6</cell><cell>?8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Challenge on Single Image Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberi</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-Trained Image Processing Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Continuous Image Representation With Local Implicit Image Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8628" to="8638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Second-Order Attention Network for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR 2021, Virtual Event. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HyperNetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilayer Feedforward Networks Are Universal Approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-SR: A Magnification-Arbitrary Network for Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single Image Super-Resolution From Transformed Self-Exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local Implicit Grid Representations for 3D Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;max&amp;quot;</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann Le-Cun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SwinIR: Image Restoration Using Swin Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image Super-Resolution With Non-Local Sparse Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Occupancy Networks: Learning 3D Reconstruction in Function Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<title level="m">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the Spectral Bias of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Nasim Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno>PMLR, 09-15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Neural Representations with Periodic Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learned Initializations for Optimizing Coordinate-Based Neural Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divi</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2846" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a Single Network for Scale-Arbitrary Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4801" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">Shaolei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021. 4</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On Single Image Scale-Up Using Sparse-Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<editor>Jean-Daniel Boissonnat, Patrick Chenin, Albert Cohen, Christian Gout, Tom Lyche, Marie-Laurence Mazure, and Larry Schumaker</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Very Deep Residual Channel Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual Dense Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
