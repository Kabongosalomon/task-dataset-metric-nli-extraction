<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Linking in 100 Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shan</surname></persName>
							<email>zifeishan@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
							<email>dgillick@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Entity Linking in 100 Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero-and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset 1 matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity linking (EL) fulfils a key role in grounded language understanding: Given an ungrounded entity mention in text, the task is to identify the entity's corresponding entry in a Knowledge Base (KB). In particular, EL provides grounding for applications like Question Answering <ref type="bibr" target="#b6">(F?vry et al., 2020b</ref>) (also via Semantic Parsing <ref type="bibr" target="#b28">(Shaw et al., 2019)</ref>) and Text Generation <ref type="bibr" target="#b24">(Puduppully et al., 2019)</ref>; it is also an essential component in knowledge base population <ref type="bibr" target="#b29">(Shen et al., 2014)</ref>. Entities have played a growing role in representation learning. For example, entity mention masking led to greatly improved fact retention in large language models <ref type="bibr" target="#b10">(Guu et al., 2020;</ref><ref type="bibr" target="#b26">Roberts et al., 2020)</ref>.</p><p>But to date, the primary formulation of EL outside of the standard monolingual setting has been cross-lingual: link mentions expressed in one language to a KB expressed in another <ref type="bibr" target="#b20">(McNamee et al., 2011;</ref><ref type="bibr" target="#b33">Tsai and Roth, 2016;</ref><ref type="bibr" target="#b31">Sil et al., 2018)</ref>.</p><formula xml:id="formula_0">1 http://goo.gle/mewsli-dataset</formula><p>The accompanying motivation is that KBs may only ever exist in some well-resourced languages, but that text in many different languages need to be linked. Recent work in this direction features progress on low-resource languages <ref type="bibr" target="#b40">(Zhou et al., 2020)</ref>, zero-shot transfer <ref type="bibr" target="#b30">(Sil and Florian, 2016;</ref><ref type="bibr" target="#b39">Zhou et al., 2019)</ref> and scaling to many languages <ref type="bibr" target="#b22">(Pan et al., 2017)</ref>, but commonly assumes a single primary KB language and a limited KB, typically English Wikipedia.</p><p>We contend that this popular formulation limits the scope of EL in ways that are artificial and inequitable.</p><p>First, it artificially simplifies the task by restricting the set of viable entities and reducing the variety of mention ambiguities. Limiting the focus to entities that have English Wikipedia pages understates the real-world diversity of entities. Even within the Wikipedia ecosystem, many entities only have pages in languages other than English. These are often associated with locales that are already underrepresented on the global stage. By ignoring these entities and their mentions, most current modeling and evaluation work tend to side-step underappreciated challenges faced in practical industrial applications, which often involve KBs much larger than English Wikipedia, with a much more significant zero-or few-shot inference problem.</p><p>Second, it entrenches an English bias in EL research that is out of step with the encouraging shift toward inherently multilingual approaches in natural language processing, enabled by advances in representation learning <ref type="bibr" target="#b14">(Johnson et al., 2017;</ref><ref type="bibr" target="#b23">Pires et al., 2019;</ref><ref type="bibr" target="#b3">Conneau et al., 2020)</ref>.</p><p>Third, much recent EL work has focused on models that rerank entity candidates retrieved by an alias table <ref type="bibr" target="#b5">(F?vry et al., 2020a)</ref>, an approach that works well for English entities with many linked mentions, but less so for the long tail of entities and languages. arXiv:2011.02690v1 [cs.CL] 5 Nov 2020</p><p>To overcome these shortcomings, this work makes the following key contributions:</p><p>? Reformulate entity linking as inherently multilingual: link mentions in 104 languages to entities in WikiData, a language-agnostic KB.</p><p>? Advance prior dual encoder retrieval work with improved mention and entity encoder architecture and improved negative mining targeting.</p><p>? Establish new state-of-the-art performance relative to prior cross-lingual linking systems, with one model capable of linking 104 languages against 20 million WikiData entities.</p><p>? Introduce Mewsli-9, a large dataset with nearly 300,000 mentions across 9 diverse languages with links to WikiData. The dataset features many entities that lack English Wikipedia pages and which are thus inaccessible to many prior cross-lingual systems.</p><p>? Present frequency-bucketed evaluation that highlights zero-and few-shot challenges with clear headroom, implicitly including lowresource languages without enumerating results over a hundred languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Multilingual Entity Linking (MEL) is the task of linking an entity mention m in some context language l c to the corresponding entity e ? V in a language-agnostic KB. That is, while the KB may include textual information (names, descriptions, etc.) about each entity in one or more languages, we make no prior assumption about the relationship between these KB languages L kb = {l 1 , . . . , l k } and the mention-side language: l c may or may not be in L kb . This is a generalization of cross-lingual EL (XEL), which is concerned with the case where L kb = {l } and l c = l . Commonly, l is English, and V is moreover limited to the set of entities that express features in l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MEL with WikiData and Wikipedia</head><p>As a concrete realization of the proposed task, we use WikiData <ref type="bibr" target="#b36">(Vrande?i? and Kr?tzsch, 2014)</ref> as our KB: it covers a large set of diverse entities, is broadly accessible and actively maintained, and it provides access to entity features in many languages. WikiData itself contains names and short descriptions, but through its close integration with all Wikipedia editions, it also connects entities to rich descriptions (and other features) drawn from the corresponding language-specific Wikipedia pages.</p><p>Basing entity representations on features of their Wikipedia pages has been a common approach in EL (e.g. <ref type="bibr" target="#b30">Sil and Florian, 2016;</ref><ref type="bibr" target="#b7">Francis-Landau et al., 2016;</ref><ref type="bibr" target="#b8">Gillick et al., 2019;</ref><ref type="bibr" target="#b38">Wu et al., 2019)</ref>, but we will need to generalize this to include multiple Wikipedia pages with possibly redundant features in many languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">WikiData Entity Example</head><p>Consider the WikiData Entity S? R?dio Q3511500 , a now defunct Valencian radio station. Its KB entry references Wikipedia pages in three languages, which contain the following descriptions: 2 ? (French) S? R?dio est une station de radio publique espagnole appartenant au groupe R?dio Televisi? Valenciana, entreprise de radio-t?l?vision d?pendant de la Generalitat valencienne.</p><p>Note that these Wikipedia descriptions are not direct translations, and contain some name variations. We emphasize that this particular entity would have been completely out of scope in the standard crosslingual task <ref type="bibr" target="#b33">(Tsai and Roth, 2016)</ref>, because it does not have an English Wikipedia page.</p><p>In our analysis, there are millions of WikiData entities with this property, meaning the standard setting skips over the substantial challenges of modeling these (often rarer) entities, and disambiguating them in different language contexts. Our formulation seeks to address this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Base Scope</head><p>Our modeling focus is on using unstructured textual information for entity linking, leaving other modalities or structured information as areas for future work. Accordingly, we narrow our KB to the subset of entities that have descriptive text available: We define our entity vocabulary V as all Wiki-Data items that have an associated Wikipedia page in at least one language, independent of the languages we actually model. 3 This gives 19,666,787 entities, substantially more than in any other task settings we have found: the KB accompanying the entrenched TAC-KBP 2010 benchmark <ref type="bibr" target="#b13">(Ji et al., 2010)</ref> has less than a million entities, and although English Wikipedia continues to grow, recent work using it as a KB still only contend with roughly 6 million entities <ref type="bibr" target="#b5">(F?vry et al., 2020a;</ref><ref type="bibr" target="#b40">Zhou et al., 2020)</ref>. Further, by employing a simple rule to determine the set of viable entities, we avoid potential selection bias based on our desired test sets or the language coverage of a specific pretrained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Supervision</head><p>We extract a supervision signal for MEL by exploiting the hyperlinks that editors place on Wikipedia pages, taking the anchor text as a linked mention of the target entity. This follows a long line of work in exploiting hyperlinks for EL supervision <ref type="bibr" target="#b2">(Bunescu and Pa?ca, 2006;</ref><ref type="bibr" target="#b32">Singh et al., 2012;</ref><ref type="bibr" target="#b17">Logan et al., 2019)</ref>, which we extend here by applying the idea to extract a large-scale dataset of 684 million mentions in 104 languages, linked to WikiData entities. This is at least six times larger than datasets used in prior English-only linking work <ref type="bibr" target="#b8">(Gillick et al., 2019)</ref>. Such large-scale supervision is beneficial for probing the quality attainable with current-day high-capacity neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mewsli-9 Dataset</head><p>We facilitate evaluation on the proposed multilingual EL task by releasing a matching dataset that covers a diverse set of languages and entities.</p><p>Mewsli-9 (Multilingual Entities in News, linked) contains 289,087 entity mentions appearing in 58,717 originally written news articles from WikiNews, linked to WikiData. <ref type="bibr">4</ref> The corpus includes documents in nine languages, representing five language families and The articles do not begin with a formulaic entity description, for example, and anchor link conventions are likely different. We treat the full dataset as a test set, avoiding any fine-tuning or hyperparameter tuning, thus allowing us to evaluate our model's robustness to domain drift.</p><p>Mewsli-9 is a drastically expanded version of the English-only WikiNews-2018 dataset by <ref type="bibr" target="#b8">Gillick et al. (2019)</ref>. Our automatic extraction technique trades annotation quality for scale and diversity, in contrast to the MEANTIME corpus based on WikiNews <ref type="bibr" target="#b21">(Minard et al., 2016)</ref>. Mewsli-9 intentionally stretches the KB definition beyond English Wikipedia, unlike VoxEL <ref type="bibr" target="#b27">(Rosales-M?ndez et al., 2018)</ref>. Both MEANTIME and VoxEL are limited to a handful of European languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[CLS] T 1 T 2 ? [SEP] L 1 L 2 ? [E] M 1 M 2 ? [/E] R 1 R 2 ? [SEP]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Prior work showed that a dual encoder architecture can encode entities and contextual mentions in a dense vector space to facilitate efficient entity retrieval via nearest-neighbors search <ref type="bibr" target="#b8">(Gillick et al., 2019;</ref><ref type="bibr" target="#b38">Wu et al., 2019)</ref>. We take the same approach.</p><p>The dual encoder maps a mention-entity pair (m, e) to a score:</p><formula xml:id="formula_1">s(m, e) = ?(m) T ?(e) ?(m) ?(e) ,<label>(1)</label></formula><p>where ? and ? are learned neural network encoders that encode their arguments as d-dimensional vectors (d=300, matching prior work). Our encoders are BERT-based Transformer networks <ref type="bibr" target="#b35">(Vaswani et al., 2017;</ref>, which we initialize from a pretrained multilingual BERT checkpoint. 7 For efficiency, we only use the first 4 layers, which results in a negligible drop in performance relative to the full 12-layer stack. The WordPiece vocabulary contains 119,547 symbols covering the top 104 Wikipedia languages by frequency-this is the language set we use in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mention Encoder</head><p>The mention encoder ? uses an input representation that is a combination of local context (mention span with surrounding words, ignoring sentence boundaries) and simple global context (document title). The document title, context, and mention span are marked with special separator tokens as well as identifying token type labels (see <ref type="figure">Figure 1</ref> for details). Both the mention span markers and 7 github.com/google-research/bert multi_cased_L-12_H-768_A-12 document title have been employed in related work <ref type="bibr" target="#b1">(Agarwal and Bikel, 2020;</ref><ref type="bibr" target="#b5">F?vry et al., 2020a)</ref>. We use a maximum sequence length of 64 tokens similar to prior work <ref type="bibr" target="#b5">(F?vry et al., 2020a)</ref>, up to a quarter of which are used for the document title. The CLS token encoding from the final layer is projected to the encoding dimension to form the final mention encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Encoders</head><p>We experiment with two entity encoder architectures. The first, called Model F, is a featurized entity encoder that uses a fixed-length text description (64 tokens) to represent each entity (see <ref type="figure">Figure 1)</ref>. The same 4-layer Transformer architecture is used-without parameter sharing between mention and entity encoders-and again the CLS token vector is projected down to the encoding dimension. Variants of this entity architecture were employed by <ref type="bibr" target="#b38">Wu et al. (2019)</ref> and <ref type="bibr" target="#b18">Logeswaran et al. (2019)</ref>.</p><p>The second architecture, called Model E is simply a QID-based embedding lookup as in <ref type="bibr" target="#b5">F?vry et al. (2020a)</ref>. This latter model is intended as a baseline. A priori, we expect Model E to work well for common entities, less well for rarer entities, and not at all for zero-shot retrieval. We expect Model F to provide more parameter-efficient storage of entity information and possibly improve on zeroand few-shot retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Entity Description Choice</head><p>There are many conceivable ways to make use of entity descriptions from multiple languages. We limit the scope to using one primary description per entity, thus obtaining a single coherent text fragment to feed into the Model F encoder.</p><p>We use a simple data-driven selection heuristic that is based on observed entity usage: Given an entity e, let n e (l) denote the number of mentions of e in documents of language l, and n(l) the global number of mentions in language l across all entities. From a given source of descriptionsfirst Wikipedia and then WikiData-we order the candidate descriptions (t l 1 e , t l 2 e , . . . ) for e first by the per-entity distribution n e (l) and then by the global distribution n(l). 8 For the example entity in Section 2.1.1, this heuristic selects the Catalan description because 9/16 training examples link to the Catalan Wikipedia page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Process</head><p>In all our experiments, we use an 8k batch size with in-batch sampled softmax <ref type="bibr" target="#b9">(Gillick et al., 2018)</ref>. Models are trained with Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> using the Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015;</ref><ref type="bibr" target="#b19">Loshchilov and Hutter, 2019)</ref>. All BERTbased encoders are initialized from a pretrained checkpoint, but the Model E embeddings are initialized randomly. We doubled the batch size until no further held-out set gains were evident and chose the number of training steps to keep the training time of each phase under one day on a TPU. Further training would likely yield small improvements. See Appendix B for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct a series of experiments to gain insight into the behavior of the dual encoder retrieval models under the proposed MEL setting, asking:</p><p>? What are the relative merits of the two types of entity representations used in Model E and Model F (embeddings vs. encodings of textual descriptions)?</p><p>? Can we adapt the training task and hardnegative mining to improve results across the entity frequency distribution?</p><p>? Can a single model achieve reasonable performance on over 100 languages while retrieving from a 20 million entity candidate set?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Data</head><p>We follow <ref type="bibr" target="#b34">Upadhyay et al. (2018)</ref> and evaluate on the "hard" subset of the Wikipedia-derived test set introduced by Tsai and Roth (2016) for crosslingual EL against English Wikipedia, TR2016 hard . This subset comprises mentions for which the correct entity did not appear as the top-ranked item in their alias table, thus stress-testing a model's ability to generalize beyond mention surface forms. Unifying this dataset with our task formulation and data version requires mapping its gold entities from the provided, older Wikipedia titles to newer WikiData entity identifiers (and following intermediate Wikipedia redirection links). This succeeded for all but 233/42,073 queries in TR2016 hard -our model receives no credit on the missing ones.</p><p>To be compatible with the pre-existing train/test split, we excluded from our training set all mentions appearing on Wikipedia pages in the full TR2016 test set. This was done for all 104 languages, to avoid cross-lingual overlap between train and test sets. This aggressive scheme holds out 33,460,824 instances, leaving our final training set with 650,975,498 mention-entity pairs. <ref type="figure" target="#fig_1">Figure 2</ref> provides a break-down by language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating Design Choices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Setup and Metrics</head><p>In this first phase of experiments we evaluate design choices by reporting the differences in Re-call@100 between two models at a time, for conciseness. Note that for final system comparisons, it is standard to use Accuracy of the top retrieved entity (R@1), but to evaluate a dual encoder retrieval model, we prefer R@100 as this is better matched to its likely use case as a candidate generator.</p><p>Here we use the TR2016 hard dataset, as well a portion of the 104-language set held out from our training data, sampled to have 1,000 test mentions per language. (We reserve the new Mewsli-9 dataset for testing the final model in Section 5.5.)</p><p>Reporting results for 104 languages is a challenge. To break down evaluation results by entity frequency bins, we partition a test set according to the frequency of its gold entities as observed in the training set. This is in line with recent recommendations for finer-grained evaluation in EL <ref type="bibr" target="#b37">(Waitelonis et al., 2016;</ref><ref type="bibr" target="#b12">Ilievski et al., 2018)</ref>.</p><p>We calculate metrics within each bin, and report macro-average over bins. This is a stricter form of the label-based macro-averaging sometimes used, but better highlights the zero-shot and few-shot cases. We also report micro-average metrics, computed over the entire dataset, without binning.  <ref type="table">Table 2</ref>: R@100 differences between pairs of models: (a) model F (featurized inputs for entities) relative to model E (dedicated embedding for each entity); (b) add cross-lingual entity-entity task on top of the mention-entity task for model F; (c) control label balance per-entity during negative mining (versus not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Entity Encoder Comparison</head><p>We first consider the choice of entity encoder, comparing Model F with respect to Model E.</p><p>Table 2(a) shows that using the entity descriptions as inputs leads to dramatically better performance on rare and unseen entities, in exchange for small losses on entities appearing more than 100 times, and overall improvements in both macro and micro recall.</p><p>Note that as expected, the embedding Model E gives 0% recall in zero-shot cases, as their embeddings are randomly initialized and never get updated in absence of any training examples.</p><p>The embedding table of Model E has 6 billion parameters, but there is no sharing across entities. Model F has approximately 50 times fewer parameters, but can distribute information in its shared, compact WordPiece vocabulary and Transformer layer parameters. We can think of these dual encoder models as classifiers over 20 million classes where the softmax layer is either parameterized by an ID embedding (Model E) or an encoding of a description of the class itself (Model F). Remarkably, using a Transformer for the latter approach effectively compresses (nearly) all the information in the traditional embedding model into a compact and far more generalizable model. This result highlights the value of analyzing model behavior in terms of entity frequency. When looking at the micro-averaged metric in isolation, one might conclude that the two models perform similarly; but the macro-average is sensitive to the large differences in the low-frequency bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Auxiliary Cross-Lingual Task</head><p>In seeking to improve the performance of Model F on tail entities, we return to the (partly redundant) entity descriptions in multiple languages. By choosing just one language as the input, we are ignoring potentially valuable information in the remaining descriptions.</p><p>Here we add an auxiliary task: cross-lingual entity description retrieval. This reuses the entity encoder ? of Model F to map two descriptions of an entity e to a score, s(t l e , t l e ) ? ?(t l e ) T ?(t l e ), where t l e is the description selected by the earlier heuristic, and t l e is sampled from the other available descriptions for the entity.</p><p>We sample up to 5 such cross-lingual pairs per entity to construct the training set for this auxiliary task. This makes richer use of the available multilingual descriptions, and exposes the model to 39 million additional high-quality training examples whose distribution is decoupled from that of the mention-entity pairs in the primary task. The multitask training computes an overall loss by averaging the in-batch sampled softmax loss for a batch of (m, e) pairs and for a batch of (e, e) pairs. Table 2(b) confirms this brings consistent quality gains across all frequency bins, and more so for uncommon entities. Again, reliance on the microaverage metric alone understates the benefit in this data augmentation step for rarer entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Hard-Negative Mining</head><p>Training with hard-negatives is highly effective in monolingual entity retrieval <ref type="bibr" target="#b8">(Gillick et al., 2019)</ref>, and we apply the technique they detail to our multilingual setting.  <ref type="table">9   war  azb  bpy  an  vo  sco  ast  io  ba  be  nds  sw  jv  gl  lb  hy  lv  oc  mk  eu  bg  it  ja  af  ceb  pms  es  pl  az  iw  ka  ca  fy  no  nn  id  sv  min  cs  nl  zh  fi  lt  ru  de  tg  sl  vi  cy  tt  zh-TW  pt  da  ko  hr  tr  el  su  uk  ro  bn  sr-Latn  hu  ms  en  sr  bs  et  kk  fr  lmo  lah  br  la  ht  mg  ar  fa  ce  ml  bar  cv  mn  pa  th  mr  sk  gu  sq  scn  kn  te  fil  ur  yo  ga  ky  uz  is  new  ta  ne</ref>   <ref type="figure">Figure B1</ref> in the Appendix for a larger view.)</p><p>In its standard form, a certain number of negatives are mined for each mention in the training set by collecting top-ranked but incorrect entities retrieved by a prior model. However, this process can lead to a form of the class imbalance problem as uncommon entities become over-represented as negatives in the resulting data set. For example, an entity appearing just once in the original training set could appear hundreds or thousands of times as a negative example. Instead, we control the ratio of positives to negatives on a per-entity basis, mining up to 10 negatives per positive.</p><p>Table 2(c) confirms that our strategy effectively addresses the imbalance issue for rare entities with only small degradation for more common entities. We use this model to perform a second, final round of the adapted negative mining followed by further training to improve on the macro-average further by +.05 (holdout) and +.08 (TR2016 hard ).</p><p>The model we use in the remainder of the experiments combines all these findings. We use Model F with the entity-entity auxiliary task and hard negative mining with per-entity label balancing, referenced as Model F + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Linking in 100 Languages</head><p>Breaking down the model's performance by language (R@1 on our heldout set) reveals relatively strong performance across all languages, despite greatly varying training sizes <ref type="figure" target="#fig_1">(Figure 2</ref>). It also shows improvement over an alias table baseline on all languages. While this does not capture the relative difficulty of the EL task in each language, it does strongly suggest effective cross-lingual transfer in our model: even the most data-poor languages have reasonable results. This validates our massively multilingual approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to Prior Work</head><p>We evaluate the performance of our final retrieval model relative to previous work on two existing   <ref type="bibr" target="#b33">(Tsai and Roth, 2016)</ref> and <ref type="bibr" target="#b34">(Upadhyay et al., 2018).</ref> datasets, noting that direct comparison is impossible because our task setting is novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Cross-Lingual Wikification Setting</head><p>We compare to two previously reported results on TR2016 hard : the WIKIME model of <ref type="bibr" target="#b33">Tsai and Roth (2016)</ref> that accompanied the dataset, and the XELMS-MULTI model by <ref type="bibr" target="#b34">Upadhyay et al. (2018)</ref>.</p><p>Both models depend at their core on multilingual word embeddings, which are obtained by applying (bilingual) alignment or projection techniques to pretrained monolingual word embeddings. As reported in <ref type="table" target="#tab_5">Table 3</ref>, our multilingual dual encoder outperforms the other two by a significant margin. To the best of our knowledge, this is the highest accuracy to-date on this challenging evaluation set. (Our comparison is limited to the four languages on which <ref type="bibr">Upadhyay et al. (2018) evaluated their multilingual model.)</ref> This is a strong validation of the proposed approach because the experimental setting is heavily skewed toward the prior models: Both are rerankers, and require a first-stage candidate gen-   <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation on</head><p>Mewsli-9 <ref type="table" target="#tab_9">Table 5</ref> shows the performance of our model on our new Mewsli-9 dataset compared with an alias table baseline that retrieves entities based on the prior probability of an entity given the observed mention string. <ref type="table" target="#tab_10">Table 6</ref> shows the usual frequencybinned evaluation. While overall (micro-average) performance is strong, there is plenty of headroom in zero-and few-shot retrieval.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Example Outputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Reranking Experiment</head><p>We finally report a preliminary experiment to apply a cross-attention scoring model (CA) to rerank entity candidates retrieved by the main dual encoder (DE), using the same architecture of <ref type="bibr" target="#b18">Logeswaran et al. (2019)</ref>. We feed the concatenated mention text and entity description into a 12-layer Transformer model, initialized from the same multilingual BERT checkpoint referenced earlier.</p><p>The CA model's CLS token encoding is used to classify mention-entity coherence. We train the model with a binary cross-entropy loss, using positives from our Wikipedia training data, taking for each one the top-4 DE-retrieved candidates plus 4 random candidates (proportional to the positive distributions). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcome</head><p>Correct: A Spanish mention of "fruit juice" linked to its German description-only "juice" has a dedicated English Wikipedia page.</p><p>Context 3 . . . ????? ??????? ????? j? ?? j? ????? ??? ???? ????????? ?????? ??? ( ?????j?? ) ?? ????????? ?????????. . .</p><p>Prediction It. men's water polo team Q261190 : La nazionale di pallanuoto maschile dell' Italia. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected</head><p>It. nat. basketball team Q261190 : La nazionale di pallacanestro italiana ? la selezione dei migliori giocatori di nazionalit? italiana. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcome</head><p>Wrong: A legitimately ambiguous mention of "Italy" in Serbian (sports context), for which model retrieved the water polo and football teams, followed by the expected basketball team entity, all featurized in Italian.  We use the trained CA model to rerank the top-5 DE candidates for Mewsli-9 <ref type="table" target="#tab_10">(Table 6</ref>). We observed improvements on most frequency buckets compared to DE R@1, which suggests that the model's few-shot capability can be improved by cross-lingual reading-comprehension. This also offers an initial multilingual validation of a similar two-step BERT-based approach recently introduced in a monolingual setting by <ref type="bibr" target="#b38">(Wu et al., 2019)</ref>, and provides a strong baseline for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a new formulation for multilingual entity linking that seeks to expand the scope of entity linking to better reflect the real-world challenges of rare entities and/or low resource languages. Operationalized through Wikipedia and WikiData, our experiments using enhanced dual encoder retrieval models and frequency-based evaluation provide compelling evidence that it is feasible to perform this task with a single model covering over a 100 languages.</p><p>Our automatically extracted Mewsli-9 dataset serves as a starting point for evaluating entity link-ing beyond the entrenched English benchmarks and under the expanded multilingual setting. Future work could investigate the use of non-expert human raters to improve the dataset quality further.</p><p>In pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the KB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy of Model F + on the 104 languages in our balanced Wikipedia heldout set, overlayed on alias table accuracy and Wikipedia training set size. (See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Context 4. . . In July 2009 , action by the Federal Bureau of Reclamation to protect threatened fish stopped irrigation pumping to parts of the California Central Valley. . .Prediction irrigation sprinkler Q998539 : ??????? ? ? ? ? ?? ? ?? ? ? ? ? ? ??? ? ? ?? ?? ?? OutcomeWrong: Metonymous mention of Central Valley Project Q2944429 in English, but model retrieved the more literal match, featurized in Chinese. Metonymy is a known challenging case for EL<ref type="bibr" target="#b16">(Ling et al., 2015)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Jovi #an ' ?n vek #iller #inden biri a? #abe #yi Valentin #ianus ' du ve 26 ?ubat 364 y?l?nda Augusto ? il titolo che fu portato dagli im #perator #i romani , dagli im #perator #i biz #anti #ni fino al 610 Dual Encoder Model F diagram. The input to the Mention Encoder is a sequence of WordPiece tokens that includes the document title (Ti), context immediately left of the mention (Li), the mention span (Mi) demarcated by [E] and [/E] markers, and context immediately right of the mention (Ri). Segment labels (SEGi) are also used to distinguish the input segments. The input to the (Model F) Entity Encoder is simply the WordPiece tokens in the entity description (Di). As usual, embeddings passed to the first transformer layer are the sum of positional embeddings (not pictured here), the segment embeddings, and the WordPiece embeddings. The example shows a Turkish mention of Augustus Q211804 paired with its Italian description.</figDesc><table><row><cell cols="2">Cosine similarity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4-Layer BERT Transformers, projecting the [CLS] token output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity Encoder</cell></row><row><cell cols="2">Segment Labels</cell><cell>SEG 0</cell><cell>SEG 1</cell><cell>SEG 2</cell><cell>SEG 3</cell><cell>SEG 0</cell></row><row><cell cols="2">WordPiece Tokens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[CLS]</cell><cell>D 1 D 2 ?</cell><cell>[SEP]</cell></row><row><cell>Mention in Turkish</cell><cell>Page title</cell><cell>Vale #ns</cell><cell></cell><cell>Augustus</cell><cell>ilan ed il #di .</cell><cell>Q211804 ( in Italian)</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Our best model outperforms previous related</cell></row><row><cell>non-monolingual models that relied on alias tables and</cell></row><row><cell>disambiguated among a much smaller set of entities.</cell></row><row><cell>Bottom half: linking accuracy on the TR2016 hard test</cell></row><row><cell>set. Top half: language coverage; entity vocabulary</cell></row><row><cell>size; and entities disambiguated among at inference</cell></row><row><cell>time. Middle columns:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison to DEER model (Gillick et al.,</cell></row><row><cell>2019) on their English WikiNews-2018 dataset.</cell></row><row><cell>eration step. They therefore only disambiguate</cell></row><row><cell>among the resulting ?20 candidate entities (only</cell></row><row><cell>from English Wikipedia), whereas our model per-</cell></row><row><cell>forms retrieval against all 20 million entities.</cell></row><row><cell>5.4.2 Out-of-Domain English Evaluation</cell></row><row><cell>We now turn to the question of how well the pro-</cell></row><row><cell>posed multilingual model can maintain competitive</cell></row><row><cell>performance in English and generalize to a domain</cell></row><row><cell>other than Wikipedia. Gillick et al. (2019) provides</cell></row><row><cell>a suitable comparison point. Their DEER model</cell></row><row><cell>is closely related to our approach, but used a more</cell></row><row><cell>light-weight dual encoder architecture with bags-of-</cell></row><row><cell>embeddings and feed-forward layers without atten-</cell></row><row><cell>tion and was evaluated on English EL only. On the</cell></row><row><cell>English WikiNews-2018 dataset they introduced,</cell></row><row><cell>our Transformer-based multilingual dual encoder</cell></row><row><cell>matches their monolingual model's performance</cell></row><row><cell>at R@1 and improves R@100 by 0.01 (reaching</cell></row><row><cell>0.99) Our model thus retains strong English per-</cell></row><row><cell>formance despite covering many languages and</cell></row><row><cell>linking against a larger KB. See</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results of our main dual encoder Model F + on the new Mewsli-9 dataset. Consistent performance across languages in a different domain from the training set points at good generalization.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model F +</cell><cell>+CA</cell></row><row><cell>Bin</cell><cell cols="4">Queries R@1 R@10 R@1</cell></row><row><cell>[0, 1)</cell><cell>3,198</cell><cell>0.08</cell><cell>0.34</cell><cell>0.07</cell></row><row><cell>[1, 10)</cell><cell>6,564</cell><cell>0.58</cell><cell>0.81</cell><cell>0.60</cell></row><row><cell>[10, 100)</cell><cell>32,371</cell><cell>0.80</cell><cell>0.93</cell><cell>0.82</cell></row><row><cell>[100, 1k)</cell><cell>66,232</cell><cell>0.90</cell><cell>0.97</cell><cell>0.90</cell></row><row><cell>[1k, 10k)</cell><cell>78,519</cell><cell>0.93</cell><cell>0.98</cell><cell>0.93</cell></row><row><cell>[10k, +)</cell><cell>102,203</cell><cell>0.94</cell><cell>0.99</cell><cell>0.96</cell></row><row><cell>micro-avg</cell><cell>289,087</cell><cell>0.89</cell><cell>0.96</cell><cell>0.91</cell></row><row><cell>macro-avg</cell><cell></cell><cell>0.70</cell><cell>0.84</cell><cell>0.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Results on the new Mewsli-9 dataset, by entity</cell></row><row><cell>frequency, attained by our main dual encoder Model F + ,</cell></row><row><cell>plus reranking its predictions with a Cross-Attention</cell></row><row><cell>scoring model (CA).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Context 1 . . . Bei den neuen Bahnen handelt es sich um das Model Tramino von der polnischen Firma Solaris Bus &amp; Coach. . . Prediction Solaris Tramino Q780281 : Solaris Tramino -rodzina tramwaj?w, kt?re s? produkowane przez firm? Solaris Bus &amp; Coach z Bolechowa ko?o Poznania. . . Outcome Correct: A family of trams originally manufactured in Poland, mentioned here in German, linked to its Polish description. 2 . . . sobre una tecnolog?a que permitir?a fabricar chocolate a partir de los zumos de fruta, agua con vitamina C o gaseosa diet?tica. . . Prediction fruit juice Q20932605 : Fruchtsaft , spezieller auch Obstsaft , ist ein aus Fr?chten einer oder mehrerer Fruchtarten gewonnenes fl?ssiges Erzeugnis. . .</figDesc><table><row><cell>Context</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Correct and mistaken examples observed in error analysis of dual encoder model F + on Mewsli-9.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We refer to the first sentence of a Wikipedia page as a description because it follows a standardized format.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">More details in Appendix C. 4 www.wikinews.org, using the 2019-01-01 snapshot from archive.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Mewsli-9 languages (code, family, script): Japanese ('ja', Japonic, ideograms); German ('de', Indo-European (IE), Latin); Spanish ('es', IE, Latin); Arabic ('ar', Afro-Asiatic, Arabic); Serbian ('sr', IE, Latin &amp; Cyrillic); Turkish ('tr', Turkic, Latin); Persian ('fa', IE, Perso-Arabic); Tamil ('ta', Dravidian, Brahmic); English ('en', IE, Latin).6  As of 2019-10-03.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The candidate descriptions (but not V ) are limited to the 104 languages covered by our model vocabulary-in general, both Wikipedia and WikiData cover more than 300 languages.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Sayali Kulkarni and the anonymous reviewers for their helpful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Mewsli-9 Dataset</head><p>Available at: http://goo.gle/mewsli- <ref type="bibr">dataset</ref> We used an automated process to construct Mewsli-9, exploiting link anchor text to identify naturally occurring entity mentions in WikiNews articles, from its inception to the end of 2018.</p><p>From a given WikiNews page dump, 9 we extracted text including link anchors and section headings using a modified version of wikiextractor. <ref type="bibr">10</ref> To obtain clean article text, we discard page-final sections that merely contain external references, etc. This is done by matching section headings against a small set of hand-collected, languagespecific patterns.</p><p>Mention candidates are filtered to those remaining links that point to Wikipedia pages in any language (not limited to our 104 languages). These Wikipedia links are redirected if necessary, and resolved to WikiData identifiers to determine the gold entity for a mention. There are many reasons why resolution may fail, including mistakes in the original markup and churn in the data sources over time. The final dataset is limited to (mention, entity) pairs where resolution to WikiData succeeded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details and Hyperparameters</head><p>All model training was carried out on a Google TPU v3 architecture, 11 using batch size 8192 and a learning rate schedule that uses linear warm-up followed by linear decay to 0.</p><p>The first phase of training our dual encoders (DE) with in-batch random negatives encompasses 500,000 steps, which takes approximately one day.</p><p>Where hard-negative training is applied, we initialize from the corresponding prior model checkpoint and continue training against the multi-task loss for a further 250,000 steps, which also takes about a day.</p><p>Other than the limit to using a 4-layer Transformer stack, our mention encoder and model F entity encoders use the same hyperparameters as mBERT-base, allowing initialization from the publicly available checkpoint-we use the weights of its first 4 layers, in addition to those of the token and positional embeddings. <ref type="bibr">9</ref>   The cross-attention scoring model (CA) in the final preliminary experiment is a full 12-layer Transformer (also mBERT-base), and was trained for 1 million steps, taking just under one day.</p><p>The learning rates were 1e-4 (DE) and 1-e5 (CA) and included warm-up phases of 10% (DE) and 1% (CA) of the respective number of training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data Preprocessing</head><p>We used the 2019-10-03 dump of Wikipedia and WikiData, parsed using in-house tools. <ref type="bibr">12</ref> Two filtering criteria are relevant in preprocessing WikiData to define our KB. The first is to exclude items that are a subclass (P279) or instance of (P31) the most common Wikimedia-internal administrative entities, detailed in <ref type="table">Table 8</ref>. The remaining entities are then filtered to retain only those for which the WikiData entry points to at least one Wikipedia page, in any language, motivated by our objective of using descriptive text as entity features.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Entity linking via dual and cross-attention encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bikel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pa?ca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirical evaluation of pretraining strategies for supervised entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07202</idno>
		<title level="m">Entities as experts: Sparse memory access with entity supervision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capturing semantic similarity for entity linking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1256" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">End-to-end retrieval in continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav Singh</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>REALM: Retrieval-augmented language model pre-training</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning<address><addrLine>Vienna, Austria. PMLR</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Systematic study of long tail phenomena in entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">Ellis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Analytics Conference</title>
		<meeting>the Text Analytics Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00065</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00141</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Barack&apos;s wife Hillary: Using knowledge graphs for fact-aware language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1598</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3449" to="3460" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crosslanguage entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MEANTIME, the NewsReader multilingual event and time corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Lyse</forename><surname>Minard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Speranza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Urizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bego?a</forename><surname>Altuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anneleen</forename><surname>Marieke Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantal</forename><surname>Schoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4417" to="4422" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot neural transfer for cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiateng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6924" to="6931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VoxEL: a benchmark dataset for multilingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rosales-M?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating logical forms from graph representations of text and entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Massey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Entity linking with a knowledge base: Issues, techniques, and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="460" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">One for all: Towards language independent named entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM- CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint multilingual supervision for cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2486" to="2495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Don&apos;t compare apples to oranges: Extending gerbil for a fine grained nel evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Waitelonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>J?rges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Sack</surname></persName>
		</author>
		<idno type="DOI">10.1145/2993318.2993334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Semantic Systems</title>
		<meeting>the 12th International Conference on Semantic Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>SEMANTiCS 2016</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zero-shot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards zero-resource cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving candidate generation for low-resource cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00303</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
