<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Global Homophily in Graph Neural Networks When Meeting Heterophily</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<email>li&lt;xiangli@dase.ecnu.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science and Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science and Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science and Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caihua</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqiang</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<settlement>Singapore. Correspondence to: Xiang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science and Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Global Homophily in Graph Neural Networks When Meeting Heterophily</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-right 2022 by the author(s).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate graph neural networks on graphs with heterophily. Some existing methods amplify a node's neighborhood with multi-hop neighbors to include more nodes with homophily. However, it is a significant challenge to set personalized neighborhood sizes for different nodes. Further, for other homophilous nodes excluded in the neighborhood, they are ignored for information aggregation. To address these problems, we propose two models GloGNN and GloGNN++, which generate a node's embedding by aggregating information from global nodes in the graph. In each layer, both models learn a coefficient matrix to capture the correlations between nodes, based on which neighborhood aggregation is performed. The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution. We further accelerate neighborhood aggregation and derive a linear time complexity. We theoretically explain the models' effectiveness by proving that both the coefficient matrix and the generated node embedding matrix have the desired grouping effect. We conduct extensive experiments to compare our models against 11 other competitors on 15 benchmark datasets in a wide range of domains, scales and graph heterophilies. Experimental results show that our methods achieve superior performance and are also very efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph-structured data is ubiquitous in a variety of domains including chemistry, biology and sociology. In graphs (networks), nodes and edges represent entities and their relations, respectively. To enrich the information of graphs, nodes are usually associated with various features. For example, on Facebook, users are connected by the friendship relation and each user has features like age, gender and school. Both node features and graph topology provide sources of information for graph-based learning. Recently, graph neural networks (GNNs) <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b27">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017)</ref> have received significant attention for the capability to seamlessly integrate the two sources of information and they have been shown to serve as effective tools for representation learning on graph-structured data.</p><p>Based on the implicit graph homophily assumption, traditional GNNs <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016</ref>) adopt a non-linear form of smoothing operation and generate node embeddings by aggregating information from a node's neighbors. Specifically, homophily is a key characteristic in a wide range of real-world graphs, where linked nodes tend to share similar features or have the same label. These graphs include friendship networks <ref type="bibr" target="#b21">(McPherson et al., 2001)</ref>, political networks <ref type="bibr" target="#b9">(Gerber et al., 2013)</ref> and citation networks <ref type="bibr" target="#b5">(Ciotti et al., 2016)</ref>. However, in the real world, there also exist graphs with heterophily, where nodes with dissimilar features or different labels are more likely to be connected. For example, different amino acid types are connected in protein structures; predator and prey are related in the ecological food webs. In these networks, due to the heterophily, the smoothing operation could generate similar representations for nodes with different labels, which lead to the poor performance of GNNs.</p><p>To generalize GNNs to heterophilous graphs, some recent works <ref type="bibr" target="#b34">(Zhu et al., 2020b;</ref><ref type="bibr" target="#b1">Bo et al., 2021;</ref><ref type="bibr" target="#b4">Chien et al., 2020)</ref> have been proposed to leverage high-pass convolutional filters and multi-hop neighbors to address the heterophily issue. On the one hand, the high-pass filters can be used to push away a node's feature vector from its neighbors' while the low-pass filters used by traditional GNNs do the opposite. The combination of low-pass and high-pass filters in arXiv:2205.07308v1 <ref type="bibr">[cs.</ref>LG] 15 May 2022 these models enforces the learned representation of a node to be close to its homophilous neighbors' and distant from heterophilous ones'. On the other hand, in heterophilous graphs, linked nodes are more likely to be dissimilar while distant nodes could share a certain similarity, so we have to jump the locality of a node to find its homophilous neighbors. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a user has only one local neighbor with homophily, while three homophilous nodes exist multi-hop away. All these four nodes exhibit the global homophily for the user, which can be used to help predict her label. Meanwhile, since it has been pointed out in <ref type="bibr" target="#b34">(Zhu et al., 2020b</ref>) that the 2-hop neighborhood of a node under some mild condition will be homophily-dominant in expectation, some models amplify a node's neighborhood with multi-hop neighbors. However, how large the neighborhood size should be set for different nodes is a challenge. Further, for those homophilous nodes excluded in the neighborhood, they will not be utilized in information aggregation. Therefore, we propose to leverage the global homophily for a node in the graph by adding all the nodes to its neighborhood. As a side effect, in this case, the traditional neighborhood aggregation will have a quadratic time complexity, which is practically infeasible. Further, more heterophilous nodes will be included in the neighborhood, which could adversely affect the model performance. Therefore, a research question arises: Can we find global homophily for a node and develop a GNN model that is both effective and efficient for heterophilous graphs?</p><p>In this paper, to find Global homophily for nodes in graphs with heterophily, we propose an effective and scalable GNN model, namely, GloGNN. In the l-th convolutional layer, inspired by the linear subspace model <ref type="bibr" target="#b15">(Liu et al., 2012)</ref>, we linearly characterize each node by all the nodes in the graph and derive a coefficient matrix Z (l) such that Z (l) ij describes the importance of node x j to node x i . We formulate the characterization problem as an optimization problem that has a closed-form solution for Z <ref type="bibr">(l)</ref> . After that, taking Z (l) as the weight matrix, we generate node embedding matrix H (l) by aggregating information from global nodes. Note that directly computing such Z (l) and Z (l) -based neighborhood aggregation lead to cubic and quadratic time complexities w.r.t. the number of nodes, respectively. Hence, we avoid calculating Z (l) directly and reorder matrix multiplication in neighborhood aggregation, which effectively reduces the time complexity to linear. Finally, we mathematically show that both Z (l) and the generated node embedding matrix H (l) have the grouping effect <ref type="bibr" target="#b18">(Lu et al., 2012)</ref>, i.e., for any two nodes in a graph, no matter how distant they are, if they share similar features and local structures, their embedding vectors will be close to each other. This helps explain the effectiveness of our models. We summarize the main contributions of our paper as:</p><p>? We propose two effective and efficient GNN models, GloGNN and GloGNN++.</p><p>? We theoretically show that both Z (l) and the generated node embedding matrix H (l) have the grouping effect.</p><p>? We combine low-pass and high-pass convolutional filters in neighborhood aggregation, as Z (l) allows signed values.</p><p>? We show the superiority of our models against 11 other methods on 15 benchmark datasets of diverse domains, sizes and graph heterophilies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GNNs have recently received significant interest for the superior performance on graph-based learning. The early model GCN <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref> extends the convolution operation from regular data to irregular graph-structured data. GCN is a spectral model <ref type="bibr" target="#b2">(Bruna et al., 2013;</ref><ref type="bibr" target="#b7">Defferrard et al., 2016)</ref>, which decomposes graph signals via graph Fourier transform and convolves on the spectral components. There are also a class of spatial GNN models that directly aggregate information from spatially nearby neighbors of a node. For example, GraphSAGE <ref type="bibr" target="#b10">(Hamilton et al., 2017)</ref> generates a node's embedding by aggregating information from a fixed number of neighbors. GAT <ref type="bibr" target="#b27">(Veli?kovi? et al., 2017)</ref> introduces the attention mechanism to learn the importance of a node's neighbors and aggregates information from these neighbors based on the learned weights. Further, GNNs have been widely studied from various perspectives, such as the over-smoothing problem <ref type="bibr" target="#b31">(Zhao &amp; Akoglu, 2019;</ref><ref type="bibr" target="#b23">Rong et al., 2019)</ref>, the adversarial attack and defense <ref type="bibr" target="#b6">(Dai et al., 2018;</ref><ref type="bibr" target="#b32">Zhu et al., 2019)</ref>, and the model explainability <ref type="bibr" target="#b28">(Vu &amp; Thai, 2020;</ref><ref type="bibr">Shan et al., 2021)</ref>.</p><p>There are also works <ref type="bibr" target="#b34">(Zhu et al., 2020b;</ref><ref type="bibr" target="#b1">Bo et al., 2021;</ref><ref type="bibr" target="#b4">Chien et al., 2020;</ref><ref type="bibr" target="#b29">Yan et al., 2021;</ref><ref type="bibr" target="#b25">Suresh et al., 2021;</ref><ref type="bibr" target="#b22">Pei et al., 2020;</ref><ref type="bibr" target="#b8">Dong et al., 2021;</ref><ref type="bibr">Lim et al., 2021;</ref><ref type="bibr" target="#b30">Yang et al., 2021;</ref><ref type="bibr" target="#b19">Luan et al., 2021;</ref><ref type="bibr" target="#b33">Zhu et al., 2020a;</ref><ref type="bibr" target="#b17">Liu et al., 2021)</ref> that extend GNNs to heterophilous graphs. Some methods propose to leverage both low-pass and high-pass convolutional filters in neighborhood aggregation. For example, GPR-GNN <ref type="bibr" target="#b4">(Chien et al., 2020)</ref> adapts to the homophily/heterophily structure of a graph by learning signed weights for node embeddings in different propagation steps. ACM-GCN <ref type="bibr" target="#b19">(Luan et al., 2021)</ref> applies both low-pass and high-pass filters for each node in a layer, and adaptively fuses the generated node embeddings from each filter. Further, some methods enlarge the node neighborhood size to include more homophilous nodes. As a representative model, H 2 GCN <ref type="bibr" target="#b34">(Zhu et al., 2020b)</ref> presents three designs to improve the performance of GNNs under heterophily: ego and neighbor embedding separation, higher-order neighborhood utilization and intermediate representation combination. WRGAT <ref type="bibr" target="#b25">(Suresh et al., 2021)</ref> transforms the original graph into a multi-relational one that contains both raw edges and newly constructed edges. The new edges can connect distant nodes and are weighted by node local structural similarity. There also exist GNNs that study the graph heterophily issue from other perspectives. For example, to jointly study the heterophily and over-smoothing problems, GGCN <ref type="bibr" target="#b29">(Yan et al., 2021)</ref> allows for signed messages to be propagated from a node's neighborhood. On the other hand, it adopts a degree correction mechanism to rescale node degrees and further alleviate the over-smoothing problem. To generalize GNNs to large-scale graphs, LINKX <ref type="bibr">(Lim et al., 2021)</ref> separately embeds node features and graph topology. After that, the two embeddings are combined with MLPs to generate node embeddings. Different from all these methods, GloGNN performs node neighborhood aggregation from the whole set of nodes in the graph, which takes more nodes in the same class as neighbors and thus boosts the performance of GNNs on graphs with heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>In this section we introduce notations and concepts used in this paper.</p><p>[Notations]. We denote an undirected graph without self-</p><formula xml:id="formula_0">loops as G = (V, E), where V = {v i } n i=1</formula><p>is a set of nodes and E ? V ? V is a set of edges. Let A denote the adjacency matrix such that A ij represents the weight of edge e ij between nodes v i and v j . For each node v i , we use N i to denote v i 's neighborhood, which is the set of nodes directly connected to v i . We further construct a diagonal matrix D where D ii = n j=1 A ij . We denote the node representation matrix in the l-th layer as H (l) , where the i-th row is the embedding vector h (l) i of node v i . For the initial node feature matrix, we denote it as X. We use Y ? R n?c to denote the ground-truth node label matrix, where c is the number of labels in node classification and the i-th row y i is the one-hot encoding of node v i 's label.</p><p>[Homophily/Heterophily]. The homophily/heterophily of a graph is typically defined based on the similarity/dissimilarity between two connected nodes w.r.t. node features or node labels. In this paper, we focus on homophily/heterophily in node labels. There have been some metrics of homophily proposed. For example, edge homophily <ref type="bibr" target="#b34">(Zhu et al., 2020b)</ref> is defined as the fraction of edges that connect nodes with the same label. Further, high homophily indicates low heterophily, and vice versa. We thus interchangeably use these two terms in this paper.</p><p>[GNN basics]. The convolution operation in GNNs is typically composed of two steps: (1) feature propagation and aggregation:?</p><formula xml:id="formula_1">(l) i = AGGREGATE(h (l) j , ?v j ? N i ); (2) node embedding updating: h (l+1) i = Update(h (l) i ,? (l)</formula><p>i ). One of the most widely used GNN models is vanilla GCN <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>, which adopts a renormalization trick to add a self-loop to each node in the graph. After that, the normalized affinity matrix? =D ? 1 2?D ? 1 2 , where? = A + I n , D = D + I n and I n ? R n?n is the identity matrix. Note that? is a low-pass filter while the corresponding Laplacian matrix L = I n ?? is a high-pass filter. The output in the (l + 1)-th layer of vanilla GCN is H (l+1) = ?(?H (l) W (l) ), where W (l) is a learnable weight matrix and ? is the Relu function. After L layers, H L is then subsequently fed into a softmax layer to generate label probability logits and a cross-entropy function for node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm</head><p>In this section, we describe our models. We first show how to capture node correlations and derive the coefficient matrix Z in each layer. After that, we introduce how to accelerate neighborhood aggregation based on Z. Further, we theoretically prove that both Z and H have the desired grouping effect. Finally, we summarize GloGNN and upgrade the model to GloGNN++. The overall framework of GloGNN is given in <ref type="figure">Figure 2</ref>. <ref type="figure">Figure 2</ref>. The overall framework of GloGNN. In each layer, we derive a coefficient matrix, based on which a node's embedding is generated by aggregating information from global nodes.</p><formula xml:id="formula_2">Inputs X A ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) %% (&amp;) %' (&amp;) %( (&amp;) %) (&amp;) %* (&amp;) %+ (&amp;) ( ) ( ) for each node ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Coefficient matrix</head><p>In graphs with heterophily, connected nodes are more likely to have different labels while distant nodes could be from the same class. We thus have to enlarge a node's neighborhood to leverage more distant nodes. A straightforward way is to use all the nodes in the graph. Inspired by the linear subspace model <ref type="bibr" target="#b15">(Liu et al., 2012)</ref>, we characterize each node by all the nodes in the graph. Specifically, in the l-th layer, we can determine a coefficient matrix Z (l) ? R n?n :</p><formula xml:id="formula_3">H (l) = Z (l) H (l) + O (l) ,<label>(1)</label></formula><p>where O (l) is a noise matrix. One can interpret Z (l)</p><p>ij as a value that reflects how well node x j characterizes x i , so Z (l) plays the role of a weight matrix. Note that Z (l) ij allows signed values. On the one hand, the more similar two nodes are, the more likely that one node can be represented by the other. Homophilous neighbors will thus be assigned large positive coefficients. On the other hand, heterophilous neighbors will be given small positive or negative coefficients due to the dissimilarities. Further, since it has been pointed out in <ref type="bibr" target="#b16">(Liu et al., 2020)</ref> that the over-smoothing problem is caused by the coupling of neighborhood aggregation and feature transformation, we decouple these two processes by first performing feature transformation to generate H (0) . Specifically, we use MLPs to map feature matrix and adjacency matrix into H (0)</p><formula xml:id="formula_4">X ? R n?c and H (0) A ? R n?c , respectively: H (0) X = MLP 1 (X), H (0) A = MLP 2 (A).</formula><p>( <ref type="formula">2)</ref> Here, c is the number of labels. Then we derive the initial node embedding matrix H (0) :</p><formula xml:id="formula_5">H (0) = (1 ? ?)H (0) X + ?H (0) A ,<label>(3)</label></formula><p>where ? ? [0, 1] is the term weight. Note that H (0) captures the information of both nodes' features and connectivities. Inspired by <ref type="bibr" target="#b12">(Klicpera et al., 2018)</ref>, we use skip connection and further modify H (l) in Equation 1 into:</p><formula xml:id="formula_6">H (l) = (1 ? ?)Z (l) H (l) + ?H (0) + O (l) ,<label>(4)</label></formula><p>where ? ? [0, 1] is a hyper-parameter that balances the term importance. Here, we characterize node correlations based on H (l) . However, as suggested in <ref type="bibr" target="#b25">(Suresh et al., 2021)</ref>, if two nodes share similar local graph structures, they are more likely to have the same label. We thus measure node correlations in terms of both feature similarity and topology similarity by further regularizing Z (l) with nodes' multi-hop reachabilities. We have the following objective function:</p><formula xml:id="formula_7">min Z (l) H (l) ?(1??)Z (l) H (l) ??H (0) 2 F +?1 Z (l) 2 F +?2 Z (l) ? K k=1 ? k? k 2 F<label>(5)</label></formula><p>where ? 1 and ? 2 are weighting factors for adjusting the importance of different components, and K is the maximum hop count. To show the importance of the k-hop graph connectivity, we further introduce a learnable parameter ? k . The objective function consists of three terms. The first term reduces noise and drives the linear representation for nodes to be close to their own embeddings, the second term is a Frobenius norm, and the third term regularizes Z (l) by the multi-hop regularized graph adjacency matrices. A closed-form solution Z (l) * to the optimization problem is</p><formula xml:id="formula_8">Z (l) * = (1 ? ?)H (l) (H (l) ) T + ?2 K k=1 ? k? k ? ?(1 ? ?)H (0) (H (l) ) T ? (1 ? ?) 2 H (l) (H (l) ) T + (?1 + ?2)In ?1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Aggregation acceleration</head><p>Based on Z (l) * , we can write neighborhood aggregation in the l-th layer:</p><formula xml:id="formula_9">H (l+1) = (1 ? ?)Z (l) * H (l) + ?H (0) .<label>(6)</label></formula><p>However, directly updating H (l+1) by Equation 6 is infeasible due to the cubic time complexity in computing Z (l) * and the quadratic time complexity in calculating Z (l) * H (l) .</p><p>To accelerate the updates of H (l+1) , instead of directly calculating Z (l) * , we transform Equation 6 into (see Section C in the appendix):</p><formula xml:id="formula_10">H (l+1) =(1 ? ?)H (l) (H (l) ) T Q (l+1) + ?2 K k=1 ? k? k Q (l+1) ? ?(1 ? ?)H (0) (H (l) ) T Q (l+1) + ?H (0)<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">Q (l+1) = 1 ? ? ?1 + ?2 H (l) ? 1 ? ? (?1 + ?2) 2 H (l) ? 1 (1 ? ?) 2 Ic + 1 ?1 + ?2 (H (l) ) T H (l) ?1 (H (l) ) T H (l)<label>(8)</label></formula><p>Here, H (l) , Q (l+1) ? R n?c , I c ? R c?c is the identity matrix and c is the number of labels. In this way, we avoid computing Z (l) * directly and can accelerate the updates of H (l+1) by matrix multiplication reordering. Specifically, we first compute Q (l+1) , where the second term is calculated from right to left. In particular, the matrix inversion is performed on a matrix in R c?c , whose time complexity is only O(c 3 ) and c is generally a very small number. This significantly improves the model efficiency. The overall time complexity of updating Q (l+1) is O(nc 2 + c 3 ), where c 2 n. After Q (l+1) is calculated, we then update H (l+1) . We compute each term of H (l+1) in a similar right-to-left manner. For example, to calculate H (l) (H (l) ) T Q (l+1) , we first compute (H (l) ) T Q (l+1) but not H (l) (H (l) ) T . This reduces the time complexity to be O(nc 2 ), but not O(n 2 c). When calculating K k=1 ? k? k Q (l+1) , we can first comput? AQ (l+1) due to the sparsity of? for a general graph, which only requires a time complexity of O(k 1 cn). Here, k 1 is the average number of nonzero entries in a row of?. Whil? AQ (l+1) generates a dense matrix, we can further employ the sparsity of? to get? 2 Q (l+1) . In this way, we can</p><formula xml:id="formula_12">sequentially derive?Q (l+1) ,? 2 Q (l+1) , ...,? K Q (l+1) in O(k 1 cn).</formula><p>In summary, the total time complexity to update H (l+1) by Equation 7 is O(k 2 n), where k 2 is a coefficient and k 2 n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Grouping effect</head><p>For a node v i , we denote? k i as the i-th row of? k , which represents v i 's k-hop node reachability in a graph. Given two nodes v i and v j , if they have similar feature vectors and local graph structures, their characterizations from other nodes are expected to be similar. Formally, we have Definition 4.1. (Grouping effect ). Given a set of nodes</p><formula xml:id="formula_13">V = {v i } n i=1 , let v i ? v j denote the condi- tion that (1) x i ?x j 2 ? 0 and (2) ? k i ?? k j 2 ? 0, ?k ? [1, K]. A matrix Z is said to have grouping effect if v i ? v j ? |Z ip ? Z jp | ? 0, ?1 ? p ? n.<label>(9)</label></formula><p>We next theoretically show the grouping effect of Z (l) * , (Z (l) * ) T and H (l+1) by giving the following lemmas. Proofs of all these lemmas are deferred to Section D in the appendix.</p><formula xml:id="formula_14">Lemma 4.2. ?1 ? i, j, p ? n, |Z (l) * ip ? Z (l) * jp | ? 1 ? ? ?1 + ?2 h (l) i ? h (l) j 2 (h (l) p ) T ? (1 ? ?) 2 (H (l) ) T R 2 + ?(1 ? ?) ?1 + ?2 h (0) i ? h (0) j 2 (h (l) p ) T ? (1 ? ?) 2 (H (l) ) T R 2 + ?2(1 ? ?) 2 ?1 + ?2 K k=1 ? k ? k i ?? k j 2 R 2 + ?2 ?1 + ?2 K k=1 ? k |? k ip ?? k jp | where R = (1 ? ?) 2 H (l) (H (l) ) T + (?1 + ?2)In ?1 H (l) (h (l) p ) T . Lemma 4.3. ?1 ? i, j, p ? n, |Z (l) * pi ? Z (l) * pj | ? ?(1 ? ?) h (l) i ? h (l) j 2 + ?2 K k=1 ? k |? k pi ?? k pj | ?1 + ?2 where ? = h (l) p ? ?h (0) p 2 2 + ?2 K k=1 ? k? k p 2 2 . Lemma 4.4. Matrices Z (l) * , (Z (l) * ) T and H (l+1) all have grouping effect.</formula><p>The grouping effect of Z (l) * , (Z (l) * ) T and H (l+1) indeed explains the effectiveness of our model. In fact, for any two nodes v i and v j , no matter how distant they are in a graph, if they share similar feature vectors and local structures, we conclude that (1) they will be given similar coefficient vectors; (2) they will play similar roles in characterizing other nodes; and (3) they will be given similar representation vectors. On the other hand, in graphs with heterophily, adjacent nodes are more likely to be dissimilar and they will thus be given different embeddings. Further, for two nodes with low feature similarity, using one to characterize the other can be enhanced by the regularization term of local graph structure, if they share high structural similarity. This also applies to nodes that have high feature similarity but low structural similarity. After L convolutional layers, we derive H (L) . We then normalize H (L) by a Softmax layer, whose results are further fed into the Cross-entropy function for classification. Finally, we summarize the pseudocodes of GloGNN in Algorithm 1 (Section A of the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">GloGNN++</head><p>Given H l , the coefficient matrix Z (l) * in Equation 6 plays the role of "longitudinal" attention that characterizes the importance of a node to another. In neighborhood aggregation, not only varies the importance of a node's neighbors, but also that of hidden features. For example, in node classification, the imbalance of node labels could lead to the various importance of hidden features corresponding to different labels. Therefore, we further upgrade our model by considering "horizontal" attention w.r.t. hidden features. We introduce a diagonal matrix ? ? R c?c such that ? ii describes the importance of the i-th dimension in H (l) . We modify Equation 5 into:</p><formula xml:id="formula_15">min Z (l) ||H (l) ?(1??)Z (l) H (l) ???H (0) || 2 F +?1 Z (l) 2 F +?2||Z (l) ? K k=1 ? k? k || 2 F ,<label>(10)</label></formula><p>and derive the optimal solution Z (l) * :</p><formula xml:id="formula_16">Z (l) * = (1 ? ?)H (l) ?(H (l) ) T + ?2 K k=1 ? k? k ? ?(1 ? ?)H (0) ?(H (l) ) T ? (1 ? ?) 2 H (l) ??(H (l) ) T + (?1 + ?2)In ?1</formula><p>Following the same procedure in Sec. 4.2 and 4.3, we can also accelerate neighborhood aggregation and further prove that such Z (l) * , (Z (l) * ) T and H (l+1) have the desired grouping effect. We omit the details due to the space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>We next discuss the major differences between our models and the adapted GAT model that takes global nodes as a node's neighbors. First, the attention weights in GAT are automatically learned and lack of interpretability, but Z (l) in our models is derived from a well-designed optimization problem and has a closed-form solution. Second, the attention weights in GAT are always non-negative values while Z (l) in our methods allows signed values. Therefore, GAT only employs low-pass convolutional filters while our methods combine both low-pass and high-pass filters. Third, for each node, the neighborhood aggregation performed by GAT over all the nodes in the graph is computationally expensive, which has a quadratic time complexity w.r.t. the number of nodes. However, our methods accelerate the aggregation and derive a linear time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we comprehensively evaluate the performance of GloGNN and GloGNN++. In particular, we compare them with 11 other methods on 15 benchmark datasets, to show the effectiveness and efficiency of our models. Due to the space limitation, we move experimental setup (Sec. E) and ablation study (Sec. F) to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For fairness, we conduct experiments on 15 benchmark datasets, which include 9 small-scale datasets released by <ref type="bibr" target="#b22">(Pei et al., 2020)</ref> and 6 large-scale datasets from <ref type="bibr">(Lim et al., 2021)</ref>. We use the same training/validation/test splits as provided by the original papers. In particular, these datasets span various domains, scales and graph heterophilies. The statistics of these datasets are summarized in <ref type="table">Tables 1 and 2</ref>. Details on these datasets can be found in Section B of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Algorithms for comparison</head><p>We compare GloGNN and GloGNN++ with 11 other baselines, including <ref type="formula" target="#formula_3">(1)</ref>   <ref type="bibr">(Lim et al., 2021)</ref>. For these methods specially designed for heterophilous graphs, LINKX is a MLP-based method while others are GNN models. Further, although several ACMvariants are proposed in <ref type="bibr" target="#b19">(Luan et al., 2021)</ref>, ACM-GCN is reported to achieve the overall best performance on the same splits of benchmark datasets from <ref type="bibr" target="#b22">(Pei et al., 2020)</ref>, so we choose it as the baseline. For other models like Geom-GCN <ref type="bibr" target="#b22">(Pei et al., 2020)</ref> and FAGCN <ref type="bibr" target="#b1">(Bo et al., 2021)</ref>, since they have been shown to be outperformed by the state-ofthe-arts, we exclude their results in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance results</head><p>Tables 1 and 2 summarize the performance results of all the methods on 15 benchmark datasets. Note that we compare the AUC score on genius as in <ref type="bibr">(Lim et al., 2021)</ref>. For other datasets, we show the results of classification accuracy. Each column in the tables corresponds to one dataset. For each dataset, we highlight the winner's score in bold and the runner-up's with underline. From the tables, we make the following observations:</p><p>(1) MLP that uses only node features performs surprisingly well on some datasets with large heterophily, such as Actor. This shows the importance of node features for node classification in heterophilous graphs.</p><p>(2) Compared with the plain GNN models GCN and GAT, MixHop and GCNII generally perform better. For example, on Wisconsin, the accuracy scores of MixHop and GCNII are 0.7588 and 0.8039, respectively, which significantly outperform that of GCN and GAT. On the one hand, MixHop amplifies a node's neighborhood with its multi-hop neighbors. This introduces more homophilous neighbors for the node. On the other hand, the initial residual and identity mapping mechanisms in GCNII implicitly combine intermediate node representations to boost the model performance.</p><p>(3) Although H 2 GCN, WRGAT and GGCN can achieve good performance on small-scale datasets, they fail to run on very large-scale datasets due to the out-of-memory (OOM) error. This hinders the wide application of these models. For ACM-GCN and LINKX, they cannot consistently provide superior results. For example, LINKX ranks third on largescale datasets, but performs poorly on small-scale ones (rank 8th). ACM-GCN is the winner on Texas, but its accuracy score on pokec is only 0.6381 (the best result is 0.8305).</p><p>While GPR-GNN leverages both low-pass and high-pass filters, it only utilizes one type of convolutional filters in each layer, which restricts its effectiveness.</p><p>(4) GloGNN++ achieves the first average rank over all the datasets while GloGNN is the runner-up. This shows that both of them can consistently provide superior results on datasets in a wide range of diversity. On the one hand, both methods learn to utilize more neighbors with homophily for node neighborhood aggregation. This boosts the model performance. On the other hand, GloGNN++ further learns the importance of hidden features of nodes, which improves the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Efficiency study</head><p>In this section, we study GloGNN's efficiency. For fairness, we compare the training time for methods that are specially designed for graphs with heterophily. In particular, we make the comparison on the large-scale datasets for better efficiency illustration. For all these methods, we use the same training set on each dataset and run the experiments for 500 epochs. We repeat the experiments three times and show the average training time of these methods w.r.t. accuracy/AUC scores on the validation set in <ref type="figure" target="#fig_3">Figure 3</ref>. Note that due to the OOM error, GGCN fails to run on these datasets and we exclude it for comparison. We also drop WRGAT because it takes long time to precompute the multirelational graph. For example, with default hyper-parameter settings, WRGAT takes around 3200 seconds to compute the multi-relational graph of Penn94 on a server with 48 CPUs. However, the runs of all other models that perform on the original graph are finished within the period. We next recap the major difference of these methods: all the methods except LINKX are GNN models. Specifically, for each node, ACM-GCN and GPR-GNN perform convolution directly from its adjacent neighbors while H 2 GCN, GloGNN and GloGNN++ amplify the neighborhood of the node. Here, H 2 GCN considers multi-hop neighbors in the node's neighborhood while both GloGNN and GloGNN++ employ the whole set of nodes in the graph. Compared with GloGNN, GloGNN++ further incorporates an attention mechanism to learn the importance of node hidden features. Finally, LINKX is a simple MLP-based model that does not include the graph convolution operation. <ref type="table">Table 1</ref>. The classification accuracy (%) over the methods on 9 small-scale datasets released in <ref type="bibr" target="#b22">(Pei et al., 2020)</ref>. The error bar (?) denotes the standard deviation score of results over 10 trials. We highlight the best score on each dataset in bold and the runner-up score with underline. Note that Edge Hom. <ref type="bibr" target="#b34">(Zhu et al., 2020b)</ref> is defined as the fraction of edges that connect nodes with the same label.  <ref type="table">Table 2</ref>. The classification results (%) over the methods on 6 large-scale datasets released in <ref type="bibr">(Lim et al., 2021)</ref>. Note that we compare the AUC score on genius as in <ref type="bibr">(Lim et al., 2021)</ref>. For other datasets, we show the classification accuracy. The error bar (?) denotes the standard deviation score of results over 5 trials. We highlight the best score on each dataset in bold and the runner-up score with underline. Note that OOM refers to the out-of-memory error. From <ref type="figure" target="#fig_3">Fig. 3</ref>, we see that GloGNN and GloGNN++ converge very fast to the best/runner-up results over all the datasets. While GPR-GNN runs faster, it generally performs poorly. For LINKX, the MLP-based model structure instead of GNN-based explains its scalability. However, the 8th-ranked accuracy score on datasets in <ref type="table">Table 1</ref> restricts its wide usage. For H 2 GCN and ACM-GCN, they are slower than GloGNN and GloGNN++. For example, GloGNN++ achieves almost 8? speedup than ACM-GCN on genius; it is also 2? faster than H 2 GCN on Penn94. These results show that GloGNN and GloGNN++ are highly effective and also efficient; hence they can be widely applied to large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Grouping effect</head><p>Lemma 4.4 shows that both the coefficient matrix Z * and the node embedding matrix H have the desired grouping effect. Considering the dataset size for clear illustration, we choose Texas, Wisconsin and Cornell as representatives to</p><p>show the grouping effect of Z * in <ref type="figure" target="#fig_4">Figure 4</ref> (a)-(c). All these datasets contain nodes in five labels. In each sub-figure, rows and columns are reordered by ground-truth labels. We use red and blue to indicate positive and negative values, respectively. We further use pixel color brightness to show the positive/negative degree of a value. The brighter a pixel, the larger the degree. From the figures, the matrix exhibits the well-defined block diagonal structure. This shows the grouping effect of Z * . For Texas and Cornell, we see only four blocks along the diagonal. This is because in both datasets, there exists one object class that includes only one node. Similarly, <ref type="figure" target="#fig_4">Figure 4</ref> (d)-(f) further show the grouping effect of the output node embedding matrix H on these datasets. We reorder columns by gold-standard classes. Each column in the matrix corresponds to a node's embedding vector. For nodes in the same class, their embedding vectors are close to each other. This further explains the superior performance of our models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Global homophily</head><p>We end this section with a study to show how GloGNN finds global homophily for nodes in the graph. Given a graph, we first calculate the average number of k-hop neighbors that share the same label with a node. We further inspect the average number of positive Z * values for these neighbors.</p><p>After that, we compare the results on 6 graphs with large heterophily in <ref type="figure" target="#fig_6">Figure 5</ref>. We see that for each node in these datasets, the average number of adjacent neighbors in the same class is less than that of multi-hop ones (2-hop to 6hop). There also exist many &gt; 6-hop neighbors that can be used to predict a node's label. This necessitates jumping the locality of a node and finding its global homophily. Further, for each node, our model GloGNN can correctly assign positive values to the global nodes in the same class, including both adjacent neighbors and those that are distant. This also explains the effectiveness of our models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we generalized GNNs to graphs with heterophily. We proposed GloGNN and GloGNN++, which generate a node's embedding by aggregating information from global nodes in the graph. In each layer, we formulated an optimization problem to derive a coefficient matrix Z that describes the relationships between nodes. Neighborhood aggregation is then performed based on Z. We accelerated the aggregation process by matrix multiplication reordering without explicitly calculating Z. We mathematically proved that both Z and the generated node embedding matrix H have the desired grouping effect, which explains the model effectiveness. We conducted extensive experiments to evaluate the performance of our models. Experimental results show that our methods performs favorably against other 11 competitors over 15 datasets of diverse heterophilies; they are also efficient and converge very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pseudocodes</head><p>Given a graph G = (V, E) and a label set C with |C| = c, let V = L ? U, where L is a set of labeled objects and U is a set of unlabeled ones, the node classification problem is to learn a mapping ?: V ? C to predict the labels of nodes in U. We next summarize the pseudocodes of GloGNN as follows. Calculate H (l+1) by Eq. 7 8: end for 9: Normalize H (L) with the Softmax function and feed the results into the Cross-entropy function 10: Optimize the objective function to update weight matrices 11: Return: Y U</p><formula xml:id="formula_17">Algorithm 1 GloGNN 1: Input: G = (V, E), V = L ? U, A, X, L, C,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>We first use 9 small-scale datasets from <ref type="bibr" target="#b22">(Pei et al., 2020)</ref> and divide them into the following four categories:</p><p>? [Citation network]. Cora, Citeseer and Pubmed are citation graphs, where each node represents a scientific paper. These graphs use bag-of-words representations as the feature vectors of nodes. Each node is assigned a label indicating the research field. Note that these three datasets are homophilous graphs.</p><p>? <ref type="bibr">[WebKB]</ref>. Texas, Wisconsin and Cornell are web page datasets collected from computer science departments of various universities. In these datasets, nodes are web pages and edges represent hyperlinks between them. We take bag-of-words representations as nodes' feature vectors. The task is to classify the web pages into five categories including student, project, course, staff and faculty.</p><p>? [Actor co-occurrence network]. Actor is a graph induced from the film-director-actor-writer network in <ref type="bibr" target="#b26">(Tang et al., 2009)</ref>, which describes the co-occurrence relation between actors in Wikipedia pages. Node features are constructed by keywords contained in the Wikipedia pages of actors. The task is to classify actors into five categories.</p><p>? [Wikipedia network]. Squirrel and Chameleon are two subgraphs of web pages in Wikipedia. Our task is to classify nodes into five categories based on their average amounts of monthly traffic.</p><p>To further show the effectiveness and efficiency of our models, we also use 6 large-scale datasets released by <ref type="bibr">(Lim et al., 2021)</ref>:</p><formula xml:id="formula_18">? [Social network].</formula><p>Penn94 is a subgraph extracted from Facebook whose nodes are students. Node features include major, second major/minor, dorm/house, year and high school. We take students' genders as nodes' labels. Pokec is a friendship network from a Slovak online social network, whose nodes are users and edges represent directed friendship relations. We construct node features from users' profiles, such as geographical region, registration time, age. The task is to classify users based on their genders. genius is a subnetwork extracted from genius.com, which is a website for crowdsourced annotations of song lyrics. In the graph, nodes are users and edges connect users that follow each other. User features include expertise scores, counts of contributions, roles held by users, etc. Some users are marked with a "gone" label on the site, which are more likely to be spam users. Our goal is to predict whether a user is marked with "gone". twitch-gamers is a subgraph from the streaming platform Twitch, where nodes are users and edges connect mutual followers. Node features include the number of views, the creation and update dates, language, life time and whether the account is dead. The task is to predict whether the channel has explicit content.</p><p>? [Citation network]. arXiv-year is a directed subgraph of ogbn-arXiv, where nodes are arXiv papers and edges represent the citation relations. We construct node features by taking the averaged word2vec embedding vectors of tokens contained in both the title and abstract of papers. The task is to classify these papers into five labels that are constructed based on their posting year. snap-patents is a US patent network whose nodes are patents and edges are citation relations. Node features are constructed from patent metadata. Our goal is to classify the patents into five labels based on the time when they were granted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Aggregation acceleration</head><p>To accelerate the updates of H (l+1) in Equation 6, we first follow the Woodbury formula <ref type="bibr" target="#b20">(Max, 1950)</ref> </p><formula xml:id="formula_19">to derive (1 ? ?) 2 H (l) (H (l) ) T + (?1 + ?2)In ?1 = 1 ?1 + ?2 In ? 1 (?1 + ?2) 2 H (l) 1 (1 ? ?) 2 Ic + 1 ?1 + ?2 (H (l) ) T H (l) ?1 (H (l) ) T<label>(11)</label></formula><p>After that, based on Eq. 6 and Eq. 11, we can easily transform Eq. 6 into Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof</head><p>In this section, we prove Lemma 4.2, Lemma 4.3 and Lemma 4.4, respectively. In the following discussion, we use z (l) * i to denote the i-th row of Z (l) * , which is the coefficient vector for representing node v i ; we denote? k i as the i-th row of? k , which represents v i 's k-hop node reachability in a graph. We first consider Lemma D.1:</p><formula xml:id="formula_20">Lemma D.1. ?1 ? i, p ? n, the optimal solution Z (l) * in Eq. 5 satisfies Z (l) * ip = (1 ? ?)[h (l) i ? (1 ? ?)z (l) * i H (l) ? ?h (0) i ](h (l) p ) T + ? 2 K k=1 ? k? k ip ? 1 + ? 2 .<label>(12)</label></formula><p>The proof of Lemma 4.4 is given as follows:</p><p>Proof. Given two nodes v i and v j , if v i ? v j , we can get by definition (1) x i ? x j 2 ? 0 and (2) ? k i ?? k j 2 ? 0, ?k ? [1, K]. Then based on Equations 2 and 3, we can easily get h</p><formula xml:id="formula_21">(0) i ? h (0) j 2 ? 0.</formula><p>Hence H (0) has grouping effect. We next show that Z (0) * has grouping effect. Since ? k i ?? k j 2 ? 0, then |? k ip ?? k jp | ? 0 and |? k pi ?? k pj | ? 0 (due to the symmetry of? k ). According to Equation 17, the R.H.S. of the equation will become close to 0, which induces that |Z (0) * ip ? Z (0) * jp | ? 0 and Z (0) * thus has grouping effect. Similarly, the R.H.S. of Equation 22 also approaches 0, leading to the grouping effect of (Z (0) * ) T . Then we show H (1) has grouping effect. From Eq. 6, H (l+1) is updated based on H (0) and Z (l) * H (l) . Due to the grouping effect of Z (0) * and H (0) , the linear representation Z (0) * H (0) also has grouping effect, which further induces that H (1) has grouping effect. In this way, we can inductively prove that Z (l) * , (Z (l) * ) T and H (l+1) all have grouping effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation study</head><p>We next conduct an ablation study to understand the main components of GloGNN. To construct the initial node embedding matrix H (0) , GloGNN first transforms both feature matrix and adjacency matrix into low-dimensional embedding vectors, respectively. To show the importance of feature matrix in constructing H (0) , we set ? = 0 in Equation 3 and derive H (0) = H (0) X . We call this variant GloGNN-na (no adjacency matrix). Similarly, to understand the importance of feature matrix, we set ? = 1 and call the variant GloGNN-nf (no feature matrix). Further, to utilize the local structural information of a node, GloGNN regularizes the coefficient matrix Z with multi-hop graph adjacency matrices, as shown in Equation 5. We thus consider a variant GloGNN-nl (no local regularization) by removing the regularization term to study the importance of local graph structures of nodes. Finally, we compare GloGNN with these variants on all the benchmark datasets and show the results in <ref type="figure">Fig. 6</ref>. From the figure, we see (1) While GloGNN-na and GloGNN-nf can achieve comparable performance with GloGNN on some datasets, GloGNN significantly outperforms them on others. This shows the necessity of GloGNN to adaptively learn the importance of feature matrix and adjacency matrix when constructing the initial node embedding vectors on various datasets.</p><p>(2) GloGNN generally performs better than GloGNN-nl. Since GloGNN-nl ignores the local regularization term, it could fail to identify two homophilous nodes that share similar local graph structures. On the other hand, GloGNN measures node similarity in terms of both node features and local graph structures, which further explains GloGNN's robustness towards graphs with various heterophilies.  <ref type="figure">Figure 6</ref>. Ablation study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental setup</head><p>We implemented GloGNN by PyTorch. For fairness, we run the experiments of 9 small-scale datasets on CPUs and optimize the models by Adam as in <ref type="bibr" target="#b29">(Yan et al., 2021)</ref>. Meanwhile, we run the experiments of 6 large-scale datasets on a single Tesla V100 GPU with 32G memory and use AdamW as the optimizer following <ref type="bibr">(Lim et al., 2021)</ref>. We perform a grid search to tune hyper-parameters based on the results on the validation set. Details of these hyper-parameters are listed in <ref type="table" target="#tab_3">Tables 3  and 4</ref>. Further, since the results of most baseline methods on these benchmark datasets are public, we directly report these results. For those cases where the results are absent, we use the original codes released by their authors and fine tune the model parameters as suggested in <ref type="bibr" target="#b29">(Yan et al., 2021;</ref><ref type="bibr">Lim et al., 2021;</ref><ref type="bibr" target="#b25">Suresh et al., 2021)</ref>. We provide our code and data at https://github.com/RecklessRonan/GloGNN.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A toy example to show global homophily. All the homophilous nodes express the global homophily of the center user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>MLP;(2) general GNN methods: GCN<ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>, GAT<ref type="bibr" target="#b27">(Veli?kovi? et al., 2017)</ref>, MixHop (Abu-El-Haija et al., 2019) and GCNII<ref type="bibr" target="#b3">(Chen et al., 2020)</ref>; (3) heterophilous-graph-oriented methods: H 2 GCN<ref type="bibr" target="#b34">(Zhu et al., 2020b)</ref>, WRGAT<ref type="bibr" target="#b25">(Suresh et al., 2021)</ref>, GPR-GNN<ref type="bibr" target="#b4">(Chien et al., 2020)</ref>, GGCN<ref type="bibr" target="#b29">(Yan et al., 2021)</ref>, ACM-GCN<ref type="bibr" target="#b19">(Luan et al., 2021)</ref> and LINKX</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Efficiency study: x-axis shows the training time and yaxis is the accuracy/AUC score on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The grouping effect of Z * (a)-(c) and H (d)-(f) on Texas, Wisconsin and Cornell (better view in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Global homophily study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Grid search space on small-scale datasets</figDesc><table><row><cell>Notation</cell><cell>Range</cell></row><row><cell>lr</cell><cell>{0.01, 0.005}</cell></row><row><cell>dropout</cell><cell>[0, 0.9]</cell></row><row><cell>early stopping</cell><cell>{40, 200, 300}</cell></row><row><cell>weight decay</cell><cell>{1e?5, 5e?5, 1e?4}</cell></row><row><cell>?</cell><cell>[0, 1]</cell></row><row><cell>? 1</cell><cell>{0, 1, 10}</cell></row><row><cell>? 2</cell><cell>{0.1, 1, 10, 10 2 , 10 3 }</cell></row><row><cell>?</cell><cell>[0, 0.9]</cell></row><row><cell>norm layers</cell><cell>{1, 2, 3}</cell></row><row><cell>max hop count K</cell><cell>[1, 6]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Grid search space on large-scale datasets</figDesc><table><row><cell>Notation</cell><cell>Range</cell></row><row><cell>lr</cell><cell>{0.01, 0.005, 0.001}</cell></row><row><cell>dropout</cell><cell>[0, 0.9]</cell></row><row><cell>weight decay</cell><cell>{0, 1e?3, 1e?2, 1e?1}</cell></row><row><cell>?</cell><cell>{0.1, 0.5, 0.9}</cell></row><row><cell>? 1</cell><cell>{0, 0.1, 1}</cell></row><row><cell>? 2</cell><cell>{0.1, 1}</cell></row><row><cell>?</cell><cell>{0.1, 0.5, 0.9}</cell></row><row><cell>norm layers</cell><cell>{1, 2, 3}</cell></row><row><cell>max hop count K</cell><cell>{1, 2, 3}</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since Z (l) * is the optimal solution of Equation 5, we have ?J</p><p>We take the derivative and get</p><p>Based on Lemma D.1, we first prove Lemma 4.2:</p><p>Proof. From Equation 12, we get</p><p>Since</p><p>and</p><p>Submission and Formatting Instructions for ICML 2022</p><p>We further have</p><p>We next prove Lemma 4.3:</p><p>Proof. From Equation 12, we get</p><p>That implies</p><p>Since Z (l) * is the optimal solution to Equation 5, we have</p><p>Hence,</p><p>Equation <ref type="formula">19</ref> can be further simplified as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beyond lowfrequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Homophily and missing links in citation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ciotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bonaventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panzarasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12840</idno>
		<title level="m">Graph neural networks with adaptive frequency response filter</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Political homophily and collaboration in regional planning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lubell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="598" to="610" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cast: A correlation-based adaptive spectral clustering algorithm on multi-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="439" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="338" to="348" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Non-local graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is heterophily a real nightmare for graph neural networks to do node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inverting modified matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Memorandum Rept. 42, Statistical Research Group</title>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Princeton Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geom-Gcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropedge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reinforcement learning enhanced explainer for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Breaking the limit of graph neural networks by improving the assortativity of graphs with local mixing patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1541" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pgm-Explainer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05788</idno>
		<title level="m">Probabilistic graphical model explanations for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06064</idno>
		<title level="m">Graph neural networks inspired by classical iterative algorithms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pairnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11468</idno>
		<title level="m">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
