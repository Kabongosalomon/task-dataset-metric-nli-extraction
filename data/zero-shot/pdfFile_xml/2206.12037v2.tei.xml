<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>?albertgu@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering, University at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering, University at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering, University at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks <ref type="bibr" target="#b6">[7]</ref>. It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties <ref type="bibr" target="#b5">[6]</ref>: as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time. These properties give S4 remarkable computational efficiency and performance, especially when modeling continuous signal data and long sequences.</p><p>Despite its potential, several aspects of the S4 model remain poorly understood. Most notably, Gu et al. <ref type="bibr" target="#b6">[7]</ref> claim that the long range effects of S4 arise from instantiating it with a particular matrix they call the HiPPO matrix. However, this matrix was actually derived in prior work for a particular time-varying system <ref type="bibr" target="#b4">[5]</ref>, and the use of this matrix in a time-invariant SSM did not have a mathematical interpretation. Consequently, the mechanism by which S4 truly models long-range dependencies is actually not known. Beyond this initialization, several other aspects of parameterizing and training S4 remain poorly understood. For example, S4 involves an important timescale parameter ?, and suggests a method for parameterizing and initializing this parameter, but does not discuss its meaning or provide a justification. <ref type="bibr">Figure 1:</ref> In this work, we focus on a more intuitive interpretation of state space models as a convolutional model where the convolution kernel is a linear combination of basis functions. We introduce a generalized framework that allows deriving state spaces x = Ax + Bu that produce particular basis functions, leading to several generalizations and new methods. (Left: LegS) We prove that the particular A matrix chosen in S4 produces Legendre polynomials under an exponential re-scaling, resulting in smooth basis functions with a closed form formula. This results in a simple mathematical interpretation of the method as orthogonalizing against an exponentially-decaying measure, granting the system better ability to model long-range dependencies. (Middle, Right: FouT) We derive a new SSM that produces approximations to the truncated Fourier basis, perhaps the most intuitive and ubiquitous set of basis functions. This method generalizes sliding Fourier Transforms and local convolutions (i.e. CNNs), and can also encode spike functions to solve classic memorization tasks.</p><p>This work aims to provide a comprehensive theoretical exposition of several aspects of S4. The major contribution of this work is a cleaner, more intuitive, and much more general formulation of the HiPPO framework. This result directly generalizes all previous known results in this line of work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. As immediate consequences of this framework:</p><p>? We prove a theoretical interpretation of S4's state matrix A, explaining S4's ability to capture long-range dependencies via decomposing the input with respect to an infinitely long, exponentially-decaying measure ( <ref type="figure" target="#fig_13">Fig. 1 (Left)</ref>).</p><p>? We derive new HiPPO matrices and corresponding S4 variants that generalize other nice basis functions. For example, our new method S4-FouT produces truncated Fourier basis functions. This method thus automatically captures sliding Fourier transforms (e.g. the STFT and spectrograms) which are ubiquitous as a hand-crafted signal processing tool, and can also represent any local convolution, thus generalizing conventional CNNs ( <ref type="figure" target="#fig_13">Fig. 1 (Middle)</ref>).</p><p>? We provide an intuitive explanation of the timescale ?, which has a precise interpretation as controlling the length of dependencies that the model captures. Our framework makes it transparent how to initialize ? for a given task, as well as how to initialize the other parameters (in particular, the last SSM parameter C) to make a deep SSM variance-preserving and stable.</p><p>Empirically, we validate our theory on synthetic function reconstruction and memorization tasks, showing that empirical performance of state space models in several settings is predicted by the theory. For example, our new S4-FouT method, which can provably encode a spike function as its convolution kernel, performs best on a continuous memorization task compared to other SSMs and other models, when ? is initialized correctly. Finally, we show that the original S4 method is still best on very long range dependencies, achieving a new state of the art of 86% average on Long Range Arena, with 96% on the most difficult Path-X task that even the other S4 variants struggle with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">State Space Models: A Continuous-time Latent State Model</head><p>The state space model (SSM) is defined by the simple differential equation <ref type="formula">(1)</ref> and <ref type="bibr">(2)</ref>. It maps a 1-D input signal u(t) to an N -D latent state x(t) before projecting to a 1-D output signal y(t).</p><p>x (t) = A(t)x(t) + B(t)u(t)</p><p>(1) y(t) = C(t)x(t) + D(t)u(t) (2)</p><formula xml:id="formula_0">K(t) = Ce tA B y(t) = (K * u)(t)<label>(3)</label></formula><p>For the remainder of this paper, we will assume D = 0 and omit it for simplicity, unless explicitly mentioned.</p><p>SSMs can in general have dynamics that change over time, i.e. the matrices A, B, C, D are a function of t in <ref type="bibr">(1)</ref> and <ref type="bibr">(2)</ref>. However, when they are constant the system is linear time invariant (LTI), and is equivalent to a convolutional system <ref type="bibr">(3)</ref>. The function K(t) is called the impulse response which can also be defined as the output of the system when the input u(t) = ?(t) is the impulse or Dirac delta function. We will call these time-invariant state space models (TSSM). These are particularly important because the equivalence to a convolution makes TSSMs parallelizable and very fast to compute, which is critical for S4's efficiency.</p><p>Our treatment of SSMs will consider the (A, B) parameters separately from C. We will refer to an SSM as either the tuple (A, B, C) (referring to (3)) or (A, B) (referring to Definition 1) when the context is unambiguous. We also drop the T in TSSM when the context is clearly time-invariant.</p><p>Definition 1. Given a TSSM (A, B), e tA B is a vector of N functions which we call the SSM basis. The individual basis functions are denoted K n (t) = e n e tA B, which satisfy x n (t) = (u * K n )(t) = t ?? K n (t ? s)u(s) ds. Here e n is the one-hot basis vector.</p><p>This definition is motivated by noting that the SSM convolutional kernel is a linear combination of the SSM basis controlled by the vector of coefficients C, K(t) = N ?1 n=0 C n K n (t).</p><p>Discrete SSM with Timescales. To be applied on a discrete input sequence (u 0 , u 1 , . . . ) instead of continuous function u(t), (1) must be discretized by a step size ? that represents the resolution of the input. Conceptually, the inputs u k can be viewed as sampling an implicit underlying continuous signal u(t), where u k = u(k?). Analogous to the fact that the SSM has equivalent forms either as an dynamical system (1) or a continuous convolution (3), the discrete-time SSM can be computed either as a recurrence or a discrete convolution. The mechanics to compute the discrete-time SSM has been discussed in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. For our purposes, we only require the following fact: for standard discretization methods used in prior work, discretizing the state space (A, B) at a step size ? is exactly equivalent to discretizing the state space (?A, ?B) at a step size 1. This allows thinking of ? simply as modulating the SSM parameters (A, B) instead of representing a step size.</p><p>A poorly understood question from prior work is how to interpret and choose this ? parameter, especially when the input u k does not actually arise from uniformly sampling an underlying continuous signal. S4 specifies to log-uniformly initialize ? in the range (? min , ? max ) = (0.001, 0.1), but do not provide a concrete justification. In Section 3.3 we show a simpler interpretation of ? directly in terms of the length of dependencies in a discrete input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">HiPPO: High-order Polynomial Projection Operators</head><p>S4 is defined as a TSSM where (A, B) is initialized with a particular formula (4). This was called the HiPPO matrix in <ref type="bibr" target="#b6">[7]</ref>, but is actually just one of several such special matrices derived in <ref type="bibr" target="#b4">[5]</ref>. To disambiguate other variants of S4, we refer to the full S4 method using this HiPPO SSM as S4-LegS. Other cases considered in this work include LegT from prior work <ref type="bibr" target="#b4">(5)</ref> and FouT that we introduce <ref type="bibr" target="#b5">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(HiPPO-LegS)</head><p>A nk = ?(2n + 1)</p><formula xml:id="formula_1">1 2 (2k + 1) 1 2 ? ? ? ? ? ? 1 n &gt; k n+1 2n+1 n = k 0 n &lt; k Bn = (2n + 1) 1 2 (4) (HiPPO-LegT) A nk = ?(2n + 1) 1 2 (2k + 1) 1 2 ? 1 k ? n (?1) n?k k ? n Bn = (2n + 1) 1 2 (5) (HiPPO-FouT) A nk = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 n = k = 0 ?2 ? 2 n = 0, k odd ?2 ? 2 k = 0, n odd ?4 n, k odd 2?k n ? k = 1, k odd ?2?n k ? n = 1, n odd 0 otherwise Bn = ? ? ? ? ? 2 n = 0 2 ? 2 n odd 0 otherwise (6)</formula><p>These matrices were originally motivated by the question of 'online memorization' of an input signal. The key idea is that for a suitably chosen SSM basis A, B, then at any time t, the current state x(t) can be used to approximately reconstruct the entire input u up to time t <ref type="figure" target="#fig_18">(Fig. 2</ref>).</p><p>The main theoretical idea is as follows. Suppose that the basis functions satisfy Definition 2.</p><p>Definition 2. We call an SSM (A(t), B(t)) an orthogonal SSM (OSSM) for the basis p n (t, s) and measure ?(t, s) ? 0 if the functions K n (t, s) = p n (t, s)?(t, s) satisfy, at all times t,</p><formula xml:id="formula_2">x n (t) = t ?? K n (t, s)u(s) ds t ?? p n (t, s)p m (t, s)?(t, s) ds = ? n,m .<label>(7)</label></formula><p>In the case of a time-invariant OSSM (TOSSM), K n (t, s) =: K n (t ? s) (depends only on t ? s) giving us Definition 1 with measure ?(t ? s) := ?(t, s) and basis p n (t ? s) := p n (t, s).</p><p>To be more specific about terminology, p n (t) and ? n (t) are called the basis and measure for orthogonal SSMs (Definition 2), while K n (t) are called the SSM basis kernels which applies more generally to all SSMs (Definition 1). The distinction will be made clear from context, notation, and the word "kernel" referring to K n (t).</p><p>For OSSMs, (p, ?) and K are uniquely determined by each other, so we can refer to an OSSM by either. One direction is obvious: (p, ?) determine K via K n (t, s) = p n (t, s)?(t, s). Proposition 1. If a set of kernel functions satisfies K n (t, s) = p n (t, s)?(t, s) where the functions p n are complete and orthogonal w.r.t. ? (equation <ref type="formula" target="#formula_2">(7)</ref> right), p and ? are unique.</p><p>Equation <ref type="formula" target="#formula_2">(7)</ref> is equivalent to saying that for every fixed t, p n , p m ? = ? n,m , or that p n are an orthonormal basis with respect to measure ?. More formally, defining p (t) n (s) = p n (t, s) and ? (t) similarly, then p (t) n are orthonormal in the Hilbert space with inner product p, q = p(s)q(s)? (t) (s) ds). By equation <ref type="formula" target="#formula_2">(7)</ref>,</p><formula xml:id="formula_3">x n (t) = t ?? u(s)K n (t, s) ds = u, p (t) n ? (t) where p (t)</formula><p>n (s) = p n (t, s). Thus at all times t, the state vector x(t) is simply the projections of u | ?t onto a orthonormal basis, so that the history of u can be reconstructed from x(t). HiPPO called this the online function approximation problem <ref type="bibr" target="#b4">[5]</ref>. Proposition 2. Consider an OSSM that satisfies <ref type="bibr" target="#b6">(7)</ref> and fix a time t. Furthermore suppose that in the limit N ? ?, the p  The main barrier to using Proposition 2 for function reconstruction is that SSMs are in general not OSSMs. For example, even though we will show that (4) is an TOSSM, its diagonalization is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3.</head><p>There is no TOSSM with the diagonal state matrix A = diag{?1, ?2, . . . }.</p><p>HiPPO can be viewed as a framework for deriving specific SSMs that do satisfy <ref type="bibr" target="#b6">(7)</ref>. The original HiPPO methods and its generalizations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> primarily focused on the case when the p n are orthogonal polynomials, <ref type="figure" target="#fig_18">Figure 2</ref>: Given an input function u(t) (black), HiPPO compresses it online into a state vector x(t) ? R N via equation <ref type="bibr">(1)</ref>. Specific cases of HiPPO matrices A, B are derived so that at every time t, the history of u up to time t can be reconstructed linearly from x(t) (red), according to a measure (green). (Left) The HiPPO-LegT method orthogonalizes onto the Legendre polynomials against a time-invariant uniform measure, i.e. sliding windows. (Right) The original HiPPO-LegS method is not time-invariant system. When used as a time-varying ODE x = 1 t Ax + 1 t Bu, x(t) represents the projection of the entire history of u onto the Legendre polynomials. It was previously unknown how to interpret the time-invariant version of this ODE using the same (A, B) matrices. and specifically looked for solutions to <ref type="bibr" target="#b6">(7)</ref>, which turn out to be SSMs. We have rephrased the HiPPO definition in Definition 2 to start directly from SSMs.</p><p>We discuss the two most important cases previously introduced.</p><p>HiPPO-LegT. (5) is a TOSSM that approximates the truncated Legendre polynomials ( <ref type="figure" target="#fig_2">Fig. 3</ref>). Definition 3. Let I(t) be the indicator function for the unit interval [0, 1]. We denote the Legendre polynomials rescaled to be orthonormal on [0, 1] as L n (t), satisfying L n (t)L m (t)I(t) dt = ? n,m . Proposition 4. As N ? ?, the SSM with (A, B) in (5) is a TOSSM with ?(t) = I(t) K n (t) = L n (t)I(t).</p><p>This particular system was the precursor to HiPPO and has also been variously called the Legendre Delay Network (LDN) or Legendre Memory Unit (LMU) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The original motivation of this system was not through the online function approximation formulation of HiPPO, but through finding an optimal SSM approximation to the delay network that has impulse response K(t) = ?(t ? 1) representing a time-lagged output by 1 time unit ( <ref type="figure" target="#fig_2">Fig. 3</ref>). We state and provide an alternate proof of this result in Section 3.2, Theorem 9.</p><p>HiPPO-LegS. Unlike the HiPPO-LegT case which is an LTI system (1) (i.e. TOSSM), the HiPPO-LegS matrix (4) was meant to be used in a time-varying system x (t) = 1 t Ax(t) + 1 t Bu(t) <ref type="bibr" target="#b4">[5]</ref>. In contrast to HiPPO-LegT, which reconstructs onto the truncated Legendre polynomials in sliding windows [t ? 1, t], HiPPO-LegS reconstructs onto Legendre polynomials on "scaled" windows [0, t]; since the window changes across time, the system is not time-invariant ( <ref type="figure" target="#fig_18">Fig. 2)</ref>.</p><formula xml:id="formula_4">Theorem 5. The SSM ( 1 t A, 1 t B) for (A, B) in (4) is an OSSM with ?(t, s) = 1 t ? I(s/t) p n (t, s) = L n (s/t).</formula><p>However, the S4 model applies the exact same formula (4) inside the time-invariant SSM (1), i.e. dropped the 1 t term, which had no mathematical interpretation. In other words, while</p><formula xml:id="formula_5">( 1 t A, 1 t B)</formula><p>is an OSSM, it was not known whether the TSSM (A, B) is a TOSSM. Given that the performance of SSM models is very sensitive to these matries A <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, it remained a mystery why this works. In Section 3 we will prove that (4) actually does correspond to a TOSSM.</p><p>LSSL. While HiPPO originally showed just the above two cases involving Legendre polynomials (and another case called LagT for Laguerre polynomials, which will not be a focus of this work), follow-up work showed that there exist OSSMs corresponding to all families of orthogonal polynomials {p n (t)}. Our more general framework will also subsume these results.</p><p>Naming convention. We use HiPPO-[SSM] to refer to a fixed OSSM (A, B) suitable for online function approximation, where [SSM] is a suffix (e.g. LegS, LegT) that abbreviates the corresponding basis functions (e.g. scaled Legendre, truncated Legendre). S4-[SSM] refers to the corresponding trainable layer (A, B, C) with randomly initialized C, trained with S4's representation and computational algorithm <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalized HiPPO: General Orthogonal Basis Projections</head><p>In Section 3.1, we prove that the LTI HiPPO-LegS is actually a TOSSM and show closed formulas for its basis functions. In Section 3.2, we include more specific results on finite-window SSMs, including introducing a new method HiPPO-FouT based on truncated Fourier functions, and proving previously established conjectures. Section 3.3 shows more general properties of TOSSMs, which establish guidelines for interpreting and initializing SSM parameters such as the timescale ?.</p><p>Our main, fully general, result is Theorem 12 in Appendix C.2, which describes a very general way to derive OSSMs for various SSM basis functions K n (t, s). This result can be instantiated in many ways to generalize all previous results in this line of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Explanation of S4-LegS</head><p>We show the matrices (A, B) in (4) are deeply related to the Legendre polynomials L n defined in Theorem 5. As more specific corollaries of Corollary 3.1, we recover both the original time-varying interpretation of the matrix in (4), as well as the instantiation of LegS as a time-invariant system. If we set a (t) = 1 t , then we recover the scale-invariant HiPPO-LegS OSSM in Theorem 5, And if a (t) = 1, this shows a new result for the time-invariant HiPPO-LegS TOSSM:</p><formula xml:id="formula_6">Corollary 3.3 (Time-Invariant HiPPO-LegS). The SSM (A, B) is a TOSSM with ?(t) = e ?t p n (t) = L n (e ?t ).</formula><p>This explains why removing the 1 t factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential "warping" or change of basis on the time axis ( <ref type="figure" target="#fig_13">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finite Window Time-Invariant Orthogonal SSMs</head><p>For the remainder of this section, we restrict to the time-invariant SSM setting <ref type="bibr">(3)</ref>. A second important instantiation of Theorem 12 covers cases with a discontinuity in the SSM basis functions K n (t), which requires infinite-dimensional SSMs to represent. The most important type of discontinuity occurs when K n (t) is supported on a finite window, so that these TSSMs represent sliding window transforms.</p><p>We first derive a new sliding window transform based on the widely used Fourier basis (Section 3.2.1). We also prove results relating finite window methods to delay networks (Section 3.2.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">S4-FouT</head><p>Using the more general framework (Theorem 12) that does not necessarily require polynomials as basis functions, we derive a TOSSM that projects onto truncated Fourier functions. This SSM corresponds to Fourier series decompositions, a ubiquitous tool in signal processing, but represented as a state space model. The basis is visualized in <ref type="figure" target="#fig_13">Fig. 1</ref> (middle) for state size N = 1024.</p><p>A benefit of using these well-behaved basis functions is that we can leverage classic results from Fourier analysis. For example, it is clear that taking linear combinations of the truncated Fourier basis can represent any function on [0, 1], and thus S4-FouT can represent any local convolution (i.e. the layers of modern convolutional neural networks).</p><p>Theorem 7. Let K(t) be a differentiable kernel on [0, 1], and letK(t) be its representation by the FouT system (Theorem 6) with state size N. If K is L?Lipschitz, then for &gt; 0, N ? L ? 2 + 2, we have K(t) ?K(t) ? . If K has k?derivatives bounded by L, then we can take N ? L ? k 2 2k?1 + 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Approximating Delay Networks</head><p>An interesting property of these finite window TSSMs is that they can approximate delay functions. This is defined as a system with impulse response K(t) = ?(t ? 1): then y(t) = (K * u)(t) = u(t ? 1), which means the SSM outputs a time-lagged version of the input. This capability is intuitively linked to HiPPO, since in order to do this, the system must be remembering the entire window u([t ? 1, t]) at all times t, in other words perform an approximate function reconstruction. Any HiPPO method involving finite windows should have this capability, in particular, the finite window methods LegT and FouT.</p><p>Theorem 8. For the FouT system A and B, let C be (twice) the vector of evaluations of the basis functions C n = 2 ? p n (1) and let D = 1. For the LegT system A and B, let C be the vector of evaluations of the basis functions C n = p n (1) = (1 + 2n) 1 2 (?1) n and let D = 0. Then the SSM kernel</p><formula xml:id="formula_7">K(t) = Ce tA B + D?(t) limits to K(t) ? ?(t ? 1) as N ? ?.</formula><p>Theorem 8 is visualized in Figs. 1 and 3 (right). Further, the result for LegT can be characterized even more tightly for finite N . In fact, this was the original motivation for the LDN/LMU <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which worked backward from the transfer function of the desired delay function impulse response K(t) = ?(t ? 1), and noticed that the SSM for Pad? approximations to this were linked to Legendre polynomials. This was not fully proven, and we state it here and provide a full proof in Appendix C.4.</p><p>Theorem 9. For A, B, C, D in the LegT system described in Theorem 8, the transfer function</p><formula xml:id="formula_8">L{K(t)}(s) is the [N ? 1/N ] Pad? approximant to e ?s = L{?(t ? 1)}(s).</formula><p>We remark that although LegT (LMU) is designed to be an "optimal" approximation to the delay function via Pad? approximants, it actually produces a weaker spike function than FouT ( <ref type="figure" target="#fig_2">Fig. 3</ref> vs. <ref type="figure" target="#fig_13">Fig. 1</ref>) and empirically performs slightly worse on synthetic tasks testing this ability (Section 4.3). This may be because Pad? approximation in the Laplace domain does not necessarily translate to localization in the time domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Properties of Time-invariant Orthogonal SSMs: Timescales and Normalization</head><p>We describe several general properties of TOSSMs, which let us answer the following questions:</p><p>? How should all parameters (A, B, C) be initialized for an SSM layer to be properly normalized?</p><p>? What does ? intuitively represent, and how should it be set in an SSM model?</p><p>It turns out that for TOSSMs, these two questions are closely related and have intuitive interpretations.</p><p>Closure properties. First, several basic transformations preserve the structure of TOSSMs. Consider a TOSSM (A, B) with basis functions p n (t) and measure ?(t). Then, for any scalar c and unitary matrix V , the following are also TOSSMs with the corresponding basis functions and measure (Appendix C.5, Proposition 13):</p><formula xml:id="formula_9">Transformation Matrices Interpretation Basis Measure Scalar Scaling (cA, cB) Timescale change p(ct) ?(ct)c Identity Shift (A + cI, B) Exponential tilting p(t)e ?ct ?(t)e 2ct Unitary Transformation (V AV * , V B) Identity V p(t) ?(t)</formula><p>Normalization. A standard aspect of training deep learning models, in general, concerns the scale or variance of activations. This has been the subject of much research on training deep learning models, touching on deep learning theory for the dynamics of training such as the exploding/vanishing gradient problem <ref type="bibr" target="#b10">[11]</ref>, and a large number of normalization methods to ensure properly normalized methods, from the simple Xavier/He initializations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> to BatchNorm and LayerNorm <ref type="bibr">[1,</ref><ref type="bibr" target="#b11">12]</ref> to many modern variants and analyses of these <ref type="bibr">[3]</ref>.</p><p>The following proposition follows because for a TOSSM, x(t) can be interpreted as projecting onto orthonormal functions in a Hilbert space (Proposition 2).</p><p>Proposition 10 (Normalization of TOSSM). Consider an (infinite-dimensional) TOSSM. For any input</p><formula xml:id="formula_10">u(t), x(t) 2 2 = u 2 ? = t ?? u(s) 2 ?(t ? s) dt. Corollary 3.4.</formula><p>For a TOSSM with a probability measure (i.e. ?(t) = 1) and any constant input u(t) = c, the state has norm x(t) 2 = c 2 and the output y(t) has mean 0, variance c 2 if the entries of C are mean 0 and variance 1.</p><p>Note that the probability measure requirement can be satisfied by simply rescaling B. Corollary 3.4 says the TOSSM preserves the variance of inputs, the critical condition for a properly normalized deep learning layer. Note that the initialization of C is different than a standard Linear layer in deep neural networks, which usually rescale by factor depending on its dimensionality such as N ? 1 2 <ref type="bibr" target="#b3">[4]</ref>.</p><p>Timescales. As discussed in Section 2, converting from continuous to discrete time involves a parameter ? that represents the step size of the discretization. This is an unintuitive quantity when working directly with discrete data, especially if it is not sampled from an underlying continuous process.</p><p>We observe the following fact: for all standard discretization methods (e.g. Euler, backward Euler, generalized bilinear transform, zero-order hold <ref type="bibr" target="#b5">[6]</ref>), the discretized system depends on (A, B), and ? only through their products (?A, ?B). This implies that the SSM (A, B) discretized at step size ? is computationally equivalent to the SSM (?A, ?B) discretized at step size 1.</p><p>Therefore, ? can be viewed just as a scalar scaling of the base SSM instead of changing the rate of the input.</p><p>In the context of TOSSMs, this just scales the underlying basis and measure (Scalar Scaling). More broadly, scaling a general SSM simply changes its timescale or rate of evolution.</p><p>Proposition 11. The ODE y = cAy + cBu evolves at a rate c times as fast as the SSM x = Ax + Bu, in the sense that the former maps u(ct) ? x(ct) if the latter maps u(t) ? x(t).</p><p>The most intuitive example of this is for a finite window TOSSM such as LegT or FouT. Discretizing this system with step size ? is equivalent to considering the system (?A, ?B) with step size 1, which produces basis functions supported exactly on [0, 1 ? ]. The interpretation of the timescale ? lends to simple discretetime corollaries of the previous continuous-time results. For example, LegT and FouT represent sliding windows of 1/? elements in discrete time. By this definition, HiPPO-LegS is timescale normalized. This motivates S4's initialization of ? log-uniformly in (0.001, 0.1), covering a geometric range of sensible timescales (expected length 10 to 1000). In Section 4 we show that the timescale can be chosen more precisely when lengths of dependencies are known.</p><p>We finally remark that HiPPO-LegT and -FouT were derived with measures I[0, 1]. However, to properly normalize them by Definition 4, we choose to halve the matrices to make them orthogonal w.r.t. ? = 1</p><formula xml:id="formula_11">2 I[0, 2].</formula><p>The S4-FouT and S4-LegT methods in our experiments use these halved versions. <ref type="table" target="#tab_0">Table 1</ref> summarizes the results for TOSSMs presented in this section, including both original HiPPO methods defined in Gu et al. <ref type="bibr" target="#b4">[5]</ref> as well as our new methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HiPPO-LagT</head><p>We note that the original HiPPO paper also included another method called LagT based on Laguerre polynomials. Because Laguerre polynomials are orthogonal with respect to e ?t , this system was supposed to represent an exponentially decaying measure. However, this method was somewhat anomalous; it generally performed a little worse than the others, and it was also empirically found to need different hyperparameters. For example, Gu et al. <ref type="bibr" target="#b4">[5]</ref> found that on the permuted MNIST dataset, setting ? to around 1 784 for most HiPPO methods was indeed optimal, as the theory predicts. However, HiPPO-LagT performed better when set much higher, up to ? = 1.0. It turns out that this method changes the basis in a way such </p><formula xml:id="formula_12">I[0, 1] 1/2 LegT equation Eq. (5) Ln(t)I[0, 1] Ln(t) I[0, 1] 1/2 LagT see [5] Lag(t)e ?t/2 Lag(t)e ?t/2 I[0, ?) ?</formula><p>that it is not orthogonal with respect to an exponentially decaying measure, but rather the constant measure I[0, ?), and has a timescale of ?; this explains why the hyperparameters for ? need to be much higher.</p><p>In summary, we do not recommend using the original HiPPO-LagT, which despite the original motivation does not represent orthogonalizing against an exponentially decaying measure. Instead, HiPPO-LegS (as a time-invariant SSM) actually represents an exponentially decaying measure.</p><p>Timescales For a timescale normalized orthogonal SSM (i.e. ? 0 ?(t) = 1 and ? 0 t?(t) = 1):</p><p>? 1 ? exactly represents the range of dependencies it captures. For example, S4-FouT can represent any finite convolution kernel of length 2 ? (so the expected length of a random kernel is 1 ? ). ? A random vector C with independent mean 0, variance 1 entries is a variance-preserving SSM, i.e. produces outputs matching the variance of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSSL and General Polynomials</head><p>The Linear State Space Layer <ref type="bibr" target="#b5">[6]</ref> succeeded HiPPO by incorporating it into a full deep SSM model, and also generalized the HiPPO theory to show that all orthogonal polynomials can be defined as the SSM kernels for some (A, B). Our framework is even stronger and immediately produces the main result of LSSL as a corollary (Appendix), and can also work for non-polynomial methods (e.g. FouT).</p><p>These results show that all orthogonal polynomial bases, including truncated and scaled variants, have corresponding OSSMs with polynomial kernels. If we define this special case as polynomial OSSMs (POSSMs), we have therefore deduced that all of the original HiPPOs are POSSMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We study the empirical tradeoffs of our proposed S4 variants. We compare several S4 variants based on the TOSSMs introduced in this work, as well as to simpler diagonal SSMs called S4D that are not orthogonal SSMs <ref type="bibr" target="#b7">[8]</ref>. Corresponding to our main contributions, we hypothesize that</p><p>? S4-LegS excels at sparse memorization tasks because it represents very smooth convolution kernels that memorize the input against an infinitely-long measure (Corollary 3.3, <ref type="figure" target="#fig_13">Fig. 1</ref>). Conversely, it is less appropriate at short-range tasks with dense information because it smooths out the signal.</p><p>? S4-FouT excels at dense memorization tasks because it can represent spike functions that pick out past elements at particular ranges (Section 3.2.2). However, it is less appropriate at very long range tasks because it represents a finite (local) window.</p><p>? ? can be initialized precisely based on known time dependencies in a given task to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Long Range Arena</head><p>The Long Range Arena (LRA) benchmark is a suite of sequence classification tasks designed to stress test sequence models on modeling long sequences. We improve S4's previous state of the art by another 6 points ( <ref type="table" target="#tab_1">Table 2)</ref>. Validating our hypothesis, S4-LegS is extremely strong at the hardest long-range task (Path-X) involving sparse dependencies of length 16384, which FouT cannot solve because it is a finite window method.</p><p>The Path-X task also serves as a validation of the theory of timescales in Section 3.3. To set these results, we lowered the initialization of ? in accordance with known length of dependencies in the task. <ref type="figure" target="#fig_6">Fig. 4</ref> illustrates the importance of setting ? correctly. (Right) A good setting of ?min can consistently solve the task. Note that the timescale of each feature is up to 1 ? min = 10 4 , which is on the order of (but not exceeding) the length of the task L = 16384. Empirically, performance is best when spreading out the range of ? with a larger ?max that covers a wider range of timescales and can potentially learn features at different resolutions, which are combined by a multi-layer deep neural network. We also show a diagonal variant of S4-LegS called S4D-Inv introduced in <ref type="bibr" target="#b7">[8]</ref> which approximates S4-LegS, but is still worse.   <ref type="figure" target="#fig_4">6</ref> shows that S4-LegT and S4-FouT, the methods that theoretically reconstruct against a uniform measure, are far better than other methods. We include the new diagonal variants (S4D) proposed in <ref type="bibr" target="#b7">[8]</ref>, which are simpler SSM methods that generally perform well but do not learn the right function on this task. We also include a method S4-(LegS/FouT) which combines both LegS and FouT measures by simply initializing half of the SSM kernels to each. Despite having fewer S4-FouT kernels, this still performs as well as the pure S4-FouT initialization.    Finally, we validate the theory of normalization in Section 3.3, which predicts that for a properly normalized TOSSM, the projection parameter C should be initialized with unit variance, in contrast to standard initializations for deep neural networks which normalize by a factor related to the size of N (in this case N = 64). <ref type="table" target="#tab_4">Table 3</ref> shows classification results on datasets Sequential CIFAR (sCIFAR) and Speech Commands (SC), using models of size at most 150K parameters. This replicates the setup of the "ablation models" of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Section 5]</ref>. Results show that using standard deviation 1.0 for C is slightly better than alternatives, although the difference is usually minor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theory: Function Reconstruction, Timescales, Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(? min, ? max) (? min, ? max)</head><formula xml:id="formula_13">(1e-3, 2e-3) (2e-3, 2e-3) (1e-3, 1e-1) (2e-3, 1e-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4-LegS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Memorization: the Delay (continuous copying) Task</head><p>Next, we study how the synthetic reconstruction ability transfers to other tasks. The Delay Task requires models to learn a sequence-to-sequence map whose output is the input lagged by a fixed time period <ref type="figure" target="#fig_10">(Fig. 7a</ref>). For recurrent models, this task can be interpreted as requiring models to maintain a memory buffer that continually remembers the latest elements it sees. This capability was the original motivation for the Legendre Memory Unit, the predecessor to HiPPO-LegT, which was explicitly designed to solve this task because it can encode a spike kernel <ref type="figure" target="#fig_2">(Fig. 3)</ref>. In <ref type="figure">Fig. 7b</ref>, we see that our new S4-FouT actually outperforms S4-LegT, which both outperform all other methods when the timescale ? is set correctly. We note that this task with a lag of just 1000 time steps is too hard for baselines such as an LSTM and Transformer, which empirically did not learn better than random guessing (RMSE 0.43).   <ref type="figure" target="#fig_10">FouT (A, B)</ref>, which encode finite window basis functions ( <ref type="figure" target="#fig_13">Fig. 1)</ref>, the model can see a history of length up to 2 ? . For example, setting ? too large means the model cannot see 1000 steps in the past, and performs at chance. Performance is best at the theoretically optimal value of ? = 2 ? 10 ?3 which can encode a spike kernel at distance exactly 1000 steps (Corollary 3.5). (Right) When ? is set optimally, the proposed S4-FouT method is the best SSM as the theory predicts. When ? is not set optimally, other methods perform better, including the simple diagonal methods proposed in <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure">Figure 7</ref>: (Delay Task.) A synthetic memorization task: definition ( <ref type="figure" target="#fig_10">Fig. 7a</ref>) and results <ref type="figure">(Fig. 7b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary: How to Train Your HiPPO</head><p>? SSMs represent convolution kernels that are linear combinations (parameterized by C) of basis functions (parameterized by A and B).</p><p>? HiPPO is a general mathematical framework for producing matrices A and B corresponding to prescribed families of well-behaved basis functions. We derive HiPPO matrices corresponding to exponentiallyscaled Legendre families (LegS) and the truncated Fourier functions (FouT).</p><p>-HiPPO-LegS corresponds to the original S4 method and produces a very smooth, long-range family of kernels ( <ref type="figure" target="#fig_13">Fig. 1</ref>) that is still the best method for long-range dependencies among all S4 variants -HiPPO-FouT is a finite window method that subsumes local convolutions (e.g. generalizing vanilla CNNs, Corollary 3.6) and captures important transforms such as the sliding DFT or STFT</p><p>? Independently of a notion of discretization, the timescale ? has a simple interpretation as controlling the length of dependencies or "width" of the SSM kernels. Most intuitively, for a finite window method such as FouT, the kernels have length exactly 1 ? , and generalize standard local convolutions used in deep learning.</p><p>A companion paper to this work builds on the theory introduced here to define a simpler version of S4 using diagonal state matrices (S4D), which are approximations to the orthogonal SSMs we introduce and can inherit S4's strong modeling abilities <ref type="bibr" target="#b7">[8]</ref>. It also includes experiments on more datasets comparing various state space models, including the S4 variants (S4-LegS and S4-FouT) introduced here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>We discuss in more detail the differences between this work and the previous results in this line of work.</p><p>Legendre Memory Unit (Legendre Delay Network). The HiPPO-LegT matrix <ref type="bibr" target="#b4">(5)</ref> was first introduced as the LMU <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The original motivation was to produce a state space model that approximates the Delay Network, which can be defined as the LTI system that transforms u(t) into u(t ? 1), i.e. lags the input by 1 time unit. This can also be defined as the system with impulse response K(t) = ?(t ? 1), i.e. convolves by the convolutional kernel with a ? spike at time 1.</p><p>The connection between the Delay Network and Legendre polynomials was made in two steps. First, the transfer function of the ideal system is L[?(t ? 1)](s) = e ?s and must be approximated by a proper rational function to be represented as an SSM. Taking Pad? approximants of this function yields "optimal" approximations by rational functions, which can then be distilled into a SSM (A, B, C) whose transfer function C(sI ? A) ?1 B matches it. Second, the SSM basis e tA B for this system can be computed and found to match Legendre polynomials. However, despite making this connection and writing out formulas for this SSM, Voelker <ref type="bibr" target="#b13">[14]</ref> did not provide a complete proof of either of these two connections.</p><p>The preceding two steps that motivated the LDN can be informally written as the chain of transformations (i) transfer function e ?s ? (ii) SSM (A, B, C) ? (iii) Legendre polynomials e tA B. The HiPPO framework in a sense proceeded in the opposite direction. Gu et al. <ref type="bibr" target="#b4">[5]</ref> started by defining the system that convolves with truncated Legendre polynomials, and with a particular differentiation technique showed that it could be written as a particular SSM which they called HiPPO-LegT. This SSM turned out to be the same (up to a minor change in scaling) as the original <ref type="figure" target="#fig_10">(A, B)</ref> defined by the LMU, thus proving the second of the two steps relating this particular SSM to the Legendre polynomials.</p><p>In this work, we show the final piece in this reverse chain of equivalences. In particular, we start from the LegT SSM (A, B, C) and directly prove that its transfer function produces Pad? approximants of the exponential. Our proof introduces new techniques in an inductive argument that can be applied to HiPPO SSMs beyond the LegT case, and relates them to continued fraction expansions of the exponential.</p><p>We comment on a minor difference between the parameterization of HiPPO-LegT and the LMU. The LMU is originally defined as</p><formula xml:id="formula_14">x (t) = 1 ? Ax(t) + 1 ? Bu(t)</formula><p>where ? is a hyperparameter that controls the length of the window. However, we point out that such constant scaling of the SSM is also controlled by the step size ? as discussed in Section 3.3. Therefore ? is redundant with ?, so the LegT matrices defined in <ref type="bibr" target="#b4">[5]</ref> and in this work do not have a concept of ?. Additionally, in this work we redefine the LegT matrices (A, B) to be scaled by a factor of 2 to make them properly timescale normalized, using the theory developed in Section 3.3.</p><p>HiPPO and LSSL. As discussed in Section 2.2, HiPPO can be thought of as a framework for deriving state space models corresponding to specific polynomial bases. The original paper did not explicitly draw the connection to state space models, and also developed systems only for a few particular cases which were called LegS (a time-varying system involving Legendre polynomials), LegT (a time-invariant system with the truncated Legendre polynomials), and LagT (involving Laguerre polynomials).</p><p>A follow-up paper on Linear State Space Layers (LSSL) generalized these results to all orthogonal polynomial families, and also generalized the flexibility of the time-varying component. They produced SSMs x (t) = A(t)x(t) + B(t)u(t) where at all times t, x(t) can be viewed as the projection of the history of u(s) | s?t onto orthogonal polynomials p n rescaled onto the interval [t ? ?(t), t], where ?(t) is an arbitrary factor. This generalized all 3 cases of the original HiPPO paper.</p><p>Compared to these works, our framework (Definition 2) simplifies and generalizes the concepts directly in terms of (time-varying) state space models. We define a more natural concept of orthogonal SSM, derive ? It allows more flexible transformations of polynomial bases, such as including a change-of-basis inside the polynomials. The previously expained case of LegS is an instance of this, which has basis functions L(e ?t ) with an exponential change of basis, instead of vanilla polynomials.</p><p>? It can be applied to non-polynomial bases, such as the truncated Fourier basis FouT.</p><p>? It does not require considering multiple cases depending on where the basis functions are supported. Instead, we handle this by considering discontinuities in the basis functions.</p><p>S4. While the preceding discussion covers theoretical interpretations of SSMs, S4 (and its predecessor LSSL) are the application of these SSMs to deep learning. In comparison to prior works such as the LMU and HiPPO which require a pre-determined system (A, B) and incorporate them naively into an RNN, LSSL and S4 use a full state space model (A, B, C) as a completely trainable deep learning layer. Doing this required resolving computational problems with the SSM, which was the main focus of S4. In this work, we make a distinction between HiPPO, which is the theoretical derivation and interpretation of particular SSMs <ref type="figure" target="#fig_10">(A, B)</ref>, and S4, which is the incorporation of those SSMs as a trainable deep learning layer with a particular algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details and Additional Experiments B.1 Delay (Continuous Copying) Task</head><p>The Delay Task consists of input-output pairs where the input is a white noise signal of length 4000 bandlimited to 1000 Hz. The output is the same signal shifted by 1000 steps <ref type="figure" target="#fig_10">(Fig. 7a</ref>). We use single layer linear SSMs with H = 4 hidden units and state size N = 1024. Models are trained with the Adam optimizer with learning rate 0.001 for 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Long Range Arena</head><p>The settings for LRA use the same hyperparameters in <ref type="bibr" target="#b7">[8]</ref>. A more detailed protocol can be found in <ref type="bibr" target="#b7">[8]</ref>. To be self-contained, we recreate the same table of parameters in <ref type="table" target="#tab_6">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof Details</head><p>We furnish the missing proofs from Section 2 in Appendix C.1. We will describe our general framework and results in Appendix C.2, and prove the results in Sections 3.1 to 3.3 in Appendices C.3 to C.5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proofs from Background</head><p>This corresponds to results from Section 2.</p><p>Proof of Proposition 1. Suppose for the sake of contradiction that there is a second basis and measure q n , ? such that q n is complete and orthogonal w.r.t. ?, and K n = q n ?. By completeness, there are coefficients c ,k such that</p><formula xml:id="formula_15">p = k c ,k q k .</formula><p>Then</p><formula xml:id="formula_16">p q j ? = k c ,k q k q j ? = k c ,k ? kj = c ,j .</formula><p>But q j ? = K j = p j ?, so</p><formula xml:id="formula_17">p q j ? = p p j ? = ? j .</formula><p>So c ,j = ? ,j which implies that p = q for all , as desired.</p><p>Proof of Proposition 3. The SSM kernels are K n (t) = e ?t(n+1) B n . Assume B n = 0 so that the kernels are not degenerate.</p><p>Suppose for the sake of contradiction that this was a TOSSM with measure ?(t). Then we must have</p><formula xml:id="formula_18">K n (s)K m (s)?(t) ?1 ds = ? n,m</formula><p>Plugging in n = 1, m = 1 and n = 0, m = 2 gives</p><formula xml:id="formula_19">1 = e ?2t B 1 e ?2t B 1 ?(t) ?1 ds = B 1 B 1 e ?4t ?(t) ?1 ds 0 = e ?1t B 0 e ?3t B 2 ?(t) ?1 ds = B 0 B 2 e ?4t ?(t) ?1 ds</formula><p>This is clearly a contradiction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 General theory</head><p>where the integrals in this paper are over the range [??, ?], unless stated otherwise. This is sufficient to derive an OSSM based on the HiPPO technique. The generalized HiPPO framework demonstrates how to build (T)OSSMs utilizing time warping to shape the time interval and tilting to construct new sets of orthogonal basis functions.</p><p>Given an general interval [ , r], we will use the notation I[ , r] to denote the indicator function for the interval [ , r]-we will drop the interval if = 0, r = 1.</p><p>We will need the notion of a "time warping" function ? as follows:</p><p>Definition 5. A time warping function is defined as</p><formula xml:id="formula_21">?(t, s) : (??, t] ? [0, 1]</formula><p>such that ?(t, t) = 1.</p><p>We will be using a special case of time-warping function, which we say has a discontinuity at t 0 for some t 0 ? (??, t]:</p><formula xml:id="formula_22">?(t, s) = I[t 0 , t]?(t, s),<label>(9)</label></formula><p>such that</p><formula xml:id="formula_23">? ?t ? ?s ? s (t, s) = c(t) ? ?s ?(t, s).<label>(10)</label></formula><p>We allow for t 0 = ??, in which case we think of the interval [t 0 , t] as (??, t].</p><p>Before proceeding, let us clarify our notation. We will use ? t and ? s to denote the partial derivatives ? ?t ?(t, s) and ? ?s ?(t, s) respectively. We will drop the parameters (t, s) and use f instead of f (t, s) when it is clear from context to reduce notational clutter. Further, we will extend this notation to function composition, i.e. write g ? f (t, s)) as g(f ) and function product, i.e. use f gh instead of f (t, s)g(t, s)g(t, s). Finally, we'll shorten f gh ? ?(t, s) as f gh(?).</p><p>We also define the tilting ? and show that regardless of warping, we can construct a new orthogonal basis (note that the result holds for warping functions as in <ref type="bibr" target="#b8">(9)</ref> and not just those as in <ref type="formula" target="#formula_23">(10)</ref>). for time-warping function ? satisfying (9) and any ?(t, s) that is non-zero in its support.</p><p>Proof. Consider the following sequence of equalities: In the above, the second equality follows from the substitution y ? ?(t, s) and hence dy = ? s ds and the final equality follows from <ref type="bibr" target="#b7">(8)</ref>. Then since ?(t, s) is always non-zero, we have (?p j (?))(?p k (?))?(?)I[t 0 , t]? s ? ?2 ds = ? jk , as desired.</p><formula xml:id="formula_24">p j (?)p k (?)?(?)I[t 0 , t]? s ds = t t0 p j (?)p k (?)?(?)? s ds = ?(t,t) ?(t,</formula><p>Without loss of generality, we can split ? into a product ?(t, s) = 1 ?(?(t, s))?(t, s) <ref type="bibr" target="#b10">(11)</ref> of one part that depends on ? and another arbitrary component.</p><p>Time Warped HiPPO. Since we have an orthonormal basis and measure, we can try to derive the (T)OSSM. For a given input signal u(t), the HiPPO coefficients are defined as the projections.</p><p>x n (t) = u, ?p n ? = u(s) ? ? ? (p n ?)(?)I[t 0 , t]? s ? ?2 ds defined as inner product of u(t) with the tilted basis functions ?p n with respect to the measure ? as defined in Lemma C.1. For additional convenience, we use the decomposition ? = ? ?1 ? ?1 from (11) to get:</p><formula xml:id="formula_25">x n (t) = u(s) ? (p n ??)(?)I[t 0 , t]? s ? ds.<label>(12)</label></formula><p>The HiPPO technique is to differentiate through this integral in a way such that it can be related back to x n (t) and other x k (t). We require for every n, we require that there are a set of coefficients {? nk } N ?1 k=0 such that</p><formula xml:id="formula_26">? t (p n ??) (?) = N ?1 k=0 ? nk (p n ??)(?)<label>(13)</label></formula><p>and for tilting component ?</p><formula xml:id="formula_27">d dt ?(t, s) = d(t)?(t, s).<label>(14)</label></formula><p>Theorem 12. Consider a set of basis functions p n orthogonal over ?, time warping ?(t, s) as in <ref type="formula" target="#formula_22">(9)</ref>, <ref type="bibr" target="#b9">(10)</ref>, and tilting ? as in <ref type="formula">(11)</ref> and <ref type="formula" target="#formula_27">(14)</ref> with the functions ?, p n , ?, ? obeying <ref type="bibr" target="#b12">(13)</ref>. If dt0 dt = 0, further assume that for some vector A , we have as N ? ?,</p><formula xml:id="formula_28">u(t 0 ) = c N ?1 k=0 A k ? x k (t) + du(t).<label>(15)</label></formula><p>Then</p><formula xml:id="formula_29">(A 0 + (c(t) + d(t))I ? cD (A ) , B ? dD)</formula><p>is an OSSM for basis functions ?p n (?) with measure ?I[t 0 , t]? s ? ?2 where A 0 nk = ? nk with ? nk as in (13), D n = (p n ??)(?(t, t 0 ))(? s ?)(t, t 0 ) ? dt 0 dt , and B n = (p n ??)(1)(? s ?)(t, t).</p><p>Proof. Applying the Leibniz rule to <ref type="bibr" target="#b11">(12)</ref>, we get</p><formula xml:id="formula_30">x n (t) = x (0) n (t) + x (1) n (t) + x (2) n (t) + x (3) n (t)</formula><p>, where</p><formula xml:id="formula_31">x (0) n (t) = u(s) ? ? t (p n ??) (?)I[t 0 , t]? s ? ds x (1) n (t) = u(s) ? (p n ??)(?)I[t 0 , t] ? ?t (? s ?) ds</formula><p>and the x </p><formula xml:id="formula_32">n (t) + x (3)</formula><p>n (t) terms capture the term we get when differentiating I[t 0 , t]. Let us consider each term separately. The first term</p><formula xml:id="formula_33">x (0) n (t) = u(s) ? ? t (p n ??) (?)I[t 0 , t]? s ? ds (16)</formula><p>corresponds to the differentiation of the basis functions and measure. In order to relate this to {x k (t)}, it suffices that ? t (p n ??) (?) satisfies <ref type="bibr" target="#b12">(13)</ref> which implies that when we vectorize this, we get</p><formula xml:id="formula_34">x (0) (t) = A 0 ? x(t).</formula><p>For additional warping and tilting terms, we consider</p><formula xml:id="formula_35">x (1) n (t) = u(s) ? (p n ??)(?)I[t 0 , t] ? ?t (? s ?) ds.</formula><p>To reduce this term to x n (t), recall from <ref type="formula" target="#formula_23">(10)</ref> that</p><formula xml:id="formula_36">? t (? s ) = c(t)? s .</formula><p>Then the above and <ref type="formula" target="#formula_27">(14)</ref> imply</p><formula xml:id="formula_37">? t (? s ?) = c(t)(? s ?) + d(t)(? s ?)</formula><p>where c(t), d(t) are defined as in <ref type="formula" target="#formula_23">(10)</ref> and <ref type="bibr" target="#b13">(14)</ref>.</p><p>We will end up with x (1) n (t) = (c(t) + d(t))x n (t). This leads to the the vectorized form x (1) (t) = (c(t) + d(t))Ix(t).</p><p>We now need to handle</p><formula xml:id="formula_38">x (2) n (t) + x (3) n (t) = u(s) ? (p n ??)(?) ? ?t I[t 0 , t] (? s ?) ds.<label>(17)</label></formula><p>For the above note that</p><formula xml:id="formula_39">I[t 0 , t](s) = H(s ? t 0 ) ? H(s ? t), where H(x) is the "heaviside step function." It is know that H (x) = ?(x), which implies ? ?t I[t 0 , t] = ?(s ? t) ? dt 0 dt ?(s ? t 0 ).</formula><p>Using the above in RHS of (17), we separate out x  = u(t) ? (p n ??)(1)(? s ?)(t, t).</p><p>In the last equality, we have used the fact that ?(t, t) = ?(t, 1) = 1 by definition. It follows that in vectorized form we have x (2) (t) = Bu(t).</p><p>Finally, define</p><formula xml:id="formula_40">x (3) n (t) = ? u(s) ? (p n ??)(?)?(s ? t 0 ) dt 0 dt ? s ? ds = ?u(t 0 ) ? (p n ??)(?(t, t 0 ))(? s ?)(t, t 0 ) ? dt 0 dt</formula><p>If dt0 dt = 0, then we have D = 0 and hence we have x (3) (t) = 0 = ?cD (A ) x(t) ? dDu(t) If dt0 dt = 0, then as N ? ?, from (15), the above comes out to</p><formula xml:id="formula_41">x (3) n (t) = ? c N ?1 k=0 A k ? x k (t) + du(t) ? (p n ??)(?(t, t 0 ))(? s ?)(t, t 0 ) ? dt 0 dt It follows that in vectorized form we have x (3) (t) = ?cD (A ) x(t) ? dDu(t).</formula><p>The result follows after combining the terms.</p><p>We see that the behavior of is the model is dictated by t 0 . In particular, in this paper, we will consider two special cases. Proof. Follows from Theorem 12. Since t 0 is independent of t, then dt0 dt = 0, and D = 0.  <ref type="formula" target="#formula_0">(13)</ref>, D n = (p n ??)(?(t, t ? ?))(? s ?)(t, t ? ?), and B n = (p n ??)(1)(? s ?(t, t).</p><p>Proof. This follows directly from Theorem 12 by setting t 0 = t ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 LegS (and LSSL?)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Explanation of S4-LegS</head><p>Consider the case when</p><formula xml:id="formula_42">? = ? ?1 ,</formula><p>i.e. the measure is completely "tilted" away, and let ? ?t ?(t, s) = a(t)?(t, s) + b(t).</p><p>Let's consider the special case of (18) where b(t) = 0. This is most generally satisfied by ?(t, s) = exp(a(t) + z(s)).</p><p>Note that the condition ?(t, t) = 1 forces z = ?a. Hence, we have ?(t, s) = exp(a(s) ? a(t)).</p><p>We now consider the following special case of Corollary C.2:</p><formula xml:id="formula_45">Corollary C.4. Let ? ? 0. The SSM (?a (t)(A + (? + 1)I), a (t)B), where t 0 is independent of t,</formula><p>is an OSSM for basis functions and measure</p><formula xml:id="formula_46">?(?) ? ? p n (?) ?(t, s) = I(?) a (s)? 2?+1 ?(?)</formula><p>where ? satisfies <ref type="formula" target="#formula_22">(19)</ref>,</p><formula xml:id="formula_47">?(t, s) = exp(?a(s) ? ?a(t)) = ? ? ,<label>(20)</label></formula><formula xml:id="formula_48">A = ? nk such that yp n (y) = n?1 k=0 ? nk p k (y)<label>(21)</label></formula><p>and B n = p n (1).</p><p>Proof. Given a orthonormal basis p 0 , p 1 , . . . , p N ?1 with respect to a measure ?. Note that time-warping function ? satisfying (19) implies that ? s = a (s)?.</p><p>We fix tilting ?(t, s) = ?(?) ? ? , which in turn follows by setting</p><formula xml:id="formula_49">? = ? ?1 .</formula><p>We show shortly that we satisfy the pre-conditions of Corollary C.2, which implies (with our choice of ? and ?) that we have an OSSM with basis functions p n (t, s) = ?(?) ? ? p n (?) and measure</p><formula xml:id="formula_50">?(t, s) = ?(?(t, s))I[t 0 , t]? s (t, s)?(t, s) ?2 = I[t 0 , t] a (s)? 2?+1 ?(?)</formula><p>To complete the proof, we show that out choice of paramters above satisfies the conditions of Corollary C.2 (by showing they satisfy the conditions of Theorem 12). We verify that ? and ? satisfy <ref type="formula" target="#formula_23">(10)</ref> and <ref type="formula" target="#formula_27">(14)</ref>, noting that</p><formula xml:id="formula_51">? t (? s ) = ?a (t)? s , and ? t (?) = ??a (t)?.</formula><p>This implies that setting c(t) = ?a (t) and d(t) = ??a (t) is enough to satisfy (10) and <ref type="bibr" target="#b13">(14)</ref>.</p><p>Further, note that (19) and the fact that ? = ? ?1 imply that ? t (p n ??) (?) = ?a (t)?p n (?).</p><p>It follows that (13) is satisfied as long as</p><formula xml:id="formula_52">?p n (?) = n?1 k=0 ? nk p k (?)</formula><p>for some set of coefficients {? nk } N ?1 k=0 , which is exactly (21). This implies the ? nk in Corollary C.2 satisfy. ? nk = ?a (t)? nk .</p><p>Let A be the matrix such that A nk = ?? nk and then note that ?a (t)(A + (? + 1)I) is exactly the first parameter of the SSM in Corollary C.2. Similarly, recall in Corollary C.2</p><formula xml:id="formula_53">B n = p n (1)(? s ?)(t, t) = p n (1)a (t),</formula><p>where the final equality follows since in our case, ? s (t, t) = a (t) exp(a(t) ? a(t)) = a (t). Overloading notation and letting B n = p n (1), all conditions of Corollary C.2 hold, from which the claimed result follows.</p><p>We are particularly interested in the following two special cases of Corollary C.4. We set ? = 0 in Corollary C.4, which in turn sets ? = ? 0 = 1. This gives the tilting</p><formula xml:id="formula_54">? = ? ?1 ? ?1 = ?.</formula><p>Then by Corollary C.4, it follows that that we can use ? and ? to build an OSSM with basis functions</p><formula xml:id="formula_55">?(?) ? ? p n (?) = ?( s t ) ? p n ( s t )</formula><p>with measure</p><formula xml:id="formula_56">I(?) a (s)? 2?+1 ?(?) = 1 t I(?) ? ?(?) .</formula><p>Then the result follows.</p><p>Corollary C.6. The SSM (?(A + I), B) is a OSSM for basis functions p n (e s?t )?(e s?t ) with measure ? = I[t 0 , t](e s?t ) e s?t ?(e s?t ) where A = ? nk as in (21) and B n = p n (1).</p><p>Proof. This is a case of Corollary C.4 where a (t) = 1, ? = exp(s ? t), and we pick ? = 0, implying that ? = ? 0 = 1. It follows that</p><formula xml:id="formula_57">? = ? ?1 ? ?1 = ?.</formula><p>Utilizing Corollary C.4, we can use ? and ? to build an OSSM with basis functions</p><formula xml:id="formula_58">?(?) ? ? p n (?) = ?(exp(s ? t)) ? p n (exp(s ? t))</formula><p>with measure</p><formula xml:id="formula_59">I(?) a (s)? 2?+1 ?(?) = I(?) exp(s ? t) ?(exp(s ? t)</formula><p>) .</p><p>This gives us our final result.</p><p>Next we instantiate Corollary C.4 to prove Corollary 3.1. (Even though strictly not needed, we instantiate Corollary C.6 and Corollary C.5 to prove Theorem 5 and Corollary 3.3.) To that end, we will need the following result:</p><p>Lemma C.7. Let the Legendre polynomials orthonormal over the interval [0, 1] be denoted as L n . Then yL n (y) = nL n (y) + ? 2n + 1</p><formula xml:id="formula_60">n?1 k=0 ? 2k + 1L k (y) ,<label>(22)</label></formula><formula xml:id="formula_61">L n (y) = 2 ? 2n + 1 ? ? 0?k?n?1,n?k is odd ? 2k + 1L k (y) ? ? ,<label>(23)</label></formula><p>and L n (0) = (2n + 1) 1 2 (?1) n and L n (1) = (2n + 1)</p><formula xml:id="formula_62">1 2 .<label>(24)</label></formula><p>Proof. The Legendre polynomials satisfy the following orthogonality condition over [?1, 1]:</p><formula xml:id="formula_63">1 ?1 P m (z)P n (z) dz = 2 2n + 1 ? mn .</formula><p>Let us denote the normalized Legendre polynomials orthogonal over [?1, 1] as ? n P n (z) where ? n = 2n+1 2 . To orthogonalize them over [0, 1], let y = 1+z 2 . It follows that z = 2y ? 1, dz = 2 dy. Note that we then have This implies that</p><formula xml:id="formula_64">1 ?1 2n + 1 2 ? 2P m (2y ? 1)P n (2y ? 1) dy = ? mn .</formula><p>Then if we let</p><formula xml:id="formula_65">L n (y) = ? 2? n P n (2y ? 1) = ? 2n + 1P n (2y ? 1),<label>(25)</label></formula><p>then we have an a set of functions over [0, 1] such that From <ref type="bibr">[2, (2.8)</ref>, (2.9)], note that P n (?1) = (?1) n and P n (1) = 1. This implies that</p><formula xml:id="formula_66">L n (0) = ? 2n + 1P n (?1), L n (1) = ? 2n + 1P n (1).</formula><p>Finally note that (25) implies:</p><p>L n (y) = 2 ? 2n + 1P n (2y ? 1)</p><formula xml:id="formula_67">= 2 ? 2n + 1P n (z).</formula><p>From <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, we get P n (z) = 0?k?n?1,n?k is odd (2k + 1)P k (z).</p><p>Using (25) on the above, we get (23).</p><p>We now consider yL n (y) = 2y ? 2n + 1P n (z)</p><formula xml:id="formula_68">= (1 + z) ? 2n + 1P n (z).</formula><p>From <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>, we get (z + 1)P n (z) = nP n (z) +  We now re-state and prove Corollary 3.1:</p><p>Corollary C.8 <ref type="figure" target="#fig_2">(Corollary 3.1, restated)</ref>. Let L n be the Legendre polynomials orthonormal over the interval Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke Corollary C.4 with ? = 1. Further, here we have t 0 = ?? and ? = 0. Now we have an SSM: ?a (t)(A 0 + I), a (t)B</p><p>where A 0 nk = ? nk as in (21) and B n = L n (1). From (24) observe that B n = (2n + 1) Thus the A and B match those in (4), which completes our claim.</p><p>We now re-state and prove Theorem 5:</p><p>Corollary C.9 (Theorem 5, restated). Let L n be the Legendre polynomials orthonormal over the interval [0, 1]. Then the SSM ( 1 t A, 1 t B) is a OSSM for basis functions L n ( s t ) and measure 1 t I[t 0 , t] where A and B are defined as in (4).</p><p>Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke Corollary C.5 with ? = 1. Now we have</p><formula xml:id="formula_69">x (t) = 1 t ?(A 0 + I)x(t) + Bu(t)</formula><p>where A 0 nk = ? nk as in (21) and B n = L n (1). From (24) observe that B n = (2n + 1) Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure, warping function ? = exp(s ? t), and with tilting ? = ?. We note that ? = exp(s ? t) satisfies <ref type="formula" target="#formula_22">(19)</ref> with, a (t) = 1. This allows us to invoke Corollary C.5.</p><p>Then x (t) = (A+I)x(t)+Bu(t) orthogonalizes against the basis functions L n (e s?t ) with measure I[??, t]e s?t where A = ? nk as in 21. Note that the SSM basis functions K n (t, s) = K n (s ? t), hence we get the claimed SSM form utilizing the same argument for A, B as in the proof of Corollary C.9</p><p>This explains why removing the 1 t factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential "warping". Proof. Out plan is to apply Corollary C.3, for which we must show that the basis functions L n (t, s), time warping ?(t, s), and tilting ?(t, s) = ? ?1 ? ?1 (t, s) satisfy (13), <ref type="bibr" target="#b9">(10)</ref>, and (14), respectively. We first set some parameters-note that because ? = 1 and set ? = ? = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Finite Windows</head><p>The above implies that we have</p><formula xml:id="formula_70">? t (L n ??) (?) = ? 1 ? L n (?).</formula><p>The above along with (23), we see that the Legendre polynomials satisfy <ref type="bibr" target="#b12">(13)</ref> with</p><formula xml:id="formula_71">? nk = 1 ? ? ?2 ? (2n + 1) 1 2 (2k + 1) 1 2 k &lt; n and n ? k is odd 0 otherwise .<label>(26)</label></formula><p>We also note that ? s = 1 ? . It follows that d dt ? s = 0, satisfying (10) trivially by setting c(t) = 0. Similarly, since ? = 1 <ref type="bibr" target="#b13">(14)</ref> is also satisfied trivially by setting d(t) = 0. Finally we note that the L n forms a complete basis over [0, 1], hence as N ? ?, we have</p><formula xml:id="formula_72">u(t ? ?) = N ?1 k=0 x k (t)L n (?(t, t ? ?)) = N ?1 k=0</formula><p>x k (t)L n (0).</p><p>The above defines A by setting A n = L n (0) (as well as c = 1 and d = 0.) Now by Corollary C.3, we have an SSM</p><formula xml:id="formula_73">A 0 ? D (A ) , B ,</formula><p>where D n = 1 ? L n (0), and by (22) A 0 nk = ? nk (as in (26)) and B n = 1 ? L n (1). From (24), we have D n = 1 ? (2n + 1) 1 2 (?1) n and B n = 1 ? (2n + 1)</p><formula xml:id="formula_74">1 2 . Thus, we have A 0 ? D (A ) nk = 1 ? ? ?(2n + 1) 1 2 (2k + 1) 1 2 2 + (?1) n?k k &lt; n and n ? k is odd ?(2n + 1) 1 2 (2k + 1) 1 2 (?1) n?k otherwise .</formula><p>The proof is complete by noting that A 0 ? D (A ) = 1 ? A and B = 1 ? B.</p><p>We note that Corollary C.11 implies Proposition 4. More specifically, Proposition 4 follows by setting ? = 1 in Corollary C.11 and noticing that the OSSM there is actually a TOSSM. (Technically we get basis function</p><formula xml:id="formula_75">L n (1 ? t) for measure I(1 ? t) but this is OK since 1 0 L k (1 ? t)L j (1 ? t)dt = 1 0 L k (t)L j (t)dt.)</formula><p>We first give a proof of Theorem 6. Then, we prove Theorem 7 as a function approximation result pertaining to S4-FouT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2 Explanation of S4-FouT</head><p>Proof of Theorem 6. We seek to derive A and B from (6) using Corollary C.3:</p><p>We use the time-warping function ?(t, s) = 1 ? (t ? s), which implies that we have</p><formula xml:id="formula_76">? s (t, s) = 1, (27) ? ?t ? s (t, s) = 0<label>(28)</label></formula><p>Thus, we can take</p><formula xml:id="formula_77">c(t) = 0 in ? ?t ? s (t, s) = c(t)? s (t, s).<label>(29)</label></formula><p>We then have ?(t, s) = 1 as we set</p><formula xml:id="formula_78">?(t, s) = ?(t, s) = 1, (30) d dt ?(t, s) = 0.<label>(31)</label></formula><p>So, we can take</p><formula xml:id="formula_79">d(t) = 0 in d dt ?(t, s) = d(t)?(t, s).<label>(32)</label></formula><p>We also have ?(?) = 1, and we order our bases in the form p n = (1, c 1 (t), s 1 (t), c 2 (t), s 2 (t), . . .) 1 , where the basis functions have derivatives:</p><p>(1) (?) = 0;</p><p>(c n ) (?) = ?2?ns n (?);</p><p>(s n ) (?) = 2?nc n (?).</p><p>Consequently, we can define ? nk as follows:</p><formula xml:id="formula_80">? nk = ? ? ? ? ? 2?n n ? k = 1, k odd ?2?k k ? n = 1, n odd 0 otherwise .<label>(33)</label></formula><p>Further, the discontinuity is at t 0 = t ? ?, ? = 1 which implies that dt0 dt = 1. We now seek to use the stored approximation to u at time t to compute u(t ? 1). </p><formula xml:id="formula_81">First, denote the latent state x(t) with coefficients x = (x 1 (t), x c 1 (t), x s 1 (t), x c 2 (t),</formula><p>Then, we claim that?</p><formula xml:id="formula_83">(t, t) = u(t) + v(t) 2 = u(t) + u(t ? 1) 2 .<label>(37)</label></formula><p>Towards that end, we examine the sine and cosine coefficients of u and v as follows:</p><p>v, c n = v(s)c n (?(t, s))</p><formula xml:id="formula_84">I[t ? 1, t]ds = u(2t ? s ? 1)c n (?(t, s))I[t ? 1, t]ds = u(s )c n (1 ? ?(t, s ))I[t ? 1, t]ds (38) = u(s )c n (?(t, s ))I[t ? 1, t]ds = u, c n . v, s n = v(s)s n (?(t, s))I[t ? 1, t]ds = u(2t ? s ? 1)s n (?(t, s))I[t ? 1, t]ds = u(s )s n (1 ? ?(t, s ))I[t ? 1, t]ds (39) = ? u(s )s n (?(t, s ))I[t ? 1, t]ds = ? u, s n .</formula><p>Here, for (38) and (39), we use the change of variables s ? 2t ? s ? 1, which gives us</p><formula xml:id="formula_85">?(t, s) = 1 ? (t ? s) = 1 ? (1 + t ? s ? 1) = 1 ? [1 ? (t ? (2t ? s ? 1))] = 1 ? (1 ? (t ? s )) = 1 ? ?(t, s ).</formula><p>1 Note that this is 0-indexed.</p><p>Then, we use the fact that c n (1 ? ?(t, s )) = c n (?(t, s )) but s n (1 ? ?(t, s )) = ?s n (?(t, s )). That is, both u and v have the same cosine coefficients but negated sine coefficients of each other. But, we know that both s n (?(t, t ? 1)) = s n (1 ? (t ? (t ? 1))) = s n (0) = 0 and s n (?(t, t)) = s n (1 ? (t ? t)) = s n (1) = 0, and hence, the reconstruction of? at the endpoints ?(t, t ? 1) = 0 and ?(t, t) = 1 depends only on the cosine coefficients, whence we assert that the reconstruction? agrees withv at both endpoints. Therefore, we hav? u(t, t) =v(t, t) implying that?(t, t) =?(t, t).</p><p>Note that w is continuous and periodic, for which the basis {1, c n , s n } n is complete, and hence, we know that as N ? ?,? ? w. Thus, at s = t, we have?(t, t) =?(t, t) = w(t) = u(t)+v(t) 2 = u(t)+u(t?1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, which completes the proof of the claim in (37).</p><p>Recall from (34) that we can express the stored approximation of u(t), given by?(t, s), as follows:</p><formula xml:id="formula_86">u(t, s) = u(s), 1 + n u(s), s n (?(t, s)) s n (?(t, s)) + n u(s), c n (?(t, s)) c n (?(t, s))</formula><p>For the value at t, the approximation?(t, t) is then given b?</p><formula xml:id="formula_87">u(t, t) = x 1 (t) + k x c k (t)c k (1) + k x s k (t)s k (1) = x 1 (t) + k ? 2x c k (t).</formula><p>Due to (37), we know u(t ? 1) = 2?(t, t) ? u(t), which combined with the above yields:</p><formula xml:id="formula_88">u(t ? 1) = 2x 1 (t) + 2 ? 2 k x c k (t) ? u(t).<label>(40)</label></formula><p>Finally, with regards to Corollary C.3, for Theorem 12, (29) satisfies <ref type="formula" target="#formula_23">(10)</ref>  </p><formula xml:id="formula_89">(A 0 + (c(t) + d(t))I ? cD (A ) , B ? dD),</formula><p>where A 0 nk = ? nk with D n and B n specified as follows:</p><formula xml:id="formula_90">D n = ? ? ? ? ? 1 n = 0 ? 2 n odd 0 otherwise (41) B n = ? ? ? ? ? 1 n = 0 ? 2, n odd 0 otherwise<label>(42)</label></formula><p>Here, the values are derived from the expressions of Corollary C.3:</p><p>D n = (p n ??)(?(t, t ? 1))(? s ?)(t, t ? 1) and B n = (p n ??)(1)(? s ?)(t, t).</p><p>Recall that we have p n ? {1, c n , s n }, ?(t, s) = 1, and from (27) and (30), ? s (t, s) = 1 with ?(t, s) = ?(t, s) = 1. Thus, (41) is due to 1(0)?1 = 1, s n (0)?1 = 0 but c n (0)?1 = ? 2. Similarly, (42) is because 1(0)?1 = 1, s n (1)?1 = 0 but again c n (1) ? 1 = ? 2.</p><p>Now, we have</p><formula xml:id="formula_91">[D (A ) ] nk = ? ? ? ? ? ? ? ? ? 2 n = k = 0 2 ?</formula><p>2 n = 0, k odd or k = 0, n odd 4 n, k odd 0 otherwise</p><formula xml:id="formula_92">[dD] n = ? ? ? ? ? ?1 n = 0 ? ? 2 n odd 0 n otherwise .</formula><p>As c(t) = d(t) = 0, we define A ? A 0 ? cD (A ) and B ? B ? dD, given by</p><formula xml:id="formula_93">A nk = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 n = k = 0 ?2 ? 2 n = 0, k odd or k = 0, n odd ?4 n, k odd 2?n n ? k = 1, k odd ?2?k k ? n = 1, n odd 0 otherwise B n = ? ? ? ? ? 2 n = 0 2 ? 2 n odd 0 otherwise C.4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.3 Function Approximation Error</head><p>Proof of Theorem 7. First, the state size being N dictates that there are N/2 s n and c n basis functions each. We fix time t and denote x c n and x s n to be the respective coefficients for s n and c n basis corresponding to S4-Fou. Since {s n , c n } n?0 forms an orthonormal basis, by Parseval's identity, we have</p><formula xml:id="formula_94">K ?K 2 2 = ? n= N/2 x c n 2 (t) + x s n 2 (t).<label>(43)</label></formula><p>Thus, in order to bound the error, it suffices to bound the high-order coefficients by integration by parts as follows:</p><p>x c n (t) = K, c n = 1 0 K(t)c n (t)dt</p><formula xml:id="formula_95">= K(t) 1 2?n s n (t) 1 0 ? 1 2?n 1 0 K (t)s n (t)dt = ? 1 2?n 1 0 K (t)s n (t)dt.</formula><p>The quantity in the bracket vanishes as s n is periodic. Therefore</p><formula xml:id="formula_96">|x c n | ? 1 2?n 1 0 K (t)s n (t)dt ? 1 2?n 1 0 |K (t)||s n (t)|dt ? L 2?n ,</formula><p>where we use the fact that K is L?Lipshitz. For x s n , a similar argument holds and we get:</p><formula xml:id="formula_97">|x s n | ? 1 2? 1 0 K (t)c n (t)dt ? 1 2? 1 0 |K (t)||c n (t)|dt ? L 2?n.</formula><p>Due to (43), this then implies that</p><formula xml:id="formula_98">K ?K 2 2 = ? n= N/2 x c n 2 (t) + x s n 2 (t) = ? n= N/2 |x c n | 2 (t) + |x s n | 2 (t) ? ? n= N/2 2L 2 (2?n) 2 = 2L 2 (2?) 2 ? n= N/2 1 n 2 = 2L 2 (2?) 2 1 N/2 ? L 2 ? 2 (N ? 2)</formula><p>.</p><p>We use (44) to get the following estimate on K ?K :</p><formula xml:id="formula_100">K ?K 2 ? L ? (N ? 2) .</formula><p>Thus, it suffices for N to satisfy the following inequality:</p><formula xml:id="formula_101">L ? (N ? 2) ? =? ? N ? 2 ? L ? =? N ? L ? 2 + 2.</formula><p>We now use the same argument as above to the fact that K has order-k bounded derivative. By iteration, we get:</p><formula xml:id="formula_102">|x s n | = |x c n | ? 1 (2?n) k 1 0 K (k) (t)s n (t)dt ? 1 (2?n) k 1 0 |K (k) ||s n (t)|dt ? L (2?n) k .</formula><p>Again, due to (43), this then gives us the following estimate on the square error:</p><formula xml:id="formula_103">K ?K 2 2 = ? n= N/2 x c n 2 (t) + x s n 2 (t) = ? n= N/2 |x c n | 2 (t) + |x s n | 2 (t) ? ? n= N/2 2L 2 (2?n) 2k = 2L 2 (2?) 2k ? n= N/2 1 n 2k = 2L 2 (2?) 2k 1 ( N/2 ) 2k?1 ? L 2 ? 2k (N ? 2) 2k?1 .<label>(45)</label></formula><p>If K has order k?bounded derivatives, then we use (45) to get the following estimate on K ?K :</p><formula xml:id="formula_104">K ?K 2 ? L ? k (N ? 2) ?k+1/2 .</formula><p>Again, it suffices for N to satisfy the following inequality: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.4 Delay Network</head><p>Finally, we prove Theorem 9. Note that this is a stronger version of the LegT portion of Theorem 8, while the FouT portion is a corollary of the proof of Theorem 6.</p><p>We start by working out some calculations concretely to provide an example. The SSM corresponding to HiPPO-LegT is (In the RHS and for the rest of this part, we will redefine A to be the ?1 matrix found above for convenience.) Case N=1. We have A = ?1, B = C = 1, and the transfer function is C(sI ? A) ?1 B = 1 1+s .</p><formula xml:id="formula_105">A = P 1 2 ? ? ? ? ?1 1 ?1 1 ?1 ?1 1 ?1 ?1 ?1 ?1 1 ?1 ?1 ?1 ?1 ? ? ? ? P 1 2 B = P</formula><p>Case N=2. The transfer function is It can be verified that this is indeed [1/2] exp (?s).</p><formula xml:id="formula_106">C(sI ? A) ?1 B = 1 ?1 sP ?1 ? ?1 1 ?1 ?1</formula><p>A General Recursion. We will now sketch out a method to relate these transfer functions recursively.</p><p>We will redefine Z to be the vector that ENDS in +1.</p><p>The main idea is to write</p><formula xml:id="formula_107">A n = A n?1 Z n?1 ?1 n?1 ?1 sP ?1 n ? A n ?1 = sP ?1 n?1 ? A n?1 ?Z n?1 1 n?1 1 + s 2n+1 ?1 .</formula><p>Now we can use the block matrix inversion formula. <ref type="bibr">3</ref> Ideally, this will produce a recurrence where the desired transfer function Z n (sP ?1 n ? A n ) ?1 1 n will depend on Z n?1 (sP ?1 n ? A n?1 ) ?1 1 n?1 . However, looking at the block matrix inversion formula, it becomes clear that there are also dependencies on terms like 1 n?1 (sP ?1 n?1 ? A n?1 ) ?1 1 n?1 and Z n?1 (sP ?1 n?1 ? A n?1 ) ?1 Z n?1 . The solution is to track all of these terms simultaneously.</p><p>We will compute the 4 transfer functions H n (s) := H 1z n (s) H 11 n (s) H zz n (s) H z1 n (s) := 1 n (sP ?1 n ? A n ) ?1 Z n 1 n (sP ?1 n ? A n ) ?1 1 n Z n (sP ?1 n ? A n ) ?1 Z n Z n (sP ?1 n ? A n ) ?1 1 n = 1 n Z n (sP ?1 n ? A n ) ?1 Z n 1 n Lemma C.12. Instead of using the explicit block matrix inversion formula, it will be easier to work with the following factorization used to derive it (block LDU decomposition 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B C D</head><formula xml:id="formula_108">= I 0 CA ?1 I A 0 0 D ? CA ?1 B I A ?1 B 0 I A B C D ?1 = I ?A ?1 B 0 I A ?1 0 0 (D ? CA ?1 B) ?1 I 0 ?CA ?1 I</formula><p>Using Lemma C.12, we can factor the inverse as (sP ?1 n ? A n ) ?1 = I n?1 (sP ?1 n?1 ? A n?1 ) ?1 Z n?1 1 (sP ?1 n?1 ? A n?1 ) ?1 1 + s 2n+1 + (?1) n?1 H 1z n?1 (s) ?1 I n?1 ?1 n?1 (sP ?1 n?1 ? A n?1 ) ?1 1</p><p>Now we compute 1 n Z n I n?1 (sP ?1 n?1 ? A n?1 ) ?1 Z n?1 1 = 1 n?1 1 ?Z n?1 1 I n?1 (sP ?1 n?1 ? A n?1 ) ?1 Z n?1 1 = 1 n?1 1 + H 1z n?1 (s) ?Z n?1 1 ? H zz n?1 (s) and I n?1 ?1 n?1 (sP ?1 n?1 ? A n?1 ) ?1 1 Z n 1 n = I n?1 ?1 n?1 (sP ?1 n?1 ? A n?1 ) ?1 1 ?Z n?1 1 n?1 1 1 = ?Z n?1 1 n?1 1 + H 1z n?1 (s) 1 ? H 11 n?1 (s) Now we can derive the full recurrence for all these functions.</p><p>We can analyze each term separately.</p><p>Case G 1z n (s). This will be the most important term, as it determines the denominator of the expressions. Simplifying the recurrence slightly gives Going forward we will also suppress the superscript of Q, Q n?1 (s) := Q 1z n?1 (s), as it will be evident that all terms have the same denominator Q n (s) Case G 11 n (s). First note that G 11 n (s) = G zz n (s) is straightforward from the fact that their recurrences are identical. The recurrence is But note that this is exactly satisfied by the Pad? approximants, by the determinantal formula of continued fractions. This shows that G 1z n?1 (s) are the Pad? approximants of e ?s , as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Normalization and Timescales</head><p>Proposition 13 (Closure properties of TOSSMs). Consider a TOSSM (A, B) for basis functions p n (t) and measure ?(t). Then, the following are also TOSSMs with the corresponding basis functions and measure:</p><p>1. Constant scaling changes the timescale: (cA, cB) is a TOSSM with basis p(ct) and measure ?(ct)c.</p><p>2. Identity shift tilts by exponential: <ref type="figure" target="#fig_10">(A + cI, B)</ref> is a TOSSM with basis p(t)e ?ct and measure ?(t)e 2ct .</p><p>3. Unitary change of basis preserves measure: (V AV * , V B) is a TOSSM with basis V p(t) and measure ?(t).</p><p>Proof. We define p(t) to be the vector of basis functions for the OSSM <ref type="figure" target="#fig_10">(A, B)</ref>,</p><formula xml:id="formula_109">p(t) = ? ? ? p 0 (t) . . . p N ?1 (t) ? ? ? .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>n</head><label></label><figDesc>are a complete basis on the support of ?. Then u(s) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>n=0 x n (t)p n (t, s) for all s ? t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(HiPPO-LegT.) (Left) First 4 basis functions Kn(t) for state size N = 1024 (Proposition 4). (Right) Choosing a particular C produces a spike kernel or "delay network" (Theorem 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 3 . 1 .</head><label>31</label><figDesc>Define ?(t, s) = exp(a(s) ? a(t)) for any differentiable function a. The SSM (a (t)A, a (t)B) is an OSSM with ?(t, s) = I(?(t, s))a (s)?(t, s) p n (t, s) = L n (?(t, s)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 6 .</head><label>6</label><figDesc>As N ? ?, the SSM for (6) is a TOSSM with ?(t) = I(t), and {p n } n?1 are the truncated Fourier basis functions orthonormal on [0, 1], ordered in the form {p n } n?0 = (1, c 0 (t), s 0 (t), . . .), where s m (t) = ? 2 sin (2?mt) and c m (t) = ? 2 cos (2?mt) for m = 0, . . . , N/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Corollary 3 . 5 .</head><label>35</label><figDesc>By Theorem 8, as N ? ?, the discrete convolutional kernel K ? e ? ?1 , i.e. the discrete delay network with lag 1 ? . Corollary 3.6. For HiPPO-FouT matrices (A, B), by Theorem 6, as N ? ?, the discrete convolutional kernel K (over the choice of C) can represent any local convolution of length ? ?1 . This discussion motivates the following definition. Properly normalized TOSSMs (A, B) will model dependencies of expected length 1, and ? modulates it to model dependencies of length 1 ? , allowing fine-grained control of the context size of a TOSSM. Definition 4 (Timescale of TOSSM). Define E[?] = ? 0 t?(t) dt ? 0 ?(t) dt to be the timescale of a TOSSM having measure ?(t). A TOSSM is timescale normalized if it has timescale 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>(Validation curves on Path-X.) (Left) Setting ?min too small can solve the task, but is inconsistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>confirms the HiPPO theory of online function reconstruction (Proposition 2) for the proposed TOSSMs LegS and FouT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Function reconstruction predicted by our general theory. An input signal of length 10000 is processed sequentially, maintaining a state vector of size only x(t) ? R 64 , which is then used to approximately reconstruct the entire history of the input. (Left) HiPPO-LegS (as an LTI system) orthogonalizes on the Legendre polynomials warped by an exponential change of basis, smoothening them out. This basis is orthogonal with respect to an exponentially decaying measure. Matching the intuition, the reconstruction is very accurate for the recent past and degrades further out, but still maintains information about the full history of the input, endowing it with long-range modeling capacity. This is the same as S4. (Right) HiPPO-FouT orthogonalizes on the truncated Fourier basis, similar to the original HiPPO-LegT or LMU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Log-MSE after training on the Reconstruction Task. (Left) When the timescales ? are set appropriately for this task, the methods that theoretically reconstruct against a uniform measure (LegT and FouT) are much better than alternatives, achieving MSE more orders of magnitude lower than other SSM initializations. (Right) Interestingly, when the timescales ? are not set correctly, these methods (LegT and FouT) actually perform worst and the diagonal methods introduced in [8] perform best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>( a )</head><label>a</label><figDesc>Models perform a mapping from R 4000 ? R 4000 where the target output is lagged by 1000 steps, with error measured by RMSE. The input is a white noise signal bandlimited to 1000Hz. We use single layer SSMs with state size N = 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Consider a measure supported</head><label></label><figDesc>on [0, 1] with density ?(t)I(t), where I(t) is the indicator function for membership in the interval [0, 1]. Let the measure be equipped with a set of orthonormal basis functions p 0 , . . . , p N ?1 , i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>p</head><label></label><figDesc>j (s)p k (s)?(s)I(s) ds = ? jk ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma C. 1 .</head><label>1</label><figDesc>For the set of orthonormal functions {p n } N ?1 n=0 orthogonal over measure ?I, the set of basis functions q t k (?(t, s)) = ?(t, s)p k (?(t, s)) are orthogonal over the measure ?(t, s) = ?(?(t, s))I[t 0 , t](s)? s (t, s)?(t, s) ?2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 0</head><label>1</label><figDesc>t0) p j (y)p k (y)?(y)dy = ?(t,t) ?(t,t0) p j (y)p k (y)?(y)dy = p j (y)p k (y)?(y)dy = p j (y)p k (y)?(y)I(y)dy = ? jk .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>( 3 )</head><label>3</label><figDesc>n (t) as follows. First, definex (2)n (t) = u(s) ? (p n ??)(?)?(s ? t)? s ? ds = u(t) ? (p n ??)(?(t, t))(? s ?)(t, t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Corollary C. 2</head><label>2</label><figDesc>(t 0 independent of t). The SSM ((A + c(t) + d(t)I), B) satisfying conditions of Theorem 12 with t 0 independent of t, is an OSSM for basis functions ?p n (?) with measure ?I[t 0 , t]? s ? ?2 where A = ? nk as in (13) and B n = (p n ??)(1)(? s ?)(t, t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Corollary C. 3</head><label>3</label><figDesc>(t 0 = t ? ?). The SSM (A 0 + (c(t) + d(t))I ? cDA , B ? dD) satisfying conditions of Theorem 12 with t 0 = t ? ? for a fixed ?, is an OSSM with basis functions ?p n (?) with measure ?I[t 0 , t]? s ? ?2 where A 0 nk = ? nk as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Corollary C. 5 .</head><label>5</label><figDesc>The SSM (? 1 Proof. Letting a (t) = 1 t implies that a(t) = ln t. Then we can observe that is a case of Corollary C.4 with time warping ?(t, s) = exp(? ln t + ln s) = exp(ln(s/t)) = s t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>1 ? 1 P 1 0</head><label>111</label><figDesc>m (z)P n (z) dz = 2P m (2y ? 1)P n (2y ? 1) dy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>1 0L</head><label>1</label><figDesc>m (y)L n (y) dy = ? mn .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>1)P k (z). Then the above becomes yL n (y) = ? 2n + 1 nP n (z) + n?1 k=0 (2k + 1)P k (z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>( 25 )</head><label>25</label><figDesc>implies that P n (z) = Ln(y) ? 2n+1 , thus yL n (y) = nL n (zk (z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>[0, 1 ].</head><label>1</label><figDesc>Define ?(t, s) = exp(a(s) ? a(t)). The SSM (a (t)A, a (t)B) is an OSSM with ?(t, s) = I(?(t, s))a (s)?(t, s) p n (t, s) = L n (?(t, s)), where A and B are defined as in (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>.</head><label></label><figDesc>We write that A = ?(A 0 + I). Indeed,?(A 0 + I) nk = ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>.</head><label></label><figDesc>We write that A = ?(A 0 + I). Indeed,?(A 0 + I) nk = ? claim.We now restate and prove Corollary 3.3.Corollary C.10 (Corollary 3.3, restated). Let L n be the Legendre polynomials orthonormal over the interval [0, 1]. Thenthe SSM (A, B)is a TOSSM for basis functions L n (e ?t ) with measure ? = I[t 0 , t]e ?t where A, B are defined as in (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>C. 4 . 1</head><label>41</label><figDesc>LegT Derivation Corollary C.11. Let L n be the Legendre polynomials orthonormal over the interval [0, 1] and let ? = 1 ? t?s ? for a constant ?. Then the SSM 1 ? A, 1 ? B is a OSSM for basis functions L n (?) with measure 1 ? I[t 0 , t](?) where A, B are defined as in (5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>x s 2</head><label>2</label><figDesc>(t), . . .) and define the functions v(s) and w(s) such that we have v(s) = u(2t ? s ? 1)andw(s) = u(s) + v(s) 2 for s ? [t ? 1, t].Now, let?,v, and? denote the reconstruction of u, v and w, where we hav?u(t, s) = u(s), 1 + n u(s), s n (?(t, s)) s n (?(t, s)) + n u(s), c n (?(t, s)) c n (?(t, s)), (34) v(t, s) = v(s), 1 + n v(s), s n (?(t, s)) s n (?(t, s)) + n v(s), c n (?(t, s)) c n (?(t, s)),(35)w(t, s) = w(s), 1 + n w(s), s n (?(t, s)) s n (?(t, s)) + n w(s), c n (?(t, s)) c n (?(t, s)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>L ? k (N ? 2 )</head><label>2</label><figDesc>?k+1/2 ? =? (N ? 2) k?1/2 ? L ? k =? N ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>1 2 1 C = Z P 1 2 P</head><label>112</label><figDesc>= diag{1 + 2n} Z = 1 ?1 1 ?1The transfer function isC(sI ? A) ?1 B = Z(sP ?1 ? A) ?1 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>n.</head><label></label><figDesc>(s)  where P, Q are polynomials. Clearing the denominator Q yields P 1z n This results in the recurrenceQ 1z n (s) = P 1z n?1 (s) + s 2(2n + 1) ? Q 1z n?1 (s) P 1z n (s) = Q 1z n?1 (s) ? s 2(2n + 1) ? P 1z n?1 (s).But this is exactly the fundamental recurrence formula for continuants of the continued fraction e s n?1 (s) are the denominators of the Pade approximants. Note that by definition of P, Q,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>1 2 Q 2 =</head><label>122</label><figDesc>) ? G 11 n?1 (s)Q n?1 (s) P 1z n?1 (s) + s 2(2n+1) ? Q n?1 (s) = s 2(2n+1) ? G 11 n?1 (s)Q n?1 (s) Q n (s) the formula G z1 n (s) = G z1 n?1 (s) ? (?1) n?1 G 11 n?1 (s)G zz n?1 (s) G 1z n?1 (s) + s 2(2n+1) = P z1 n?1 (s) Q n?1 (s) ? (?1) (s)Q n (s) Q n?1 (s)Q n (s) ? (?1) n (s)Q n?1 (s)By definition of P z1 , P z1 n?1 (s)Q n (s) ? (?1) P z1 n (s)Q n?1 (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of time-invariant orthogonal SSMs. Original HiPPO results (Bottom) and new results (Top). Method SSM Matrices SSM kernels e tA B Orthogonal basis pn(t) Measure ?(t) Timescale LegS equation Eq. (4) Ln(e ?t )e ?t Ln(e ?t ) e ?t 1 FouT equation Eq. (6) (cos, sin)(2?nt)I[0, 1] (cos, sin)(2?nt)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>(Long Range Arena) Accuracy (std.) on full suite of LRA tasks. Hyperparameters in Appendix B. .07) 86.82 (0.13) 90.90 (0.15) 88.65 (0.23) 94.20 (0.25) 96.35 (-) 86.09 S4-FouT 57.88 (1.90) 86.34 (0.31) 89.66 (0.88) 89.07 (0.19) 94.46 (0.24)</figDesc><table><row><cell>Model</cell><cell>ListOps</cell><cell>Text</cell><cell>Retrieval</cell><cell>Image</cell><cell cols="2">Pathfinder Path-X</cell><cell>Avg</cell></row><row><cell>S4-LegS</cell><cell cols="7">59.60 (077.90</cell></row><row><cell cols="5">S4-LegS/FouT 60.45 (0.75) 86.78 (0.26) 90.30 (0.28) 89.00 (0.26)</cell><cell>94.44 (0.08)</cell><cell></cell><cell>78.50</cell></row><row><cell>S4 (original)</cell><cell>58.35</cell><cell>76.02</cell><cell>87.09</cell><cell>87.26</cell><cell>86.05</cell><cell>88.10</cell><cell>80.48</cell></row><row><cell>Transformer</cell><cell>36.37</cell><cell>64.27</cell><cell>57.46</cell><cell>42.44</cell><cell>71.40</cell><cell></cell><cell>53.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We additionally construct a synthetic Reconstruction Task (for a uniform measure) to test if S4 variants can learn to reconstruct. The input is a white noise sequence u ? R 4000 . We use a single layer linear S4 model with state size N = 256 and H = 256 hidden units. Models are required to use their output at the last time step, a vector y 4000 ? R 256 , to reconstruct the last 1000 elements of the input with a linear probe. Concretely, the loss function is to minimize u 3000:4000 ? W y 4000 2 2 , where W ? R 1000?256 is a learned matrix.</figDesc><table /><note>Fig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation of the initialization standard deviation of C for S4-LegS on classification datasets.</figDesc><table><row><cell>Init std. ? of C</cell><cell>0.01</cell><cell>0.1</cell><cell>1.0</cell><cell>10.0</cell><cell>100.0</cell></row><row><cell cols="6">Sequential CIFAR 85.91 (0.41) 86.33 (0.01) 86.49 (0.51) 84.40 (0.16) 82.05 (0.61)</cell></row><row><cell cols="6">Speech Commands 90.27 (0.31) 90.00 (0.11) 90.67 (0.19) 90.30 (0.36) 89.98 (0.51)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The values of the best hyperparameters found for LRA. LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization. Features H Norm Pre-norm Dropout LR Batch Size Epochs WD (?min, ?max)</figDesc><table><row><cell cols="2">Depth ListOps 8</cell><cell>128</cell><cell>BN</cell><cell>False</cell><cell>0</cell><cell>0.01</cell><cell>50</cell><cell>40</cell><cell>0.05</cell><cell>(0.001, 0.1)</cell></row><row><cell>Text</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell>0.01</cell><cell>16</cell><cell>32</cell><cell>0.05</cell><cell>(0.001, 0.1)</cell></row><row><cell>Retrieval</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell>0.01</cell><cell>64</cell><cell>20</cell><cell>0.05</cell><cell>(0.001, 0.1)</cell></row><row><cell>Image</cell><cell>6</cell><cell>512</cell><cell>LN</cell><cell>False</cell><cell>0.1</cell><cell>0.01</cell><cell>50</cell><cell>200</cell><cell>0.05</cell><cell>(0.001, 0.1)</cell></row><row><cell cols="2">Pathfinder 6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell>0.004</cell><cell>64</cell><cell>200</cell><cell>0.03</cell><cell>(0.001, 0.1)</cell></row><row><cell>Path-X</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell cols="2">0.0005 32</cell><cell>50</cell><cell>0.05</cell><cell>(0.0001, 0.01)</cell></row><row><cell cols="11">very general instantiations of it (Section 3.1), and flesh out its properties (Section 3.3). Our general result</cell></row><row><cell cols="11">subsumes all prior cases including all cases of the LSSL as a direct corollary. Some concrete advantages include:</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Corollary 3.2 (Scale-Invariant HiPPO-LegS, Theorem 5). The SSM ( 1 t A, 1 t B)is a TOSSM for basis functions K n (t) = s t L n ( s t ) and measure ? = 1 t I[0, 1] where A and B are defined as in (4).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (A + I), 1 t B) is a OSSM for basis functions p n ( s t )?( s t ) with measure 1 t I[t 0 , t] s t ? ?( s t )where A = ? nk as in (21) and B n = p n (1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Recall that, like the coefficients, the matrices are 0-indexed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://en.wikipedia.org/wiki/Block_matrix#/Block_matrix_inversion</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://en.wikipedia.org/wiki/Schur_complement#/Background</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of DARPA under Nos. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>H n (s) = H 1z n (s) H 11 n (s) H zz n (s) H z1 n (s)</p><p>Now we'll define a few transformations which will simplfy the calculations. Define G 1z n (s) =</p><p>These satisfy the following recurrences:</p><p>n?1 (s) + </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An introduction to orthogonal polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chihara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
	<note>ISBN 9780486479293</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Catformer: Designing stable transformers via sensitivity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14343</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dynamical systems in spiking neuromorphic hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Russell</forename><surname>Voelker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The SSM kernels are e t(cA) (cB) = ce (ct)A) B = cp(ct)?(ct)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">It remains to show that the p n (ct) are orthonormal with respect to measure c?(ct): p j (ct)p k (ct)?(ct)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Using the commutativity of A and I, the SSM kernels are e t(A+cI) B = e tA e ctI B = e ct p(t)?(t)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">It remains to show that p n (t)e ?ct are orthonormal with respect to measure ?(t)e 2ct : p j (t)e ?ct p k (t)e ?ct ?(t)e 2ct = p j (t)p k (t)?(t) = ? jk</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Av * V B = V E Ta B = V P</surname></persName>
		</author>
		<title level="m">The SSM basis is e tV</title>
		<imprint/>
	</monogr>
	<note>t)?(t</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Note that orthonormality of a set of basis functions can be expressed as p(t)?(t)p(t) = I, so that (V p(t))?(t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">It remains to show that the basis functions V p(t) are orthonormal with respect to ?(t)</title>
		<imprint/>
	</monogr>
	<note>t)?(t)p(t</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">=</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
