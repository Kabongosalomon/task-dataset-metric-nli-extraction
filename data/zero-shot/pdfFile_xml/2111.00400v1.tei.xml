<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FANS: Fusing ASR and NLU for on-device SLU</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Radfar</surname></persName>
							<email>radfarmr@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Alexa Machine Learning</orgName>
								<address>
									<settlement>Amazon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Mouchtaris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Alexa Machine Learning</orgName>
								<address>
									<settlement>Amazon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Kunzmann</surname></persName>
							<email>kunzman@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Alexa Machine Learning</orgName>
								<address>
									<settlement>Amazon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariya</forename><surname>Rastrow</surname></persName>
							<email>arastrow@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Alexa Machine Learning</orgName>
								<address>
									<settlement>Amazon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FANS: Fusing ASR and NLU for on-device SLU</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Spoken language understanding (SLU)</term>
					<term>Auto- matic speech recognition (ASR)</term>
					<term>Natural language understand- ing (NLU)</term>
					<term>sequence-to-sequence</term>
					<term>neural transformer</term>
					<term>encoder- decoder</term>
					<term>intent-slot</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spoken language understanding (SLU) systems translate voice input commands to semantics which are encoded as an intent and pairs of slot tags and values. Most current SLU systems deploy a cascade of two neural models where the first one maps the input audio to a transcript (ASR) and the second predicts the intent and slots from the transcript (NLU). In this paper, we introduce FANS, a new end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU decoder to infer the intent, slot tags, and slot values directly from a given input audio, obviating the need for transcription. FANS consists of a shared audio encoder and three decoders, two of which are seq-to-seq decoders that predict non null slot tags and slot values in parallel and in an auto-regressive manner. FANS neural encoder and decoders architectures are flexible which allows us to leverage different combinations of LSTM, self-attention, and attenders. Our experiments show compared to the state-of-the-art end-toend SLU models, FANS reduces ICER and IRER errors relatively by 30% and 7%, respectively, when tested on an in-house SLU dataset and by 0.86% and 2% absolute when tested on a public SLU dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spoken language understanding (SLU) systems translate the meaning of a spoken utterance to a format that is readable by machine <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Today's commercial neural SLU systems deploy a cascade of ASR and NLU to infer semantic information from audio <ref type="bibr" target="#b3">[4]</ref>. In these models, after a spoken utterance is transcribed by an ASR module, an NLU system decodes the desired NLU output from the transcript. This disjoint SLU framework has several drawbacks: First, ASR and NLU are optimized independently and NLU performance highly depends on ASR accuracy. Second, many ASR errors occurred in words that have null slot tags and, therefore, have no impact on NLU performance. Accordingly, training ASR to get a full transcript is sub-optimal for NLU. Third, disjoint SLU models restrict the model compression and sharing parameters end-to-end, a requirement for on-device SLU where computational resource is limited.</p><p>In this paper, we go beyond disjoint SLU paradigm and introduce an end-to-end SLU system which is optimized to maximize the NLU accuracy directly from audio. The outputs of an SLU system are often: an intent and two sequences that contain slot tags and values. In recent years, several end-to-end SLU approaches have beed proposed <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref> which can be classified into two broad categories: The models in the first category approach SLU as a multi-label classification problem aiming mostly to predict the intent of a spoken utterance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. In <ref type="bibr" target="#b24">[24]</ref>, stacked layers of biGRU along with MLP are used to classify intent directly from audio files. In <ref type="bibr" target="#b4">[5]</ref>, a SincNet layer <ref type="bibr" target="#b26">[26]</ref> is augmented to the architecture proposed in <ref type="bibr" target="#b24">[24]</ref> and leverage a pre-training strategy to further improve accuracy. In addition to RNNs, convolutional neural networks (CNNs) also have been used alone or joint with RNNs for speech-to-intent classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref>. In <ref type="bibr" target="#b25">[25]</ref>, a cascade of CNN and GRU layers is used with different training strategy called Reptile, to infer the intents. In <ref type="bibr" target="#b27">[27]</ref>, a multi-modal model was proposed in which a pre-trained text embedding incorporated as an auxiliary model along with a biLSTM audio encoder to predict intent.</p><p>The second category of SLU models infer not only intent but also slot tags and slot values. There have been different proposals, though less than the first category, to achieve this goal <ref type="bibr">[12, 17-22, 28, 29]</ref> . In <ref type="bibr" target="#b18">[18]</ref>, an SLU model is proposed that leverages text BERT pre-training that is jointly optimized with ASR. In <ref type="bibr" target="#b19">[19]</ref>, a jointly trained ASR and NLU is proposed by introducing an interface to couple ASR and NLU neural models. In this model, ASR and NLU are first pre-trained and then finetuned on the SLU task. In <ref type="bibr" target="#b30">[30]</ref>, an SLU model is proposed that adapts the pre-trained CTC-and attention-based ASR modules to directly predict NLU output. In <ref type="bibr" target="#b20">[20]</ref>, a transformer-based SLU is proposed which is shown to be a competitive E2E SLU models but this architecture assumes the slot values are associated to a fixed, ordered, pre-defined slot tags. In <ref type="bibr" target="#b21">[21]</ref>, another transformer based SLU model was proposed that leverages selfsupervised pre-trained acoustic features, pre-trained model initialization that allows to use large training data and fine tuned on SLU data. In <ref type="bibr" target="#b11">[12]</ref>, multi-task learning is leveraged for the SLU task. The authors proposed different model architectures to decode ASR and NLU outputs alone or together. In this multitask learning scheme, one task is to decode the transcript and the other task is to decode serialized semantics (slot values and slot tags). They also proposed a direct model to decode the serialized slot tag and slot values.</p><p>In this paper, we propose, FANS, a new end-to-end neural architecture that fuses the ASR encoder directly to the NLU decoder. FANS infers intent, slot tags, and slot value directly from audio with no dependency on transcription. In contrast to <ref type="bibr" target="#b11">[12]</ref>, FANS avoids serialized semantics by deploying two parallel decoders to infer only non null slot tags and their corresponding slot values. In addition, FANS does not use transcription which is used in <ref type="bibr" target="#b11">[12]</ref> as an auxiliary task to help predict semantic. Using attention mechanism, FANS learns to attend to audio segments that are solely relevant to NLU. FANS uses predicted intent as a prior to help better slot tag prediction. FANS model architecture leverages both LSTM and transformer in the encoder and decoder with different attention mechanisms to get best of both worlds. FANS learns parameters that are optimized for the NLU output rather than ASR so that makes FANS more accurate, compact, and a promising candidate for on-device SLU. We evaluate the performance of FANS on both public and inhouse SLU datasets. Our experiments show a significant performance improvement when FANS is compared with the state-of-the-art-models.</p><p>The rest of this paper is organized as follows: In Section 2, we describe and formalize FANS architecture with its variants. In Section 3, we demonstrate the superiority of FANS over a competitive model using both in-house and public datasets. Finally, in Section 4, we draw the conclusions.  <ref type="figure">Figure 1</ref>: The FANS output target consists of an intent and two sequences that contain pair-wise match slot tag and slot values. We discard all slot values associated with slot labeled as Other-note that we also use the term Null interchangeably in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FANS Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">FANS input and output structures</head><p>The input audio signal is transferred to a time-frequency space As such, the target output would be the slot tag y = [y1, . . . , yj, . . . , yM ] T , and the slot valu? z = [z1, . . . ,zj, . . . ,z M ] T , where M is the length of the sequence after discarding N ull tags in the tag sequence and corresponding slot values. In addition to y andz, there is an intent, u ? vocab(U), corresponding to each x. We insert u as the first element intoz to get z = [u;z] T . Hence, we generate two sequences y and z which comprise the output of FANS and together tuple (x, y, z) forms the input and outputs of FANS, where x is a vector of audio features, y a vector of slot values, and z is a vector of slot tag plus intent.</p><formula xml:id="formula_0">represented by x = [x1, . . . , xi, . . . , xT ] T where xi ? R D ,</formula><p>Different from our output structure, in <ref type="bibr" target="#b11">[12]</ref>, the slot tag and slot values are serialized-what the authors called serialized semantics; for our example, the serialized semantics given b? s = [ArtistN ame, Depeche, ArtistN ame, M ode, DeviceLocation, Downstairs] T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">FANS model</head><p>FANS consists of a shared audio encoder and two sequence decoders: slot tag and slot value decoders and one intent decoder whose parameters are shared with the slot tag decoder <ref type="figure" target="#fig_2">(Figure 2)</ref>. The neural structure of the encoder and the decoders can be any types of known neural architectures but here we use different combinations of LSTM and self-attention-self-attention is technically the self-attention mechanism used in Transformers <ref type="bibr" target="#b31">[31]</ref> . In addition, FANS deploys an attender that helps the decoders to attend to the encoder for better prediction. We use both additive attention <ref type="bibr" target="#b32">[32]</ref> and cross-attention <ref type="bibr" target="#b31">[31]</ref>.</p><p>The shared audio encoder transforms x to the higher level feature representations h enc = [h1, . . . , hi, . . . , hT ] T where hi ? R K , and K is the dimension of the encoder output feature space. Given the input vectors, previous decoded outputs, and the model parameters ? = {? enc , ? dec-tag , ? dec-value , ? dec-intent }, FANS infers the slot values, slot tags, and intent by maximizing the following posterior probabilities: *</p><formula xml:id="formula_1">yi = argmax vocab(Y) p(yi|x, fatt(h enc , y?i), ? enc , ? dec-tag ), * zi = argmax vocab(Z) p(zi|x, fatt(h enc , z?i, u), ? enc , ? dec-value ), * u = argmax vocab(U ) p(u|x, h enc , ? enc , ? dec-intent ),<label>(1)</label></formula><p>where the subscript ?i represents all previous decoded tokens before the ith token. Also fatt(?, ?) denotes the attender function which is either the cross attention of the form:</p><formula xml:id="formula_2">fCA(h enc , q?i) = softmax wqq ?i h encT w k T ? K wvh enc (2)</formula><p>where q?i = y?i for the slot value decoder and q?i = z?i for the slot tag decoder; also, wq, w k , and wv are learnable query, key, and value matrices that transform the input vector into a K -dimensional space. The additive attention given by</p><formula xml:id="formula_3">fAA(h enc , qi?1) = T j=1 softmax g(qi?1, h enc j ) h enc j<label>(3)</label></formula><p>where qi?1 = yi?1, the hidden state just before emitting yi for the slot value decoder and qi?1 = zi?1, the hidden state just before emitting zi for the slot tag decoder. Also, g(?) is a feedforward neural network. Given N training samples drawn from distribution p(x, y, z), the model parameters ? are learned by maximizing the likelihood function *</p><formula xml:id="formula_4">? = argmax ? ? log p(Y|X , ? enc , ? dec-tag ) +?1 log p(Z|X , U, ? enc , ? dec-value ) +?2 log p(U|X , ? enc , ? dec-intent )<label>(4)</label></formula><p>where ? = (X , Y, Z, U) is the set of the training data and ?1 and ?2 are two tuning parameters that can control the contribution of each likelihood function during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and evaluation metrics</head><p>We use two SLU datasets: a public and an in-house. Compared to ASR and NLU datasets that contain thousands of hours  of transcribed audio files and numerous text, current popular public SLU datasets have limited audio files. Three popular ones are FSC, containing 30,000 utterances <ref type="bibr" target="#b4">[5]</ref>, SNIP containing 14,000 utterances <ref type="bibr" target="#b33">[33]</ref>, and ATIS, containing 4,500 utterances <ref type="bibr" target="#b34">[34]</ref> which makes training SLU models end-to-end very challenging. We opted for FSC <ref type="bibr" target="#b4">[5]</ref> to be able to leverage endto-end training. This dataset contains 11 intents, 3 slot tags, and slot values belong to a vocabulary of size 100. Because FSC is not annotated for intent, we consider the action labels as intents and the object and location labels as slot tags. The SNIP and ATIS datasets are too small to be used for E2E training so these datasets usually are used as fine tuning datasets when models are pre-trained with large ASR and NLU corpa separately. We also used a subset of 500 hours of de-identified in-house SLU data in which the data were split into train (80%), evaluation (10%) and test (10%) sets. Our SLU dataset contains 57 intents and 175 slot tags and the slot values belong to a vocabulary of size ?100,000. We used 64-dimensional log short time Fourier transform vectors obtained by segmenting the utterances with a Hamming window of the length 25 ms and frame rate of 10 ms. The 3 frames are stacked with a skip rate of 3, resulting in 192-dimensional input features; these features are normalized using the global mean and variance normalization.</p><p>We evaluate the SLU models using two well-known metrics: intent classification error rate (ICER) and interpretation error rate (IRER). These metrics are defined as follows. Let I denote the number of utterances with miss-recognized intent. Let E denote the number of utterances that contains any of missrecognized intent, slot tags, or slot values; e.g., if we have an utterance with one intent, 10 slot values and 10 slot tags and only one of these 21 (10+10+1) entities is miss-recognized, we count one error for the entire utterance. Hence: ICER = I N and IRER = E N where N is the total number of utterances. It should be noted for the in-house SLU data, we report performance in terms of relative increase in ICER (RI-ICER) and IRER (RI-IRER) against the best model, meaning that we set IRER and ICER to zero for the best model and compute the relative increase in ICER and IRER for other models when compared to the best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model parameters</head><p>We built three different versions of FANS by deploying LSTM and self-attention (SA) <ref type="bibr" target="#b31">[31]</ref> neural structures in the encoder and decoders. We also used cross-attention (CA) <ref type="bibr" target="#b31">[31]</ref> or additive attention (AA) <ref type="bibr" target="#b32">[32]</ref> as the attender. We tried to keep all models within the same size to make comparison not biased on the size of the model. We compared FANS with the multi-task and direct A2I models proposed in <ref type="bibr" target="#b11">[12]</ref> .</p><p>The three versions of FAN are as follows: 1-FAN-a: an LSTM based encoder, two LSTM based decoders, and an additive attention attender; 2-FAN-b: an LSTM based encoder, two self-attention based decoders, a cross-attention attender; 3-FAN-c: a self-attention based encoder and two self-attention based decoders, and a cross-attention attender. In FAN-a, we used a 3 ? 612 LSTM in the encoder and two decoders. In FAN-b, we used a 4 ? 768 LSTM in the encoder and 3 layers of self-attention layers in each decoder. In FAN-c, we used 12 self-attention layers in the encoder, 5 self-attention layers in the slot value decoder, and 3 self-attention layers in the slot tag decoder. The input dimension (d-model) was set to 256. For all self-attention layers, we used 4 heads and a 2,048-hidden unit feedforward net is used. For the multi-task and direct A2I in <ref type="bibr" target="#b11">[12]</ref>, the size of the encoder and decoder were set to 3 ? 612 and 4 ? 612, respectively. We trained these models using 500 hours of data in-house data.</p><p>For the FSC dataset, we built smaller models to prevent overfitting as the FSC dataset is very small and not complex. We observed models of size ? 3 millions deliver good results and don't suffer from overfitting. For FSC, the FANS models were built as follows: In FAN-a, we used a 1 ? 256 LSTM in the encoder and two decoders. In FAN-b, we used a 2 ? 232 LSTM in the encode and 2 layers of self-attention layers in each decoder with 1 head and a 800-hidden unit feedforward net. In FAN-c, we used 2 self-attention layers in the encoder, 1 self-attention layer in the slot value decoder, and 1 self-attention layer in the slot tag decoder with 4 heads and a 1024-hidden unit feedforward net. The input dimension (d-model) for all these models was set to 128. For the multi-task and direct A2I in <ref type="bibr" target="#b11">[12]</ref>, the size of the encoder and decoder were set to 1 ? 256 and 1 ? 350 LSTM, respectively.</p><p>The self-attention sub-space dimension (the column dimension of the query, key and value matrices) was set to 64. The dropout rate and label smoothing were set to 0.1. We used Adam optimizer with ?1 = 0.9, ?2= 0.98, and = 1e-9. The learning curve was chosen by setting the pre-defined factor k and warm-up rate w to 0.99 and 16,000, respectively. We used step size of 1000 and the model trained until no improvement observed. For all experiments, ?1 and ?2 were set to 1. For the cascade ASR-NLU model we used 12/4 and 7/2 layers selfattention in the encoder and decoder of ASR for large/small models, respectively. For the NLU model, we used a transformer encoder with 2 layers that was trained on the full transcript and the NLU output including N ull tags . The large models were trained using four machines each of which has eight GPUs and the small models were trained on one GPU. <ref type="table">Table 1</ref>: Results in terms of relative increase in ICER (RI-ICER) and IRER (RI-IRER) against the best model; The last column gives the model size in terms of the number of parameters in million; In this experiments we used our in-house SLU dataset. SA: self-attention, CA: cross attention <ref type="bibr" target="#b31">[31]</ref> and AA: additive attention <ref type="bibr" target="#b32">[32]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Encoder Decoder Attender RI-ICER (%) RI-IRER(%) Size (M) A2I-multi-task <ref type="bibr">[</ref>   <ref type="table">Table 1</ref> and 2 show the RI-ICER/RI-IRER and ICER/IRER results which are obtained by using the in-house and FSC datasets, respectively. As shown in <ref type="table">Table 1</ref>, the best performance is pertained to FANS-c which deploys self-attention both in the encoder and the decoders with a cross attention attenderthis model is basically a transformer with three decoders. We observed 30 % and 7 % relative reduction in ICER and IRER compared to the multi-task A2I <ref type="bibr" target="#b11">[12]</ref>. We also observe averaging the ICER and IRER , all variants of FANS outperform three other models. Similar to what is reported in <ref type="bibr" target="#b11">[12]</ref>, we also observed the A2I direct model underperforms compared to the A2I multi-task model suggesting that in the serialized semantic models having a transcript learner helps the NLU decoder. It is also seen that the joint ASR NLU model <ref type="figure" target="#fig_3">(Figure 3</ref>) exhibits worst performance confirming that the performance of these non E2E SLU models highly depends on separately pretrained ASR and NLU models. A closer look at slot tag and slot values reveals the slot values prediction is more challenging as the number of slot values are far exceeding the number of slot tags. None of the models compared here are pre-trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>We also compared FANS with other models when we used the public FSC SLU dataset as shown in <ref type="table" target="#tab_1">Table 2</ref>. In order to avoid over-fitting, we reduced the size of the models to ? 3 millions. Using the FSC dataset, we observed FANS-b outperforms all other models with 0.86 % and 2.00 % ICER and IRER reduction, respectively. We speculate two reasons as why FANS-b outperforms FANS-c on FSC: First, the utterances in FSC are very short so the attention mechanism in the encoder is not as effective as when we deal with longer utterances contained in the larger datasets. Second, FSC is not semantically complex and has very low entropy with repetitive utterances and small number of intent and slot tags; as such, for this dataset, a smaller model is sufficient to provide satisfactory results. We also observe the gap between ICER and IRER is smaller than large data set; this is however mainly because FSC has much smaller number of slot tags compared to the large dataset (3 vs 175). We also observe IRER for the joint ASR NLU model is strikingly high; one explanation is that the 20 hours FSC is not enough to train the ASR model which requires to deliver transcript to the NLU modules with as low WER as possible. Overall, the results given in <ref type="table">Table 1</ref> and 2 suggest that multitask learners for slot tag and slot values is a better strategy than serializing them into one stream and using one learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we introduce FANS, a new neural architecture for end-to-end SLU. FANS fuses ASR and NLU modules to decode outputs that matters for NLU. FANS obviates the need for recognizing the transcript, allowing decoders to attend to the parts of input acoustics that are relevant for inferring slot tags and slot values. We showed that using different learners for slot tags and slot values leads to better performance compared to when all semantic information serialized. Majority of words in ASR transcripts are associated with null slot tags; therefore what the NLU module expects from the audio encoder is a high level representation for words corresponds to non null slot tags. FANS architecture learns to only focus on these words. We showed through experiments on both in-house data and public datasets that FANS outperforms competitive neural SLU models. The compact structure of FANS and its high accuracy make it a promising candidate for on-device SLU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T and D denote the number of frames and the dimension of the time-frequency space, respectively; also T denotes the transpose operation. Each audio signal, x, has a transcript and a slot tag annotation represented by? = [?1, . . . ,?j, . . . ,?L] T and z = [?1, . . . ,?j, . . . ,?L] T , respectively, where L is the number of words in the transcript and zj ? {vocab(Z) ? N ull}. In other words, each word in the transcript belongs either to a slot tag from a slot tag dictionary or it is labelled as N ull (or Other). An example is demonstrated inFigure 1in which the transcript is? = [play, Depeche, M ode, in, Downstairs] T and? = [N ull, ArtistN ame, ArtistN ame, N ull, DeviceLocation] T . For the SLU task, we are only interested in inferring non N ull slot tags and words in the transcripts associated with non N ull slot tags-we call them slot values. Accordingly, we exclude all N ull tags and words associated with the N ull tags to generate two new sequences: y = [Depeche, M ode, downstairs] T andz = [ArtistN ame, ArtistN ame, DeviceLocation] T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>FANS architecture. The model consists of a shared audio encoder, three decoders, and an attender .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A high-level block-diagram of cascade ASR-NLU SLU models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Resutls in terms of ICER and IRER, and model size; In these experiments, we used public FSC SLU dataset<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="6">Encoder Decoder Attender ICER (%) IRER(%) Size (M)</cell></row><row><cell>A2I-multi-task [12]</cell><cell>LSTM</cell><cell>LSTM</cell><cell>AA</cell><cell>2.05</cell><cell>3.45</cell><cell>2.2</cell></row><row><cell cols="2">A2I-direct-model [12] LSTM</cell><cell>LSTM</cell><cell>AA</cell><cell>2.37</cell><cell>3.28</cell><cell>2.3</cell></row><row><cell>FANS-a</cell><cell>LSTM</cell><cell>LSTM</cell><cell>AA</cell><cell>2.14</cell><cell>3.03</cell><cell>2.2</cell></row><row><cell>FANS-b</cell><cell>LSTM</cell><cell>SA</cell><cell>CA</cell><cell>0.71</cell><cell>1.03</cell><cell>2.3</cell></row><row><cell>FANS-c</cell><cell>SA</cell><cell>SA</cell><cell>CA</cell><cell>2.03</cell><cell>3.9</cell><cell>2.3</cell></row><row><cell>Joint ASR NLU</cell><cell>SA</cell><cell>SA</cell><cell>CA</cell><cell>1.56</cell><cell>14.49</cell><cell>2.31</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="31" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spoken language understanding: Systems for extracting semantic information from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Easy contextual intent prediction and slot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8337" to="8341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spoken content retrieval: A survey of techniques and technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="235" to="422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Capsule networks for low resource spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Renkens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02922</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6189" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recent advances in end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caubri?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bi-directional recurrent neural network with ranking loss for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6060" to="6064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Topic identification for speech without asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07476</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning for context-aware spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adapting pretrained transformer to lattices for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="845" to="852" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dialogue history integration into end-to-end signal-toconcept spoken language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caubriere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06012</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A data efficient end-to-end spoken language understanding architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05955</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale unsupervised pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7999" to="8003" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semisupervised spoken language understanding via self-supervised speech and language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13826</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Speechbert: An audio-and-text jointly learned language model for end-to-end spoken question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shan Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11559</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dheram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06173</idno>
		<title level="m">Speech to semantics: Improve asr and nlu jointly via all-neural interfaces</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end neural transformer based spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Athanasios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siegfried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="799" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-end spoken language understanding using transformer networks and self-supervised pre-trained features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tuske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08238</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring asr-free end-to-end modeling to improve spoken language understanding in a cloudbased dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanaryanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsuprun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Splat: Speechlanguage joint pre-training for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02295</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5754" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving end-to-end speech-to-intent classification with reptile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Gorinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01994</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging unpaired text data for training end-to-end speech-to-intent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7984" to="7988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spoken content retrieval-beyond cascading speech recognition with text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1389" to="1420" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speechbert: An audio-and-text jointly learned language model for end-to-end spoken question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shan Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11559</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Endto-end spoken language understanding without full transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lastras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14386</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of spoken language systems: The atis domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
