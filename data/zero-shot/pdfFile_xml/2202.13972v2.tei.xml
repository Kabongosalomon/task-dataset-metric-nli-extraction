<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The impact of lexical and grammatical processing on generating code from natural language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathana?l</forename><surname>Beau</surname></persName>
							<email>n.beau@groupeonepoint.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Paris</orgName>
								<orgName type="institution" key="instit2">LLF</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">onepoint</orgName>
								<address>
									<addrLine>29 rue des Sablons</addrLine>
									<postCode>F-75116</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Crabb?</surname></persName>
							<email>benoit.crabbe@u-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Paris</orgName>
								<orgName type="institution" key="instit2">LLF</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The impact of lexical and grammatical processing on generating code from natural language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Considering the seq2seq architecture of <ref type="bibr" target="#b18">Yin and Neubig (2018)</ref> for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Translating natural language program descriptions to actual code is meant to help programmers to ease writing reliable code efficiently by means of a set of advanced code completion mechanisms.</p><p>There are mainly two classes of methods for obtaining code corresponding to a query expressed in natural language. The first one is code retrieval, which consists of searching and retrieving an appropriate code snippet from a code database. The second one is code generation, where the goal is to generate code fragments from a natural language description, generating potentially previously unseen code. In this work, we are interested in Python code generation. Code generation features a mismatch between an ambiguous and noisy natural language input and the structured nature of the generated code. Although Python's vocabulary has a finite number of keywords, the set of values that can be assigned to a variable is infinite and constitutes one of the issues in predicting code corresponding to natural language.</p><p>Like many other NLP tasks, current architectures for natural language to code generally take advantage of pre-trained language models such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> or GPT <ref type="bibr">(Brown et al., 2020)</ref> based on the transformer architecture <ref type="bibr" target="#b14">(Vaswani et al., 2017)</ref>. In particular, these architectures are used for code generation where parallel data is limited due to the human expertise required for alignment. The best results on code generation are reached by pretraining seq2seq models on external sources, then by fine-tuning those models on smaller data sets. For instance, <ref type="bibr" target="#b12">Orlanski and Gittens (2021)</ref> fine-tune BART <ref type="bibr" target="#b9">(Lewis et al., 2020)</ref> on data pairs of natural language and code and by taking advantage of external informations. Similarly, <ref type="bibr" target="#b11">Norouzi et al. (2021)</ref> used BERT and a transformer decoder in a semi-supervised way by taking advantage of a large amount of additional monolingual data. Another popular method is to train large language models on code <ref type="bibr" target="#b0">(Austin et al., 2021;</ref><ref type="bibr">Hendrycks et al., 2021)</ref>. Notably, GPT-3 has been finetuned on a large quantity of data from Github to obtain a powerful language model named Codex <ref type="bibr">(Chen et al., 2021</ref>) that powers Github Copilot, a tool to help developers.</p><p>Overall the above mentioned solutions aim to take advantage of large amounts of training data available nowadays, but few of them care about generating code that is guaranteed to be syntactically correct nor well typed. Let us mention some exceptions from semantic parsing like <ref type="bibr" target="#b6">Dong and Lapata (2016)</ref>; <ref type="bibr" target="#b13">Rabinovich et al. (2017)</ref>; <ref type="bibr" target="#b17">Yin and Neubig (2017)</ref> that rely on grammatical constraints to ensure that the generated code can be executable.</p><p>In this work, we study variations around the TranX seq2seq architecture <ref type="bibr" target="#b18">(Yin and Neubig, 2018)</ref> for translating natural language to code. Rather than generating directly code tokens from natural language, the architecture generates an Abstract Syntax Tree (AST) constrained by the programming language grammar.</p><p>The paper reports state of the art results on the task and specifically introduces:</p><p>? A formalization of the grammar constrained code generator relying on the <ref type="bibr" target="#b7">Earley (1970)</ref> parser transition system.</p><p>? A study of the impact of key components of the architecture on the performance of the system: we study the impact of the grammatical component itself, the impact of the language model chosen, the impact of variable naming and typing and the impact of the input/output copy mechanisms.</p><p>It is structured as follows. Section 2 formalizes the symbolic transition system used for generating the grammatically correct code, Section 3 describes a family of variants around the TranX architecture that will be used to study the impact of these variations in the experimental part of the paper (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A transition system for code generation</head><p>Among the models tested in the paper, some are generating syntactically constrained code. In the context of our study, we propose a transition model that meets two objectives: the code generated is grammatically valid in terms of syntax and the whole translation process still reduces to a seq2seq transduction mechanism that allows us to leverage standard machine learning methods. To this end we introduce a transition system for code generation that generates an AST as a sequence of actions. The derivations can then be translated into ASTs and in actual Python code by means of deterministic functions. The set of valid ASTs is a set of trees that are generated by an ASDL grammar <ref type="bibr" target="#b16">(Wang et al., 1997)</ref>. An ASDL grammar is essentially a context free grammar abstracting away from low level syntactic details of the programming language and aims to ease the semantic interpretation of the parse trees. To this end ASDL grammar rules come with additional decorators called constructors and field names ( <ref type="figure">Figure  1</ref>).</p><p>Our transition system generates derivations, or sequences of actions, that can be translated to a syntactically correct Python code. We adapt to code generation the transition system of the Earley parser <ref type="bibr" target="#b7">(Earley, 1970)</ref> as formalized in <ref type="figure" target="#fig_0">Figure  2</ref>. The generator state is a stack of dotted rules. A dotted rule is a rule of the form A ? ??X? where ? is a sequence of grammar symbols whose subtrees are already generated and X? is a sequence of grammar symbols for which the subtrees are yet to be generated. The ?X symbol is the dotted symbol or the next symbol for which the system has to generate the subtree. The Python ASDL grammar includes rules with star ( * ) qualifiers allowing zero or more occurrences of the starred symbol. The transition system uses an additional set of starred actions and a CLOSE action to stop these iterations ( <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>Each <ref type="bibr">PREDICT(C)</ref> action starts the generation of a new subtree from its parent. The GENERATE action adds a new leaf to a tree. The COMPLETE action finishes the generation of a subtree and continues the generation process with its parent. The set of PREDICT actions is parametrized by the ASDL rule constructor (C), thus there are as many predict actions as there are constructors in the ASDL grammar. Constructors are required in order to generate the actual ASTs from the derivations. <ref type="bibr">GENERATE(V)</ref> actions are actions responsible for generating the terminal or primitive symbols. The Python ASDL grammar generates ASTs with primitive leaf types (identifier, int, string, constant) that have to be filled with actual values for the AST to be useful. To generate actual primitive values the set of generate actions is also parametrized by the actual values V for the primitive types. The set of such values is infinite and consequently the set of generate actions is also infinite.</p><p>Non-Determinism comes from the use of PRE-DICT(C), GENERATE(V) and CLOSE rules. By contrast the application of the COMPLETE action is entirely deterministic: once the generator has a completed dotted rule on the top of its stack, it has no other choice than applying the complete rule.</p><p>The sequential generation process is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Given a start state, at each time step, the generator has to decide which action to perform according to the current state of the stack and updates the stack accordingly. Once the generator reaches the goal state, we collect the list of actions performed (the derivation) in order to build the AST that we finally translate into actual Python code 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Factors influencing code prediction</head><p>All architectures analyzed in this study are variations around a seq2seq architecture. We describe the several variants of this architecture used in this paper both on the encoder and decoder side. We identify key factors that have an impact on the natural-language-to-code translation architecture expr = BinOp expr left, operator op, expr right operator = Add expr = Constant constant value expr = List expr * elts <ref type="figure">Figure 1</ref>: Example of ASDL rules for the Python language. Each rule is built from a set of grammatical symbols (in blue), is uniquely identified by a constructor name (in red) and provides names to its right hand side symbols, its fields (in green). Grammatical symbols are split in nonterminals (like expr) and terminals or primitives (like constant). Grammatical symbols can also be annotated with qualifiers ( * ) that allow for zero or more iterations of the symbol. The state of the generator is a stack of dotted rules whose bottom is S. As in the the Earley parser, the PREDICT rule starts the generation of a new subtree by pushing a new dotted rule on the stack, the GENERATE rule adds a leaf to the tree by swapping the top of the stack and the COMPLETE rule attaches a generated subtree into its parent by popping the top two elements of the stack and pushing an updated dotted rule. To handle * qualifiers we add the starred inference rules where COMPLETE * and GENERATE * implement an iteration that stops with the CLOSE * rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Transition Condition</head><formula xml:id="formula_0">START(C) A ? ?? GOAL A ? ?? PREDICT(C) S|A ? ? ? B? ? S|A ? ? ? B?|B ? ?? (B ? ? ? rules) GENERATE(V) S|A ? ? ? t? ? S|A ? ?t ? ? (t ? primitives) COMPLETE S|A ? ? ? B?|B ? ?? ? S|A ? ?B ? ? PREDICT * (C) S|A ? ? ? B * ? ? S|A ? ? ? B * ?|B ? ?? (B ? ? ? rules) GENERATE * (V) S|A ? ? ? t * ? ? S|A ? ?t ? t * ? (t ? primitives) COMPLETE * S|A ? ? ? B * ?|B ? ?? ? S|A ? ?B ? B * ? CLOSE * S|A ? ? ? X * ? ? S|A ? ? ? ?</formula><p>Generator State (stack) Action   <ref type="bibr">,4]</ref>. The derivation starts with expr as axiom symbol and applies transitions until the goal is reached. The list of actions performed is called the generator derivation. Given a generated derivation we can design a straightforward deterministic procedure to translate it into an AST. The actual Python code is generated from the AST by the astor library. and we formalize a family of models that allow to test variations of these factors.</p><formula xml:id="formula_1">expr ? ?expr * START(List) expr ? ?expr * |expr ? ?expr operator expr PREDICT * (BinOp) expr ? ?expr * |expr ? ?expr operator expr|expr ? ?constant PREDICT(Constant) expr ? ?expr * |expr ? ?expr operator expr|expr ? constant? GENERATE(7) expr ? ?expr * |expr ? expr ? operator expr COMPLETE expr ? ?expr * |expr ? expr ? operator expr|expr ? ? PREDICT(Add) expr ? ?expr * |expr ? expr operator ? expr COMPLETE expr ? ?expr * |expr ? expr operator ? expr|expr ? ?constant PREDICT(Constant) expr ? ?expr * |expr ? expr operator ? expr|expr ? constant? GENERATE(5) expr ? ?</formula><p>We consider a family of models generating Python code y from a natural language description x, that have the generic form:</p><formula xml:id="formula_2">p(y|x) = t p(y t |y &lt;t , x)<label>(1)</label></formula><p>y is either a sequence of code tokens in case we do not use a grammar, or a sequence of actions from a derivation in case we use a grammar. The decoding objective aims to find the most-probable hypothesis among all candidate hypotheses by solving the following optimization problem:</p><formula xml:id="formula_3">y = argmax y p(y|x)<label>(2)</label></formula><p>The family of models varies according to four key qualitative factors that we identify in the TranX architecture. First we describe a substitution procedure managing variables and lists names in section 3.1). Second, in section 3.2, we test the architectural variations for encoding the natural language sequence. Third, in section 3.3, we describe variations related to constraining the generated code with grammatical constraints and architectural variations that allow to copy symbols from the natural language input to the generated code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Substitution</head><p>Programming languages come with a wide range of variable names and constant identifiers that make the set of lexical symbols infinite. Rather than learning statistics on a set of ad-hoc symbols, we rather normalize variable and constant names with a pre-processing method, reusing the method of <ref type="bibr" target="#b18">Yin and Neubig (2018)</ref>.</p><p>Preprocessing amounts to substitute the actual names of the variables with a normalized set of predefined names known to the statistical model. The substitution step renames all variables both in the natural language and in the code with conventional names such as var_0, var_1, etc. for variables and lst_0,lst_1, etc. for lists. A post processing step substitutes back the predicted names with the original variable names in the system output. For example, given the natural language intent: create list done containing permutations of each element in list <ref type="bibr">[a, b, c, d]</ref> with variable x as tuples is transformed into: create list var_0 containing permutations of each element in list lst_0 with variable var_1 as tuples</p><p>The predicted code such as var_0 = <ref type="bibr">[(el, var_1)</ref> for el in [lst_0]] is transformed back into done = [(el, x) for el in <ref type="bibr">[a, b, c, d]</ref>].</p><p>Models using variable replacement as illustrated above, are identified with the notation SUBSTITU-TION = TRUE in section 4. Implementing this heuristic is made easy by the design of the CoNaLa data set where all such names are explicitly quoted in the data while for Django we had to detect variable names by comparing natural language with its corresponding code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>We switched between a classic bi-LSTM and a pretrained BERT BASE to encode the input natural language {x i , i ? 1, n } of n words into a vectorial representations {h (enc) i , i ? 1, n } which are later used to compute the attention mechanism. We set the BERT factor to TRUE when using it and FALSE when using the bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>At each time step t, the LSTM decoder computes its internal hidden state h</p><formula xml:id="formula_4">(dec) t : h (dec) t = LSTM([e t?1 : a t?1 ], h (dec) t?1 ) (3)</formula><p>where e t?1 is the embedding from the previous prediction, a t?1 is the attentional vector.</p><p>We compute the attentional vector a t as in <ref type="bibr" target="#b10">Luong et al. (2015)</ref> combining the weighted average over all the source hidden state c t and the decoder hidden state h</p><formula xml:id="formula_5">(dec) t : a t = W a [c t : h (dec) t ]<label>(4)</label></formula><p>It is the attention vector a t which is the key to determine the next prediction y t . We use several variants of the code generator, that we describe by order of increasing complexity. The basic generator is a feed forward that uses the attention vector to generate a code token v from a vocabulary V : <ref type="figure">Figure 4</ref>: Illustration of the seq2seq model with the variables SUBSTITUTION, GRAMMAR, BERT, POINTERNET set to TRUE. We describe here the complete process where we predict a derivation sequence composed of grammar rules and CLOSE (PREDRULE) or Python variables/built-in (GENERATE). The astor library is used to transform the AST constructed with the derivation sequence into Pyton code. In the case where GRAMMAR = FALSE, we only have the GENERATE action which exclusively predicts unconstrained code tokens (as for a classical seq2seq).</p><formula xml:id="formula_6">p(y t = GENERATE[v]|x, e &lt;t ) = softmax(e v ? W g ? a t )<label>(5)</label></formula><p>These models are not constrained by the Python grammar and we identify these models with GRAM-MAR = FALSE. We also use a pointer network that may either copy symbols from input to output or generate symbols from V . Then the probability of generating the symbol v is given by the marginal probability: p(y t = GENERATE[v]|x, e &lt;t ) = p(gen|x, e &lt;t )p(v|gen, x, e &lt;t ) +p(copy|x, e &lt;t )p(v|copy, x, e &lt;t )</p><p>The probabilities p(gen|.) and p(copy|.) sum to 1 and are computed with softmax(W ? a t ). The probability of generating v from the vocabulary V p(v|gen, .) is defined in the same way as (5). We use the pointer net architecture <ref type="bibr" target="#b15">(Vinyals et al., 2015)</ref> to compute the probability p(v|copy, .) of copying an element from the natural language x. Models that use a pointer network are identified with PN = TRUE, otherwise with PN = FALSE .</p><p>Finally we use a set of models that are constrained by the Python grammar and that rely on the transition system from section 2. Rather than directly generating Python code, these models generate a derivation whose actions are predicted using two prediction tasks. When the generator is in a state where the dot of the item on the top of the stack points on a nonterminal symbol, the PREDRULE is used. This task either outputs a PREDICT(C) action or the CLOSE action:</p><formula xml:id="formula_8">p(y t = PREDRULE[c]|x, e &lt;t ) = softmax(e r ? W p ? a t )<label>(7)</label></formula><p>When the generator is in a state where the dot of the item on the top of the stack points on a terminal symbol, the generate task is used. This amounts to reuse either equation (5) or equation <ref type="formula" target="#formula_7">(6)</ref> according to the model at hand. Models constrained by the grammar are labelled with GRAMMAR = TRUE.</p><p>Recall that the COMPLETE action of the transition system is called deterministically (Section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we describe the characteristics of the data sets on which we have tested our different setups and the underlying experimental parameters 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>In this study we use two available data sets, Django and CoNaLa, to perform our code generation task. The Django data set provides line-by-line comments with code from the Django web framework. About 70% of the 18805 examples are simple Python operation ranging from function declarations to package imports, and including exception handling. Those examples strongly share the natural language structure (e.g. call the function cache.close ? cache.close()). More than 26% of the words in the natural language are also present in the code, BLEU score between the natural language and code is equal to 19.4.</p><p>CoNaLa Delete an element 0 from a dictionary 'a') and the associated code is diverse and idiomatic (e.g. {i: a[i] for i in a if (i != 0)}). Compared to Django, the code is much more challenging to generate. Especially because the number of words shared between the NL and the code is much lower (BLEU = 0.32). Also, the code is longer and more complex with an AST depth of 7.1 on average against 5.1 for Django.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vocabulary generation</head><p>The vocabulary of natural language and code is essential. Usually, this vocabulary is created by adding all the words present in the training data set. There are however exceptions that are detailed in this section.</p><p>The natural language vocabulary relies on a byte pair encoding tokenizer when BERT = TRUE. As explained in section 3.1, the variable names are replaced with special tokens var_i and lst_i. These new tokens are crucial to our problem, and added to the BERT vocabulary . We can then finetune BERT with this augmented vocabulary on our data sets.</p><p>For the decoder part, when GRAMMAR = TRUE, the vocabulary of grammatical actions is fixed, while the vocabulary of AST leaves has to be built. This associated vocabulary can be composed of built-in Python functions, libraries with their associated functions or variable names. Its creation is consequently a major milestone in the generation process.</p><p>To create this external vocabulary, we proceed as in TranX. From the code, we create the derivation sequence composed of the action of the grammar as well as the primitives. All primitives of the action sequences are incorporated into our external vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Setup</head><p>When BERT = FALSE, the size of the representations is kept small to prevent overfitting. Encoder and decoder embedding size is set to 128. The hidden layer size of the encoder and decoder bi-LSTM is set to 256 and the resulting attention vector size is 300. We have two dropout layers: for embeddings and at the output of the attention. We use Adam optimizer with learning rate ? = 5.10 ?3 .</p><p>When BERT = TRUE, encoder embeddings have a natural size of 756 with BERT. We therefore apply a linear transformation to its output to get an embedding size equal to 512. The size of LSTM decoder hidden state and attention vector are set to 512. We regularize only the attentional vector in that case. We use Adam optimizer with learning rate ? = 5.10 ?5 . In both cases, we use a beam search size of 15 for decoding.</p><p>Evaluation To compare with previous work, we report the standard evaluation metrics for each data set: exact match accuracy and corpus-level BLEU.</p><p>Python version As the grammar slightly changes between Python versions, let us mention that all our experiments have been carried out with Python 3.7. To highlight the contribution of the different factors, SUBSTITUTION, BERT, GRAMMAR, PN on the Django and CoNaLa data sets we report a detailed study of their impact in <ref type="table" target="#tab_1">Table 1</ref>. The results are analyzed by distinguishing lexical and grammatical aspects and by identifying relations between the different factors. We start by a comparison of the marginal mean of the BLEU score for each of our variables in both conditions. <ref type="figure" target="#fig_3">Figure 5</ref> highlights the mean difference between the conditions by contrasting the case where the value is TRUE with the case where the value is FALSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Pointer network The pointer network can improve the results, especially when SUBSTITUTION = FALSE. This is because the only way to obtain the name of the variables is to copy them. Combined with substitution, the pointer network offers an additional possibility to predict the var_i, lst_i which allows to achieve the best results with a BLEU score of 39.01 on CoNaLa and an exact match accuracy of 76 on Django.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substitution and Typing</head><p>The scores are stabilised and much higher with substitution. We gain more than 9 points of BLEU on CoNaLa (respectively 20 points on Django) thanks to substitution. The "weakest" configuration where all variables are FALSE except the substitution gives better results than all configurations where SUBSTITUTION = FALSE. The increase in BLEU with substitution can be explained in two ways. On the one hand, we remark that the model has difficulties to memorize the val-ues to fill the lists with GENERATE. For example, four tokens of code must be generated to predict the list <ref type="bibr">[a, b, c, d]</ref>. Using substitution, the model can just predict lst_0 which will be replaced by <ref type="bibr">[a, b, c, d]</ref> during postprocessing. This avoids a potential error in the creation of the list and directly gives a valid 4-gram. This contributes to greatly increase the BLEU, which shows the importance of replacing lists. On CoNaLa, BLEU score on the development set drops from an average of 37.99 to an average of 30.66 without list replacement. Besides list replacement, the architecture has also a weakness with respect to variable typing. When using the grammar without substitution, the results are lower than without grammar. This effect is the result of a type checking failure. The model predicts ill-typed AST structures. For instance it predicts an AST whose corresponding code should be 1.append <ref type="bibr">([6,7]</ref>). However the AST library we used prevents from generating such ill-typed code. The absence of code generation in such cases explain the decrease in BLEU score.</p><p>The use of substitution partially corrects for these typing errors because the substituted symbols var_i, lst_i are generally more likely to be predicted and are likely to have the right type thanks to the mapping.</p><p>Grammatical aspect The transition system doesn't improve the results on average because System CoNaLa BLEU CoNaLa accuracy Django BLEU Django accuracy <ref type="bibr" target="#b18">(Yin and Neubig, 2018)</ref> 27.  of the empty predictions when SUBSTITUTION = FALSE. The use of the transition system leads to better results when SUBSTITUTION = TRUE but not as drastically as one would have expected. However the real contribution of the grammar associated with substitution is the syntactic validity of the code in 100% of the cases, as tested with our architecture obtaining the best results. In scenarios where we do not use the grammar, it is never the case to have an empty output. But then the proportion of code sequences that are actually syntactically valid in this setup is 92% on average.</p><p>BERT As expected when using BERT to encode the natural language input we get an improvement of about 6 marginal BLEU on CoNaLa (respectively +3 BLEU on Django). More interestingly, this effect is lower than the one of the substitution operation.</p><p>We conclude that the use of a pre-trained model increases the results but less than substitution, despite what one might think and it suggests that improving the management of variable names and lists is one of the key elements for improving the system. The contribution of grammatical constraints in BLEU may seem detrimental but we could see that this is a side effect of typing constraints in adversarial scenarios. Overall the nonconstrained generated code is syntactically incorrect in 8% of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Test</head><p>We compare in table 2 our results with other systems on CoNaLa and Django test sets. We report our best performing models on the development set with and without grammatical constraints. We also use models trained on the full CoNaLa including mined examples to get relevant comparisons.</p><p>Among the other systems <ref type="bibr" target="#b18">Yin and Neubig (2018)</ref> is the only one that uses grammatical constraints.</p><p>Our architecture differs with the use of a BERT encoder whereas <ref type="bibr" target="#b18">Yin and Neubig (2018)</ref> use an LSTM. The other systems do not use grammatical constraints but rather try to take advantage of additional data. <ref type="bibr" target="#b12">Orlanski and Gittens (2021)</ref> and <ref type="bibr" target="#b11">Norouzi et al. (2021)</ref> aim to take advantage of the CoNaLa mined examples. As these mined examples are noisy, <ref type="bibr" target="#b12">Orlanski and Gittens (2021)</ref> takes advantage of BART <ref type="bibr" target="#b9">(Lewis et al., 2020)</ref>, a denoising encoder. They also enrich the natural language input with the results of queries from StackOverflow by adding the title of the post, its associated tags, etc. <ref type="bibr" target="#b11">Norouzi et al. (2021)</ref> use BERT as encoder and a transformer decoder. They apply the Target Autoencoding method introduced by <ref type="bibr" target="#b4">Currey et al. (2017)</ref>. During training, the encoder parameters are frozen and the decoder is trained to reconstruct code examples. They use this method on the mined examples to take maximal advantage of the additional noisy data.</p><p>We observe that our grammar based model with BERT encoder is state of the art on CoNaLa while the transformer encoder/decoder architecture of <ref type="bibr" target="#b11">Norouzi et al. (2021)</ref> performs best on Django. Quite interestingly the exact match accurracy of these models remain weak on CoNaLa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We formalized a transition system that allows us to guarantee the generation of syntactically correct code. A detailed study of the components of the seq2seq architecture reveals that the models have difficulties at managing accurately variable names and list encodings. The comparison with models trained on larger noisy data sets reveals that our grammatically constrained architecture without explicit denoising remains competitive. This further highlights the importance of grammatical constraints and of specific processes dedicated to manage variables, list naming and typing.</p><p>Finally, we observe that BLEU and exact match, used in this paper, although commonly used in the literature, are not ideal metrics especially because high BLEU scores do not guarantee that the code will be executable. Even exact match is not satifactory since a single natural language query can be solved by several python programs. In future work, we plan to build extensions to the datasets used here with additional test cases assessing the correction of the generated code. These tests are likely to support more relevant metrics for code generation evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An Earley inspired transition system for generating Abstract Syntactic Trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example derivation for the generation of the Python list expression [7+5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is made up of 600k NL-code pairs from StackOverflow, among which 2879 examples have been been manually cleaned up by developers. All results are reported on the manually curated examples, unless stated otherwise. The natural language descriptions are actual developer queries (e.g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Difference between the marginal mean of each variable for the TRUE and FALSE conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>expr * |expr ? expr operator expr? expr ? expr ? expr * |expr ? ?constant PREDICT * (Constant) expr ? expr ? expr * |expr ? constant?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>expr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(List)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>expr:elts</cell><cell>expr:elts</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(BinOp)</cell><cell>(Constant)</cell></row><row><cell></cell><cell></cell><cell>expr:left</cell><cell>operator:op</cell><cell>expr:right</cell><cell>constant:value</cell></row><row><cell></cell><cell></cell><cell>(Constant)</cell><cell>(Add)</cell><cell>(Constant)</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>constant:value</cell><cell></cell><cell>constant:value</cell></row><row><cell></cell><cell>GENERATE(4)</cell><cell>7</cell><cell></cell><cell>5</cell></row><row><cell>expr ? expr expr ? expr  *</cell><cell>COMPLETE  *</cell><cell></cell><cell></cell></row><row><cell>expr ? expr expr?</cell><cell>CLOSE  *</cell><cell></cell><cell></cell></row></table><note>COMPLETE expr ? expr ? expr* COMPLETE*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Substitution BERT Grammar PNCoNaLa BLEU CoNaLa accuracy Django BLEU Django accuracy Performances with different natural language encoders on the development sets with and without a grammatical component. The scores reported are the mean and standard deviation resulting from training with 5 different seeds. The * refers to the use of 100k CoNaLa mined data in addition to clean examples.</figDesc><table><row><cell>False</cell><cell>False True</cell><cell>False True False True</cell><cell>False True False True False True False True</cell><cell>21.05 ? 0.81 22.33 ? 0.78 20.59 ? 0.74 22.16 ? 1.93 30.83 ? 4.08 30.98 ? 1.33 25.88 ? 0.94 28.43 ? 0.64</cell><cell>0.9 ? 0.42 1.7 ? 0.90 2.87 ? 0.48 3.87 ? 1.65 2 ? 0.94 3.3 ? 1.48 3.8 ? 1.96 4.4 ? 1.67</cell><cell>42.58 ? 1.54 64.79 ? 1.00 43.23 ? 1.62 62.55 ? 1.60 53.18 ? 0.87 58.69 ? 1.28 47.32 ? 0.50 52.55 ? 0.51</cell><cell>26.86 ? 1.15 62.85 ? 1.21 30.12 ? 0.63 65.20 ? 0.03 30.28 ? 0.26 37.96 ? 0.27 29.62 ? 0.33 37.38 ? 0.38</cell></row><row><cell></cell><cell>False</cell><cell>False True</cell><cell>False True False True</cell><cell>31.17 ? 0.88 32.10 ? 1.06 33.36 ? 1.63 32.86 ? 1.75</cell><cell>3.1 ? 1.52 3.1 ? 1.24 6.37 ? 0.63 5 ? 1.67</cell><cell>70.4 ? 0.25 70.28 ? 0.38 70.82 ? 0.22 70.62 ? 0.49</cell><cell>70.40 ? 0.29 70.46 ? 0.37 71.3 ? 0.19 71.47 ? 0.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>False</cell><cell>36.43 ? 0.41</cell><cell>4.5 ? 1.84</cell><cell>76.97 ? 0.15</cell><cell>74.58 ? 0.27</cell></row><row><cell>True</cell><cell>True</cell><cell>False</cell><cell>True False</cell><cell>36.29 ? 2.27 35.42 ? 1.75  *  35.04 ? 1.03</cell><cell>5 ? 1.32 5.2 ? 1.33  *  7.3 ? 1.25</cell><cell>76.62 ? 0.50 -76.20 ? 0.46</cell><cell>76 ? 0.71 -74.88 ? 0.56</cell></row><row><cell></cell><cell></cell><cell>True</cell><cell>True</cell><cell>37.99 ? 1.85 39.01 ? 1.08  *</cell><cell>7.5 ? 1.12 7.7 ? 1.92  *</cell><cell>76.32 ? 0.59 -</cell><cell>75.32 ? 1.54 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of the systems trained without external data sources on CoNaLa and Django test sets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the astor library to this end.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The code of our experiments is public and available at https://gitlab.com/codegenfact/BertranX</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Qualitative Examples</head><p>We present examples of code generated by our best models with and without grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>declare an array </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Program synthesis with large language models. CoRR, abs/2108.07732</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<title level="m">Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis</title>
		<editor>Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N</editor>
		<meeting><address><addrLine>Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedant</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mira</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccandlish</surname></persName>
		</author>
		<idno>abs/2107.03374</idno>
		<title level="m">Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Copied monolingual data improves low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heafield</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-4715</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-07" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient context-free parsing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Earley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="102" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge competence with APPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2105.09938</idno>
	</analytic>
	<monogr>
		<title level="j">Collin Burns</title>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1166</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Code generation from natural language with less prior knowledge and more monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.98</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2021-08-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="776" to="785" />
		</imprint>
	</monogr>
	<note>Virtual Event. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reading stackoverflow encourages cheating: Adding question text improves extractive code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Orlanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gittens</surname></persName>
		</author>
		<idno>abs/2106.04447</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1139" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The zephyr abstract syntax description language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Domain-Specific Languages</title>
		<meeting>the Conference on Domain-Specific Languages<address><addrLine>Santa Barbara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10-15" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
