<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How does Disagreement Help Generalization against Label Corruption?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
						</author>
						<title level="a" type="main">How does Disagreement Help Generalization against Label Corruption?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promising for handling noisy labels. This fosters the state-of-the-art approach "Co-teaching" that cross-trains two deep neural networks using the small-loss trick. However, with the increase of epochs, two networks converge to a consensus and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we propose a robust learning paradigm called Co-teaching+, which bridges the "Update by Disagreement" strategy with the original Co-teaching. First, two networks feed forward and predict all data, but keep prediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back propagates the small-loss data from its peer network and updates its own parameters. Empirical results on benchmark datasets demonstrate that Co-teaching+ is much superior to many state-of-theart methods in the robustness of trained models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In weakly-supervised learning, learning with noisy labels is one of the most challenging questions, since noisy labels are ubiquitous in our daily life, such as web queries <ref type="bibr" target="#b18">(Liu et al., 2011)</ref>, crowdsourcing <ref type="bibr" target="#b36">(Welinder et al., 2010)</ref>, medical images <ref type="bibr" target="#b5">(Dgani et al., 2018)</ref>, and financial analysis <ref type="bibr" target="#b0">(A?t-Sahalia et al., 2010)</ref>. Essentially, noisy labels are systematically corrupted from ground-truth labels, which inevitably degenerates the accuracy of classifiers. Such degeneration becomes even more prominent for deep learning models (e.g., convolutional and recurrent neural networks), since Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). these complex models can fully memorize noisy labels <ref type="bibr" target="#b39">(Zhang et al., 2017;</ref><ref type="bibr" target="#b1">Arpit et al., 2017)</ref>.</p><p>To handle noisy labels, classical approaches focus on either adding regularization <ref type="bibr" target="#b23">(Miyato et al., 2016)</ref> or estimating the label transition matrix <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref>. Specifically, both explicit and implicit regularizations leverage the regularization bias to overcome the label noise issue. Nevertheless, they introduced a permanent regularization bias, and the learned classier barely reaches the optimal performance. Meanwhile, estimating the label transition matrix does not introduce the regularization bias, and the accuracy of classifiers can be improved by such accurate estimation. However, the label transition matrix is hard to be estimated, when the number of classes is large.</p><p>Recently, a promising way of handling noisy labels is to train on small-loss instances <ref type="bibr" target="#b13">(Jiang et al., 2018;</ref><ref type="bibr" target="#b29">Ren et al., 2018)</ref>. These works try to select small-loss instances, and then use them to update the network robustly. Among those works, the representative methods are MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> and Co-teaching <ref type="bibr" target="#b9">(Han et al., 2018b)</ref>. For example, MentorNet pre-trains an extra network, and then it uses the extra network for selecting clean instances to guide the training of the main network. When the clean validation data is not available, self-paced MentorNet has to use a predefined curriculum (e.g., small-loss instances). Nevertheless, the idea of self-paced MentorNet is similar to the self-training approach, and it inherits the same inferiority of accumulated error.</p><p>To solve the accumulated error issue in MentorNet, Coteaching has been developed, which simultaneously trains two networks in a symmetric way <ref type="bibr" target="#b9">(Han et al., 2018b)</ref>. First, in each mini-batch data, each network filters noisy (i.e., big-loss) samples based on the memorization effects. Then, it teaches the remaining small-loss samples to its peer network for updating the parameters, since the error from noisy labels can be reduced by peer networks mutually. From the initial training epoch, two networks having different learning abilities can filter different types of error. However, with the increase of training epochs, two networks will converge to a consensus gradually and Coteaching reduces to the self-training MentorNet in function. the training epochs, or how to slow down the speed that two networks will reach a consensus with the increase of epochs. Fortunately, we find that a simple strategy called "Update by Disagreement" <ref type="bibr" target="#b20">(Malach &amp; Shalev-Shwartz, 2017</ref>) may help us to achieve the above target. This strategy conducts updates only on selected data, where there is a prediction disagreement between two classifiers.</p><p>To demonstrate that the "Disagreement" strategy can keep two networks diverged during training, we train two 3-layer MLPs  on MNIST simultaneously for 10 trials, and report total variations of Softmax outputs between two networks in <ref type="figure">Figure 1</ref>. We can clearly observe that two networks trained by Co-teaching (blue in <ref type="figure">Figure 1</ref>) converge to a consensus gradually, while two networks trained by the "Disagreement" strategy (orange in <ref type="figure">Figure 1</ref>) often keep diverged.</p><p>Motivated by this phenomenon, in this paper, we propose a robust learning paradigm called Co-teaching+ ( <ref type="figure" target="#fig_0">Figure 2</ref>), which naturally bridges the "Disagreement" strategy with Co-teaching. Co-teaching+ trains two deep neural networks similarly to the original Co-teaching, but it consists of the disagreement-update step (data update) and the cross-update step (parameters update). Initially, in the disagreement-update step, two networks feed forward and predict all data first, and only keep prediction disagreement data. This step indeed keeps two networks (trained by Co-teaching+) diverged (green in <ref type="figure">Figure 1</ref>). Then, in the crossupdate step, each network selects its small-loss data from such disagreement data, but back propagates the small-loss data from its peer network and updates its own parameters. Intuitively, the idea of disagreement-update comes from In each mini-batch data, each network selects its small-loss data to teach its peer network for the further training.</p><p>Right panel: Co-teaching+ also maintains two networks (A &amp; B). However, two networks feed forward and predict each minibatch data first, and keep prediction disagreement data (!=) only. Based on such disagreement data, each network selects its smallloss data to teach its peer network for the further training.</p><p>Co-training <ref type="bibr" target="#b3">(Blum &amp; Mitchell, 1998)</ref>, where two classifiers should keep diverged to achieve the better ensemble effects. The intuition of cross-update comes from culture evolving hypothesis <ref type="bibr" target="#b2">(Bengio, 2014)</ref>, where a human brain can learn better if guided by the signals produced by other humans.</p><p>We conduct experiments on both simulated and real-world noisy datasets, including noisy MNIST, CIFAR-10, CIFAR-100, NEWS, T-ImageNet and three Open-sets . Empirical results demonstrate that the robustness of deep models trained by the Co-teaching+ approach is superior to many state-of-the-art methods, including Coteaching, MentorNet and F-correction <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref>. Before delving into details, we clearly emphasize our contribution as follows.</p><p>? We denote that "Update by Disagreement" (i.e., the Decoupling algorithm) itself cannot handle noisy labels, which has been empirically justified in Section 3.</p><p>? We realize that the "Disagreement" strategy can keep two networks diverged, which significantly boosts the performance of Co-teaching.</p><p>? We summarize three key factors towards training robust deep networks with noisy labels: (1) using the small-loss trick; (2) cross-updating parameters of two networks; and (3) keeping two networks diverged.</p><p>The rest of this paper is organized as follows. In Section 2, we propose our robust learning paradigm Co-teaching+. Experimental results are discussed in Sections 3 and 4. Conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Co-teaching+: Towards Training of Robust Deep Networks with Noisy Labels</head><p>Similar to Co-teaching, we also train two deep neural networks. As in <ref type="figure" target="#fig_0">Figure 2</ref>, in each mini-batch data, each network conducts its own prediction, then selects instances for which there is a prediction disagreement between two networks. Based on such disagreement data, each network further selects its small-loss data, but back propagates the small-loss data selected by its peer network and updates itself parameters. We call such algorithm as Co-teaching+ (Algorithm 1), which consists of disagreement-update step and cross-update step. This brings the question as follows.</p><p>How does disagreement benefit Co-teaching? To answer this question, we should first understand the main drawback of Co-teaching. In the early stage of training, the divergence of two networks mainly comes from different (random) parameter initialization. Intuitively, this divergence between two networks pushes Co-teaching to become more robust than self-paced MentorNet, since two diverged networks have different abilities to filter different types of error. However, with the increase of training epochs, two networks will gradually converge to be close to each other (blue in <ref type="figure">Figure 1</ref>). Thus, Coteaching degenerates to self-paced MentorNet, and will not promote the learning ability to select clean data any more. To overcome this issue, we need to keep the constant divergence between two networks or slow down the speed that two networks reach a consensus. This intuition comes from Co-training algorithm, where in semi-supervised learning <ref type="bibr" target="#b4">(Chapelle et al., 2009)</ref>, the better ensemble effects require to keep diverged more between two classifiers.</p><p>Fortunately, the "Disagreement" strategy <ref type="bibr" target="#b20">(Malach &amp; Shalev-Shwartz, 2017)</ref> can help us to keep two networks diverged (orange in <ref type="figure">Figure 1</ref>), since this strategy conducts algorithm updates only on selected data, where there is a prediction disagreement between the two classifiers. Therefore, within the whole training epochs, if two networks always select the disagreement data for further training, the divergence of two networks will be always maintained. Specifically, during the training procedure of Co-teaching, if we use the "Disagreement" strategy to keep two networks diverged, then we can prevent Co-teaching reducing to self-training MentorNet in function. This brings us the new robust training paradigm Co-teaching+ (Algorithm 1, green in <ref type="figure">Figure 1</ref>).</p><p>Take "complementary peer learning" as an illustrative example for Co-teaching+. When students prepare for their exams, the peer learning will normally more boost their review efficiency than the solo learning. However, if two students are identically good at math but not good at literature, their review process in literature will have no any Algorithm 1 Co-teaching+.</p><p>Step 4: disagreement-update;</p><p>Step 5-8: cross-update.</p><p>1: Input w (1) and w (2) , training set D, batch size B, learning rate ?, estimated noise rate ? , epoch E k and Emax; for e = 1, 2, . . . , Emax do 2: Shuffle D into |D| B mini-batches; //noisy dataset for n = 1, . . . , |D| B do 3: Fetch n-th mini-batchD from D; 4: Select prediction disagreementD by Eq. (1); 5: GetD (1) = arg min D :|D |??(e)|D | (D ; w (1) ); //sample ?(e)% small-loss instances 6:</p><formula xml:id="formula_0">GetD (2) = arg min D :|D |??(e)|D | (D ; w (2) ); //sample ?(e)% small-loss instances 7: Update w (1) = w (1) ? ?? (D (2) ; w (1) ); //update w (1) byD (2) ; 8: Update w (2) = w (2) ? ?? (D (1) ; w (2) ); //update w (2) byD (1) ; end 9: Update ?(e) = 1 ? min{ e E k ?, ? } or 1 ? min{ e E k ?, (1 + e?E k Emax?E k )? }; end 10: Output w (1) and w (2) .</formula><p>progress. Thus, the optimal peer should be complementary, which means that a student who is good at math should best review with another student who is good at literature. This point also explains why the diverged peer has more powerful learning ability than the identical peer.</p><p>Algorithm description. Algorithm 1 consists of the disagreement-update step (step 4) and the cross-update step (step 5-8), where we train two deep neural networks in a mini-batch manner.</p><p>In step 4, two networks feed forward and predict the same mini-bach of dataD={(x 1 , y 1 ), (x 2 , y 2 ), ? ? ? , (x B , y B )} first, where the batch size is B. Then, they keep prediction disagreement dataD (Eq. (1)) according to their predictions {?</p><formula xml:id="formula_1">(1) 1 ,? (1) 2 , . . . ,? (1) B } (predicted by w (1) ) and {? (2) 1 ,? (2) 2 , . . . ,? (2) B } (predicted by w (2) ): D = {(x i , y i ) :? (1) i =? (2) i },<label>(1)</label></formula><p>where i ? {1, . . . , B}. The intuition of this step comes from Co-training, where two classifiers should keep diverged to achieve the better ensemble effects.</p><p>In step 5-8, from the disagreement dataD , each network w (1) (resp. w (2) ) selects its own small-loss dataD (1) (resp.D (2) ), but back propagates the small-loss dataD (1) (resp.D (2) ) to its peer network w (2) (resp. w (1) ) and updates parameters. The intuition of step 5-8 comes from the aforementioned culture evolving hypothesis <ref type="bibr" target="#b2">(Bengio, 2014)</ref>, where a human brain can learn better if guided by the signals produced by other humans.</p><p>In step 9, we update ?(e), which controls how many smallloss data should be selected in each training epoch. Due to the memorization effects, deep networks will fit clean data first and then gradually over-fit noisy data.</p><p>Thus, at the beginning of training, we keep more smallloss data (with a large ?(e)) in each mini-batch, which is equivalent to dropping less data. Since deep networks will fit clean data first, noisy data do not matter at the initial training epochs. With the increase of epochs, we keep less small-loss data (with a small ?(e)) in each mini-batch. As deep networks will over-fit noisy data gradually, we should drop more data. The gradual decrease of ?(e) prevents deep networks over-fitting noisy data to some degree.</p><p>Similar to Co-teaching, we decrease ?(e) quickly at the first E k epochs to stop networks over-fitting to the noisy data, namely ?(e) = 1 ? e E k ? . However, after E k epochs, Co-teaching+ has two types of ?(e). The first type keeps a constant ?(e), where ?(e) = 1 ? ? ; while the second type further decreases ?(e) slowly, where ?(e) = 1 ? (1 + e?E k Emax?E k )? . We take an example to explain the difference. Assume that the estimated noise rate ? is 30%. It means that, after E k epochs, the first type will constantly fetch 70% small-loss data in each mini-batch as "clean" data. However, the ? estimation tends to be inaccurate in practice. Therefore, given the estimated ? , we should fetch less data, e.g., 60% small-loss data, to keep remained data more clean. This explains why, in real-world noisy datasets, Co-teaching+ chooses the second type to further decrease ?(e) slowly after E k epochs (Section 4).</p><p>Relations to other approaches. We compare our Co-teaching+ with related approaches in <ref type="table">Table 1</ref>. We try to find the connections among them, and pinpoint the key factors that can handle noisy labels. First, self-paced MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> employs the small-loss trick to handle noisy labels. However, this idea is similar to the selftraining approach, and it inherits the same inferiority of accumulated error caused by the sample-selection bias. Inspired by Co-training <ref type="bibr" target="#b3">(Blum &amp; Mitchell, 1998</ref>) that trains double classifiers and cross updates parameters, Coteaching <ref type="bibr" target="#b9">(Han et al., 2018b)</ref> has been developed to cross train two deep networks, which addresses the accumulated error issue in MentorNet. Note that, Co-training does not exploit the memorization in deep neural networks, while Co-teaching does (i.e., leveraging small-loss trick). However, with the increase of training epochs, two networks trained by Co-teaching will converge to a consensus, and Co-teaching will reduce to the self-training MentorNet. This brings us to think how to address the consensus issue in Co-teaching. Although Decoupling algorithm <ref type="bibr" target="#b20">(Malach &amp; Shalev-Shwartz, 2017</ref>) (i.e., "Update by Disagreement") itself cannot combat with noisy labels effectively, which has been empirically justified in Section 3, we clearly realize that the "Disagreement" strategy can always keep two networks diverged. Such divergence effects can boost the performance of Co-teaching and bring us Co-teaching+, since the better ensemble effects require to keep diverged more between two classifiers due to Co-training.</p><p>To sum up, there are three key factors that can contribute to effectively handle noisy labels (first column of <ref type="table">Table 1</ref>). First, we should leverage the memorization effects of deep networks (i.e., the small-loss trick). Second, we should train two deep networks simultaneously, and cross update their parameters. Last but not least, we should keep two deep networks diverged during the whole training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments on Simulated Noisy Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setup</head><p>Datasets. First, we verify the efficacy of our approach on four benchmark datasets <ref type="table" target="#tab_1">(Table 2)</ref>, including three vision datasets (i.e., MNIST, CIFAR-10, and CIFAR-100) and one text dataset (i.e., NEWS). Then, we verify our approach on a larger and harder dataset called Tiny-ImageNet (abbreviated as T-ImageNet) 1 . These datasets are popularly used for the evaluation of learning with noisy labels in the literature <ref type="bibr" target="#b28">(Reed et al., 2015;</ref><ref type="bibr" target="#b6">Goldberger &amp; Ben-Reuven, 2017;</ref><ref type="bibr" target="#b14">Kiryo et al., 2017)</ref>.</p><p>Since all datasets are clean, following <ref type="bibr" target="#b28">(Reed et al., 2015;</ref><ref type="bibr" target="#b25">Patrini et al., 2017)</ref>, we need to corrupt these datasets manually by the label transition matrix Q, where Q ij = Pr(? = j|y = i) given that noisy? is flipped from clean y. Assume that the matrix Q has two representative structures:</p><p>(1) Symmetry flipping (van Rooyen et al., 2015); (2) Pair flipping <ref type="bibr" target="#b9">(Han et al., 2018b)</ref>: a simulation of fine-grained classification with noisy labels, where labelers may make mistakes only within very similar classes.</p><p>Baselines. We compare Co-teaching+ (Algorithm 1) with the following state-of-art approaches, and implement all methods with default parameters by PyTorch, and conduct all the experiments on a NVIDIA Titan Xp GPU.</p><p>(i). MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018</ref>). An extra teacher network is pre-trained and then used to filter out noisy instances for its student network to learn robustly under noisy labels. Then, student network is used for classification. We used self-paced MentorNet in this paper;</p><p>(ii). Co-teaching <ref type="bibr" target="#b9">(Han et al., 2018b)</ref>, which trains two networks simultaneously and cross-updates parameters of peer networks. This method can deal with a large number of classes and is more robust to extremely noisy labels;</p><p>(iii). Decoupling <ref type="bibr" target="#b20">(Malach &amp; Shalev-Shwartz, 2017)</ref>, which <ref type="table">Table 1</ref>. Comparison of state-of-the-art and related techniques with our Co-teaching+ approach. In the first column, "small loss": regarding small-loss samples as "clean" samples, which is based on the memorization effects of deep neural networks; "double classifiers": training two classifiers simultaneously; "cross update": updating parameters in a cross manner instead of a parallel manner; "divergence": keeping two classifiers diverged during the whole training epochs.</p><p>MentorNet Co-training Co-teaching Decoupling Co-teaching+ small loss double classifiers cross update divergence updates the parameters only using the instances which have different prediction from two classifiers.</p><p>(iv). F-correction <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref>, which corrects the prediction by the label transition matrix. As suggested by the authors, we first train a standard network to estimate the transition matrix Q.</p><p>(v). As a simple baseline, we compare Co-teaching+ with the standard deep network that directly trains on noisy datasets (abbreviated as Standard).</p><p>Network structure. For MNIST, we use a 2-layer MLP.</p><p>For CIFAR-10, we use a network architecture with 2 convolutional layers and 3 fully connected layers. For CIFAR-100, the 7-layer network architecture in our paper follows . For NEWS, we borrowed the pre-trained word embeddings from GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>, and a 3-layer MLP is used with Softsign active function. For T-ImageNet, we use a 18-layer Preact ResNet <ref type="bibr">(He et al., 2016)</ref>. The network structure here is standard test bed for weakly-supervised learning, and the details are in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Optimizer. Adam optimizer (momentum=0.9) is with an initial learning rate of 0.001, and the batch size is set to 128 and we run 200 epochs. The learning rate is linearly decayed to zero from 80 to 200 epochs. As deep networks are highly nonconvex, even with the same network and optimization method, different initializations can lead to different local optimal. Thus, following <ref type="bibr" target="#b20">(Malach &amp; Shalev-Shwartz, 2017)</ref>, we also take two networks with the same architecture but different initializations as two classifiers.</p><p>Initialization. Assume that the noise rate ? is known. To conduct a fair comparison in benchmark datasets, we set the ratio of small-loss samples ?(e) as identical as Coteaching:</p><formula xml:id="formula_2">?(e) = 1 ? min{ e E k ?, ? },<label>(2)</label></formula><p>where E k = 10.</p><p>If ? is not known in advanced, ? can be inferred using validation sets <ref type="bibr" target="#b17">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b38">Yu et al., 2018)</ref>. Note that ?(e) only depends on the memorization effect of deep networks but not any specific datasets.</p><p>Measurement. To measure the performance, we use the test accuracy, i.e., test accuracy = (# of correct predictions) / (# of test dataset). Intuitively, higher test accuracy means that the algorithm is more robust to the label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with the State-of-the-Arts</head><p>Results on MNIST. <ref type="figure" target="#fig_1">Figure 3</ref> shows test accuracy vs. number of epochs on MNIST. In all three plots, we can clearly see the memorization effects of deep networks. For example, test accuracy of Standard first reaches a very high level since deep network will first fit clean labels. Over the increase of epochs, deep network will over-fit noisy labels gradually, which decreases its test accuracy accordingly. Thus, a robust training method should alleviate or even stop the decreasing trend in test accuracy.</p><p>In the easiest Symmetry-20% case, all new approaches work better than Standard obviously, which demonstrates their robustness. Co-teaching+ and F-correction work significantly better than Co-teaching, MentorNet and Decoupling. However, F-correction cannot combat with the other two harder cases, i.e., Pair-45% and Symmetry-50%. Especially in the hardest Pair-45% case, F-correction can learn nothing at all, which greatly restricts its practical usage in the wild. Besides, in two such cases, Co-teaching+ achieves higher accuracy than Co-teaching and MentorNet.</p><p>Results on CIFAR-10. <ref type="figure">Figure 4</ref> shows test accuracy vs. number of epochs on CIFAR-10. Similarly, we can clearly see the memorization effects of deep networks, namely test accuracy of Standard first reaches a very high level then decreases gradually. In the easiest Symmetry-20% case, Co-teaching+ works much better than all other baselines,  where F-correction works similar to MentorNet but a bit worse than Co-teaching.</p><p>However, F-correction cannot combat with two harder cases easily, i.e., Pair-45% and Symmetry-50%. In the Symmetry-50% case, F-correction works better than Standard and Decoupling, but worse than Co-teaching and Co-teaching+. In the hardest Pair-45% case, F-correction almost learns nothing. In such two harder cases, our Co-teaching+ consistently achieves higher accuracy than Coteaching and MentorNet.</p><p>Results on CIFAR-100. <ref type="figure">Figure 5</ref> shows test accuracy vs. number of epochs on CIFAR-100. Similarly, we can clearly see the memorization effects of deep networks, namely test accuracy of Standard first reaches a very high level then decreases gradually. In the easiest Symmetry-20% case, Co-teaching+ and F-correction work significantly better than Co-teaching, MentorNet and Decoupling.</p><p>However, F-correction cannot combat with two harder cases easily, i.e., Pair-45% and Symmetry-50%. In the Symmetry-50% case, F-correction works better than Standard and Decoupling, but worse than the other three approaches. In the hardest Pair-45% case, F-correction almost learns nothing. In such two harder cases, our Co-teaching+ consistently achieves higher accuracy than Coteaching and MentorNet. An interesting phenomenon is, in the easiest case, Co-teaching+ not only fully stop the decreasing trend in test accuracy, but also performs better and better with the increase of epochs. Results on NEWS. To verify Co-teaching+ comprehensively, we conduct experiments not only on vision datasets, but also on text dataset NEWS. <ref type="figure" target="#fig_2">Figure 6</ref> shows test accuracy vs. number of epochs on NEWS.</p><p>Similar to results on vision datasets, we can still see the memorization effects of deep networks in all three plots, i.e., test accuracy of Standard first reaches a very high level and then gradually decreases. However, Co-teaching+ mitigates such memorization issue, and works much better than others across three cases. Note that F-correction cannot combat with all three cases, even in the easiest Symmetry-20% case. This interesting phenomenon in Fcorrection does not occur in vision datasets.</p><p>Results on T-ImageNet. To verify our approach on a complex scenario,  . Specifically, Open-sets are built by replacing some training images in CIFAR-10 by outside images, while keeping the labels and the number of images per class unchanged. The "mislabeled" images come from different outside datasets, including CIFAR-100, ImageNet-32 (32 ? 32 ImageNet images) and SVHN. Note that outside images whose labels exclude 10 classes in CIFAR-10 are considered.</p><p>Network &amp; Optimizer &amp; Initialization. We follow the experimental settings in . Specifically, we use a network architecture with 6 convolutional layers and 1 fully-connected layer, and its details can be found in the third column of <ref type="table" target="#tab_2">Table 3</ref>. Batch normalization (BN) is applied in each convolutional layer before the ReLU activation, and a max-pooling layer is implemented every two convolutional layers. All networks are trained by Stochastic Gradient Descent (SGD) with learning rate 0.01, weight decay 10 ?4 and momentum 0.9, and the learning rate is divided by 10 after 40 and 80 epochs (100 in total).</p><p>Note that Open-sets are real-world noisy datasets. To handle these complex scenarios, we should set the ratio of small-loss samples ?(e) as follows.</p><formula xml:id="formula_3">?(e) = 1 ? min{ e E k ?, (1 + e ? E k E max ? E k )? },<label>(3)</label></formula><p>where E k = 10 and E max = 200.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-Arts</head><p>Results on three Open-sets. Following , we report the classification accuracy on CIFAR-10 noisy datasets with 40% open-set noise in <ref type="table">Table 5</ref>. The Standard and Iterative results are borrowed from . For MentorNet, Co-teaching and Co-teaching+, we report the averaged/maximal test accuracy over the last 10 epochs. As can be seen, our approach outperforms other baselines on all three open-set noisy datasets. For CIFAR-100 noise and ImageNet-32 noise, both Co-teaching and Co-teaching+ are better than Iterative. For SVHN noise, Co-teaching+ is significantly better than Iterative; while MentorNet and Co-teaching also work better than Iterative.</p><p>Reflection of results. Different algorithm designs lead to different results. To sum up, self-paced MentorNet is concluded as training single deep network using the smallloss trick. Co-teaching moves further step, which is viewed as cross-training double deep networks using the smallloss trick. Based on Co-teaching, Co-teaching+ is regarded as cross-training double diverged deep networks using the small-loss trick. Thus, keeping two deep networks diverged is one of the key ingredients to train robust deep networks. This point has been empirically verified by the result difference between Co-teaching and Co-teaching+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a robust learning paradigm called Co-teaching+, which trains deep neural networks robustly under noisy supervision. Our key idea is to maintain two networks simultaneously that find the prediction disagreement data. Among such disagreement data, our method cross-trains on data screened by the "small loss" criteria. We conduct experiments to demonstrate that, our proposed Co-teaching+ can train deep models robustly with the extremely noisy supervision beyond Co-teaching and MentorNet. More importantly, we summarize three key points towards training robust deep networks with noisy labels: (1) using small-loss trick based on memorization effects of deep networks; (2) cross-updating parameters of two networks; and (3) keeping two deep networks diverged during the whole training epochs. In future, we will investigate the theory of Co-teaching+ from the view of disagreement-based algorithms <ref type="bibr" target="#b34">(Wang &amp; Zhou, 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of error flow among MentorNet (M-Net), Co-teaching and Co-teaching+. Assume that the error flow comes from the selection of training instances, and the error flow from network A or B is denoted by red arrows or blue arrows, respectively. Left panel: M-Net maintains only one network (A). Middle panel: Co-teaching maintains two networks (A &amp; B) simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Test accuracy vs. number of epochs on MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Test accuracy vs. number of epochs on NEWS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To address the consensus issue in Co-teaching, we should consider how to always keep two networks diverged within arXiv:1901.04215v3 [cs.LG] 12 May 2019</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Disagreement</cell><cell></cell><cell cols="2">Co-teaching</cell><cell></cell><cell cols="2">Co-teaching+</cell></row><row><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total Variation</cell><cell>0.6 0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row></table><note>Figure 1. Comparison of divergence (evaluated by Total Varia- tion) between two networks trained by the "Disagreement" strat- egy, Co-teaching and Co-teaching+, respectively. Co-teaching+ naturally bridges the "Disagreement" strategy with Co-teaching.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of data sets used in the experiments.</figDesc><table><row><cell></cell><cell cols="3"># of train # of test # of class</cell><cell>size</cell></row><row><cell>MNIST</cell><cell>60,000</cell><cell>10,000</cell><cell>10</cell><cell>28?28</cell></row><row><cell>CIFAR-10</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell><cell>32?32</cell></row><row><cell>CIFAR-100</cell><cell>50,000</cell><cell>10,000</cell><cell>100</cell><cell>32?32</cell></row><row><cell>NEWS</cell><cell>11,314</cell><cell>7,532</cell><cell>7</cell><cell>1000-D</cell></row><row><cell cols="2">T-ImageNet 100, 000</cell><cell>10, 000</cell><cell>200</cell><cell>64?64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>MLP and CNN models used in our experiments on MNIST, CIFAR-10, CIFAR-100/Open-sets, and NEWS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MLP on MNIST</cell><cell></cell><cell></cell><cell cols="5">CNN on CIFAR-10</cell><cell></cell><cell cols="6">CNN on CIFAR-100/Open-sets</cell><cell></cell><cell></cell><cell cols="3">MLP on NEWS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">28?28 Gray Image</cell><cell></cell><cell></cell><cell cols="5">32?32 RGB Image</cell><cell></cell><cell></cell><cell cols="4">32?32 RGB Image</cell><cell></cell><cell></cell><cell></cell><cell cols="3">1000-D Text</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">3?3 Conv, 64 BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell cols="4">300-D Embedding</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">5?5 Conv, 6 ReLU</cell><cell></cell><cell cols="5">3?3 Conv, 64 BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Flatten ? 1000?300</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">2?2 Max-pool</cell><cell></cell><cell></cell><cell></cell><cell cols="4">2?2 Max-pool</cell><cell></cell><cell></cell><cell cols="6">Adaptive avg-pool ? 16?300</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">3?3 Conv, 128 BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Dense 28?28 ? 256, ReLU</cell><cell></cell><cell cols="5">5?5 Conv, 16 ReLU</cell><cell></cell><cell cols="6">3?3 Conv, 128 BN, ReLU</cell><cell></cell><cell cols="5">Dense 16?300 ? 4?300</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">2?2 Max-pool</cell><cell></cell><cell></cell><cell></cell><cell cols="4">2?2 Max-pool</cell><cell></cell><cell></cell><cell></cell><cell cols="3">BN, Softsign</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">3?3 Conv, 196 BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Dense 16?5?5 ? 120, ReLU</cell><cell cols="6">3?3 Conv, 196 BN, ReLU</cell><cell></cell><cell></cell><cell cols="4">Dense 4?300 ? 300</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Dense 120 ? 84, ReLU</cell><cell></cell><cell></cell><cell cols="4">2?2 Max-pool</cell><cell></cell><cell></cell><cell></cell><cell cols="3">BN, Softsign</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dense 256 ? 10</cell><cell></cell><cell></cell><cell cols="4">Dense 84 ? 10</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Dense 256 ? 100/10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dense 300 ? 7</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Standard</cell><cell></cell><cell cols="4">Decoupling</cell><cell></cell><cell></cell><cell cols="2">F-correction</cell><cell></cell><cell></cell><cell cols="3">MentorNet</cell><cell></cell><cell cols="4">Co-teaching</cell><cell></cell><cell cols="4">Co-teaching+</cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(MNIST, Pair-45%)</cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell cols="4">(MNIST, Symmetry-50%)</cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(MNIST, Symmetry-20%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>0.40 0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.70 0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.85 0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell>0.50</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell>0.75</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Pair-45%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Symmetry-50%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Symmetry-20%.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Test accuracy vs. number of epochs on CIFAR-10 dataset. Test accuracy vs. number of epochs on CIFAR-100 dataset.</figDesc><table><row><cell>shows averaged/maximal test</cell></row><row><cell>accuracy on T-ImageNet over last 10 epochs. As we can</cell></row><row><cell>see, for both Symmetry cases, Co-teaching+ is the best. For</cell></row><row><cell>the Pair case, Co-teaching and Co-teaching+ outperform</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Averaged/maximal test accuracy (%) of different approaches on T-ImageNet over last 10 epochs. The best results are in bold. Averaged/maximal test accuracy (%) of different approaches on Open-sets over last 10 epochs. The best results are in bold.</figDesc><table><row><cell>Flipping-Rate(%)</cell><cell></cell><cell>Standard</cell><cell cols="2">Decoupling F-correction</cell><cell>MentorNet</cell><cell>Co-teaching Co-teaching+</cell></row><row><cell>Pair-45%</cell><cell cols="3">26.14/26.32 26.10/26.61</cell><cell>0.63/0.67</cell><cell cols="2">26.22/26.61 27.41/27.82</cell><cell>26.54/26.87</cell></row><row><cell>Symmetry-50%</cell><cell cols="6">19.58/19.77 22.61/22.81 32.84/33.12 35.47/35.76 37.09/37.60</cell><cell>41.19/41.77</cell></row><row><cell>Symmetry-20%</cell><cell cols="6">35.56/35.80 36.28/36.97 44.37/44.50 45.49/45.74 45.60/46.36</cell><cell>47.73/48.20</cell></row><row><cell>Open-set noise</cell><cell></cell><cell>Standard</cell><cell>MentorNet</cell><cell cols="3">Iterative (Wang et al., 2018) Co-teaching Co-teaching+</cell></row><row><cell cols="2">CIFAR-10+CIFAR-100</cell><cell>62.92</cell><cell>79.27/79.33</cell><cell></cell><cell>79.28</cell><cell>79.43/79.58</cell><cell>79.28/79.74</cell></row><row><cell cols="2">CIFAR-10+ImageNet-32</cell><cell>58.63</cell><cell>79.27/79.40</cell><cell></cell><cell>79.38</cell><cell>79.42/79.60</cell><cell>79.89/80.52</cell></row><row><cell>CIFAR-10+SVHN</cell><cell></cell><cell>56.44</cell><cell>79.72/79.81</cell><cell></cell><cell>77.73</cell><cell>80.12/80.33</cell><cell>80.62/80.95</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">CAI, University of Technology Sydney 2 RIKEN-AIP 3 Alibaba Damo Academy 4 University of Tokyo. Correspondence to: Xingrui Yu &lt;xingrui.yu@student.uts.edu.au&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://tiny-imagenet.herokuapp.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>MS was supported by JST CREST JPMJCR18A2. IWT was supported by ARC FT130100746, DP180100106 and LP150100671. XRY was supported by China Scholarship Council No. 201806450045. We gratefully acknowledge the support of NVIDIA Corporation with the donation of Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-frequency covariance estimates with noisy and asynchronous financial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>A?t-Sahalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">492</biblScope>
			<biblScope unit="page" from="1504" to="1517" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolving culture versus local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Growing Adaptive Machines</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training a neural network based on unreliable human annotation of medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dgani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISBI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Positive-unlabeled learning with non-negative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Noise resistant graph ranking for improved web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Decoupling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the design of loss functions for classification: theory, robustness to outliers, and savageboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from corrupted binary labels via classprobability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual adversarial training for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Class proportion estimation with application to multiclass anomaly rejection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04403</idno>
		<title level="m">Theoretical foundation of co-training and disagreement-based algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="291" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An efficient and provable approach for mixture proportion estimation using linear independence assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>How does Disagreement Help Generalization against Label Corruption?</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistical learning contributed a lot to the problem of learning with noisy labels, especially in theoretical aspects. Statistical learning approaches can be categorized into three strands: surrogate loss, noise rate estimation and probabilistic modeling. For example, in the surrogate losses category</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the noise rate estimation category, both Menon et al. (2015) and Liu &amp; Tao</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>A. Related literature Statistical learning methods. proposed a class-probability estimator using order statistics on the range of scores</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">) proposed a unified framework to distill the knowledge from clean labels and knowledge graph, which can be exploited to learn a better model from noisy labels. Veit et al. (2017) trained a label cleaning network by a small set of clean labels, and used this network to reduce the noise in large-scale noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott ; Raykar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the probabilistic modeling category</title>
		<editor>Rodrigues &amp; Pereira</editor>
		<imprint>
			<publisher>Zhang &amp; Sabuncu</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>) proposed a human-assisted approach that conveys human cognition of invalid class transitions, and derived a structure-aware deep probabilistic model incorporating a speculated structure prior. robust decision boundary under any softmax neural classifier pre-trained on noisy datasets. Their idea is to induce a generative classifier on top of hidden feature spaces of the discriminative deep model</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">For other datasets, we use a warmup strategy to achieve a higher test accuracy. Specifically, for CIFAR-10, we warm-up Co-teaching+ with training Coteaching for the first 20 epochs (i.e., only conducting cross-update for the first 20 epochs). For CIFAR-100, we warm-up Co-teaching+ with training Co-teaching for the first 5 epochs. For T-ImageNet, we start disagreement-update in the middle of training, i.e., we warm-up Co-teaching+ with training Co-teaching for the first 100 epochs. For Open-sets, we warm-up Co-teaching+ with training two networks in parallel for the first 55 epochs, where both networks leverage the small-loss trick. Inevitably, there is few chance that we cannot find enough small-loss instances for cross-update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">For</forename><surname>Mnist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>News</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>We Train Co-Teaching+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>By</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In that case, we only conduct disagreement-update in a mini-batch data during training</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
