<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SYNTACTICALLY GUIDED GENERATIVE EMBEDDINGS FOR ZERO-SHOT SKELETON ACTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<settlement>Hyderabad</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyanshu</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<settlement>Hyderabad</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran Sarvadevabhatla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<settlement>Hyderabad</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SYNTACTICALLY GUIDED GENERATIVE EMBEDDINGS FOR ZERO-SHOT SKELETON ACTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-ZSL</term>
					<term>skeleton action recognition</term>
					<term>VAE</term>
					<term>deep learning</term>
					<term>language and vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce SynSE, a novel syntactically guided generative approach for Zero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined generative embedding spaces constrained within and across the involved modalities (visual, language). The inter-modal constraints are defined between action sequence embedding and embeddings of Parts of Speech (PoS) tagged words in the corresponding action description. We deploy SynSE for the task of skeleton-based action sequence recognition. Our design choices enable SynSE to generalize compositionally, i.e., recognize sequences whose action descriptions contain words not encountered during training. We also extend our approach to the more challenging Generalized Zero-Shot Learning (GZSL) problem via a confidence-based gating mechanism. We are the first to present zero-shot skeleton action recognition results on the largescale NTU-60 and NTU-120 skeleton action datasets with multiple splits. Our results demonstrate SynSE's state of the art performance in both ZSL and GZSL settings compared to strong baselines on the NTU-60 and NTU-120 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Advances in human action recognition have been predominantly driven by the abundance of online RGB videos. However, with the advent of accurate depth sensing technologies (e.g. Microsoft Kinect, Intel Real Sense), action recognition from 3D human skeleton data has also gained traction. Skeleton representations can be advantageous since they are compact, robustly separate the action subject (human) from background and enable privacy-preserving action capture.</p><p>The introduction of large scale skeleton action datasets such as NTU-60 <ref type="bibr" target="#b0">[1]</ref> and NTU-120 <ref type="bibr" target="#b1">[2]</ref>, have allowed researchers to develop high-performance approaches for skeleton action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. However, these approaches are resource intensive, prone to overfitting and fail to generalize on classes outside the training set. Therefore, there is a strong motivation for Zero-Shot Learning (ZSL) approaches in an attempt to readily generalize across actions outside the training set.</p><p>In ZSL, visual representations and corresponding labels for seen classes are assumed to be available. During test time, the model is evaluated using data from unseen classes which are not present during training. Typically, side information (e.g. class attributes) is leveraged to transfer knowledge from the seen to unseen classes. As a popular approach, ZSL approaches employ a shared embedding strategy wherein the visual (image or video) features and semantic attribute features of the corresponding class labels are projected into a common embedding space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Generative ZSL approaches present an alternative strategy wherein unseen samples <ref type="bibr" target="#b13">[14]</ref> or features from unseen samples <ref type="bibr" target="#b10">[11]</ref> are generated using Generative Adversarial Networks(GANs). Owing to the instability in training GANs, Variational Auto-Encoders(VAEs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> have also been used for feature generation.</p><p>ZSL has been previously explored for skeleton action recognition. In the only available work <ref type="bibr" target="#b17">[18]</ref> (arXiv), embedding based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> are used to align visual feature embedding of skeleton action sequence with the text embedding of the descriptive action phrase (e.g 'take off jacket', 'put on glasses'). The visual features are represented by the final layer features of a skeleton action recognition model and the action phrase embedding is typically obtained by pooling the individual embeddings of words comprising the phrase. However, this approach does not enable alignment of visual embedding with respect to the individual contributors of phrase semantics -the verb ('action') and the noun(s) ('participating entities'). This inability is a major shortcoming since it does not enable generalization, i.e., being able to map the test action sequences to a description containing novel combinations of verbs and nouns, some of which might be from training action descriptions themselves.</p><p>To address these shortcomings, we propose an approach wherein the visual embedding is aligned based on the Parts of Speech (PoS) tags (verb, noun) of the phrasal words. Instead of directly mapping the visual and PoS-wise embeddings in a discriminative setting <ref type="bibr" target="#b19">[20]</ref>, we use group (per-PoS, visual) specific generative models with cross-group latent objective <ref type="bibr" target="#b16">[17]</ref> for improved ZSL performance (Section 2). We also extend our approach to the Generalized Zero-Shot Learning (GZSL) problem, a more challenging and realistic variant of ZSL wherein good performance is required from seen and unseen classes. We do so by incorporating a confidence-based gating mechanism. (Section 2.5). Our approach enables state-of-theart performance for ZSL and GZSL compared to strong baselines on the NTU-60 and the much larger NTU-120 dataset. (Section 4).</p><p>The source code and pre-trained models can be accessed at https://github.com/skelemoa/synse-zsl.   <ref type="figure">Fig. 1</ref>. Architectural diagram for our approach (SynSE). (left) The dotted path represents the process flow for the GZSL setting while the solid arrows represent the flow for ZSL. The Generative Multimodal Alignment Module is detailed on right side. It contains modality VAEs, where Part-of-Speech (PoS) specific latent generative embeddings zv (verb), zn (noun) are jointly aligned with segments (zs,v, zs,n) of latent generative skeleton embedding zs via cross-modal alignment -refer Section 2 for more details. Note that the RGB images have been included for reference. Only the skeleton sequence is provided as input to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYNSE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem definition</head><p>quence, y tr s ? Ys is the corresponding member from the label set of seen classes. On similar lines, Du = {(x u s , y u s )} denotes the set of test samples with the subscript u standing for unseen. Suppos? y represents the test time class prediction. For ZSL, we have? ? Yu, Ys ?Yu = ? while for GZSL, we have? ? Yu ?Ys, Ys ?Yu = ?. For simplicity, we drop the subscript for seen, unseen and refer to the class names as y and the visual feature embedding as xs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning modality-wise latent generative spaces</head><p>A crucial requirement for a ZSL approach is the ability to correctly map novel inputs. For this, we employ a Variational Auto Encoder (VAE) <ref type="bibr" target="#b20">[21]</ref> as the base architecture to learn the generative space of latent representations. A VAE is trained by maximizing the Evidence Lower Bound (ELBO):</p><formula xml:id="formula_0">L = E q ? (z|x) [log p ? (x|z)] ? ?DKL(q ? (z|x)||p ? (z))<label>(1)</label></formula><p>Here, the first term on the right hand side is the reconstruction error and the second term is the Kullback-Leibler divergence between likelihood p ? (z) and the prior q ? (z|x). ? is a hyperparameter which acts as a trade-off between the two error terms. A popular choice for the prior is the multivariate Gaussian distribution, q ? (z|x) = N (?, ?). The VAE maps the input x initially to representations for ?, ? and eventually to the randomized latent representation z via the reparameterization trick <ref type="bibr" target="#b20">[21]</ref>.</p><p>The first stage in our approach involves learning individual latent generative latent spaces for visual and linguistic representations. This is achieved by using a VAE for each space. To enable semantically aware compositional generalization, the text description for class label y is tokenized into constituent Part-of-Speech (PoS) specific sets -yv for verb and yn for noun. The tokens are encoded using a natural language encoder module to obtain the corresponding PoS-wise embeddings ev and en (see <ref type="figure">Figure 1</ref>). Since our approach employs independent VAEs for skeleton (s) and linguistic (v, n) representations, the overall cost function for a single sample can be written as:</p><formula xml:id="formula_1">LV AE = m?{s,v,n} E q ? (zm|xm) [log p ? (xm|zm)]? ?DKL(q ? (zm|xm)||p ? (zm)) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross-modal alignment</head><p>The VAEs optimize latent representations for individual modalities. To achieve alignment between the skeleton sequence and linguistic latent representations, a cross-modal reconstruction objective is formulated <ref type="bibr" target="#b16">[17]</ref>. First, the latent embeddings from the PoS embeddings (zv, zn) are concatenated (see z l in <ref type="figure">Figure 1</ref>) and the result x l is used to reconstruct the visual representation via the skeleton representation VAE's posterior decoder Ds. Next, the skeleton sequence latent embedding (zs) is uniformly mapped to as many embeddings (zs,v, zs,n) as the number of PoS tags. Complementary to the processing of z l , each of the split embedding is used to reconstruct the corresponding PoS token embedding (en, ev) via the corresponding PoS token embedding's decoder (Dv, Dn). Overall, the cross-modal reconstruction objective for a training sample is formulated as:</p><formula xml:id="formula_2">LCMR = |xs ? Ds(z l )|2 + m?{v,n} |em ? Di(zs,m)|2<label>(3)</label></formula><p>Finally, the VAE loss and the cross-modal reconstruction loss are optimized together as:</p><formula xml:id="formula_3">L = LV AE + ?LCMR<label>(4)</label></formula><p>where ? is a trade-off weight factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Zero-shot Classification using Latent Embedding</head><p>The PoS tag embeddings of each unseen class are respectively transformed by the PoS encoders (Ev, En) and used to obtain samples from the latent generative space (z l -see <ref type="figure">Figure 1</ref>). A softmax classifier fu : z l ? Yu is trained to classify these latent samples into the unseen classes.  <ref type="table">Table 1</ref>. ZSL accuracy (%) on the NTU-60 and NTU-120 datasets.</p><p>The cross-modal VAE setup described earlier aims to align the visual features with the language features in the common latent generative space. In other words, zs and z l are optimized to be interchangeable. Taking advantage of this, during inference, the unseen class skeleton sequence representation xs is first obtained. Supplying xs to the visual VAE encoder (Es) enables us to obtain the mean visual latent embedding (?s) of the sequence 1 . The corresponding class prediction is obtained using ?s and the classifier fu mentioned previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Gating Module for GZSL</head><p>For a given skeleton sequence representation xs, the probability distribution cs over seen classes is obtained from the skeleton action recognition model fs : xs ? Ys from which the action sequence embedding has been sourced all along. The unseen class classifier fu : Es(xs) ? Yu, is a part of our ZSL approach described in the previous section which provides the unseen class probabilities cu. The probability distribution over all the classes can be written as: p(y|x) = csp gate (s; cs, cu) + cup gate (u; cs, cu)</p><p>Further, we use a gating model (due to its superior performance for GZSL in other domains) to first decide whether the sample belongs to a seen class or an unseen class <ref type="bibr" target="#b21">[22]</ref>. For this, the seen and unseen class probabilities are used as features to train a probabilistic binary classifier p gate (s; cs, cu) <ref type="bibr" target="#b21">[22]</ref>. The resulting outputs are used to determine the probability distribution over all (Ys ? Yu) classes (Equation 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Implementation Details</head><p>Visual and Textual features: The visual features xs, are realised using the 256 dimensional penultimate layer feature from 4s-ShiftGCN <ref type="bibr" target="#b2">[3]</ref>, a state-of-the-art deep network for skeleton action recognition. To maintain the zero-shot assumption, we train 4s-ShiftGCN only on the seen classes. We use the Sentence BERT model <ref type="bibr" target="#b22">[23]</ref> to obtain 1024-dimensional PoS-wise word embeddings. Before splitting into verbs and nouns, the class names are modified to fill the missing PoS tag, e.g. 'reading' is changed to 'reading book', 'drop' to 'drop object', 'headache' to 'have headache'. For actions where adding the missing tag (usually a noun) would be unreasonable (e.g. 'jump up', 'stand up'), the average of all noun embeddings is used as a placeholder. Architectural Details: We have a single dense layer as the encoder (Es) and decoder (Ds), which map the input features (xs, ev, en) to the latent space (zs, zv, zn) and vice versa. xs is 256-dimensional and ev, en are 1024-dimensional. The size of the latent dimension is based on the number of unseen classes. For small number <ref type="bibr" target="#b4">(5)</ref> of unseen classes, the skeleton latent dimension is set as 100 and the latent dimension for the PoS tags is 50. For larger number of unseen classes, the latent dimensions are doubled to 200 and 100 for the skeleton latent dimension and PoS tags respectively. The ZSL classifier has a single dense layer which takes latent features as input and returns the softmax probabilities for unseen classes. Training Details: The VAEs within the Generative Multimodal Alignment Module are optimized using the Adam optimizer with a learning rate of 1e ?4 and a batch size of 64. The VAEs are trained using a cyclic annealing schedule <ref type="bibr" target="#b23">[24]</ref> for multiple cycles to mitigate the vanishing KL divergence problem. The ? hyperparameter for the KL divergence is turned on after 1000 epochs, starting with 0 and is increased with a rate of 0.0021 per epoch in each cycle. Similarly, the ? parameter for the cross modal reconstruction is turned on after 1400 epochs for experiments on NTU-60 and 1500 epochs on NTU-120 and is kept constant with a value of 1. One cycle is completed in 1700 epochs for NTU-60 and 1900 epochs for NTU-120 dataset. The zero-shot classifier (Section 2.4) is also optimized using Adam with a learning rate for 1e ?3 . 500 features per unseen class are generated and the classifier is trained for 300 epochs.</p><p>The input to the gating model (Section 2.5) is the concatenation of the top k softmax probabilities from the outputs of the seen and unseen classifiers. We set k equal to the number of unseen classes and we temperature scale <ref type="bibr" target="#b24">[25]</ref> the seen classifier probabilities as well. The gating model is implemented as a binary logistic regression classifier and optimized using LBFGS solver from the scikit-learn library with the default aggressiveness hyperparameter (C = 1). For training the gating model, we set aside a few samples from the training set and refer to them as the gating train set. Similarly, we set aside a few samples from validation set (gating validation set). We train the gating model using the gating train set and determine the hyperparameters (temperature coefficient, threshold) using the gating validation set. The gating module is configured for use in 'hard' gating mode wherein p gate (s; cs, cu) and p gate (u; cu, cu) in Equation 5 take binary values <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU-60 [1]</head><p>: This is a large-scale dataset curated for 3D human action analysis. It contains 56,880 samples belonging to 60 action classes, with 40 different subjects captured from 80 distinct camera viewpoints. The action sequences of skeleton representations are in the form of 3D coordinates for 25 human body joints. We create two splits for ZSL evaluation -a 55/5 split with 55 seen classes, 5 randomly chosen unseen classes and a more challenging 48/12 split. NTU-120 <ref type="bibr" target="#b1">[2]</ref>: NTU-120 builds upon NTU-60 and contains 60 additional fine-grained action classes. It contains a total of 114,480 samples spread across 120 actions performed by 106 different subjects captured from 155 different camera viewpoints. Analogous to the NTU-60 ZSL evaluation setup, we create two splits -110 (seen)/10 (unseen) and 96 (seen)/24 (unseen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Details</head><p>We perform ZSL and GZSL experiments on the NTU-60 and NTU-120 datasets on the described splits. Since no previous works for skeleton ZSL exist, we modify representative state-of-the-art approaches from other problem domains and implement from scratch.   <ref type="table">Table 3</ref>. SynSE ZSL accuracy (%) on the NTU-60 dataset for various ablations (55/5 split).</p><p>CADA-VAE <ref type="bibr" target="#b16">[17]</ref> learns a generative latent space under a cross aligned and distribution aligned objective. Since we found the distribution alignment objective to induce instability in training, we omit it during optimization. ReViSE <ref type="bibr" target="#b11">[12]</ref> aims to align the latent embeddings realised via autoencoders using a Maximum Mean Discrepancy criterion. JPose <ref type="bibr" target="#b19">[20]</ref> attempts to learn PoS aware embeddings of word2vec representations for video retrieval tasks. It learns a series of progressively refined embeddings under inter/intra modal constraints in a discriminative setting. For fair comparison, the visual features and PoS embeddings are the same as ones used in our approach (Section 2.6). <ref type="table">Table 1</ref> shows the ZSL results of the various approaches on the NTU-60 and NTU-120 datasets. For the 55/5 split of NTU-60, the VAE-based generative approaches significantly outperform the discriminative embedding based approaches. SynSE's performance is comparable to that of CADA-VAE. Predictably, results on the more challenging 48/12 split show that having a larger number of unseen classes impacts performance across the board. However, SynSE offers significant improvement over other baseline approaches, including CADA-VAE. On the larger NTU-120 dataset, SynSE outperforms other methods on both the splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ZSL results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GZSL results</head><p>Since we use a gating-based strategy for GZSL in SynSE (Section 2.5), we compare against other baselines by incorporating the same strategy. Specifically, the seen class classifier is kept the same while the specific baseline approach provides the corresponding unseen class probabilities. Following standard convention for GZSL, we report the average seen class accuracy (s), the average unseen class accuracy (u) and their harmonic mean (h). <ref type="table" target="#tab_3">Table 2</ref> shows the results for datasets and the associated pre-defined splits. Similar to the trend in ZSL for the 55/5 split of NTU-60, SynSE performs poorer compared to CADA-VAE on the harmonic scale for the 55/5 NTU-60 split. However, it outperforms other approaches on the harmonic scale for other splits of NTU-60 and NTU-120. We also compare our hard gating strategy <ref type="bibr" target="#b26">[27]</ref> with the soft gating based strategy <ref type="bibr" target="#b21">[22]</ref>. The results in <ref type="table" target="#tab_3">Table 2</ref> show that soft gating is biased towards seen classes, resulting in poor harmonic accuracy. Additionally, <ref type="table" target="#tab_3">Table 2</ref> also shows the significant performance hit when temperature scaling (Sec. 2.6) is removed <ref type="bibr" target="#b24">[25]</ref>.</p><p>To further demonstrate the effectiveness of our GZSL strategy (i.e. gating model), we explored an alternative based on the approach used for CADA-VAE <ref type="bibr" target="#b16">[17]</ref>, which does not involve gating. As <ref type="table" target="#tab_3">Table 2</ref> shows, the resulting setup ends up too heavily skewed for seen classes and is unable to classify the unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>We perform ablation experiments on the 55/5 split of the NTU-60 dataset to analyse the importance of the building blocks of our alignment module and design choices affecting its ZSL performance. As the results show <ref type="table">(Table 3)</ref>, Sentence-BERT is a superior choice to Word2Vec <ref type="bibr" target="#b25">[26]</ref> for embedding PoS-tagged words. Similarly, 4s-ShiftGCN provides better visual embeddings compared to another state-of-the-art skeleton action recognition model MS-G3D <ref type="bibr" target="#b3">[4]</ref>. We further ablate on the architectural choices by varying the size of the latent dimension. As shown in <ref type="table">Table 3</ref>, we see that both an increase and decrease in the size of the latent embedding causes reduction in ZSL performance. In order to validate our choice of 500 latent features per class, we experiment with varying number of latent features with results as shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we have presented SynSE, a compositional approach for infusing latent visual representations of skeleton-based human actions with syntactic information derived from corresponding textual descriptions. We present the first set of zero-shot skeleton action recognition results on the large-scale NTU-60 and NTU-120 datasets. Our experiments show that SynSE outperforms strong baselines for ZSL and the more challenging GZSL setup. Going forward, we would like to explore the viability of SynSE for zero-shot RGB video action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Let</head><label></label><figDesc>Dtr = {(x tr s , y tr s )} denote the set of Ntr training samples where x tr s denotes visual feature embedding of a skeleton action se-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>GZSL</figDesc><table><row><cell></cell><cell cols="3">Accuracy (%) for seen (s) classes, unseen (u) classes and their harmonic mean (h) on NTU-60 and NTU-120 datasets</cell></row><row><cell>Component</cell><cell>Default in SynSE</cell><cell>Ablation</cell><cell>Accuracy</cell></row><row><cell cols="3">Language Embedding Sentence-BERT [23] Word2Vec [26]</cell><cell>60.76</cell></row><row><cell>Visual Features</cell><cell>4s-ShiftGCN [3]</cell><cell>MS-G3D [4]</cell><cell>68.80</cell></row><row><cell>Latent Dimension</cell><cell>100</cell><cell>50</cell><cell>73.83</cell></row><row><cell>Latent Dimension</cell><cell>100</cell><cell>200</cell><cell>74.67</cell></row><row><cell>Latent features</cell><cell>500</cell><cell>250</cell><cell>73.89</cell></row><row><cell>Latent features</cell><cell>500</cell><cell>1000</cell><cell>73.82</cell></row><row><cell></cell><cell>original</cell><cell></cell><cell>75.81</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that zs = ?s + ?s E, where E = N (0, I) by the VAE reparameterization trick.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning robust visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Kang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep matching autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06047</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundeep</forename><surname>Vinay Kumar Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4281" to="4289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generative model for zero shot learning using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2188" to="2196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generalized zero-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">red</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavan</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshaan</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11344</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple partsof-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive confidence smoothing for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cyclical annealing schedule: A simple approach to mitigating kl vanishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10145</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
