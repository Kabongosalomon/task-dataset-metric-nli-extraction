<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PSUMNet: Unified Modality Part Streams are All You Need for Efficient Pose-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
							<email>neel.trivedi@research.</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran Sarvadevabhatla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PSUMNet: Unified Modality Part Streams are All You Need for Efficient Pose-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human action recognition</term>
					<term>skeleton</term>
					<term>dataset</term>
					<term>human activ- ity recognition</term>
					<term>part</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>25 joints ? Only Body ? 22 joints ? Only Fingers ? 67 joints ? Body + Fingers PSUMNet Part based Skeleton Dataset Dense Skeleton Dataset Sparse Skeleton Dataset Efficiency Generalizability NTU-X NTU RGB+D SHREC Fig. 1: The plot on left shows accuracy against # parameters for our proposed architecture PSUMNet ( ) and existing approaches for the large-scale NTURGB+D 120 human actions dataset (cross subject). PSUMNet achieves state of the art performance while competing recent methods use 100%-400% more parameters. The diagram on right illustrates that PSUMNet scales to sparse pose (SHREC [6]) and dense pose (NTU-X [26]) configurations in addition to the popular NTURGB+D[15] configuration.</p><p>Abstract. Pose-based action recognition is predominantly tackled by approaches which treat the input skeleton in a monolithic fashion, i.e. joints in the pose tree are processed as a whole. However, such approaches ignore the fact that action categories are often characterized by localized action dynamics involving only small subsets of part joint groups involving hands (e.g. 'Thumbs up') or legs (e.g. 'Kicking'). Although part-grouping based approaches exist, each part group is not considered within the global pose frame, causing such methods to fall short. Further, conventional approaches employ independent modality streams (e.g. joint, bone, joint velocity, bone velocity) and train their network multiple times on these streams, which massively increases the number of training parameters. To address these issues, we introduce PSUMNet, a novel approach for scalable and efficient pose-based action recognition. At the representation level, we propose a global frame based part stream approach as opposed to conventional modality based streams. Within each part stream, the associated data from multiple modalities is unified and consumed by the processing pipeline. Experimentally, PSUMNet achieves state of the art performance on the widely arXiv:2208.05775v1 [cs.CV] 11 Aug 2022 2 N. Trivedi et al.</p><p>used NTURGB+D 60/120 dataset and dense joint skeleton dataset NTU 60-X/120-X. PSUMNet is highly efficient and outperforms competing methods which use 100%-400% more parameters. PSUMNet also generalizes to the SHREC hand gesture dataset with competitive performance. Overall, PSUMNet's scalability, performance and efficiency makes it an attractive choice for action recognition and for deployment on computerestricted embedded and edge devices. Code and pretrained models can be accessed at https://github.com/skelemoa/psumnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fig. 2: Comparison between conventional training procedure used in most of the previous approaches (left) and our approach (right). Conventional methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">16]</ref> use dedicated independent streams and train separate instances of the same network for each of the four modalities, i.e joint, bone, joint velocity and bone velocity. This method increases the number of total parameters by a huge margin and involves a monolithic representation. Our method processes the modalities in a unified manner and creates part group based independent stream with a superior performance compared to existing methods which use 100%-400% more parameters -see <ref type="figure" target="#fig_0">Fig. 3</ref> for architectural details of PSUMNet.</p><p>Skeleton based human action recognition at scale has gained a lot of focus recently, especially with the release of large scale skeleton action datasets such as NTURGB+D <ref type="bibr" target="#b20">[19]</ref> and NTURGB+D 120 <ref type="bibr" target="#b15">[15]</ref>. A plethora of RNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">9]</ref>, CNN <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b10">10]</ref> and GCN <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b17">16]</ref> based approaches have been proposed to tackle this important problem. The success of approaches such as ST-GCN <ref type="bibr" target="#b29">[28]</ref> which modeled spatio-temporal joint dynamics using GCN has given much prominence to GCN-based approaches. Furthermore, approaches such as RA-GCN <ref type="bibr" target="#b24">[23]</ref> and 2s-AGCN <ref type="bibr" target="#b22">[21]</ref> built upon this success and demonstrated additional gains by introducing multi modal (bone and velocity) streams -see <ref type="figure" target="#fig_1">Fig. 2</ref> (left). This multi stream approach has been adopted as convention by state of the art approaches.</p><p>However, the conventional setup has three major drawbacks. First, each modality stream is trained independently and the results are combined using late (decision) fusion. This deprives the processing pipeline from taking advantage of correlations across modalities. Second, with addition of each new modality, the number of parameters increase by a significant margin since a separate network with the same model architecture is trained for each modality. Third, the skeleton is considered in a monolithic fashion. In other words, the entire input pose tree at each time step is treated as a whole and at once. This is counter intuitive to the fact that a lot of action categories often involve only a subset of the available joints. For example, action categories such as "Cutting paper" or "Writing" can be easily identified using only hand joints whereas action categories such as "Walking" or "Kicking" can be easily identified using only leg joints. Additionally, monolithic processing increases compute requirements when the number of joints in the pose representation increases <ref type="bibr" target="#b27">[26]</ref>. Non-monolithic approaches which decompose the pose tree into disjoint part groups do exist <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b25">24]</ref>. However, each part group is not considered within the global pose frame, causing such methods to fall short.</p><p>Our proposed approach tackles all of the aforementioned drawbacks -see <ref type="figure" target="#fig_1">Fig. 2</ref> (right). Our contributions are the following:</p><p>-We propose a unified modality processing approach as opposed to conventional independent modality approaches. This enables a significant reduction in the number of parameters. (Sec. 3.2) -We propose a part based stream processing approach which enables richer and dedicated representations for actions involving a subset of joints (Sec. 3.1). The part stream approach also enables efficient generalization to dense joint (NTU-X <ref type="bibr" target="#b27">[26]</ref>) and small joint (SHREC <ref type="bibr" target="#b5">[6]</ref> The high accuracy provided by PSUMNet, coupled with its efficiency in terms of compute (number of parameters and floating-point operations) makes our approach an attractive choice for real world deployment on compute restricted embedded and edge devices -see <ref type="figure">Fig. 1</ref>. Code and pretrained models can be accessed at https://github.com/skelemoa/psumnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Skeleton action recognition: Since the release of large scale skeleton based datasets <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b15">15]</ref> various CNN <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b14">14]</ref>, RNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b32">31]</ref> and recently GCN based methods have been proposed for skeleton action recognition. ST-GCN <ref type="bibr" target="#b29">[28]</ref> was the first successful approach to model the spatio-temporal relationships for skeleton actions at scale. Many state of the art approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b1">2]</ref> have adopted and modified this approach to achieve superior results. However, these approaches predominantly process the skeleton joints in a monolithic manner, i.e these approaches process the entire input skeleton at once which can create a bottleneck when the input skeleton becomes denser, e.g. NTU-X <ref type="bibr" target="#b27">[26]</ref>.</p><p>Part based approaches: The idea of grouping skeleton joints into different groups has few precedents. Du et al. <ref type="bibr" target="#b7">[8]</ref> propose a RNN-based hierarchical grouping of part group representations. Thakkar et al. <ref type="bibr" target="#b26">[25]</ref> propose a GCN based approach which applies modified graph convolutions to different part groups. Huang et al. <ref type="bibr" target="#b12">[12]</ref> propose a GCN-based approach in which they utilize the higher order part level graph for better pooling and aligning of nodes in the main skeleton graph. More recently, Song et al. <ref type="bibr" target="#b25">[24]</ref> propose a part-aware GCN method which utilizes part factorization to aid an attention mechanism to find the most informative part. Some previous part based approaches segment the limbs based on left and right orientation as well (left/right arm, left/right leg etc.) <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>. Such segmentation leads to disjoint part groups which contain very small number of joints and are unable to convey useful information. In contrast, our part stream approach creates overlapping part groups with sufficient number of joints to model useful relationships. Also, each individual part group in our setup is registered to the global frame unlike the per-group coordinate system setup in existing approaches. In addition, we employ a combination of part group and coarse version of the full skeleton instead of part-group only approach seen in existing approaches. Our part stream approach allows each part based sub-skeleton to contribute towards the final prediction via decision fusion. To the best of our knowledge, such globally registered independent part stream approach has never been used before.</p><p>Multi stream Training: Earlier approaches <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">16]</ref> and more recent approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">27]</ref> create multiple modalities termed joint, bone and velocity from the raw input skeleton data. The conventional method is to train the given architecture multiple times using different modality data followed by decision fusion. However, this conventional approach with multiple versions of the base architecture greatly increases the total number of parameters. Song et al. <ref type="bibr" target="#b25">[24]</ref> attempt a unified modality pipeline wherein early fusion of different modality streams is used to achieve a unified modality representation. However, before the fusion, each modality is processed via multiple independent networks which again increases the count of trainable parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We first describe our approach for factorizing the input skeleton into part groups and a coarser version of the skeleton (Sec. 3.1). Subsequently, we provide the architectural details of our deep network PSUMNet which processes these part streams (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Part Stream Factorization</head><p>Let X (? R 3?T ?N ) represent the T -frames, N -joint skeleton configuration of a 3D skeleton pose sequence for an action. We factorize X into following three part groups -see 1. Coarse body (X b ): This is comprised of all joints in the original skeleton for NTURGB+D skeleton topology, 25 joints in total. For the 67-joint dense skeleton topology of NTU-X <ref type="bibr" target="#b27">[26]</ref>, this stream comprises of all the body joints but without the intermediate joints of each finger for each hand. Specifically, only 6 joints out of 21 finger joints are considered per hand resulting in total of 37 joints for NTU-X. 2. Hands (X h ): This contains all the finger joints in each hand and the arm joints. Note that the arms are rooted at the throat joint. For NTURGB+D dataset, the number of joints for this stream is 13 and for NTU-X, the total number of joints is 48. 3. Legs (X l ): This includes all the joints in each of the legs. The leg joints are rooted at the hip joint. For NTURGB+D dataset the number of joints for this stream is 9 and for NTU-X, the total number of joints is 13.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (right), the part group sub skeletons are used to train three correspodning independent streams of our proposed PSUMNet ( Sec. 3.2). As explained previously, our hypothesis is that many of the action categories are dominated by certain part groups and hence can be classified using only a subset of the entire input skeleton. To leverage this, we perform late decision fusion by performing a weighted average of the prediction scores from each of the part streams to obtain the final classification. Crucially, we change the number of layers in each of the streams in proportion to number of input joints. We use 10, 6 and 4 layers respectively for body, hands and legs streams. This helps restrict the total number of parameters used for the entire protocol.</p><p>In contrast with other part based approaches, the part groups in our setting are not completely disjoint. More crucially, the part groups are defined with respect to a shared global coordinate space. Though seemingly redundant due to multiple common joints across part groups, this design choice actually enables global motion information to propagate to the corresponding groups. Another significant advantage of such part stream approach is the better scalability to much denser skeleton datasets such as NTU-X <ref type="bibr" target="#b27">[26]</ref> and to sparser datasets such as SHREC <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PSUMNet</head><p>In what follows, we explain the architecture of a single part stream of PSUMNet (e.g. X = X b ) since the architecture is shared across the part streams. An overview of PSUMNet's single stream architecture can be seen in <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>. First, the input skeleton X is passed through Multi Modality Data Generator (MMDG) to create a rich modality aware representation. This feature representation is processed by Spatio-Temporal Relation Module (STRM). Global average pooling (GAP ) of the processed result is transformed via fully connected layers (FC ) to obtain the per-layer prediction for the single part stream.</p><p>Next, we provide details for various modules included in our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi Modality Data Generator (MMDG)</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 3</ref> (b), this module processes the raw skeleton data and generates the corresponding multi modality data, i.e. joint, bone, joint-velocity and bonevelocity. The joint modality is the raw skeleton data represented by X = {x ? R C?T ?N }, where C, T and N are channels, time steps and joints. The bone modality data is obtained using the following equation:</p><formula xml:id="formula_0">X bone = {x[:, :, i] ? x[:, :, i nei ] | i = 1, 2, ..., N }<label>(1)</label></formula><p>where i nei denotes neighboring joint of i based on predefined adjacency matrix. Next we create joint-velocity and bone-velocity modality data using following equations:</p><formula xml:id="formula_1">X joint?vel = {x[:, t + 1, :] ? x[:, t, :] | t = 1, 2, ..., T, x ? X joint } (2) X bone?vel = {x[:, t + 1, :] ? x[:, t, :] | t = 1, 2, ..., T, x ? X bone }<label>(3)</label></formula><p>Finally, we concatenate all these four modality data into channel dimension to generate X ? R 4C?T ?N which is fed as input to the network. Concatenating the modality data helps model the inter-modality relations in a more direct manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Spatio Temporal Relational Module (STRM)</head><p>The modality aware representation obtained from MMDG is processed by the Spatial Temporal Relational Module (STRM) as shown in <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>. STRM consists of multiple Spatio Temporal Relational Blocks (STRB) stacked one after another. The architecture of a single STRB is shown in <ref type="figure" target="#fig_0">Fig. 3 (c)</ref>. Each STRB block contains a Spatial Attention Map Generator (SAMG) to dynamically model different spatial relations between joints followed by Temporal Relational Module (TRM) to model temporal relations between joints. Spatial Attention Map Generator (SAMG): We dynamically model an Spatial Attention Map for the spatial graph convolutions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">20]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 3 (d)</ref>, we pass the input skeleton through two parallel branches, each consisting a 1 ? 1 convolution and a temporal pooling block. We pair-wise subtract outputs from the parallel branches to model the Attention Map. We add the predefined adjacency matrix A as a regularization to the Attention Map to generate the final hybrid adjacency matrix A hyb , i.e.</p><formula xml:id="formula_2">A hyb = ?M (X in ) + A<label>(4)</label></formula><p>where ? is a learnable parameter and A is the predefined adjacency matrix. M is defined as:</p><formula xml:id="formula_3">M (X i ) = ?(TP(?(X in )) ? TP(?(X in )))<label>(5)</label></formula><p>where ?, ? and ? are 1x1 convolutions, TP is temporal pooling.</p><p>Once we obtain this adjacency matrix A hyb , we pass the original input through a 1 ? 1 convolution and multiply the results with the dynamic adjacency matrix to characterize the spatial relations between the joints as follows:</p><formula xml:id="formula_4">X out = A hyb (?(X in ))<label>(6)</label></formula><p>where ? is 1x1 convolution block. is matrix multiplication operation. Temporal Relation Module (TRM): We use multiple parallel convolution blocks to model the temporal relation between the joints of the input skeleton as shown in <ref type="figure" target="#fig_0">Fig. 3 (e)</ref>. Each temporal convolution block is a standard 2D convolution with varying kernel sizes in temporal dimension and with dilation. This helps model temporal relations at multiple scales. The outputs from each of the temporal convolution blocks are concatenated. The result is processed by GAP and FC layers and mapped to a prediction (softmax) layer as mentioned previously.</p><p>Since each part group (body, hands, legs) contains significantly different number of joints, we adjust the number of STRBs and depth of the network for each stream accordingly as shown in <ref type="figure" target="#fig_1">Fig. 2 (Right)</ref>. This design choice provides two advantages. First, it reduces the total number of parameters by 50%-80%. Second, adjusting the depth of the network in proportion to the joint count enables richer dedicated representations for actions whose dynamics are confined to the corresponding part groups, resulting in better performance overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NTURGB+D <ref type="bibr" target="#b20">[19]</ref> is a large scale skeleton action recognition dataset with 60 different actions performed by 40 different subjects. The dataset contains 25 joints human skeleton captured using Microsoft Kinect V2 cameras. There are a total of 56,880 action sequences. There are two evaluation protocols for this dataset -First, Cross Subject (XSub) split where action performed by 20 subjects falls into training set and rest into the test set. Second, Cross View (XView) protocol where actions captured via camera ID 2 and 3 are used as training set and actions captured via camera ID 1 are used as test set. NTURGB+D 120 <ref type="bibr" target="#b15">[15]</ref> is an extension of NTURGB+D dataset with additional 60 action categories and a total of 113,945 action samples. The actions are performed by a total of 106 subjects. There are two evaluation protocols for this dataset -First, Cross Subject (XSub) split where action performed by 53 subjects falls into training set and rest into the test set. Second, Cross Setup (XSet) protocol where actions even setup IDs are used as training set and rest as test set. NTU60-X <ref type="bibr" target="#b27">[26]</ref> is a RGB derived skeleton dataset for the sequences of the original NTURGB+D dataset. The skeleton provided in this dataset is much denser and contains 67 joints. There are total of 56,148 action samples and the evaluation protocols are same as the NTURGB+D dataset. NTU120-X <ref type="bibr" target="#b27">[26]</ref> is the extension of NTU60-x dataset and corresponds to the action sequences provided by NTURGB+D 120 dataset. There are total of 113,821 samples in this dataset and the evaluation protocols are same as the NTURGB+D 120 dataset. Following <ref type="bibr" target="#b27">[26]</ref>, we evaluate our model on only Cross Subject protocol of NTU60-X and NTU120-X datasets.</p><p>SHREC <ref type="bibr" target="#b5">[6]</ref> is a 3d skeleton based hand gesture recognition dataset. There are a total of 2800 samples with 1960 samples in train set and 840 samples in test set. Each samples has 20-50 frames and gestures are performed by 28 participants ones using only one finger and ones using the whole hand. There are 14 gestures and 28 gestures splits provided by the creators and we report results on both of these splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Optimization details</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2 (right)</ref>, the input skeleton to each of the part stream contains different number of joints. For NTURGB+D dataset, the body stream has input skeleton with a total of 25 joints, hands stream has the input skeleton with a total of 13 joints and legs stream with a total of 9 joints. Within the PSUMNet architecture, we use 10 STRBs for the body stream, 6 STRBs for the hands stream and 4 STRBs to process the legs stream.</p><p>We implement PSUMNet using the Pytorch deep learning framework. We use SGD optimizer with 0.1 as the base learning rate and a weight decay of 0.0005. All the models are trained on 4 1080Ti 12 GB GPU systems. For training of 25 joints datasets-NTU60 and NTU120, we use a batch size of 200. For 67 joints datasets-NTU60-X and NTU120-X, due to much denser skeleton, smaller batch size of 65 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Tab. 1 compares the performance of proposed PSUMNet with other approaches on Cross Subject (XSub) and Cross View (XView) splits of NTURGB+D dataset <ref type="bibr" target="#b20">[19]</ref> and Cross subject (XSub) and Cross Setup (Xset) splits of the NTURGB+D 120 dataset <ref type="bibr" target="#b15">[15]</ref>. As can be seen from the Params. column in Tab. 1, PSUMNet uses the least number of parameters compared to other methods and achieves better or very comparable results across different splits of the datasets. For the harder Cross Subject split of both NTURGB+D and NTURGB+D 120, PSUMNet achieves state of the art performance compared to other approaches which use 100%-400% more parameters. This shows the superiority of PSUMNet both in terms of performance and efficiency -also see <ref type="figure">Fig. 1</ref>.</p><p>We also compare the performance of only body stream of PSUMNet with single stream (i.e only joint, only bone) performance of other approaches in Tab. 2 for Xsub split of NTURGB+D and NTURGB+D 120 datasets. As can be seen, PSUMNet outperforms other approaches by a margin of 1-2% for NTURGB+D and by 2-3% for NTURGB+D 120 using almost the same or lesser number of parameters. This also supports our hypothesis that part stream based unified modality approach is much more efficient compared to conventional independent modality streams approach.</p><p>Trivedi et al. <ref type="bibr" target="#b27">[26]</ref> introduced NTU60-X and NTU120-X, extensions of existing NTURGB+D and NTURGB+D 120 datasets with 67 joint dense skeletons containing fine-grained finger joints within the full body pose tree. Handling such large number of joints while keeping the total parameters of the model in bounds    as compared to PSUMNet. This shows the benefit of using part based streams approach for dense skeleton representation as well.</p><p>To further explore the generalization capability of our proposed method, we evaluate performance of PSUMNet for skeleton based hand gestures recognition dataset, SHREC <ref type="bibr" target="#b5">[6]</ref>. Taking advantage of part based stream approach, we train only the hands stream of PSUMNet. As shown in Tab. 4, PSUMNet achieves comparable results to existing state of the art method (DSTANet <ref type="bibr" target="#b23">[22]</ref>) which uses 1400% more parameters. PSUMNet outperforms the second best approach (DDNet <ref type="bibr" target="#b30">[29]</ref>) which uses 100% more parameters.</p><p>Overall, Tab. 1, 3 and 4 comprehensively show that proposed PSUMNet achieves state of the art performance, generalizes easily across a range of action datasets and uses a significantly smaller number of parameters compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>As explained in Sec. 3.1, we train PSUMNet using three part streams namely body, hands and legs streams and report the ensembled results from all the three streams. To understand the advantage of the proposed part stream approach, <ref type="figure">Fig. 4</ref>: Comparing per class accuracy after training PSUMNet using only Hands stream and only body stream for NTU120-X dataset (Left) and only Legs stream with only body stream for NTU60-X datset (Right). On observing the class labels we can see that all the actions in the left plot are dominated by hand joints movements and all the actions in the right plot are dominated by leg joints movement and hence streams corresponding to these parts are able to classify these classes better which is in line with our hypothesis we compare stream wise per class accuracy for NTU120-X and NTU60-X of PSUMNet. <ref type="figure">Fig. 4</ref> (left) depicts the per class comparison setting for per class accuracy comparison between the 'only hands stream' and 'only body stream' setting of PSUMNet for NTU120-X dataset. The classes shown correspond to those with largest (positive) gain in per class accuracy while using only hand stream. Upon observing the action labels of these classes, ("Cutting Paper", "Writing", "Folding Paper"), it is evident that these classes are dominated by hand joints movements and hence are better classified using only a subset of input skeleton which has dedicated representations for hand joints as opposed to using entire skeleton in a monolithic fashion.</p><p>Similarly, we also compare the per class accuracy while using only legs stream against only body stream of PSUMNet for NTU60-X dataset as shown in <ref type="figure">Fig. 4</ref> (right). In this case too, the class labels with highest positive gain while using only legs stream are dominated correspond to expected classes such as "Walking", "Kicking".</p><p>The above results can also be appreciated better by studying the number of parameters in each of the part based stream. The body stream in PSUMNet has 1.4M parameters, Hands stream has 0.9M and legs stream has 0.5M parameters. Hence, hands stream while using only 65% of the total parameters of the body stream and legs stream while using only 35% of the body stream parameters can identify those classes better which are dominated by joints corresponding to each part stream. Early action recognition: In the experiments so far, evaluation was conducted on the full action sequence. In other words, the predicted label is known only after all the frames are provided to the network. However, there can be anticipatory scenarios where we may wish to know the predicted action label without waiting for the entire action to finish. To examine the performance in such scenarios, we create test sequences whose length is determined in terms of a fraction of the total sequence length. We study the trends in accuracy as the % of total sequence length is steadily increased. For comparison, we study PSUMNet with the state of the art network, CTR-GCN <ref type="bibr" target="#b1">[2]</ref>. As can be seen in <ref type="figure" target="#fig_2">Fig. 5</ref>, PSUMNet consistently outperforms CTR-GCN for partially observed sequences, indicating its suitability for early action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablations</head><p>To understand the contribution of each part stream in PSUMNet, we provide individual stream wise performance of PSUMNet on NTU60 and NTU120 datasets Cross Subject splits as ablations in Tab. 5.</p><p>At a single stream level, the body stream achieves higher accuracy compared to hands and legs stream. This is expected since the body stream includes a coarse version of all the joints. However, as mentioned previously (Sec. 4.4), hands and legs streams classify actions dominated by respective joints better. Therefore, accuracies of Body+Hands (row 4 in Tab. 5) and Body+Legs (row 5) variants are higher than only the body stream. Legs stream achieves lower accuracy as compared to body and hands stream because there are only a small subset of action categories which are dominated by leg joints movements. However, as with hands stream, legs stream benefits classes which involve significant leg joints movements.</p><p>Our proposed part groups factorization registers each group's sub-skeleton in a global frame of reference (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Further, all the part groups are not disjoint and have overlapping joints to better propagate global motion information through the network (Sec. 3.1). To justify our choice of globally registered part groups, we perform an ablation with a different part grouping strategy, with each part group being disjoint and in a local frame of reference. Specifically, the ablation setup for body stream includes on 9 torso joints (including shoulders and hips joints), hands stream includes only 12 joints and legs stream includes only 8 joints. It is important to notice here that unlike our original strategy, both legs and hands in corresponding part stream are not connected. As expected, such  strategy fails to capture global motion information unlike our proposed method (c.f. 'Disjoint parts' row and last row in Tab. 5).</p><p>To further investigate contribution of each data modality in our proposed unified modality method, we provide ablation studies with PSUMNet trained on single and two modalities instead of four (c.f. 'Modalities in PSUMNet' rows and last row in Tab. 5). We notice that PSUMNet benefits most by joint and bone modalities compared to velocity modalities. However, the best performance is obtained by utilizing all the modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present Part Streams Unified Modality Network PSUMNet to efficiently tackle the challenging task of scalable pose-based action recognition. PSUMNet uses part based streams and avoids treating the input skeleton in monolithic fashion as done by contemporary approaches. This choice enables richer and dedicated representations especially for actions dominated by a small subset of localized joints (hands, legs). The unified modality approach introduced in this work enables efficient utilization of the inter-modality correlations. Overall, the design choices provide two key benefits -(1) they help attain state of the art performance using significantly smaller number of parameters compared to existing methods (2) they allow PSUMNet to easily scale to both sparse and dense skeleton action datasets in distinct domains (full body, hands) while maintaining high performance. PSUMNet is an attractive choice for pose-based action recognition especially in real world deployment scenarios involving compute restricted embedded and edge devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Overall Architecture of one stream of the proposed architecture. The input skeleton is passed through Multi modality data generator (MMDG), which generates joint, bone, joint velocity and bone velocity data from input and concatenates each modality data into channel dimension as shown in (b). This multi-modal data is processed via Spatio Temporal Relational Module (STRM) followed by global average pooling and FC. (c) Spatio Temporal Relational Block (STRB), where input data is passed through Spatial Attention Map Generator (SAMG) for spatial relation modeling, followed by Temporal Relational Module. As shown in (a) multiple STRB stacked together make the STRM. (d) Spatial Attention Map Generator (SAMG), dynamically models adjacency matrix (A hyb )to model spatial relations between joints. Predefined adjacency matrix (A) is used for regularization. (e) Temporal Relational Module (TRM) consists of multiple temporal convolution blocks in parallel. Output of each temporal convolution block is concatenated to generate final features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 (</head><label>2</label><figDesc>right):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Comparing PSUMNet with current state of the art method, CTR-GCN on partially observed sequences for NTURGB+D 120 (XSub) dataset. Annotated numbers for each line plot denote accuracy of both models on partial sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state of the art approaches for NTURGB+D and NTURGB+D 120 dataset. Model parameters are in millions (?10 6 ) and FLOPs are in billions (?10 9 ).</figDesc><table><row><cell>NTU60</cell><cell>NTU120</cell></row></table><note>* : These numbers are cumulative over all the streams used by respective models as per their training protocol.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of only body stream of PSUMNet with the best performing modality (i.e only joint, only bone) of state of the art approaches for NTURGB+D 60 and 120 dataset on Cross Subject protocol.</figDesc><table><row><cell>is a difficult task. However, as shown in Tab. 3, PSUMNet achieves state of the</cell></row><row><cell>art performance for both NTU60-X and NTU120-X datasets. Total parameters</cell></row><row><cell>increase by a small margin for PSUMNet to handle the additional joints, yet it is</cell></row><row><cell>worth noting that other competing approaches use 100%-400% more parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Comparison with state of the art approaches for dense skeleton datasets</cell></row><row><cell>NTU60-X and NTU120-X.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Params. (M) 14 gestures 28 gestures</cell></row><row><cell>Key-Frame CNN [6]</cell><cell>7.9</cell><cell>82.9</cell><cell>71.9</cell></row><row><cell>CNN+LSTM [17]</cell><cell>8.0</cell><cell>89.8</cell><cell>86.3</cell></row><row><cell>Parallel CNN[7]</cell><cell>13.8</cell><cell>91.3</cell><cell>84.4</cell></row><row><cell>STA-Res TCN [11]</cell><cell>6.0</cell><cell>93.6</cell><cell>90.7</cell></row><row><cell>DDNet [29]</cell><cell>1.8</cell><cell>94.6</cell><cell>91.9</cell></row><row><cell>DSTANet [22]</cell><cell>14.0</cell><cell>97.0</cell><cell>93.9</cell></row><row><cell>PSUMNet(Ours)</cell><cell>0.9</cell><cell>95.5</cell><cell>93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state of the art approaches for SHREC skeleton hand gesture recognition dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Indivdual streams performance on NTURGB+D and NTURGB+D 120 Cross Subject dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning multigranular spatio-temporal graph network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Channel-wise topology refinement graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-scale spatial temporal graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decoupling gcn with dropgraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 4</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 4</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d hand gesture recognition using a depth and skeletal dataset: Shrec&apos;17 track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on 3D Object Retrieval</title>
		<meeting>the Workshop on 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for multivariate time series classification using both inter-and intra-channel parallel convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Devineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP&apos;2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2015.7298714</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298714" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Space-time representation of people based on 3d skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2017.01.011</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S10773142173002792,4" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d cnns on distance matrices for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3123266.3123299</idno>
		<ptr target="http://doi.acm.org/10.1145/3123266.3123299" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>MM &apos;17</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial-temporal attention res-tcn for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Part-level graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6759</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/67594" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11045" to="11052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ddgcn: A dynamic directed graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2019.2916873</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.29168731" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition via spatial and temporal transformer networks. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled spatial-temporal attention network for skeleton-based action-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for robust skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413802</idno>
		<ptr target="https://doi.org/10.1145/3394171.34138023,4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACMMM)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="http://bmvc2018.org/contents/papers/1003.pdf" />
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntu-x: An enhanced large-scale dataset for improving pose-based recognition of subtle human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thatipelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2021) 1, 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Topology-aware convolutional neural network for efficient skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM multimedia asia</title>
		<meeting>the ACM multimedia asia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian graph convolution lstm for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
