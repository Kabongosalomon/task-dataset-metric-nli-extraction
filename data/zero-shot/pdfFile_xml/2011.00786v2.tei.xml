<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actor and Action Modular Network for Text-based Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhanyu</forename><forename type="middle">Ma</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Actor and Action Modular Network for Text-based Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video object segmentation</term>
					<term>language attention mechanism</term>
					<term>modular network</term>
					<term>multi-modal learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based video segmentation aims to segment an actor in video sequences by specifying the actor and its performing action with a textual query. Previous methods fail to explicitly align the video content with the textual query in a fine-grained manner according to the actor and its action, due to the problem of semantic asymmetry. The semantic asymmetry implies that two modalities contain different amounts of semantic information during the multi-modal fusion process. To alleviate this problem, we propose a novel actor and action modular network that individually localizes the actor and its action in two separate modules. Specifically, we first learn the actor-/actionrelated content from the video and textual query, and then match them in a symmetrical manner to localize the target tube. The target tube contains the desired actor and action which is then fed into a fully convolutional network to predict segmentation masks of the actor. Our method also establishes the association of objects cross multiple frames with the proposed temporal proposal aggregation mechanism. This enables our method to segment the video effectively and keep the temporal consistency of predictions. The whole model is allowed for joint learning of the actor-action matching and segmentation, as well as achieves the state-of-the-art performance for both single-frame segmentation and full video segmentation on A2D Sentences and J-HMDB Sentences datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the explosion of video data on the Internet, understanding video content becomes increasingly important and has attracted significant research interest in recent years. Action recognition is one of the key tasks in the field of video analysis, which mainly focuses on human-centric action classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or localization <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> in videos. Since traditional action recognition lacks fine-grained understanding of video content, recently there is a growing interest in simultaneously understanding actors and actions in videos. Xu et al. <ref type="bibr" target="#b4">[5]</ref> first proposed a new actor-action segmentation challenge on a large-scale video dataset, i.e., actor-action <ref type="figure">Fig. 1</ref>. Illustration of text-based video segmentation. The goal of the task is to attain segmentation masks of the actor in video sequences by specifying the actor and its action with a textual query. Note that the mask color corresponds to the textual query color (Better viewed in color). dataset (A2D). Actor-action semantic segmentation requires to spatio-temporally localize and recognize seven classes of actors (e.g., "adult" and "bird") and eight classes of actions (e.g., "climb" and "fly") at pixel-level in a video. Various approaches under supervision <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> or weak supervision <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> have been proposed to tackle this problem and achieved significant advance. However, the actors and actions of interest are much more diverse in the real world, thus the small number of pre-defined actor and action classes largely limits applications of the aforementioned methods in some cases, such as automatic video editing, intelligent vision search, and human-robot interaction.</p><p>Compared with word-level actor and action classes, textual queries are more flexible so that they can be used to specify various actors and actions of interest. While most related works focus on either spatial object localization from textual queries in static images <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> or temporal action localization from textual queries in videos <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. There are few works that consider the actor and the corresponding action simultaneously in a video when the actor and action are specified by a given textual query. To bridge this gap, the new task of text-based video segmentation was originally introduced by Gavrilyuk et al. <ref type="bibr" target="#b18">[19]</ref>, whose goal was to attain pixel-level segmentation of the actor in video content by specifying the actor and its performing action with a textual query. Some illustrative examples of the new task are shown in <ref type="figure">Fig. 1</ref>, where four textual queries are given to specify four different actors and their actions, respectively. The model is required to predict segmentation masks of actors frame-arXiv:2011.00786v2 [cs.CV] 22 Aug 2022 <ref type="figure">Fig. 2</ref>. An overview of our proposed actor and action modular network (AAMN) for text-based video segmentation (Better viewed in color). The AAMN takes a textual query, RGB and Flow clips as well as corresponding proposals generated from an external detector as inputs. The temporal proposal aggregation mechanism will generate a set of actor-/action-related tubes by computing linking scores. The language attention model will adaptively learn the actor-/actionrelated representation from the given textual query. Two modules are proposed to localize the target tube that involves the actor and action referred to by the textual query. The fully convolutional network (FCN) is used to predict the masks of the actor within the target tube. by-frame conditioned by the textual queries. The variety of textual queries provides a flexible way to select the actors of interest according to actors' appearance information and motion information. Appearance information contains visual cues about categories, colors, and shapes of actors, while motion information represents actions that are performed by actors in the video. Considering either video or textual query understanding has been extensively studied in its related fields, the major challenge of this task is to establish a suitable video-query alignment by associating with actors and actions. Despite much progress has been achieved in this task, existing methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> suffer from the following two aspects.</p><p>First, the fine-grained alignment between video and query features is not explored well. Existing methods adopt a bottomup paradigm to address the text-based video segmentation, they fail to capture entity-level (i.e., actor and action) information for explicitly modeling the fine-grained alignment between two modalities. More concretely, these methods first integrate video and query features with various fusion strategies, and then they implicitly align each convolved local volume (i.e., Spatio-temporal feature unit) of the 3D feature map produced by a 3D convolutional neural network (CNN) with a holistic representation of the textual query. Finally, they measure the binary similarity map from the decoder network. However, for the query representation, the arbitrary input query could include various words referring to actors, actions, attributes, locations, objects, etc., which contains much actor-/action-related information. Besides, the variation of linguistic structures of queries makes these methods difficult to precisely capture the semantic information related to the actor and its action. For the video features, the convolved local volume usually provides either redundant or incomplete visual content for a single actor depending on relative sizes of the actor and convolutional filters. In addition, since each local volume could not include sufficient information to recognize an action, modeling spatio-temporal relationships among local volumes <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> is helpful to recognize the action. Therefore, the local volume of the video features and the holistic representation of the textual query contain different amounts of semantic information, namely, semantic asymmetry between the local volume and the query representation. The semantic asymmetry fails to align the textual query with referred actor and its action in video content, and eventually degrades segmentation results.</p><p>Second, existing methods are inefficient for full video segmentation and suffer from the discrepancy of predictions in consecutive frames, as they do not consider object association between adjacent frames. As A2D dataset <ref type="bibr" target="#b4">[5]</ref> is sparsely annotated 3 to 5 frames for a video, existing methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> directly aggregated the spatio-temporal features into the annotated frames to train and evaluate their models. Consequently, there are two limitations for these methods: 1) They are time-consuming to segment all frames of the video by densely sampling lots of video clips, because their models only predict one-frame segmentation results using 2D decoder networks with multi-frame video clips as inputs. 2) The temporal consistency of predictions cross multiple frames is not established well as the outputs of the models are treated independently. The scene changes and motion blur would lead to the discrepancy of predictions in consecutive frames. To alleviate these issues, McIntosh et al. <ref type="bibr" target="#b21">[22]</ref> replaced 2D deconvolutional layers with 3D deconvolutional layers in the decoder network, and used annotations of bounding boxes to train and evaluate their model. However, the block-like outputs from the model are not precise enough for pixel-level actor segmentation in videos.</p><p>To overcome the limitation of semantic asymmetry, an intuitive idea is to first find actor-/action-related video content and words, rather than equally split local volumes of video features and directly use the holistic representation from the textual query. This can be easily achieved by leveraging the recent advances of object detection <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and language attention mechanism <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Then the cross-modal matching between extracted instance-level regions and actor-related words is straightforward, as it is similar to the referring expression methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. If the region contains the actor referred to by the query, namely, semantic symmetry between the region and the actor-related words, it is desired region. However, the referring expression methods have two drawbacks for text-based video segmentation. On the one hand, the referring expression methods do not refer to the dynamic information or actions of the actor, the action-related matching is still a challenge. The action-related matching additionally requires understanding the dynamic motion rather than only static actors. It is more complicated as the video content also includes other disturbing relationships, e.g., affiliation and position, which can easily lead to confusion. Moreover, the actor-related matching and action-related matching are closely related, i.e., it is the actor that actually performs the action. But how to make them collaborate with each other to improve the final performance is rarely investigated. On the other hand, the referring expression methods fail to establish data association for objects cross multiple frames. These methods process the video frame-by-frame independently without considering the valuable temporal information cross frames, which could result in the discrepancy of predictions due to the scene or appearance variations in the video.</p><p>In this paper, we propose a novel model which named Actor and Action Modular Network (AAMN) for text-based video segmentation. Specifically, to effectively utilize the limited annotations and establish the association of objects in consecutive frames, we first generate a set of actor-/action-related tubes by introducing a temporal proposal aggregation mechanism. This allows us to obtain appearance and motion representations for each tube of the clip. In order to realize semantically symmetrical matching, we implement the language attention mechanism on the textual query to adaptively focus on actor-/action-related words, and then construct two modules in AAMN. One module is called actor module, which is designed to identify the actor based on actor's appearance information. It associates actor-related tubes with the corresponding words to produce actor matching scores. Another module is called action module, which is designed to identify the actor based on actions performed by the actor in the video. This module utilizes optical flow features to model the dynamic motion information of the actor. To further keep our model from being disturbed by ambiguous motion information or background, the action module contains a contextual long short-term memory network (LSTM) to model the potential relationships among pairwise tubes. The action module integrates motion features and contextual features as well as action-related words to predict the action-matching scores. Finally, the feature maps of the target tube are fed into a small fully convolutional network (FCN) <ref type="bibr" target="#b30">[31]</ref> to predict masks of the input clip. Our method can not only achieve single-frame video segmentation like previous methods in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, but also full video segmentation like the method in <ref type="bibr" target="#b21">[22]</ref>. To demonstrate the effectiveness of the proposed AAMN, we conduct experiments on two benchmark datasets, and experimental results show that our method outperforms the state-of-the-art methods.</p><p>The contributions of this paper are highlighted as follows: i) We study on a practically important problem, i.e., text-based video segmentation, and accordingly propose a novel Actor and Action Modular Network (AAMN) to deal with the intrinsic semantic asymmetric problem. ii) We propose a temporal proposal aggregation mechanism to establish objects association cross adjacent frames. This enables AAMN to achieve multi-frame segmentation at a time with limited annotations, and keep the temporal consistency of predictions in consecutive frames. iii) We especially focus on the action-related matching between the video content and the textual query, by constructing an action module to jointly model the dynamic motion and visual context. iv) We quantitatively and qualitatively validate the effectiveness of our method and achieve the state-of-the-art results on A2D Sentences and J-HMDB Sentences datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Actor-Action Semantic Segmentation in Videos</head><p>There are many emerging works on video object segmentation that aims to segment out a particular object in an entire video <ref type="bibr" target="#b31">[32]</ref>. Actor-action semantic segmentation in videos differs from them by assigning an action label to the target actor. Such a detailed actor-action understanding task has attracted growing attention in recent years. Xu et al. made the first effort on actor-action semantic segmentation problem in <ref type="bibr" target="#b4">[5]</ref>, where they collected a large-scale video dataset, i.e., A2D, to jointly consider various types of actors performing various actions. Early works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> solved this problem based on Conditional Random Fields (CRF) with supervoxels features. With the success of deep learning in the field of computer vision, deep learning-based approaches have been widely studied for actor-action semantic segmentation. Qiu et al. <ref type="bibr" target="#b6">[7]</ref> exploited 2D and 3D FCN to model spatio-temporal dependency from a video for semantic segmentation. Dang et al. <ref type="bibr" target="#b8">[9]</ref> relied on region masks to enhance consistent labeling of action for actor-action semantic segmentation. Kalogeiton et al. <ref type="bibr" target="#b7">[8]</ref> proposed a multitask architecture for joint actoraction detection and then performed actor-action segmentation with SharpMask <ref type="bibr" target="#b32">[33]</ref>. Ji et al. <ref type="bibr" target="#b9">[10]</ref> leveraged multiple input modalities and contextual information from videos to realize pixel-wise actor-action segmentation and detection in a unified network with joint multitask learning. Rana et al. <ref type="bibr" target="#b10">[11]</ref> proposed an effective method to perform pixel-level actor-action detection in a single-shot. Moreover, the weakly supervised actor-action semantic segmentation, which only has access to video-level actor and action tags, has also been studied in recent works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although promising results have been achieved by the aforementioned approaches, the pre-defined actor and action categories limit their applications in human-computer interaction scenarios. Our work focuses on solving a more challenging task, namely, text-based video segmentation, which aims to segment a particular actor specified by a textual query referring to the actor and its performing action in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Referring Expression Comprehension and Segmentation</head><p>The referring expression comprehension (REC) aims to localize the object with a bounding box in the image that corresponds to a given textual query <ref type="bibr" target="#b33">[34]</ref>. To address this problem, most of proposed approaches focus on mining contextual information from the language and image <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b34">[35]</ref>, modeling the relationship between different objects <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and analyzing the linguistic structures <ref type="bibr" target="#b28">[29]</ref>. To precisely describe the shape of the referred object, the referring expression segmentation (RES) goes a step further to predict the segmentation mask instead of the bounding box. Hu et al. first proposed the baseline by up-sampling the concatenation of visual and linguistic features using a deconvolutional layer. Margffoy-Tuay et al. <ref type="bibr" target="#b36">[37]</ref> integrated the visual and linguistic features by generating the dynamic filter for each word of the textual query. Li et al. <ref type="bibr" target="#b37">[38]</ref> explored to fuse multi-level visual features to recurrently refine the local details of segmentation masks. To enable better multi-modal interactions, more recent methods <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> focus on establish complex cross-modal relations between visual and linguistic features. For example, Ye et al. <ref type="bibr" target="#b40">[41]</ref> integrated every word with multi-level visual features and introduce self-attention to adaptively focus on informative words of query and regions of the image. Hu et al. <ref type="bibr" target="#b41">[42]</ref> proposed a bi-directional attention module to establish cross-modal correlations, Liu et al. <ref type="bibr" target="#b42">[43]</ref> leveraged attention mechanism to capture different concepts for enhancing the multi-modal fusion process.</p><p>Text-based video segmentation is closely related to the task of RES, some works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref> also explored to address textbased video segmentation with RES methods. However, there are still significant differences between these two tasks. First, for the RES, the target object is mainly referred to by its appearance or location in a given static image. While for text-based video segmentation, the target object is referred to by not only its attributes (e.g., colors, shapes), but also its performing actions in the video. This indicates that the text-based video segmentation is more complicated than RES. Second, RES methods segment the video frame-by-frame without considering the data association between adjacent frames, which usually leads to the discrepancy of predicted objects. It is essential to establish the association of objects cross multiple frames for text-based video segmentation. Third, textbased video segmentation adopts more metrics to evaluate the segmentation performance except the overall intersection over union that is often used in RES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Text-based Video Segmentation</head><p>The task of text-based video segmentation was originally introduced by Gavrilyuk et al. <ref type="bibr" target="#b18">[19]</ref>, whose goal was to segment the actor specified by a textual query referring to the actor and its performing action in the video. Existing approaches address this problem by following a bottom-up procedure, which extracts the video and query features separately and then predicts the segmentation mask from the merged heterogeneous features. Specifically, Gavrilyuk et al. <ref type="bibr" target="#b18">[19]</ref> proposed to correlate the video contents and sentences using dynamic convolutions. Wang et al. <ref type="bibr" target="#b19">[20]</ref> introduced vision guided language attention and language guided vision attention mechanisms to obtain a robust language representation and aggregate global visual context for performance gains. Wang et al. <ref type="bibr" target="#b20">[21]</ref> constructed a context modulated dynamic convolutional network to incorporate contextual information into the correlation learning of heterogeneous modalities. McIntosh et al. <ref type="bibr" target="#b21">[22]</ref> further encoded the visual and language features as capsules and integrated the visual and language information via a routing algorithm. Unlike aforementioned works, we propose to solve the textbased video segmentation from another perspective, namely, a top-down procedure. We first localize the referred actor and action with bounding boxes sequence in a symmetric matching way, then predict the masks of the actor along the bounding boxes sequence. Our method can effectively localize the query described actor and action, and reduce disturbances of irrelevant objects and the background in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we elaborate on the proposed method for text-based video segmentation, the overall framework of our method is illustrated in <ref type="figure">Fig. 2</ref>.</p><p>Task definition. Given a video sequence I = {I n ? R W ?H?3 } N n=1 with N frames, and a corresponding textual query W = {w t } T t=1 with T words. The textual query usually refers to the actor and its performing action in the video. The text-based video segmentation aims to produce N -frame binary segmentation masks M = {M n } N n=1 , M n ? R H?W for the referred actor in the video sequence.</p><p>Overview. Except appearance information, which contains visual cues about categories, colors and shapes of the actors, dynamic motion information is another important visual cue to identify the actors in the video. This motivated us to localize the actor cross frames by aligning appearance information and motion information between the video content and the textual query. To explicitly model such fine-grained alignment and achieve semantically symmetric matching between two modalities, we propose an actor and action modular network (AAMN) for text-based video segmentation.</p><p>Formally, our AAMN first generates a set of proposals per frame in a video clip via an external object detector. Then AAMN feeds RGB and Flow clips as well as corresponding proposals into two parallel CNN architectures to learn the appearance and motion representations for each proposal. To link the proposals cross frames and generate a set of actor-/actionrelated tubes for input clips, a temporal proposal aggregation mechanism is introduced after fully connected (FC) layers (Sec. III-A). Meanwhile, in order to obtain related representations about the actor and action from the textual query, AAMN decomposes the query into two separate components relying on a language attention mechanism (Sec. III-B). Afterward, we construct two modules, i.e., actor module and action module, to identify the actor based actor's appearance information and motion information, respectively. These two modules take actor-/action-related tubes and language representations as inputs and localize the target tube according to the actor and action matching scores (Sec. III-C). Finally, a tiny FCN is followed by two modules to predict the segmentation masks of the actor within the target tube (Sec. III-D). We train the whole network with a multi-task learning strategy by combining the actor-action matching and the actor segmentation (Sec. III-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Actor-/Action-related Tubes Generation and Representation</head><p>The AAMN takes RGB and Flow clips as well as pregenerated proposals as inputs, where each clip contains 2L+1 frames. For proposals generation, we follow the previous work <ref type="bibr" target="#b7">[8]</ref> to choose the commonly used object detector, i.e., Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>, to extract a fixed number of proposals with high confidence in each frame of the video. We simply denote these proposals as {{r n k } K k=1 } N n=1 , where the video has N frames and each frame contains K proposals. r n k denotes the k-th proposal in the frame n, and it is comprised of the top-left and bottom-right coordinates. The Faster R-CNN model has been trained on a large-scale detection dataset MSCOCO <ref type="bibr" target="#b44">[45]</ref>.</p><p>In our experiments, we further finetune the detector on a new dataset, i.e., A2D, to improve the detection accuracy for actors.</p><p>Considering that only 3 to 5 frames are annotated in videos of A2D dataset, we sample 3 to 5 clips around the annotated frames from a video to train our model. For each clip, the middle frame, i.e., the (L+1)-th frame is annotated with bounding box and binary mask, while forward L frames and backward L frames are not annotated. To localize and segment the queryspecified actor in consecutive frames of the clip, we first link the detected proposals {{r n k } K k=1 } 2L+1 n=1 in temporal dimension and generate K tubes (i.e., bounding boxes sequences). To this end, we introduce a temporal proposal aggregation mechanism that expects two proposals in adjacent frames contain the same actor or object if they possess similar spatial locations and features. Specifically, we utilize a CNN architecture to extract the appearance features in RGB stream and further obtain fixed-size (e.g., 7 ? 7) feature maps via RoIAlign operation <ref type="bibr" target="#b26">[27]</ref> for the generated proposals. Similar to Mask R-CNN <ref type="bibr" target="#b26">[27]</ref> which has multi-task heads for classes prediction, bounding boxes regression, and object segmentation, the small feature maps in RGB stream are fed into two different branches for actor-action matching and segmentation, respectively. For actor-action matching branch, we transform each small map into a holistic appearance representation v n k ? R Cv (k ? [1, K], n ? [1, 2L + 1]) for the k-th proposal in the n-th frame via two FC layers. Similarly, we can obtain the holistic motion</p><formula xml:id="formula_0">representation f n k ? R Cv (k ? [1, K], n ? [1, 2L + 1])</formula><p>for the k-th proposal in the n-th frame in Flow stream.</p><p>Next, the linking score between proposals r n1 k1 and r n2 k2 is formulated as:</p><formula xml:id="formula_1">s link (r n1 k1 , r n2 k2 ) = IoU (r n1 k1 , r n2 k2 ) + ? 1 + ED(v n1 k1 , v n2 k2 ) ,<label>(1)</label></formula><p>where the IoU (?) is utilized to measure the spatial similarity by computing the intersection-over-union between proposals r n1 k1 and r n2 k2 . The features of r n1 k1 and r n2 k2 are denoted as v n1 k1 and v n2 k2 , respectively. The ED(?) means the Euclidean distance which is adopted to measure the semantic similarity between features v n1 k1 and v n2 k2 . The ? is a balanced parameter. For each proposal in the n 1 -th frame, we select the proposal with maximal linking score from the n 2 -th frame. In this way, we can obtain K tubes for each video clip. Note that we only use visual features from RGB stream to calculate the semantic similarity.</p><p>To obtain the representation of each tube in the RGB stream, we apply mean pooling over features from forward L frames and backward L frames to aggregate them as temporal context of the (L+1)-th frame, and perform addition with the feature of the (L+1)-th frame:</p><formula xml:id="formula_2">v L+1 k = v L+1 k + 1 2L 2L+1 n=1;n =L+1 v n k .<label>(2)</label></formula><p>Finally, we use one FC layer with Rectified Linear Unit (ReLU) function to transform v L+1 k and obtain the appearance representation for the k-th tube. For simplicity, we denote the representation as v k ? R C v . In Flow stream, we apply the same way to obtain the motion representation f k ? R C v for the k-th tube in the Flow clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Textual Query Representation Learning</head><p>As AASTQ takes an arbitrary textual query as input, the query may contain diversely linguistic structures or irrelevant semantics with relation to the actor and its action. To alleviate the interference of the linguistic structures and irrelevant semantics, a straightforward idea is to utilize external language parser <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> or pre-defined templates <ref type="bibr" target="#b47">[48]</ref> to parse the query into different components and then extract the actor-/action-related individual words for the actor and action localization. Nevertheless, previous works <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b48">[49]</ref> have demonstrated that the language parser cannot work well in the query-region matching problem, because individual words lack essential contextual information for scene understanding of images. As an alternative way, Hu et al. <ref type="bibr" target="#b27">[28]</ref> proposed to automatically parse the query into a triplet (subject, relationship, object) with a soft attention mechanism for relational reasoning. Similar to <ref type="bibr" target="#b27">[28]</ref>, Yu et al. <ref type="bibr" target="#b28">[29]</ref> learned to parse the query into a triplet (subject, location, relationship) for object localization. In our work, we aim to explicitly model the actor and action localization from a given query. Hence, we adopt the attention mechanism to adaptively learn to attend to the words that are relevant to the actor and action, respectively.</p><p>More specifically, for a given textual query with T words {w t } T t=1 , we use Glove <ref type="bibr" target="#b49">[50]</ref> to embed each word and obtain a vector sequence {e t } T t=1 ? R Ce . Then a two-layer Bidirectional LSTM <ref type="bibr" target="#b50">[51]</ref> takes the vector sequence {e t } T t=1 as input and encodes the query as follows:</p><formula xml:id="formula_3">? ? h 1 t = ? ??? ? LSTM ??? h 1 t?1 , e t ;</formula><p>information not only from the t-th word but also from the contextual words before and after the t-th word. Afterward, two separate FC layers followed by softmax layers are used to compute the attention weight ? m t (m ? {actor, action}) on each word,</p><formula xml:id="formula_4">? m t = exp (W m h t ) T i=1 exp (W m h i ) ,<label>(4)</label></formula><p>where the attention weight ? m t denotes the probability of the t-th word belong to the actor or action. Finally, the ? m t is applied to the embedding vector sequence {e t } T t=1 to derive the actor-related and action-related query representations,</p><formula xml:id="formula_5">q m = T t=1 ? m t ? e t .<label>(5)</label></formula><p>According to the above, we adopt the attention mechanism to adaptively learn the related information about the actor and action. The learning of attention weights is weakly supervised by collaborating with the actor and action modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Actor and Action Matching in the Modular Network</head><p>To segment the actor according to descriptions about actor and its action, we first localize the tube involving the actor and its action in the video clip. We formulate this problem as a matching process in two modules, namely, the actor module and the action module. The former module is applied to localize the tube which involves the actor referred by the given query. It will compute a set of matching scores s(t k |q actor ) (k ? [1, K]) to measure the similarity between the actorrelated query representation q actor and the k-th tube t k . The later module is used to localize the tube which involves the action described by the given query. This module computes a set of matching scores s(t k |q action ) (k ? [1, K]) to measure the similarity between the action-related query representation q action and the k-th tube t k . The combined matching score s k = s(t k |q actor ) + s(t k |q action ) determines the similarity between the textual query q and the k-th tube t k . Finally, the tube with the highest score is selected as the target tube which contains the actor and action referred by the textual query. The detailed design of two modules will be discussed in the following parts.</p><p>1) The Actor Module: As human usually tends to describe a specific actor according to appearance and location information in videos, we design the actor module to identify the query-specified actor through appearance features from RGB stream in AAMN. The actor module is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. We measure the similarity score between the actor-related query representation q actor and feature vector v actor k of each tube. The tube representation captures appearance information but lacks necessary location information to distinguish those actors who have similar appearance information. Therefore, following the previous work <ref type="bibr" target="#b13">[14]</ref>, we introduce a 5-dimensional  of the actor, i.e., v actor</p><formula xml:id="formula_6">location feature l k = x tl W ,</formula><formula xml:id="formula_7">k = [v k , l k ] ? R C v +5 .</formula><p>The visual representation v actor k and linguistic representation q actor are linearly transformed with FC layers and l 2 -normalized, and then perform element-wise multiplication to integrate two representations. Finally, the integrated representation is fed into one FC layer to predict an unary matching score for the actor as follows:</p><formula xml:id="formula_8">v actor k = W 1 v actor k + b 1 , q actor = W 2 q actor + b 2 , x actor k = v actor k 2 q actor 2 , s actor k = W 3 x actor k + b 3 ,<label>(6)</label></formula><p>where denotes element-wise multiplication, ? 2 means</p><formula xml:id="formula_9">l 2 -normalization, {W 1 , b 1 , W 2 , b 2 , W 3 , b 3 } are learnable pa- rameters in three FC layers.</formula><p>2) The Action Module: The action matching needs to introduce dynamic motion information as the visual cues for action perception. It is rarely investigated in previous works. Motivated by the previous approaches in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> where they introduce optical flow as dynamic motion information for action recognition, we introduce another CNN architecture, i.e., Flow stream, which takes a Flow clip as input, to learn motion information for each proposal. We obtain the holistic representation of motion information f k (k ? [1, K]) for the kth tube t k with the same way as the appearance representation v k . However, the dynamic motion representation of each tube sometimes is not enough for action inferring. For instance, given a video with the query of "a man is standing and watching on the behind", the actor of "man" performs actions of "standing" and "watching" without obviously spatial displacements. Thus the action cannot be shown from the dynamic motion information. Besides, the tubes with similar motion patterns also affect the result of action matching. To mitigate these problems, we devise a contextual LSTM to model the relationship between the target tube and others, then utilize the information from the contextual LSTM as auxiliary information for the action matching.</p><p>The proposed action module is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. This module contains a contextual LSTM network which assists to match the action under the ambiguous motion cues, and a gated fusion network used to weight the importance of motion information and appearance information for action matching. Concretely, the contextual LSTM takes the sequential appearance feature v j ? R C v (j ? [1, K] ? j = k) of tubes as well as the whole clip feature v 0 ? R C v as inputs. To obtain v 0 , we take the video clip as a large tube. The feature maps of this tube from the last convolutional layer is resized into fixed sized (e.g., 7 ? 7) via RoIAlign and then fed into two fully connected layers. We aggregate these features using the same way in Equation 2. In our model, v 0 represents global contextual information and is arranged in the first place of input sequence, i.e., v seq</p><formula xml:id="formula_10">k = {v 0 , v 1 , ..., v k?1 , v k+1 , ..., v K?1 } (k ? [1, K ? 1]).</formula><p>The tube features are ordered from top to down and left to right according to the proposal locations of the (L+1)-th frame in the video clip. Then the sequential features v seq k are consecutively fed into LSTM to encode each tube and obtain hidden representations at each time step, i.e.,</p><formula xml:id="formula_11">h v t = LSTM h v t?1 , v t ? R Cc (t ? [0, K]).</formula><p>Thereafter, we concatenate all hidden states from each time step</p><formula xml:id="formula_12">h v = h v 0 , h v 1 , .</formula><p>.., h v K?1 ? R K?Cc and use an attention mechanism to adaptively associate the tubes together to obtain a contextual representation v c k ? R Cc for the k-th tube. The attention is calculated by:</p><formula xml:id="formula_13">y t = ?(W 4 h v t + b 4 ), ? t = exp (W 5 y t + b 5 ) T K j=1 exp (W 5 y j + b 5 ) T , v c k = K t=1 ? t ? y t ,<label>(7)</label></formula><p>where ? (?) is ReLU function,</p><formula xml:id="formula_14">{W 4 , b 4 , W 5 , b 5 } are learnable parameters in FC layers. The contextual representation v c k of the k-th tube is concate- nated with appearance representation v k , i.e., v k = [v k , v c k ] ? R C v +Cc .</formula><p>Then v k and the motion representation f k ? R Cv are fused with a gated way:</p><formula xml:id="formula_15">g a k = W 6 v k + b 6 ; g b k = W 7 f k + b 7 , ? = ? W 8 g a k , g b k + b 8 , v action k = ? g a k + (1 ? ?) g b k ,<label>(8)</label></formula><p>where ? (?) is the sigmoid function which rescales the gate vector ? ? R Ce to [0, 1]. Then the ? performs element-wise multiplication with sum of g a i ? R Ce and g b i ? R Ce for weighted fusion them.</p><formula xml:id="formula_16">{W 6 , b 6 , W 7 , b 7 , W 8 , b 8 } are learnable parameters in FC layers.</formula><p>After obtaining the visual representation of action v action k ? R Ce , we compute the action matching score with the same way as the actor matching score: </p><p>where {W 9 , b 9 , W 10 , b 10 , W 11 , b 11 } are corresponding parameters in FC layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Actor Segmentation Network</head><p>Based on the computed matching score s k (i ? [1, K]) above, we can select the target tube with the highest score, which involves the actor and action referred to by the given query. Then we take 2L + 1 feature maps of the target tube into actor segmentation network to predict masks of the actor. The network is a small FCN which takes the fixed size, e.g., 14 ? 14, feature maps from RoIAlign layer as input. Similar to Mask R-CNN <ref type="bibr" target="#b26">[27]</ref>, the FCN is comprised of 4 consecutive convolution layers and 1 deconvolutional layer. The first layer consists of 256 1 ? 1 filters to fuse the appearance feature of the tube in RGB stream, and motion feature of the tube in Flow stream. The other three layers are consists of 256 3 ? 3 filters. The final deconvolutional layer up-samples features with factor 2. Since only the (L+1)-th frame has mask annotation for input video clip, we choose the (L+1)-th feature map of the target tube to train the actor segmentation network with input batch size as 1. During testing, we set input batch as 2L+1 and take total feature maps of the target tube to predict the actor masks in the actor segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Learning with Joint Actor-Action Matching and Segmentation</head><p>During training, AAMN enables joint learning for the actor-action matching and segmentation by minimizing the following combined loss function:</p><formula xml:id="formula_18">L = L mat + ?L seg ,<label>(10)</label></formula><p>where ? is a hyperparameter to balance the matching loss L mat and the segmentation loss L seg . Assuming the ground truth of the actor is contained in the k-th region proposal of the (L+1)-th frame, then L seg will only be computed on the k-th mask. L seg is the average binary cross-entropy loss which is defined as:</p><formula xml:id="formula_19">L seg = ? 1 h w w i=1 h j=1 (? ij log(? ij ) + (1 ? ? ij )log(1 ? ? ij )),<label>(11)</label></formula><p>where h and w are height and width of the feature map, respectively. ? is the binary ground-truth label, ? is the predicted value of the segmentation mask.</p><p>Since the triplet hinge loss has shown its strength in many cross-modal matching problem <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, here we adopt it as the matching loss L mat for the actor and action matching in AAMN. L mat encourages the scores of matched proposals and textual queries to be larger than those of mismatched ones:</p><formula xml:id="formula_20">L mat = q max [0, ? s (p, q) + s (p, q)] + p max [0, ? s (q, p) + s (q, p)],<label>(12)</label></formula><p>where p and q indicate region proposal and textual query, respectively. (p, q) and (q, p) mean the matched proposal and query pairs, and (p, q) and (q, p) are the mismatched pairs. s (p, q) is the computed similarity between p and q. There are K ? 1 mismatched pairs for each matched pair in our experiments. is a margin parameter between matched and mismatched pairs.</p><p>During testing, we follow previous works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> to sample video clips round the annotated frames as inputs. We predict segmentation masks for the selected tube and recover predicted masks into the same resolution as the input clip. Finally, we select the mask in the middle frame (i.e., annotated frame) to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS A. Datasets and Evaluation Metrics</head><p>A2D Sentences. Gavrilyuk et al. <ref type="bibr" target="#b18">[19]</ref> augmented Actor-Action Dataset (A2D) with textual queries that describe actors and actions of interest in videos. The original A2D <ref type="bibr" target="#b4">[5]</ref> consists of 7 actor classes which perform one of the 9 action classes. Each video provides around 3 to 5 frames sparse annotations with joint pixel-wise semantic mask and bounding box per actor-action pair. A2D sentences manually annotated 6,655 textual queries which describe actors and actions presented in videos. It is divided into 3,017 and 737 videos for training and testing. Furthermore, McIntosh et al. <ref type="bibr" target="#b21">[22]</ref> annotated actors with bounding boxes in all frames of videos. The extended A2D Sentences makes it available to evaluate the full video segmentation from a textual query.</p><p>J-HMDB Sentences. J-HMDB Sentences is extended from J-HMDB <ref type="bibr" target="#b53">[54]</ref>, which is a benchmark dataset for action recognition. It comprises of 928 video clips for 21 action classes and provides pixel-wise segmentation mask for the human in action for each frame. There are 928 textual queries are annotated in J-HMDB dataset to describe what action the humans is performing in the video.</p><p>Evaluation Metrics. We adopt four types of metrics to evaluate our method as prior works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. For singleframe video segmentation <ref type="bibr" target="#b18">[19]</ref>, each key-frame with a query is regarded as a single sample. While for the full video segmentation <ref type="bibr" target="#b21">[22]</ref>, each video with a query is considered as a single sample. The Overall Intersection-over-Union (Overall IoU) calculates the total intersection area between predictions and ground truth masks divided by total union area accumulated over all test samples, which tends to favor large actors and objects. The Mean IoU, which treats large and small actors or objects equally, is calculated as the average over the IoU of each testing samples. The precision at different threshold P@X (X ? {0.5, 0.6, 0.7, 0.8, 0.9}), which is the percentage of testing samples whose IoU higher than specific threshold. The mean average precision (mAP) over [0.50 : 0.05 : 0.95] is also computed to measure the segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In AAMN implementation, we choose Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> with VGG-16 network as two CNN architectures to extract fixed-size proposal features with RoIAlign layer <ref type="bibr" target="#b26">[27]</ref>. AAMN takes a textual query, a RGB clip and a Flow clip as well as pre-generated proposals as inputs. The frames of the video clip are resized and padded to 512 ? 512 as previous works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The optical flow is a tensor of three channels with x and y coordinates of the flow as well as the flow magnitude <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Since the generation of high-quality proposals is important for actor and action localization, we choose Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> with a backbone of ResNet-101 as an external object detector. This model is trained on MSCOCO dataset <ref type="bibr" target="#b44">[45]</ref> and further finetuned on A2D <ref type="bibr" target="#b18">[19]</ref>. In our experiments, the dimension of C v is 4096, the dimension of C v is set as 1024. For the textual query encoding, we adopt two-layer Bi-directional LSTM (Bi-LSTM). The dimension of the word embeddings that are input into Bi-LSTM is C e = 300, and the forward and backward hidden states from Bi-LSTM are C h = 512. In order to obtain fixed dimensional query representation, long queries are truncated and short queries are padded with zeros. The fixed length of the query is set to 20 in our experiments. Besides, the dimension of hidden states in the contexual LSTM is C c = 512.</p><p>We implement our proposed model with Tensorflow. Two CNN architectures are initialized with pre-trained weights on MSCOCO dataset <ref type="bibr" target="#b44">[45]</ref>. During the model training, we use stochastic gradient descent (SGD) with initial learning rate of 0.0001, momentum of 0.95, weight decay of 0.0005. The learning rate decreases by 10 times at 1.0 ? 10 5 iterations and the model is trained for 1.5 ? 10 5 iterations. The balanced parameter ? in Equation 1 is selected as 1.0. The margin is set to 0.1 in the matching loss L mat in Equation 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies 1) Selection of Proposal Number:</head><p>The selection of proposal number is critical to model visual region context in the contextual LSTM of the action module. Thus we conduct experiments with various numbers of proposals to analyze the impact of proposals number in our model. We first extract a set of proposals per frame with high confidence utilizing an object detector. Then we select a specific number of proposals and feed them into our model. In our experiments, the number is set to increase from 2 to 12 with an interval of 2. Before evaluating the segmentation results with different proposals number, we first analyze the actor and action localization results with bounding boxes. Here, the precision of localization P@X (X ? {0.5, 0.6, 0.7, 0.8, 0.9}) is evaluated by calculating the IoU ratio between the true bounding box and the top predicted box in annotated frames. If the IoU is larger than the threshold, the prediction is considered as a true positive, otherwise it is counted as a false positive. The total true positive counts are then averaged over all testing samples to obtain P@X. <ref type="table" target="#tab_1">Table I</ref> shows the precision of localization at versus thresholds with various numbers of proposals. It can be seen that the precision increases with increment of proposals number. When the number reaches a certain value, i.e., K = 6, the precision tends to decline in most metrics. This reveals that the contextual LSTM progressively encodes local features into a discriminate vector representation which contains more details of video content, but too more proposals may bring noises. The segmentation results with different number of proposals are shown in <ref type="table" target="#tab_1">Table II</ref>. Since our model is a two-stage method, the segmentation performance is closely relevant to the localization performance, the best segmentation performance is achieved when the number of proposals equals 6. Thus we set the number of proposals as 6 in our model.</p><p>2) Impact of Input Frame Number: The AAMN takes RGB and Flow clips as inputs, where each clip contains 2L + 1 frames. To investigate the impact of frame number in the clip, we perform experiments with different L, i.e., L = {0, 2, 4, 8}, the corresponding frames are {1, 5, 9, 17} in a clip. The experimental results are shown in <ref type="table" target="#tab_1">Table III</ref>. We observe that the single frame input, i.e., L = 0, has the worst performance in all metrics. By increasing the number of input frames, the performance is increased, especially in P@0.6 and P@0.7. It demonstrates that the aggregation of temporally contextual information is helpful for actor and action matching in two modules, and leads to the improvement of actor segmentation. The best results are achieved under most metrics when the frame number is 9. The performance drops by further increasing the frame number. The possible reason is that proposals of the actor or object cannot link well by computing similarity of spatial locations over a long frames sequence, irrelevantly temporal information is aggregated into the annotated frame. Therefore, we select the frame number as 9, i.e., L = 4, in our experiments.</p><p>3) Study of Ablation Models: To systematically evaluate the relative contributions of different components in AAMN, we design seven ablation models as shown in Lines 1-7 of Table IV. a) "Baseline Model" utilizes concatenation of the appearance feature v i and the motion feature f i of the ith tube as the visual representation, the mean pooling of embedded word sequence as a language representation. The matching score is computed using the same way as the actor matching score. b) "w/o Actor Module" means that AAMN only contains the action module. c) "w/o Actor Module" means that AAMN only contains the action module. d) "w/o Location Feature" indicates that the actor module without the location feature l i . e) "w/o Contextual LSTM" indicates that the action module without the contextual LSTM. f) "w/o Attention" refers to using element-wise mean pooling over the concatenated hidden states h v in the contextual LSTM to obtain the contextual representation v c i . g) "w/o Gated Fusion" refers to using concatenation instead of gated fusion to merge the appearance feature v i and the motion feature f i in the action module. As a comparison, the "Full Model" is shown in the last Line <ref type="table" target="#tab_1">of Table IV</ref>.</p><p>From the experimental results in Lines 1-7 and Line 14 of <ref type="table" target="#tab_1">Table IV</ref>, we can observe that: i) Benefiting from the modular design, the "Full Model" outperforms the "Baseline Model" by a large margin in terms of different metrics. ii) The performance of individual modules is close to or slightly worse than the performance of "Baseline Model", while the combination of them can significantly improve the performance. This demonstrates that two modules can collaborate and facilitate each other for the actor and action matching. iii) Modeling the contextual information using the contextual LSTM with attention is helpful to promote the action matching and further improve the segmentation performance. iv) Compared with the concatenation way to fuse the appearance feature v i and the motion feature f i , the gated fusion way can learn to weight the importance of the motion feature f i and the appearance feature v i for accurately matching the described action in a textual query. 4) Impact of Optical Flow on Different Components: The impact of optical flow have been studied when we add optical flow features in different components of AAMN, the corresponding results are shown in Lines 8-13 of Table IV. a) "w/o Flow Stream" denotes that AAMN only contains RGB stream and the motion feature f i is replaced with the appearance feature v i in action module. b) "w/o RGB Stream" means that AAMN only contains flow stream and the appearance feature v i is replaced with the motion feature f i in two modules. c) "(FCN) w/o Optical Flow" indicates that AAMN predicts the segmentation mask from the appearance features v i . d) "(Action Module) w/o Optical Flow" denotes that AAMN only uses the appearance feature v i for action matching in the action module. e) "(Action Module) w/o RGB" means that AAMN only utilizes the motion feature f i for action matching in the action module. f) "(Actor Module) w/ Optical Flow" refers to adding the motion feature f i into the actor module.</p><p>From the experimental results in Lines 8-14 of <ref type="table" target="#tab_1">Table IV</ref>, we can conclude that: i) The two-stream framework performs better than single stream framework, because the appearance feature v i is essential for actor matching and the motion feature f i plays a key role for action matching. ii) The combination of the motion feature f i and the appearance feature v i can improve the segmentation performance in AAMN. iii) The individual appearance feature v i or motion feature f i performs worse than their combination for action matching, since the appearance feature v i encoded region contextual information can distinguish those actions which have ambiguous motion cues. iv) The motion feature f i cannot promote the actor matching, this may be because the input of two modules are same, the modular network cannot learn discriminative information for the actor and action matching. We also investigate the impact of optical flow on the semantic similarity of linking score in Equation 1. Specifically, we generate tubes by calculating the semantic similarity using visual features from Flow stream, RGB stream, and combination of two streams. Experimental results are summarized in <ref type="table">Table V</ref>. It can be observed that our model achieves the best performance by calculating the semantic similarity only using visual features from RGB stream, while the performance degrades obviously using visual features from Flow stream or combination of two streams. This is reasonable because we expect to link objects in consecutive frames by capturing invariant features about these objects. Specifically, visual features from RGB stream learn appearance information about objects. Such information contains visual cues about categories, colors, and shapes of objects. Thus, appearance information is usually invariant between adjacent frames. However, visual features from Flow stream mainly learn dynamic motion information performed by objects. For each object, the learned information is not stable in single frames for different actions, view angles, movement speeds, and occlusions. Lots of works <ref type="bibr" target="#b54">[55]</ref> usually aggregate multiple frames of visual features from Flow stream for action recognition. Calculating the semantic similarity with such dynamic motion information is not accurate to link objects. Therefore, we only use visual features from RGB stream to calculate the semantic similarity in our work. 5) Impact of Temporal Aggregation Strategy: Due to sparse annotations of A2D dataset, we generate the tube representation by aggregating visual features from adjacent frames into the annotated frame (i.e., the middle frame) to train our model. Except the temporal aggregation strategy in Equation 2, we also explored another aggregation strategy by directly averaging visual features over all frames of the clip. Experimental results are summarized in <ref type="table" target="#tab_1">Table VI</ref>. It can be observed that the strategy in Equation 2 obtains better results than averaging visual features over all frames in most metrics. This is because visual features from the annotated frame are more discriminative for actor and action matching, and assigning larger weights to visual features from annotated frame can improve the localization precision of the actor. 6) Analysis of Learning Strategy: Our AAMN is a twostage method which first localizes the actor-/action-related tube and then performs segmentation within the localized tube. Thus we can choose two learning strategies for AAMN: jointly learn actor-action matching and segmentation at same time, or separate learn them one-by-one. To compare with our main joint learning strategy, we perform two separate learning experiments, i.e., "M + S" and "S + M". The "M + S" means we first train the matching branch until convergence and then train the FCN branch for the actor segmentation. The "S + M" denotes we first train the FCN branch until convergence and then train the matching branch. The experimental results with different learning strategies are shown in <ref type="table" target="#tab_1">Table VII</ref>. Note that the actor-action matching and segmentation branches share the same backbone of feature extractor, therefore when separately trained, the final AAMN may be bias to the actor-action matching or segmentation. By comparing the results in <ref type="table" target="#tab_1">Table  VII</ref>, we can conclude that joint learning is helpful to avoid subtask biases and achieves best performance. 7) Analysis of Hyper-parameter: In Equation 10, a hyperparameter ? is introduced to balance the matching loss L mat and the segmentation loss L seg in joint learning process. ? = 1.0 means L mat plays an equally important role as L seg . Here we conduct experiments to analyze the impacts of ?. We vary ? from 0.001 to 15.0 in experiments and present corresponding results in <ref type="table" target="#tab_1">Table VIII</ref>. It can be found that the segmentation performance is improved with increasing of ?, and reaches the best performance at most evaluation metrics when ? = 5.0. After that, the performance tends to descend when ? keeps increasing.</p><p>8) Analysis of Word Attention Weights: As the lack of wordlevel annotations, the language attention learning is weakly supervised by collaborating with the actor and action modules. According to the role of each word plays in two modules, the related information about the actor and action is adaptively learned from the textual query. Since it is hard to quantitatively evaluate the accuracy of learned word attention weights, we visualize word attention weights and corresponding segmentation results on key frames of two video from A2D Sentences in <ref type="figure">Fig. 5</ref>.</p><p>In <ref type="figure">Fig. 5</ref>, the first row shows the visualization of learned word attention weights for the textual query, where ? 1 and ? 2 denote word attention weights corresponding to the actor module and action module, respectively. The darker color indicates larger value of weight. We can observe that the language attention model enables to attend to right words that are relevant to the actor and action respectively. Specifically, (b) Q: a man with white shirt is throwing a ball (a) Q: a person in black clothes is running <ref type="figure">Fig. 5</ref>. Visualization of word attention weights and segmentation results on A2D Sentences. For each example, we sample three annotated frames from a video. The first row shows the learned attention weights for the textual query, the second row shows the localization results in annotated frames, the third row shows the the ground truth masks (red), the last row shows the predicted masks (green) for the referred actor. ? 1 has higher values on those word that are nouns and attribute words referring to the actor. While ?2 obtains higher values on those words that are verbs or verb-related phrases. In the second row, it is obvious that our model can accurately localize the actor with green bounding boxes in different frames. This can be attributed to the modular design in our model. The predicted bounding box from the detector is very close to the ground truth bounding box (red dashed line). The yellow bounding box is the most relevant contextual region which has the highest attention weight in the contextual LSTM of the action module. The third row shows the ground truth segmentation masks of the actor in key frames. The last row shows the predicted segmentation mask of the referred actor with our AAMN. We can observe that our model is able to predict accurate binary masks for the referred actor in key frames.</p><formula xml:id="formula_21">? 1 ? 2 ? 1 ? 2 ? 1 ? 2 ? 1 ? 2 ? 1 ? 2 ? 1 ? 2</formula><p>To further demonstrate our model can effectively capture related information about the actor and action from the textual query, we conduct an additional experiment by randomly setting the word attention weights for the textual query during the model testing process. The experimental results are shown in <ref type="table" target="#tab_1">Table IX</ref>. It can be seen that the segmentation performance deteriorates notablely by assigning random attention weights for the textual query. This indicates that the language attention learning enables to capture key-word information for the actor matching and action matching in AAMN. In <ref type="figure" target="#fig_3">Fig. 6 (b)</ref> and (c), we present two testing examples with predicted attention weights and random attention weights, respectively. It can be observed that the predicted attention weights focus on key words about the actor and action, and thus the model can correctly localize and segment the referred actor. However, the model with random attention weights fails to localize the referred actor, as the model ignores meaningful information about the actor and action from the textual query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-art Methods</head><p>1) Single-Frame Segmentation from a Textual Query: Since sparse annotations on A2D dataset, previous approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> take a video clip around the annotated frame and a textual query as inputs. They utilized pre-trained 3D CNN to extract the clip features, and conduct temporal average pooling to obtain a 2D feature map. The segmentation performance is evaluated on the annotated frame by upsampling the fused multi-modal features from the low resolution to high resolution by introducing 2D deconvolutional layers. While other approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b55">[56]</ref> are proposed for image segmentation from a textual query, they directly take the annotated frame and a textual query as inputs and ignore the temporal modeling. Therefore, aforementioned approaches are single-frame segmentation models as they only output singleframe predictions at a time. Different from them, AAMN is a multi-frame model which outputs multi-frame predictions at a time. Specifically, by taking a video clip and a textual query as inputs, AAMN first localizes the tube that contains the referred actor and action in the video clip, and then feeds the tube into a FCN. The FCN is only trained with the annotated frame, i.e., middle frame, of the selected tube during the model learning. The FCN can predict all masks of the referred actor along the selected tube during the model testing. In this set of experiments, we present the segmentation results of AAMN on annotated frames as previous works.</p><p>In <ref type="table" target="#tab_5">Table X</ref>, we present comparisons of our model with previous methods on A2D Sentences. For fair comparison with methods that only take the RGB clip as input, we remove the Flow stream and the gated fusion in the action module. The exprimental results are shown in Line 7 of Table X. It can be observed that our method outperforms previous methods under most metrics, especially for the Overlap in P@0.5, P@0.6, and P@0.7, which surpass the best results by 4.5%, 5.5%, and 4.0%, respectively. Previous works <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> have demonstrated that integrating the optical flow is beneficial to Q: women in green dress is walking on the street Q: a small black dog on top near a white dog is walking improve the performance of video object segmentation. The methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> also explored two-stream models that take RGB and Flow clips as inputs. Their methods fused RGB and Flow streams by computing a weighted average of response maps from each stream. We also extend the single-stream model in <ref type="bibr" target="#b20">[21]</ref> to a two-stream model by adopting the same way to fuse RGB and Flow streams. Our method is a two stream model that takes RGB and Flow clips as inputs. The optical flow is introduced to localize the action which performs by the referred actor. The experimental results of our full model are shown in Line 11 of <ref type="table" target="#tab_5">Table X</ref>. We can see that the performance of our model significantly outperforms stateof-the-art results from single-stream models, and our model achieves the best performances compared with previous twostream models. Specifically, compared with the best results from two-stream model in <ref type="bibr" target="#b20">[21]</ref>, our model brings 5.6%, 9.2%, 11.7% and 6.7% improvements under metrics of P@0.5, P@0.6, P@0.7, and P@0.8. The performance under metrics of mAP, Overall IoU, and Mean IoU also achieves 7.0%, 0.5%, and 1.8% improvements, respectively. We note that the performance of our model is slightly worse than the best results for the Overlap in P@0.9, and the performance improvement is small in Overall IoU. The main reason is that we adopt proposal-based approach <ref type="bibr" target="#b26">[27]</ref> to predict the mask of actor on a 28 ? 28 grid irrespective of actor size. Such a grid is sufficient for small actors, but for large actors further upsampling with bilinear interpolation will over-smooth the fine-grained details <ref type="bibr" target="#b59">[60]</ref>. While previous works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> predict the mask of the actor by gradually upsampling the full feature map with multiple deconlutional layers. They introduce skip-connections in the model <ref type="bibr" target="#b21">[22]</ref> or supervisions on different resolutions during model training <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Therefore, these methods tend to preserve fine-grained details and possess higher overlaps for both small and large actors.</p><p>In addition, we also compare the inference speed for different methods in <ref type="table" target="#tab_5">Table X</ref>. The inference speed indicates the total time for the method predicts the final segmentation masks from a clip-query pair. It can be observed that the inference speed of our method is slower than other methods. This is because our method requires more time for proposal detection per frame in the input video clip. However, our method provides a new solution for text-based video segmentation from another perspective, which is significantly different from existing methods. And our method is more efficient than other methods, as they only predict one-frame segmentation results from the clip-query pair, while our method can segment all frames of the video clip.</p><p>Following <ref type="bibr" target="#b18">[19]</ref>, we also evaluate our method on J-HMDB Sentences to validate the generalization ability of our method. In our experiments, the testing model is pre-trained on A2D Sentences, and J-HMDB Sentences is used as a testing dataset. For fair comparisons, we also uniformly sample three clips for each video as previous works. The experimental results are shown in <ref type="table" target="#tab_1">Table XI</ref>. In Line 6 of Table XI, we show experimental results that we remove the Flow stream in our model. It can be observed that our method achieves superior results compared with previous methods. In Line 10 of <ref type="table" target="#tab_1">Table  XI</ref>, we show the experimental results from full model which takes RGB and Flow clips as inputs. Compared with twostream models, our method achieves state-of-the-art performance under most metrics. Specifically, compared with the best results from two-stream method in <ref type="bibr" target="#b20">[21]</ref>, our method achieves 1.9%, 3.4% and 1.7% improvements in terms of P@0.5, P@0.6 and P@0.7. The mAP and Overall IoU also surpass the best results by 1.2% and 1.4%, respectively. The results in terms of P@0.8 and Mean IoU are slightly worse than the best results. It is probably caused by the backbone that cannot extract fine representations without finetuning on J-HMDB Sentences.</p><p>To validate the importance of temporal modeling by associating objects cross frames for text-based video segmentation, we also compare our method with some state-of-the-art referring expression segmentation methods on A2D Sentences. The experimental results are shown in <ref type="table" target="#tab_1">Table XII</ref>. It can be observed that our method achieves superior performance than these methods. This demonstrates that the temporal modeling by associating objects cross frames is truly helpful to localize and segment the referred actor in the video.</p><p>2) Full Segmentation from a Textual Query: Previous methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> are very time consuming for full video segmentation as they only predict one frame segmentation at a time. McIntosh et al. <ref type="bibr" target="#b21">[22]</ref> further replaced 2D deconvolutional layers with 3D deconvolutional layers in their decoder network, which enables the model to predict multi-frame segmentation results at a time. Similar to <ref type="bibr" target="#b21">[22]</ref>, our method is a multi-frame segmentation model, which can be applied to full video segmentation by segmenting video clip-by-clip.</p><p>To explore full video segmentation, McIntosh et al. further annotated actors with bounding box in all frames of the video. They utilized bounding boxes to train and evaluate their model. The predictions of the model is block-like, which is not precise enough for pixel-level segmentation of the actor. Different from them, we only train our model with ground truth masks in key frames. For ease of comparison with <ref type="bibr" target="#b21">[22]</ref>, we also evaluate our model with bounding box annotations in all frames of the video. The experimental results are shown in <ref type="table" target="#tab_1">Table XIII</ref>. The performance is evaluated with pixel-wise segmentation output in the first and second lines. In the third and last lines, the bounding box is placed around the fine-grained segmentation masks for evaluation. As observed from <ref type="table" target="#tab_1">Table  XIII</ref>, our method achieves superior performance compared with previous method in <ref type="bibr" target="#b21">[22]</ref> for the full video segmentation. E. Visualization and Failure Cases 1) Qualitative Results: To qualitatively validate the effectiveness of our method, we provide some segmentation results in sampled frames for two videos on A2D Sentences. The visualization results are shown in <ref type="figure">Fig. 7</ref>, where mask colors correspond to query colors. For each example, the first row shows input video frames. The second row shows the results produced by the method in <ref type="bibr" target="#b18">[19]</ref>. The third row shows the results produced by the method in <ref type="bibr" target="#b19">[20]</ref>. The last row shows the results produced by our method. It can be observed that our method can produce better visual qualitative results than previous methods, and more details of the actor are presented in our results. For example, the boundaries of the toddler in the first video and two man in the second video are clearer than the results produced by two other methods. Besides, segmentation results from methods in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> involve confused pixels between different actors in two videos. Such confused pixels indicate that these methods cannot precisely distinguish the referred actors in sampled frames. Different from them, our method enables to correctly distinguish the referred actors and generate more precise segmentation masks.</p><p>2) Failure Cases: Despite our AAMN has achieved significant improvements than other methods, there are still some challenging scenarios that cannot be handled well. Some typical failure cases from A2D Sentences are shown in <ref type="figure" target="#fig_4">Fig.  8</ref>. We argue that these failure cases are mainly due to the following three aspects. First, the ambiguous description for the target actor. For example, in the first video, it aims to (a) Q: a black dog is walking on the left Q: the toddler in a yellow shirt is walking a black lab (b) Q: man in red shirt standing in the middle Q: man in black jumping up and down <ref type="figure">Fig. 7</ref>. The qualitative results shown in sampled frames for several videos from A2D Sentences (Best viewed in color). For each example, from the first row to the fourth row show the sampled video frames, the segmentation results from <ref type="bibr" target="#b18">[19]</ref>, the segmentation results from <ref type="bibr" target="#b19">[20]</ref>, and the segmentation results from our AAMN. The queries are given on the top and their colors correspond to mask colors.</p><p>segment the duck on the bottom-left with the textual query "the duck is pecking around". Our model suffers from difficulty to identify the referred duck and ducks around it, because they have similar appearance and motion information. Second, the occlusion between different instances. The occlusion in the video makes it difficult to correctly localize the target actor with inaccurate detection results. It also deteriorates the segmentation results as the model cannot distinguish different instances within the localized region. For example, in the second video, our model fails to segment the man in white as he is partially occluded by the man in blue. Third, the blurry boundary of the target actor. Our model fails to produce sharper predictions around actor boundaries when the actor involves similar colors with background or fast motion in the video. For example, in the third video, our predictions cannot precisely segment the right thigh of the man because the part of right thigh is hard to be distinguished from background. Some of failure cases caused by such blurry boundary may be improved by enhancing shape information from adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have studied a challenging problem which is text-based video segmentation. To deal with this problem, we proposed an actor and action modular network to achieve video-query symmetrical matching. We first generate actor-/action-related tubes for input video clip, and learn the actor-/action-related language representations from the textual query. Then we use the modular network to locate the target tube, which involves the referred actor and action. Finally, the target tube is fed into a tiny FCN to predict the binary masks of the actor within the tube. We conducted extensive experiments to demonstrate the effectiveness of our method, and our method achieves state-of-the-art performance on two benchmark datasets. To explore full video segmentation, McIntosh et al. annotated actors with bounding boxes in all frames of the video. But the segmentation results are not precise enough for the model trained with bounding boxes. Considering pixellevel annotation for all frames of video is time-consuming and cost expensive, we will explore weakly supervised text-based video segmentation in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the actor module in AAMN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>+ b 9 ,</head><label>9</label><figDesc>q action = W 10 q action + b 10 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the action module in AAMN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of experimental results with predicted and random attention weights. (a) The ground truth. (b) Segmentation results with predicted word attention weights. (c) Segmentation results with random word attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>the duck is pecking around Q: man in the white kimono is fighting on the tatami Q: guy climbing on the blue Some failure cases on A2D Sentences. (a) The original video frame. (b) The ground truth. (c)The failures of prediction caused by the ambiguous description for the target actor (the first video), the occlusion between different instances (the second video), and the blurry boundary of the target actor (the third video). Note that we sample two annotated frames from each video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>y tl H , x br W , y br H , w?h W ?H in the middle frame of the clip, where (x tl , y tl , x br , y br ) denotes the top-left and bottom-right coordinates, (w, h) and (W,H)are the width and height of the middle proposal in the k-th tube and the video frame, respectively. We concatenate the location feature l k with the tube representation v k as the visual representation</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell>,</cell></row><row><cell>=</cell><cell>1 ,</cell><cell>1 ,</cell><cell>2 ,</cell><cell>2 ,</cell><cell>?</cell><cell></cell><cell>FC layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? FC layer</cell><cell>( |</cell><cell>)</cell></row><row><cell cols="6">0.30 0.11 0.27 0.10 0.08 0.06 0.08</cell><cell></cell><cell>FC layer</cell></row><row><cell cols="6">man in black jumping up and down</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ANALYSIS</head><label>I</label><figDesc>OF LOCALIZATION PRECISION AT DIFFERENT THRESHOLD WITH DIFFERENT PROPOSALS NUMBER IN EACH VIDEO FRAME</figDesc><table><row><cell>Models</cell><cell></cell><cell>P@0.5</cell><cell>P@0.6</cell><cell cols="2">P@0.7</cell><cell>P@0.8</cell><cell cols="2">P@0.9</cell></row><row><cell>K = 2</cell><cell></cell><cell>67.5</cell><cell>63.8</cell><cell cols="2">58.7</cell><cell>48.5</cell><cell cols="2">20.1</cell></row><row><cell>K = 4</cell><cell></cell><cell>69.1</cell><cell>65.2</cell><cell cols="2">59.6</cell><cell>49.1</cell><cell cols="2">20.7</cell></row><row><cell>K = 6</cell><cell></cell><cell>69.0</cell><cell>66.1</cell><cell cols="2">60.2</cell><cell>48.7</cell><cell cols="2">21.5</cell></row><row><cell>K = 8</cell><cell></cell><cell>68.2</cell><cell>65.5</cell><cell cols="2">59.7</cell><cell>48.1</cell><cell cols="2">20.9</cell></row><row><cell>K = 10</cell><cell></cell><cell>67.9</cell><cell>65.0</cell><cell cols="2">59.4</cell><cell>47.9</cell><cell cols="2">21.0</cell></row><row><cell>K = 12</cell><cell></cell><cell>67.2</cell><cell>63.5</cell><cell cols="2">57.9</cell><cell>46.5</cell><cell cols="2">19.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">THE ANALYSIS OF SEGMENTATION RESULTS WITH DIFFERENT PROPOSAL</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NUMBER</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell><cell></cell></row><row><cell cols="9">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell cols="2">K = 2 66.9</cell><cell>60.6</cell><cell>50.7</cell><cell>26.5</cell><cell>1.7</cell><cell>36.8</cell><cell>59.9</cell><cell>53.6</cell></row><row><cell cols="2">K = 4 68.7</cell><cell>64.6</cell><cell>53.6</cell><cell>30.4</cell><cell>3.1</cell><cell>39.5</cell><cell>62.9</cell><cell>55.7</cell></row><row><cell cols="2">K = 6 68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row><row><cell cols="2">K = 8 67.7</cell><cell>63.2</cell><cell>53.1</cell><cell>31.6</cell><cell>2.9</cell><cell>40.6</cell><cell>62.7</cell><cell>54.8</cell></row><row><cell cols="2">K = 10 67.1</cell><cell>62.9</cell><cell>52.6</cell><cell>29.3</cell><cell>2.2</cell><cell>39.9</cell><cell>60.9</cell><cell>53.3</cell></row><row><cell cols="2">K = 12 66.6</cell><cell>61.8</cell><cell>50.5</cell><cell>27.8</cell><cell>2.1</cell><cell>37.2</cell><cell>58.8</cell><cell>52.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">THE IMPACT OF INPUT FRAME NUMBER</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell cols="2">IoU</cell></row><row><cell cols="9">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell>L = 0</cell><cell>68.1</cell><cell>62.9</cell><cell>52.3</cell><cell>29.4</cell><cell>2.8</cell><cell>39.6</cell><cell>61.7</cell><cell>55.2</cell></row><row><cell>L = 2</cell><cell>68.4</cell><cell>64.1</cell><cell>53.2</cell><cell>30.1</cell><cell>2.9</cell><cell>39.9</cell><cell>62.7</cell><cell>55.3</cell></row><row><cell>L = 4</cell><cell>68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row><row><cell>L = 8</cell><cell>68.9</cell><cell>64.4</cell><cell>54.2</cell><cell>31.9</cell><cell>3.2</cell><cell>40.5</cell><cell>62.8</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON A2D SENTENCES</figDesc><table><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap</cell><cell>mAP</cell><cell>IoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P@0.5</cell><cell cols="2">P@0.6</cell><cell>P@0.7</cell><cell>P@0.8</cell><cell>P@0.9</cell><cell>0.5:0.95</cell><cell>Overall</cell><cell>Mean</cell></row><row><cell cols="2">Baseline Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.3</cell><cell cols="2">56.7</cell><cell>39.5</cell><cell>17.2</cell><cell>1.1</cell><cell>31.9</cell><cell>55.2</cell><cell>48.5</cell></row><row><cell cols="4">AAMN w/o Actor Module</cell><cell></cell><cell></cell><cell>48.0</cell><cell cols="2">44.3</cell><cell>36.8</cell><cell>20.7</cell><cell>1.9</cell><cell>27.8</cell><cell>45.7</cell><cell>39.7</cell></row><row><cell cols="4">AAMN w/o Action Module</cell><cell></cell><cell></cell><cell>63.7</cell><cell cols="2">58.7</cell><cell>42.5</cell><cell>22.3</cell><cell>1.4</cell><cell>33.7</cell><cell>58.9</cell><cell>51.8</cell></row><row><cell cols="4">AAMN w/o Location Feature</cell><cell></cell><cell></cell><cell>68.2</cell><cell cols="2">63.7</cell><cell>53.4</cell><cell>31.8</cell><cell>2.7</cell><cell>39.5</cell><cell>62.4</cell><cell>54.7</cell></row><row><cell cols="4">AAMN w/o Contextual LSTM</cell><cell></cell><cell></cell><cell>67.9</cell><cell cols="2">62.5</cell><cell>51.8</cell><cell>27.8</cell><cell>2.0</cell><cell>37.8</cell><cell>60.3</cell><cell>53.6</cell></row><row><cell cols="3">AAMN w/o Attention</cell><cell></cell><cell></cell><cell></cell><cell>68.3</cell><cell cols="2">63.4</cell><cell>52.8</cell><cell>31.7</cell><cell>2.1</cell><cell>39.9</cell><cell>61.5</cell><cell>54.8</cell></row><row><cell cols="4">AAMN w/o Gated Fusion</cell><cell></cell><cell></cell><cell>68.1</cell><cell cols="2">62.7</cell><cell>52.6</cell><cell>29.7</cell><cell>2.2</cell><cell>38.9</cell><cell>61.7</cell><cell>53.8</cell></row><row><cell cols="4">AAMN w/o Flow Stream</cell><cell></cell><cell></cell><cell>65.2</cell><cell cols="2">58.0</cell><cell>44.5</cell><cell>20.8</cell><cell>1.6</cell><cell>34.8</cell><cell>58.8</cell><cell>51.3</cell></row><row><cell cols="4">AAMN w/o RGB Stream</cell><cell></cell><cell></cell><cell>55.1</cell><cell cols="2">43.3</cell><cell>22.7</cell><cell>4.5</cell><cell>0.1</cell><cell>22.2</cell><cell>46.5</cell><cell>42.0</cell></row><row><cell cols="4">AAMN (FCN) w/o Optical Flow</cell><cell></cell><cell></cell><cell>67.5</cell><cell cols="2">61.9</cell><cell>49.1</cell><cell>26.4</cell><cell>2.0</cell><cell>38.0</cell><cell>61.8</cell><cell>53.9</cell></row><row><cell cols="6">AAMN (Action Module) w/o Optical Flow</cell><cell>67.1</cell><cell cols="2">62.2</cell><cell>51.6</cell><cell>29.3</cell><cell>1.6</cell><cell>38.0</cell><cell>60.9</cell><cell>53.3</cell></row><row><cell cols="5">AAMN (Action Module) w/o RGB</cell><cell></cell><cell>65.9</cell><cell cols="2">61.7</cell><cell>52.2</cell><cell>31.4</cell><cell>1.4</cell><cell>37.8</cell><cell>59.8</cell><cell>52.5</cell></row><row><cell cols="5">AAMN (Actor Module) w/ Optical Flow</cell><cell></cell><cell>68.0</cell><cell cols="2">63.1</cell><cell>53.0</cell><cell>30.0</cell><cell>2.4</cell><cell>39.0</cell><cell>61.7</cell><cell>54.3</cell></row><row><cell cols="3">AAMN (Full Model)</cell><cell></cell><cell></cell><cell></cell><cell>68.9</cell><cell cols="2">64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">THE IMPACT OF OPTICAL FLOW ON THE SEMANTIC SIMILARITY</cell></row><row><cell>Features</cell><cell></cell><cell></cell><cell cols="2">Overlap</cell><cell></cell><cell>mAP</cell><cell cols="2">IoU</cell></row><row><cell></cell><cell cols="8">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell>Flow</cell><cell cols="4">67.7 62.0 49.4 23.7</cell><cell>0.8</cell><cell>37.4</cell><cell cols="2">61.1 53.5</cell></row><row><cell>RGB</cell><cell cols="4">68.9 64.2 54.5 32.4</cell><cell>3.4</cell><cell>41.2</cell><cell cols="2">63.4 55.6</cell></row><row><cell cols="5">Flow+RGB 68.4 64.0 54.0 32.4</cell><cell>2.6</cell><cell>40.7</cell><cell cols="2">62.0 55.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON OF TWO TEMPORAL AGGREGATION STRATEGIES</cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell><cell></cell></row><row><cell cols="9">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell cols="2">Average 69.1</cell><cell>63.4</cell><cell>53.0</cell><cell>30.4</cell><cell>2.8</cell><cell>40.7</cell><cell>62.8</cell><cell>55.7</cell></row><row><cell>Ours</cell><cell>68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VII THE</head><label>VII</label><figDesc>ANALYSIS OF DIFFERENT TRAINING STRATEGIES</figDesc><table><row><cell>Models</cell><cell></cell><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell><cell></cell></row><row><cell></cell><cell cols="8">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell>M + S</cell><cell>66.2</cell><cell>55.2</cell><cell>34.3</cell><cell>9.3</cell><cell>0.4</cell><cell>29.8</cell><cell>55.7</cell><cell>50.0</cell></row><row><cell>S + M</cell><cell>65.4</cell><cell>58.5</cell><cell>41.9</cell><cell>15.9</cell><cell>1.6</cell><cell>33.1</cell><cell>57.9</cell><cell>50.8</cell></row><row><cell>Joint</cell><cell>68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE VIII</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">THE ANALYSIS OF HYPER-PARAMETER ?</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell cols="2">Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell><cell></cell></row><row><cell cols="9">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell cols="2">0.01 59.9</cell><cell>44.1</cell><cell>20.2</cell><cell>2.9</cell><cell>0</cell><cell>21.9</cell><cell>51.8</cell><cell>46.3</cell></row><row><cell>0.1</cell><cell>64.8</cell><cell>54.5</cell><cell>35.2</cell><cell>10.5</cell><cell>0.7</cell><cell>29.7</cell><cell>56.9</cell><cell>49.7</cell></row><row><cell>1.0</cell><cell>67.0</cell><cell>60.7</cell><cell>45.0</cell><cell>21.8</cell><cell>1.2</cell><cell>35.7</cell><cell>59.5</cell><cell>52.9</cell></row><row><cell>2.5</cell><cell>67.7</cell><cell>61.6</cell><cell>48.6</cell><cell>25.0</cell><cell>2.5</cell><cell>37.5</cell><cell>60.7</cell><cell>53.9</cell></row><row><cell>5.0</cell><cell>68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell></row><row><cell>7.5</cell><cell>68.9</cell><cell>64.1</cell><cell>53.8</cell><cell>30.6</cell><cell>2.3</cell><cell>40.5</cell><cell>63.0</cell><cell>55.6</cell></row><row><cell cols="2">10.0 68.2</cell><cell>63.2</cell><cell>53.3</cell><cell>30.6</cell><cell>2.2</cell><cell>40.2</cell><cell>62.0</cell><cell>55.1</cell></row><row><cell cols="2">12.5 68.3</cell><cell>63.5</cell><cell>53.1</cell><cell>28.9</cell><cell>1.6</cell><cell>38.8</cell><cell>61.3</cell><cell>54.2</cell></row><row><cell cols="2">15.0 67.4</cell><cell>62.4</cell><cell>51.3</cell><cell>28.0</cell><cell>1.9</cell><cell>37.7</cell><cell>60.1</cell><cell>53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>WITH RANDOM ATTENTION WEIGHTS.</figDesc><table><row><cell>Weights</cell><cell>Overlap</cell><cell>mAP</cell><cell>IoU</cell></row><row><cell></cell><cell cols="3">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell>Random</cell><cell>65.2 60.0 49.2 26.8 1.3</cell><cell>36.5</cell><cell>58.3 51.8</cell></row><row><cell cols="2">Prediction (Ours) 68.9 64.2 54.5 32.4 3.4</cell><cell>41.2</cell><cell>63.4 55.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE X COMPARISON</head><label>X</label><figDesc>WITH THE STATE-OF-THE-ART METHODS ON A2D SENTENCES</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell></cell><cell></cell><cell cols="2">Overlap</cell><cell></cell><cell>mAP</cell><cell>IoU</cell><cell></cell><cell>Speed</cell></row><row><cell></cell><cell></cell><cell>P@0.5</cell><cell>P@0.6</cell><cell>P@0.7</cell><cell>P@0.8</cell><cell>P@0.9</cell><cell>0.5:0.95</cell><cell>Overall</cell><cell>Mean</cell><cell></cell></row><row><cell>Hu et al. [15]</cell><cell>RGB</cell><cell>34.8</cell><cell>23.6</cell><cell>13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>13.2</cell><cell>47.4</cell><cell>35.0</cell><cell>60.6 ms</cell></row><row><cell>Li et al. [56]</cell><cell>RGB</cell><cell>38.7</cell><cell>29.0</cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>16.3</cell><cell>51.5</cell><cell>35.4</cell><cell>-</cell></row><row><cell>Gavrilyuk et al. [19]</cell><cell>RGB</cell><cell>47.5</cell><cell>34.7</cell><cell>21.1</cell><cell>8.0</cell><cell>0.2</cell><cell>19.8</cell><cell>53.6</cell><cell>42.1</cell><cell>95.6 ms</cell></row><row><cell>ACGA [20]</cell><cell>RGB</cell><cell>55.7</cell><cell>45.9</cell><cell>31.9</cell><cell>16.0</cell><cell>2.0</cell><cell>27.4</cell><cell>60.1</cell><cell>49.0</cell><cell>108.9 ms</cell></row><row><cell>CMDy [21]</cell><cell>RGB</cell><cell>60.7</cell><cell>52.5</cell><cell>40.5</cell><cell>23.5</cell><cell>4.5</cell><cell>33.3</cell><cell>62.3</cell><cell>53.1</cell><cell>125.3 ms</cell></row><row><cell>VT-Capsule [22]</cell><cell>RGB</cell><cell>52.6</cell><cell>45.0</cell><cell>34.5</cell><cell>20.7</cell><cell>3.6</cell><cell>30.3</cell><cell>56.8</cell><cell>46.0</cell><cell>-</cell></row><row><cell>AAMN (Ours)</cell><cell>RGB</cell><cell>65.2</cell><cell>58.0</cell><cell>44.5</cell><cell>20.8</cell><cell>1.6</cell><cell>34.8</cell><cell>58.8</cell><cell>51.3</cell><cell>446.6 ms</cell></row><row><cell>Gavrilyuk et al. [19]</cell><cell>RGB+Flow</cell><cell>50.0</cell><cell>37.6</cell><cell>23.1</cell><cell>9.4</cell><cell>0.4</cell><cell>21.5</cell><cell>55.1</cell><cell>42.6</cell><cell>174.7 ms</cell></row><row><cell>ACGA [20]</cell><cell>RGB+Flow</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.7</cell><cell>60.6</cell><cell>50.3</cell><cell>184.8 ms</cell></row><row><cell>CMDy [21]</cell><cell>RGB+Flow</cell><cell>63.3</cell><cell>55.0</cell><cell>42.8</cell><cell>25.7</cell><cell>4.9</cell><cell>34.2</cell><cell>62.9</cell><cell>53.8</cell><cell>201.3 ms</cell></row><row><cell>AAMN (Ours)</cell><cell>RGB+Flow</cell><cell>68.9</cell><cell>64.2</cell><cell>54.5</cell><cell>32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4</cell><cell>55.6</cell><cell>546.5 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE XI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">COMPARISON WITH THE STATE-OF-THE-ART METHODS ON J-HMDB SENTENCES</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Input</cell><cell></cell><cell></cell><cell cols="2">Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell>IoU</cell></row><row><cell></cell><cell></cell><cell>P@0.5</cell><cell cols="2">P@0.6</cell><cell>P@0.7</cell><cell>P@0.8</cell><cell>P@0.9</cell><cell>0.5:0.95</cell><cell>Overall</cell><cell>Mean</cell></row><row><cell>Hu et al. [15]</cell><cell>RGB</cell><cell>63.3</cell><cell>35.0</cell><cell></cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>17.8</cell><cell>54.6</cell><cell>52.8</cell></row><row><cell>Li et al. [56]</cell><cell>RGB</cell><cell>57.8</cell><cell>33.5</cell><cell></cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>17.3</cell><cell>52.9</cell><cell>49.1</cell></row><row><cell>ACGA [20]</cell><cell>RGB</cell><cell>75.6</cell><cell>56.4</cell><cell></cell><cell>28.7</cell><cell>3.4</cell><cell>0.0</cell><cell>28.9</cell><cell>57.6</cell><cell>58.4</cell></row><row><cell>CMDy [21]</cell><cell>RGB</cell><cell>74.2</cell><cell>58.7</cell><cell></cell><cell>31.6</cell><cell>4.7</cell><cell>0.0</cell><cell>30.1</cell><cell>55.4</cell><cell>57.6</cell></row><row><cell>VT-Capsule [22]</cell><cell>RGB</cell><cell>67.7</cell><cell>51.3</cell><cell></cell><cell>28.3</cell><cell>5.1</cell><cell>0.0</cell><cell>26.1</cell><cell>53.5</cell><cell>55.0</cell></row><row><cell>AAMN (Ours)</cell><cell>RGB</cell><cell>74.6</cell><cell>57.3</cell><cell></cell><cell>25.6</cell><cell>1.5</cell><cell>0.0</cell><cell>27.8</cell><cell>55.6</cell><cell>56.3</cell></row><row><cell>Gavrilyuk et al. [19]</cell><cell>RGB+Flow</cell><cell>69.9</cell><cell>46.0</cell><cell></cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>23.3</cell><cell>54.1</cell><cell>54.2</cell></row><row><cell>ACGA [20]</cell><cell>RGB+Flow</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.5</cell><cell>57.9</cell><cell>59.1</cell></row><row><cell>CMDy [21]</cell><cell>RGB+Flow</cell><cell>76.6</cell><cell>60.0</cell><cell></cell><cell>32.9</cell><cell>5.0</cell><cell>0.0</cell><cell>30.8</cell><cell>56.2</cell><cell>58.0</cell></row><row><cell>AAMN (Ours)</cell><cell>RGB+Flow</cell><cell>78.5</cell><cell>63.4</cell><cell></cell><cell>34.6</cell><cell>3.8</cell><cell>0.0</cell><cell>32.0</cell><cell>59.3</cell><cell>59.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE XII COMPARISON</head><label>XII</label><figDesc>WITH STATE-OF-THE-ART REFERRING EXPRESSION SEGMENTATION METHODS P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="2">Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell></row><row><cell cols="5">P@0.5 RRN [38] 42.0 35.5 27.5 15.4</cell><cell>2.5</cell><cell>22.4</cell><cell>56.4 37.3</cell></row><row><cell cols="5">CMSA [41] 46.7 38.5 27.9 13.6</cell><cell>1.7</cell><cell>-</cell><cell>59.2 40.5</cell></row><row><cell cols="5">BRINet [42] 55.7 46.4 33.0 17.5</cell><cell>3.3</cell><cell>28.3</cell><cell>63.0 47.4</cell></row><row><cell cols="5">CMPC [43] 59.0 52.7 43.4 28.4</cell><cell>6.8</cell><cell>35.1</cell><cell>64.9 51.5</cell></row><row><cell>Ours</cell><cell cols="4">68.9 64.2 54.5 32.4</cell><cell>3.4</cell><cell>41.2</cell><cell>63.4 55.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE XIII</cell><cell></cell><cell></cell></row><row><cell cols="8">EXPERIMENTAL RESULTS ON A2D SENTENCES FOR FULL VIDEO</cell></row><row><cell></cell><cell></cell><cell cols="3">SEGMENTATION</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell>mAP</cell><cell>IoU</cell></row><row><cell></cell><cell cols="7">P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</cell></row><row><cell cols="2">VT-Capsule (pixel) 9.6</cell><cell>1.6</cell><cell>0.4</cell><cell>0.0</cell><cell>0.0</cell><cell>1.8</cell><cell>34.4 26.6</cell></row><row><cell>AAMN (pixel)</cell><cell cols="2">13.9 2.8</cell><cell>0.3</cell><cell>0.0</cell><cell>0.0</cell><cell>2.5</cell><cell>35.6 29.8</cell></row><row><cell cols="6">VT-Capsule (bbox) 41.9 33.3 22.2 10.0 0.1</cell><cell cols="2">21.2 51.5 41.3</cell></row><row><cell>AAMN (bbox)</cell><cell cols="4">46.7 37.8 24.0 9.1</cell><cell>0.2</cell><cell cols="2">21.5 52.1 43.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? ? h 1 t = ? ??? ? LSTM ??? h 1 t+1 , e t , h 1 t = ? ? h 1 t , ? ? h 1 t , ? ? h 2 t = ? ??? ? LSTM ??? h 2 t?1 , h 1 t ; ? ? h 2 t = ? ??? ? LSTM ??? h 2 t+1 , h 1 t , h 2 t = ? ? h 2 t , ? ? h 2 t .(3)Both directions of hidden states at each time step in the first layer are concatenated into h 1 t and fed into the second layer. Two directional hidden states of the second layer are concatenated into h 2 t . Then the t-th word representation is the concatenation of h 1 t andh 2 t , i.e., h t = h 1 t , h 2 t ? R C h (t ?[1, T ]). The encoded word representation h t contains the</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A hierarchical model of shape and appearance for human action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human focused action localization in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">6553</biblScope>
			<biblScope unit="page" from="219" to="233" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Actor-action semantic segmentation with grouping process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3083" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep spatio-temporal dependence for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TMM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="939" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4163" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Actoraction semantic segmentation with region masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08430</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end joint semantic segmentation of actors and actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="702" to="717" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">We don&apos;t need thousand proposals: Single shot actor-action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="2960" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised actor-action segmentation via robust multi-task ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a weakly-supervised video actor-action segmentation model with a wise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9901" to="9911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1280" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5958" to="5966" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3939" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Context modulated dynamic networks for actor and action video segmentation with language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual-textual capsule routing for text-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9939" to="9948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="8401" to="8408" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1307" to="1315" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via languageguided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1960" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video object segmentation and tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bundled object context for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2749" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic graph attention for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4644" to="4653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Margffoytuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="630" to="645" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5745" to="5753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="38" to="54" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7454" to="7463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Referring segmentation in images and videos with cross-modal self-attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4424" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-modal progressive comprehension for referring segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cmf: Cascaded multi-model fusion for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="2289" to="2293" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="787" to="798" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph-structured referring expression reasoning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9952" to="9961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving descriptionbased person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5542" to="5556" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Human action recognition and prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>IJCV</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9796" to="9805" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
