<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DO WE STILL NEED AUTOMATIC SPEECH RECOGNITION FOR SPOKEN LANGUAGE UNDERSTANDING?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Borgholt</surname></persName>
							<email>borgholt@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">D</forename><surname>Havtorn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<addrLine>3 Corti</addrLine>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Abdou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><forename type="middle">Edin</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<addrLine>3 Corti</addrLine>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DO WE STILL NEED AUTOMATIC SPEECH RECOGNITION FOR SPOKEN LANGUAGE UNDERSTANDING?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>representation learning</term>
					<term>spoken language understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spoken language understanding (SLU) tasks are usually solved by first transcribing an utterance with automatic speech recognition (ASR) and then feeding the output to a text-based model. Recent advances in self-supervised representation learning for speech data have focused on improving the ASR component. We investigate whether representation learning for speech has matured enough to replace ASR in SLU. We compare learned speech features from wav2vec 2.0, state-of-the-art ASR transcripts, and the ground truth text as input for a novel speech-based named entity recognition task, a cardiac arrest detection task on real-world emergency calls and two existing SLU benchmarks. We show that learned speech features are superior to ASR transcripts on three classification tasks. For machine translation, ASR transcripts are still the better choice. We highlight the intrinsic robustness of wav2vec 2.0 representations to out-of-vocabulary words as key to better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Pre-training large transformers with self-supervised learning (SSL) has dramatically advanced automatic speech recognition (ASR) <ref type="bibr" target="#b0">[1]</ref> and shown promise for taking spoken language understanding (SLU) to the next level <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. However, it remains an open question whether better ASR is the key to improving SLU or if the speech representations learned with SSL offer a better alternative as input to downstream models. Labeled data for ASR training can be difficult to obtain, so if self-supervised representations are a competitive alternative, this has the potential to democratize SLU.</p><p>Work on speech representation learning has primarily focused on ASR <ref type="bibr" target="#b0">[1]</ref> and other speech-specific tasks, such as emotion recognition <ref type="bibr" target="#b3">[4]</ref>, speaker identification <ref type="bibr" target="#b4">[5]</ref> and phoneme classification <ref type="bibr" target="#b5">[6]</ref>. Tasks that require high-level </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream SLU model</head><p>Self-supervised representation <ref type="figure">Fig. 1</ref>. Self-supervised models, such as wav2vec 2.0, yield state-of-the-art results when fine-tuned for automatic speech recognition. However, we show this step is redundant for many downstream spoken language understanding tasks where self-supervised representations can be used as input.</p><p>semantics have been studied less <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Accordingly, the number of SLU tasks is limited, and many text-based natural language understanding tasks cannot be directly translated to the speech domain due to the difficulty of obtaining word segmentation. For this reason, existing tasks only contain little data <ref type="bibr" target="#b6">[7]</ref> or make use of synthetic speech <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this work, we compare speech features from the wav2vec 2.0 model, state-of-the-art ASR transcripts and ground truth text as input for four SLU tasks that all require knowledge about high-level semantic concepts. While the quality of an ASR model can always be debated, the ground truth text defines an upper bound on ASR performance and serves as a baseline. We consider existing intent classification (IC) and machine translation (MT) tasks and present a novel speech-based named entity recognition (NER) task. Finally, we use a proprietary dataset of 911-calls to define a noisy real-world task of cardiac arrest detection (CAD). Our contributions are as follows: 1. We present the first systematic comparison between text and speech features as input to a broad range of SLU tasks since the recent surge in SSL for speech.</p><p>2. We show that wav2vec 2.0 speech representations are better than ASR transcripts for downstream IC, NER and CAD tasks when labeled data is limited and competitive when data is abundant. For MT, the speech representations fall short of text.</p><p>3. We introduce a speech-based NER task derived from the most widely used dataset for research in ASR and SSL for speech, LibriSpeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASKS</head><p>We start by presenting the four tasks, all based on natural speech. Dataset statistics are found in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Named entity recognition: LibriSpeech</head><p>LibriSpeech <ref type="bibr" target="#b7">[8]</ref> is derived from audiobooks part of the Lib-riVox project <ref type="bibr" target="#b0">1</ref> . Training data for wav2vec 2.0 consist of 60K hours of speech from LibriVox, while the open-source ASR models used in this work are trained on LibriSpeech unless stated otherwise. Defining a downstream task on data from the same domain used to train the SSL model and ASR model corresponds to a common scenario where training data for the different modeling steps overlap. LibriSpeech comes with multiple standardized training subsets <ref type="bibr" target="#b8">[9]</ref> allowing us to study how the downstream model is affected by varying the amount of training data. Finally, LibriSpeech contains two validation and test subsets, clean and other, which offer insight into the importance of recording quality. We provide silver label named entity tags for LibriSpeech. The labels were obtained by using an off-the-shelf Electra language model 2 fine-tuned on the CoNLL-2003 NER task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and applying the model to the ground truth text. We manually reviewed model-induced labels for the validation set to get a sense of data quality. For 1,000 randomly selected examples, human-model agreement is high, with a Krippendorff's alpha of 0.98. 1 https://librivox.org 2 https://huggingface.co/dbmdz</p><p>The task is to predict whether a named entity is contained in the input example. In contrast to classic text-based NER, where each word is tagged with a label, we considered this a binary sequence-based classification task to keep the model setup for text and speech features as similar as possible.</p><p>Conveniently, we find that the dataset is balanced such that approximately 50% of the examples in the training subsets contain a named entity. For validation and test, the fraction is around 30%. We make the dataset available with more details 3 , such as entity type (i.e., person, location, organization or miscellaneous) and entity alignment obtained from <ref type="bibr" target="#b6">[7]</ref> which used the Montreal Forced Aligner 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cardiac arrest detection: Seattle Fire Department</head><p>From a proprietary dataset of 911 emergency calls provided by Seattle Fire Department, WA, USA, we constructed a binary sequence classification task where the objective is to predict whether the caller describes an out-of-hospital cardiac arrest (OHCA) or not. The original dataset contains 1303 OHCA calls and many more not-OHCA calls. We did a random 80-10-10 split of the OHCA calls and sampled a not-OHCA call of similar length to each of the OHCA calls to keep the dataset balanced in terms of target distribution and hours of speech per class. We did not have ground truth text available for this task but report word error rate on a separate subset in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Intent classification: Fluent Speech Commands</head><p>The Fluent Speech Commands (FSC) dataset <ref type="bibr" target="#b6">[7]</ref> consists of 248 unique read-speech commands from 97 speakers instructing a hypothetical intelligent home device to perform an action (e.g., "Turn the lights on in the kitchen"). Recording of the commands was crowd-sourced, resulting in a varied selection of English speakers from the US and Canada. The task was originally phrased as a multi-slot task with three slots: action, object and location. However, due to the small number of slot classes, the task is commonly rephrased as a simple classification task with 31 unique classes.  <ref type="table">Table 2</ref>. Named entity recognition results on the LibriSpeech test sets. All results are given in F1-scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Machine translation: CoVoST 2</head><p>CoVoST 2 is a multilingual speech-to-text translation dataset <ref type="bibr" target="#b11">[12]</ref> derived from the Common Voice speech corpus <ref type="bibr" target="#b12">[13]</ref>. Translations were made by professional translators and the corresponding speech recordings were crowd-sourced. We focused on the English-to-German task using the so-called CoVoST training set and the Common Voice test and validation sets as in the original work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task-specific models</head><p>We are interested in comparing the information content of the input representations, not the models, so we chose a minimalist architecture. All models take as input a sequence of vectors x 1:T = x 1 , x 2 , . . . , x T where x t ? R K and share a similar encoder. A fully connected layer without activation maps each x t to a D-dimensional linear subspace. This linear mapping is the only source of variation in terms of model parameterization between the input representations as it depends on the input dimensionality K; 1024 for wav2vec 2.0 representations, 29 for character-level text and 1,296 to 41,341 for word-level text. The linearly projected features are fed to a bidirectional LSTM with a D-dimensional recurrent state. Hereafter, each task requires a different architecture. For the binary NER and CAD tasks, the LSTM output h 1:T is max-pooled and fed into a single neuron with a sigmoid activation to parameterize a Bernoulli distribution. Similarly, for the IC task, the LSTM output is pooled and mapped to a 31-dimensional vector with softmax normalization to parameterize a categorical distribution. For the MT task, we used an LSTM-based autoregressive decoder with scaled dot-product attention <ref type="bibr" target="#b13">[14]</ref>. We used a vocabulary of 10K subword units <ref type="bibr" target="#b4">5</ref> for the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Speech representations and ASR models</head><p>The wav2vec 2.0 models use contrastive self-supervised learning and are fine-tuned for ASR with a connectionist temporal classification loss. See <ref type="bibr" target="#b0">[1]</ref> for more details. We 5 https://github.com/google/sentencepiece considered two SSL-ASR model pairs downloaded from the FAIRSEQ sequence modeling toolkit <ref type="bibr" target="#b5">6</ref> . For the first pair, the self-supervised wav2vec2.0 model has been trained on 60K hours of speech from LibriLight and fine-tuned on 960 hours from LibriSpeech <ref type="bibr" target="#b0">[1]</ref>. The second pair, which is more robust, adds 3000 hours of conversational and crowd-sourced speech from the Fisher, Switchboard and CommonVoice corpora to the self-supervised training, while the ASR model was finetuned using the 300 hours from Switchboard <ref type="bibr" target="#b14">[15]</ref>. All models use the same architecture. We tested the two ASR models on the validation set for each task and chose the model pair corresponding to the lowest word error rate. For the IC and CAD tasks, the robust ASR model was better.</p><p>As shown in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, the top layers of wav2vec 2.0 are a poor choice of input to phoneme classification and ASR. We ran a small initial experiment with limited training data to determine which output from the 24 transformer layers in the wav2vec 2.0 architecture to use as input to the downstream tasks. We found that layer 15 yielded the best results. This layer has also been found to provide the best results for phoneme classification <ref type="bibr" target="#b5">[6]</ref>, and layers 13 through 16 have been shown to contain the highest level of correlation with text-based word embeddings <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>All models were trained by minimizing cross-entropy and use D = 256. In the very low-resource settings, we also tested smaller dimensionalities to reduce overfitting, but this did not improve results. We used the Adam optimizer <ref type="bibr" target="#b18">[18]</ref> with a fixed learning rate of 3 ? 10 ?4 for the first half of training before annealing it to 5 ? 10 ?5 during the second half. Batch size and validation frequency were tuned for each task on the ASR character-level. We ensured that the number of training steps was large enough to reach convergence for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Results for each of the four tasks are presented below. We report the metric commonly used in previous work for the existing tasks. GT refers to ground truth in tables 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Named entity recognition: LibriSpeech</head><p>The wav2vec 2.0 representations showed impressive performance on the 10-hour subset, as seen in table 2, where textbased models were only slightly better than a random baseline. Even with 100 hours of labeled data, they were superior. The gap closed at 960 hours. In general, models trained on ground truth text performed better on the other subset, whereas speech-based models always performed best on the clean subset, highlighting the speech features' sensitivity to noisy conditions. Although the ASR transcripts are also affected by noise, they gave more robust results, as these models performed better on the other subset in all but one case.</p><p>On examples that exclusively contain named entities that are out-of-vocabulary, wav2vec 2.0 representations gave an error rate of 23% when trained on 100 hours. ASR transcripts gave a substantially higher error rate of 36%. This underscores the large amount of data needed for robust outof-vocabulary named entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cardiac arrest detection: Seattle Fire Department</head><p>Considering the observation that ASR transcripts are more noise-robust than wav2vec 2.0 representations, we might expect them to fare better on noisy 911-calls. However, as seen in table 3, the wav2vec 2.0 representations still yielded better results. Unlike the NER task, it is possible that speech-based features, such as emotion and rate of speech, might prove useful for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Intent classification: Fluent Speech Commands</head><p>As mentioned in the task description, every speaker in this dataset read the same 248 commands. As a result, training, validation and test subsets contain the same 248 identical examples when we consider ground truth text, which leads to an accuracy of 100% as seen in table 3. While the task is generally considered to require semantic understanding <ref type="bibr" target="#b2">[3]</ref>, which is also why we include it here, it can be solved to perfection by a many-to-one sentence recognizer (i.e., different sentences map to the same intent). The wav2vec 2.0 representations were slightly better than the ASR transcripts and very close to the more complex state-of-the-art ASR-based system from <ref type="bibr" target="#b19">[19]</ref> which reached 99.7% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Machine translation: CoVoST 2</head><p>Unsurprisingly, we find that our simple ASR-based MT system was a lot worse than ground truth text. The wav2vec 2.0 representations were even worse. These results are not surprising considering the generally large gap between speech and text-based approaches <ref type="bibr" target="#b11">[12]</ref>. We hypothesize that the lack of simple morphological features, like word boundaries, is a challenge to overcome for a shallow model trained on speechbased representations. To test this hypothesis, we trained the  <ref type="table">Table 3</ref>. Results for cardiac arrest detection, intent classification and machine translation.</p><p>model on the ASR character-level transcripts without whitespaces (e.g., HOW ARE YOU ? HOWAREYOU) which resulted in a notable drop from BLEU 11.5 to 9.7, but not enough to explain the gap between the two representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>This work should not be seen as a quest to remove ASR from the SLU pipeline. Automatically generated transcripts offer an important layer of interpretability in modern speech applications. Furthermore, we did not explore how text-based language models can be modified to handle error-prone transcripts, which is a promising direction for SLU <ref type="bibr" target="#b1">[2]</ref>. However, this work is highly relevant when large quantities of unlabeled speech data can be easily obtained, but no or limited text data is readily available -such as in an emergency call center. Our work suggests that ASR fine-tuning can be avoided for downstream SLU tasks. Interestingly, it was recently found that word meaning is shifted towards the output layer of the model when wav2vec 2.0 is fine-tuned for ASR <ref type="bibr" target="#b17">[17]</ref>. Our work highlights the feasibility of extracting this knowledge directly from the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We compared self-supervised speech features from wav2vec 2.0 with automatic speech recognition transcripts and ground truth text as input to a simple model on four spoken language understanding tasks. Interestingly, it turned out that wav2vec 2.0 representations yielded better performance than speech recognition transcripts with up to 100 hours of training data for cardiac arrest detection, named entity recognition and intent classification. Only when 960 hours of labeled training data was available, the speech recognition-based approach yielded a slight improvement on the named entity recognition task. For machine translation, the wav2vec 2.0 representations were inferior to the text-based features. Our results on the classification tasks have implications for how to tackle spoken language understanding tasks with limited training data demonstrating that the traditional automatic speech recognition step can be bypassed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>arXiv:2111.14842v1 [eess.AS] 29 Nov 2021 Basic dataset statistics for the SLU tasks. Number of examples in the subsets. Duration in hours for all subsets.Median example length in seconds. Word error rate (WER) on the validation set for the ASR models presented in 3.2.</figDesc><table><row><cell>Number of examples</cell><cell>Duration Median</cell><cell>WER</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/borgholt/ner-librispeech 4 https://montreal-forced-aligner.readthedocs.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/pytorch/fairseq</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised spoken language understanding via self-supervised speech and language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7468" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Superb: Speech processing universal performance benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Shu-Wen Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I Jeff</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01051</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning problemagnostic speech representations from multiple selfsupervised tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03416</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Chun</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6419" to="6423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11084</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7669" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CoVoST 2 and massively multilingual speech-to-text translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10310</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020</title>
		<meeting>the 12th Conference on Language Resources and Evaluation (LREC 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4211" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On scaling contrastive representations for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Borgholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Tycho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Havtorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3885" to="3889" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Layer-wise analysis of a self-supervised speech representation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Pasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04734</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech-language pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximo</forename><surname>Bianv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7458" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
