<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-07">Date of publication September 7, 2021, date of current version September 17, 2021.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
							<email>khoavoho@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72703</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
							<email>kyamazak@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72703</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Truong</surname></persName>
							<email>sangt@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72703</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science -VNU-HCM</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Sugimoto</surname></persName>
							<email>sugimoto@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ngan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72703</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-07">Date of publication September 7, 2021, date of current version September 17, 2021.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.3110973</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TERMS Temporal Action Proposal Generation</term>
					<term>Temporal Action Detection</term>
					<term>Agent-Aware Bound- ary Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation (TAPG) aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet plays an important role in many tasks of video analysis and understanding. Despite the great achievement in TAPG, most existing works ignore the human perception of interaction between agents and the surrounding environment by applying a deep learning model as a black-box to the untrimmed videos to extract video visual representation. Therefore, it is beneficial and potentially improve the performance of TAPG if we can capture these interactions between agents and the environment. In this paper, we propose a novel framework named Agent-Aware Boundary Network (ABN), which consists of two sub-networks (i) an Agent-Aware Representation Network to obtain both agent-agent and agents-environment relationships in the video representation, and (ii) a Boundary Generation Network to estimate the confidence score of temporal intervals. In the Agent-Aware Representation Network, the interactions between agents are expressed through local pathway, which operates at a local level to focus on the motions of agents whereas the overall perception of the surroundings are expressed through global pathway, which operates at a global level to perceive the effects of agents-environment. Comprehensive evaluations on 20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone networks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly outperforms state-ofthe-art methods regardless of the employed backbone network on TAPG. We further examine the proposal quality by leveraging proposals generated by our method onto temporal action detection (TAD) frameworks and evaluate their detection performances. The source code can be found in this URL a . a https://github.com/vhvkhoa/TAPG-AgentEnvNetwork.git Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</p><p>exhaustively; (ii) cover multi-duration actions; (iii) generate reliable confidence scores so that proposals can be retrieved properly [6]. Despite recent endeavors [6], [12], [13], TAPG remains as an open problem, especially when facing realworld problems such as action duration variability, activity complexity, camera motion, viewpoint changes, etc. In spite of good achievements on benchmarking datasets, the existing TAPG approaches have some limitations as follows:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Temporal action proposal generation (TAPG) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b13">[14]</ref> is one of the most key and fundamental tasks in video understanding i.e. action recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, video summarization <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, video captioning <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, video recommendation <ref type="bibr" target="#b20">[21]</ref>, video highlight detection <ref type="bibr" target="#b21">[22]</ref>, and smart surveillance <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Given an untrimmed video, TAPG aims to propose temporal intervals with specific starting and ending timestamps for each action. Most of existing TAPG approaches first detect a set of possible starting and ending timestamps of all actions separately, and then a proposal evaluation module is employed to evaluate every possible pair of starting and ending timestamps by predicting its confidence score. The non-maximum suppression (NMS) function is finally used to eliminate redundant candidate proposals based on their confidence scores and overlapping metrics. A robust TAPG method should be able to (i) generate temporal proposals with actual boundaries to cover action instances precisely and ? In previous works, video visual representation is extracted by directly applying a backbone model, e.g. C3D/I3D <ref type="bibr" target="#b24">[25]</ref>, Two-Stream network <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b25">[26]</ref> or SlowFast network <ref type="bibr" target="#b15">[16]</ref> into the whole spatial dimensions of the video (or entire snippet). This makes the predictions biased to the background (or environment) instead of agents who commit actions because the agents with their actions usually occupy a small region compared to the entire frame. ? A temporal action proposal is a combination of three entities i.e. agent, action, and environment; however, the existing approaches do not have any mechanism to present such combination as well as express the relationship between these entities.</p><p>To address the aforementioned limitations, we leverage human perception process of a temporal action proposal which is a combination of three entities i.e. agent, action, and environment and we propose a novel Contextual Agent-Aware Boundary Network (ABN). Our ABN consists of two main sub-networks i.e. Agent-Aware Representation Network and Boundary Generation Network. The first sub-network aims to extract video visual representation i.e. contextual agentaware visual feature, given an untrimmed video whereas the second sub-network aims to estimate the confidence score matrices and the probabilities of starting time and ending time given a video feature. To interpret those entities of agent, action, and environment, our Agent-Aware Representation Network comprises of two semantic pathways corresponding to local pathway, which locally extracts information from the agents who commit actions and global pathway, which globally extracts information from entire environment. Furthermore, the number of agents in a given video can be arbitrary; however, a few of them are actually committing the action. To extract a semantic local feature, we apply a selfattention module. The final video feature combines both local feature and global feature through a self-attention module. The second sub-network, Boundary Generation Network, takes contextual agent-aware visual feature as an input and consists of three modules corresponding to Base Module to model the temporal relationship as well as provide a shared feature sequence for later modules of Temporal Assessment Module (TAM) and Proposal Assessment Module(PAM). The overall flowchart of our proposed ABN is given in <ref type="figure">Fig.1</ref>.</p><p>Our main contributions are summarized as follows:</p><p>1) Leveraging the human perception process of understanding an action which combines agents, action and environment, we propose an end-to-end contex-tual ABN for TAPG. Our ABN contains two subnetworks corresponding to (i) Agent-Aware Representation Network for extracting semantic video feature given untrimmed video and (ii) Boundary Generation Network to evaluate confidence scores of densely distributed proposals. 2) Introducing Agent-Aware Representation Network, a novel video contextual visual representation, for extracting video feature from an untrimmed video. Our semantic Agent-Aware Representation Network involves two parallel pathways: (i) local pathway to tell what agents are doing (ii) global pathway to express the relationship between the agents and the environment. 3) Investigating the impacts of agents and the environment as well as the interaction between agents and their environment in our proposed ABN framework. 4) Examining the action proposal quality and effectiveness of our proposed ABN by putting proposals that it generated to TAD framework and evaluate its detection performance. 5) Benchmarking the proposed ABN on popular datasets in both TAPG and TAD, namely ActivityNet-1.3 with three different backbone networks (i.e. C3D, SlowFast and Two-Stream) and THUMOS-14 with two backbone networks (i.e. C3D and Two-Stream). Our proposed ABN has achieved state-of-the-art performance on both TAPG and TAD regardless of backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. TEMPORAL ACTION PROPOSAL GENERATION (TAPG)</head><p>TAPG aims to propose temporal intervals that may contain an action instance with their temporal boundaries and confidence in untrimmed videos. In general, TAPG can be categorized into three groups i.e. anchor-based and boundary-based and hybrid anchor-boundary-based as follows:</p><p>? The anchor-based TAPG methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[27]</ref> refer to the temporal boundary refinements of predefined anchors or sliding windows. Those methods are inspired by the achievements of anchor-based object detectors in still images like Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, Reti-naNet <ref type="bibr" target="#b28">[29]</ref>, or YOLOv3 <ref type="bibr" target="#b29">[30]</ref>. These methods discretize the proposal task into a classification task where multiple predefined anchors with different lengths are used as classes and a class that best fits the ground truth action length is regarded as ground truth true class for training. In such approaches, a large number of proposals are densely generated. Although this approach helps to save computational costs, this approach lacks the flexibility of action duration. ? The boundary-based TAPG methods <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> resolve the above problem by breaking every action interval into starting and ending points and learn to predict them. In those methods, there are two stages corresponding to generating the boundary probability sequence and applying the Boundary Matching mechanism to generate candidate proposals. In inference time, starting and ending probabilities at every time in the given video are predicted, then, those with local peaks will be chosen as potential boundaries. The potential starting points are paired with potential ending points to become a potential action interval when their interval fits in the predefined upper and lower threshold, along with a confidence score being a multiplication of the starting and ending probabilities. As one of the first boundary-based methods, <ref type="bibr" target="#b10">[11]</ref> defined actionness scores by grouping continuous high-score regions as the proposal. Later, boundary-sensitive method <ref type="bibr" target="#b11">[12]</ref> proposed a two-stage strategy where boundaries and actionness scores at every temporal point are predicted in the first stage and fused together, filtered by Soft-NMS to get the final proposals at the second stage. ? In order to make use of the advantages of both anchorbased methods and boundary-based methods, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref> proposed hybrid approaches in which the boundary detection and the dense confidence regression are performed simultaneously by using ROI align. Based on the observation that anchor-based methods can uniformly cover all segments in videos but imprecise while boundary-based methods may have more precise boundaries but it may omit some proposals when the quality of actionness score is low, <ref type="bibr" target="#b1">[2]</ref> proposed Complementary Temporal Action Proposal (CTAP) generator. BMN <ref type="bibr" target="#b5">[6]</ref> is an improvement of BSN <ref type="bibr" target="#b11">[12]</ref>. In BMN, a boundary-matching matrix is generated instead of actionness scores to capture an action-duration score for more descriptive final scores, which help to improve the final proposals' prediction. Continuously, drawing the inspiration from BSN <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> proposed Dense Boundary Generator (DBG) and implemented the boundary classification and action completeness regression for densely distributed proposals.</p><p>The TAPG approaches can be summarized in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Temporal Action Proposal Generation (TAPG) Anchor-based Length and language model <ref type="bibr" target="#b4">[5]</ref> Sparse dictionaries <ref type="bibr" target="#b6">[7]</ref> SMulti-stage CNNs <ref type="bibr" target="#b7">[8]</ref> Temporal Unit Regression <ref type="bibr" target="#b8">[9]</ref> Single Stream <ref type="bibr" target="#b0">[1]</ref> Complementary Temporal Action Proposal Generation <ref type="bibr" target="#b1">[2]</ref> Single Shot Action Detector <ref type="bibr" target="#b26">[27]</ref> Boundary-based Structured Segment Networks <ref type="bibr" target="#b10">[11]</ref> Boundary Sensitive Network <ref type="bibr" target="#b11">[12]</ref> Multi-granularity generator <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TEMPORAL ACTION DETECTION (TAD)</head><p>Depending on spatial or temporal domain, action detection approaches can be categorised into either TAD (TAD) or spatial action detection (SAD) or spatial-temporal action detection. TAD aims to find the temporal intervals of starting action and ending action whereas SAD searches for human region and the corresponding human action in spatial domain. In this work, we focus on on TAD which provides the answer of what and when the action happens in a video.</p><p>Due to action recognition is a part of TAD, thus, most of the early TAD methods were built based on hand-crafted features, the same as action recognition. Early TAD methods are based on efficient spatio-temporal feature representations and motion propagation across frames in videos such as HOG3D <ref type="bibr" target="#b30">[31]</ref>, SIFT3D <ref type="bibr" target="#b31">[32]</ref>, ESURF <ref type="bibr" target="#b32">[33]</ref>, MBH <ref type="bibr" target="#b33">[34]</ref> etc. As the performance of the methods using hand-crafted features became stabilized, TAD has reached a levelling off. With the convolutional neural networks (CNNs) was developed <ref type="bibr" target="#b34">[35]</ref>, a lot of effective TAD approaches have proposed. In general, TAD can be divided into either onestage detection or two-stage detection.</p><p>In one-stage framework, both temporal proposal and action classification are learnt simultaneously. Due to the similarity between TAD and object detection example, SSAD <ref type="bibr" target="#b26">[27]</ref>, SS-TAD <ref type="bibr" target="#b35">[36]</ref> made use of single-shot detector to solve TAD with one-stage detection. While both SSAD and SS-TAD make use of C3D feature <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, SS-TAD adopts the anchor mechanism and the stacked GRU units.</p><p>Unlike one-stage framework, two-stage approach is based on the paradigm of proposal generation-and-then classification i.e. extracts temporal proposals first, and then processes with the classification and regression operations. Similar to proposal generation in object detection, TAPG plays the most important role in TAD in this two-stage approach paradigm. Two-stage framework is the mainstream method, so most papers adopt this. TAD can be implemented by: (i) sliding windows such as S-CNN <ref type="bibr" target="#b7">[8]</ref> which fixes some size sliding windows to generate various sizes video segments, and then deal with them by a multi-stage network. S-CNN is build on C3D feature and contains thee sub-networks corresponding to TAPG, classification and localization; However, S-CNN is time consumption when increasing the overlap between the windows to obtains good performance, TURN <ref type="bibr" target="#b8">[9]</ref> leverages Faster RCNN to improe S-CNN by intergrating boundary regression network. (ii) boundary network such as BSN <ref type="bibr" target="#b11">[12]</ref>, BMN <ref type="bibr" target="#b5">[6]</ref>, DBG <ref type="bibr" target="#b13">[14]</ref> which aim to deal with video actions of different lengths and with precise temporal boundaries as well as reliable confidence scores. BSN first locates the boundaries of the temporal action segments i.e. starting time and ending time. Both starting time and ending time are then combined into temporal proposal. Based on the sequence of action confidence scores for each candidate proposal, a 32-dimensional proposal-level feature is extracted and benchmarked for evaluating the confidence of the temporal proposals. BMN and DBG are both improvements of BSN with a new confidence evaluation and boundary-matching mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. VIDEO FEATURE REPRESENTATION</head><p>Following the success of CNNs on image tasks. In <ref type="bibr" target="#b39">[40]</ref>, Tran et al. proposed a simple linear model named C3D which outperforms all previous best-reported methods. By transferring the 2D pre-trained model to 3D model, <ref type="bibr" target="#b40">[41]</ref> proposed I3D. In I3D, the 3D filters are replaced by a set of repeated 2D filters. [15] and then they have been improved in <ref type="bibr" target="#b25">[26]</ref>. Two-Stream networks explore video appearance and motion clues with two separate networks. One network is to exploit spatial information from individual frames while the other uses temporal information from optical flow. The outputs of the two networks are then combined by late fusion. RNN/LSTM is believed to cope with sequential information better, and thus many proposed methods <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> attempted to incorporate LSTM to deal with action recognition. 3D networks, which were first introduced by <ref type="bibr" target="#b44">[45]</ref>, extract features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. Later on, C3D features, 3D CNN architectures and their improvements <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> have been proposed. In <ref type="bibr" target="#b39">[40]</ref>, Tran et al. proposed a simple linear model named C3D which outperforms all previous best-reported methods. By transferring the 2D pretrained model to 3D model, <ref type="bibr" target="#b40">[41]</ref> proposed I3D. In I3D, the 3D filters are replaced by a set of repeated 2D filters. Inspired by the success of ResNet in image classification, Hara, et al. extended ResNet architecture to 3D CNN and proposed 3D ResNet <ref type="bibr" target="#b41">[42]</ref>. In their work, they examined various 3D CNN architecture under different backbone such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152, ResNet-200, DenseNet-121 and ResNeXt-101. Recently, SlowFast network <ref type="bibr" target="#b15">[16]</ref> is a variation of 3D CNN networks category, in which two parallel pathways are utilized to capture appearances of video scene and object motion in each pathway. SlowFast networks have been proposed to tackle the action recognition and action spatial localization tasks and got the highest scores in many benchmark datasets e.g. Kinetics, Charades, AVA, etc.</p><p>Our proposed ABN belongs to the category of two-stage TAD and boundary-based TAPG approach where our focus is human perception-based video feature extraction which aims to obtain semantic video representation followed by human perception of action understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR PROPOSED METHOD: AGENT-AWARE BOUNDARY NETWORK (ABN) A. PROBLEM FORMULATION</head><p>Considering an untrimmed video V = {x l } L l=1 with L frames, our goal is to generate a set of temporal segments which inclusively and tightly contain actions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a set of ground truth action segments</head><formula xml:id="formula_0">A = {a i = (s i , e i )} M i=1</formula><p>having M temporal segments with an action segment comprised of a starting timestamp (s i ) and an ending timestamp (e i ), our objective is formalized by minimizing the following objective function:</p><formula xml:id="formula_1">L = M i=1 log p(a i |V)<label>(1)</label></formula><p>As proposed by <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b5">[6]</ref>, the above objective function may also be achieved indirectly by decomposing the action proposal generation problem into detecting starting and end-ing timestamps of every actions together with their duration, as formulated below:</p><formula xml:id="formula_2">L = M i=1 log p(s i |V) + log p(e i |V) + log p(e i ? s i |s i ) (2)</formula><p>Instead of representing the video frames separately as individual feature vectors, in our proposed framework, we divide the video V into T = L ? non-overlapping snippets of ? consecutive frames. Each snippet of ? frames captures the motions taking place alongside the appearances, which are crucial in detecting the starting and ending timestamps for each action.</p><p>We denote ? as a feature extraction function which is applied to every ?-frame snippet, the visual representation feature sequence F of the entire video V can be defined as:</p><formula xml:id="formula_3">F = {f i } T i=1 = {?(x ??(i?1)+1 , ..., x ??i )} T i=1<label>(3)</label></formula><p>In most of the previous works <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, function ? is simply defined as the extraction of a feature vector from a hidden layer of C3D Network <ref type="bibr" target="#b24">[25]</ref>, Two-Stream Network <ref type="bibr" target="#b14">[15]</ref>, or SlowFast Network <ref type="bibr" target="#b15">[16]</ref> given a ?-frame snippet. However, as stated in the above sections, this may cause insufficient or noisy information capture because actions and the agents, who create the actions, usually take place in small spatial regions of the video.</p><p>Hence, in this work, we propose a novel action proposal generation model in videos, named Agent-Aware Boundary Network, equipped with a new feature extraction mechanism for the function ? namely Contextual Agent-Aware Representation Network (described in Sec. III-B1) to be able to incorporate information from both agents and the interaction between them and surrounding environment. Our proposed ABN with the Contextual Agent-Aware Representation Network can be developed on any backbone such as C3D Network <ref type="bibr" target="#b24">[25]</ref>, Two-Stream Network <ref type="bibr" target="#b14">[15]</ref>, or the latest model of SlowFast Network <ref type="bibr" target="#b15">[16]</ref>. More details are discussed in section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. NETWORK DESIGN</head><p>The ABN consists of two sub-networks and is demonstrated in <ref type="figure">Fig. 1</ref>. The first sub-network, Contextual Agent-Aware Representation Network, extracts contextual Agent-Aware visual representation of a ?-frame snippet at both global and local levels and is detailed in section III-B1. The second subnetwork, Boundary Generation Network, takes the first component as the input and generates the action proposals and is described in section III-B2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Contextual Agent-Aware Representation Network</head><p>The contextual Agent-Aware representation network extracts contextual video visual representation of a ?-frame snippet, which makes a significant contribution in TAPG. Considering our goal is extracting features for a ?-frame snippet from frame t to frame t + ?, our Agent-Aware representation network is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref> and consists of four steps, i.e.</p><p>(i) backbone feature extraction, (ii) global feature extraction, (iii) local feature extraction, and (iv) feature fusion. Notably, there are two kinds of feature extracted in our proposed network, i.e., environment feature extracted through step 2 plays a role of global semantic level, and agent feature extracted through step 3 plays a role of local semantic level.</p><p>Step 1: Backbone Feature Extraction: The backbone network is used to encode global semantic information of the entire ?-frame snippet. In order to prove the robustness of our proposed ABN, which is independent to the backbone network, we adopt Two-Stream, C3D <ref type="bibr" target="#b24">[25]</ref> and SlowFast <ref type="bibr" target="#b15">[16]</ref> networks in our experiments. These networks process a snippet of frames through multiple blocks of residual convolutional layers, with each block</p><formula xml:id="formula_4">B i ? {B i } 4 i produces a feature map S i ? {S i } 4</formula><p>i . Assume N is the last block of the backbone network, the feature map S N is then used as input for the two parallel pathways as in the next two steps.</p><p>Step 2: Global Feature Extraction: To extract global feature, the feature map S 4 keeps going through average pooling and several fully connected layers until the second last layer, forming a vector which captures the overall scene, namely the global feature ? e . Because all spatial dimensions are processed, this pathway captures the abstract information at the global level of the scene, however, it may not able to capture the details like motions of agents inside.</p><p>Step 3: Local Features Extraction: The local features extraction consists of two procedures. First, the local semantic feature vectors of each agent appearing in the video snippet are extracted. Then, all local feature vectors extracted from the first step are fused together to form an agent-aware feature vector. In order to fuse an arbitrary number of local semantic features, we employ a self-attention module with an average pooling layer, which is discussed below.</p><p>For local semantic feature vectors extraction, we first detect agent appear in the ?-frame snippet by a human detector. The center frame of the snippet is heuristically selected to feed into the detector because it is least diverged compared to frames at both ends of the snippet. We utilize Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> model pre-trained on COCO dataset <ref type="bibr" target="#b47">[48]</ref> as our human detector after eliminating all object classes except the 'person' class. Detected human bounding boxes with confidence scores above 0.5 are then used to guide the RoIAlign <ref type="bibr" target="#b48">[49]</ref> to extract features from S 4 , each feature storing local information about appearance and motion of the corresponding agent, called the local semantic feature.</p><p>After a set of local semantic features is formed, we employ a self-attention module to fuse them together into a single local agent-aware feature ? a . The self-attention module looks at the local semantic feature of each agent and assigns upweights to agents who play important roles in the video snippet or are committing observed actions while assigning down-weights to the minor role agents.</p><p>In this step, the Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> works as a hard attention module which eliminates all the background and only emphasizes humans or agents moving in the scene. On the other hand, the self-attention module works as a soft <ref type="bibr">VOLUME 4, 2016</ref> Contextual Agent Aware Feature ... Center frame ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 1: Backbone Feature Extraction Snippet</head><p>Human Detector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROI Align</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Pooling</head><p>Step 2: Global Feature Extraction Global Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Agent Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Attention</head><p>Local feature (Agent feature)</p><p>...</p><p>Step 3: Local Feature Extraction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Attention</head><p>Step 4: Feature Fusion Agent-Aware Representation Network attention module which helps to concentrate on the right agents but also keeps information of the other agents because the activities we observe may require the interaction between these agents.</p><p>Step 4: Feature Fusion: Finally, the environment feature ? e and the agent aware feature ? a are fused by another selfattention module (discussed below). While simultaneously processing these features, the self-attention module would reweight them by a proper ratio, which helps the overall model to know which type of information to consider while reasoning the action proposals, i.e. deciding whether to emphasize on local information of the agents or global information of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Module</head><p>In both TAPG and TAD, an arbitrary number of agents may appear in each snippet, which leads to difficulty to combine them into a single feature vector attentively to represent the snippet. Inspired by that problem, we propose a self-attention module which adopts the Transformer Encoder model <ref type="bibr" target="#b49">[50]</ref> to learn to re-weight the importance of the semantic features set based on each of themselves and fuse them together by an average pooling operation.</p><p>The self-attention module is employed twice within a snippet, i.e. (i) encode the list of individual agent features to a single multi-agent feature and (ii) fuse both the environment feature ? e and the multi-agent feature ? a to a snippet feature f . Generally, a Transformer Encoder model will encode the set of input features ? = {? i } ? i=1 to three matrices of latent states, namely keys K = (k i = ? k (? i )), queries Q = (q i = ? q (? i )) and values V = (v i = ? v (? i )). Notably, in the agent feature extraction (step 2), ? is equivalent to the number of agents, ? is corresponding to individual feature and ? is a single multi-agent feature whereas in the feature fusion (step 4) ? is set as 2 and ? is corresponding to environment feature and multi-agent feature and ? is snippet feature. ? is defined as a fully connected layer. For each query state q i of an input feature, an attention function defined in Eq. 4 maps a query q i and a set of key-value pairs (K, V ) to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key as follows:</p><formula xml:id="formula_5">A(q i , K, V ) = softmax( q i ? K T ? d K )V<label>(4)</label></formula><p>where d K is the number of dimensions in key states. Then, an average pooling layer is applied to fuse the resulting matrix ? A = A(q i , K, V ) ? i=1 and form the overall context feature based on input features set.</p><p>The proposed self-attention model is utilized in our proposed ABN in a differentiable fashion and is trained along with the other parts of our network in an end-to-end way, hence, the resulting model may be able to properly generate the contextual Agent-Environment feature, which decreases the impact of background information in every snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Boundary Generation Network</head><p>Our ABN belongs to the category of boundary-based approach and the boundary sub-network, i.e. boundary generation network, contains three modules i.e. Base Module, Temporal Assessment Module (TAM) and Proposal Assessment Module (PAM). These modules are described follows:</p><p>Base Module first processes the feature sequence F , which is extracted from the video by our contextual Agent-Aware representation network, through several 1D convolutional layers to extract temporal relationships between nearby snippet features. Those 1D convolutional layers are designed with a stride of 1 and same padding to reserve temporal length of the output feature sequence. </p><formula xml:id="formula_6">? D ? T O 6 : 512 ? 1 ? D ? T 7 Squeeze O 6 : 512 ? 1 ? D ? T O 7 : 512 ? D ? T 8 2DConv. 128 ? 1 ? 1/(0, 0) , ReLU O 7 : 512 ? D ? T O 8 : 128 ? D ? T 9 2DConv. 128 ? 3 ? 3/(1, 1) , ReLU O 8 : 128 ? D ? T O 9 : 128 ? D ? T 10 2DConv. 2 ? 1 ? 1/(0, 0) , Sigmoid O 9 : 128 ? D ? T O P : 2 ? D ? T</formula><p>Temporal Assessment Module (TAM) takes the features sequence from base module and estimates probabilities of every temporal location being a starting or ending boundary.</p><p>Proposal Assessment Module (PAM) also takes the features sequence from base module and produces two matrices, each of which densely contains the confidence scores of every possible duration at every starting temporal point, but are trained by two different types of loss functions as suggested by <ref type="bibr" target="#b5">[6]</ref>. These matrices would have a shape of D ? T with D is the maximum length of the proposals in snippets that we consider and T is the number of snippets. In this work, we set D = T for ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> and D = T /2 for THUMOS-14 <ref type="bibr" target="#b51">[52]</ref> as suggested by <ref type="bibr" target="#b5">[6]</ref>.</p><p>Network architecture of Boundary Generation Network is given in <ref type="table" target="#tab_2">Table 1</ref>. In <ref type="table" target="#tab_2">Table 1</ref>, base module is represented by layer 1 to layer 3, temporal assessment module is represented by layer 4 and proposal assessment module is represented by layer 5 to layer 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TRAINING PHASE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Label Generation</head><p>We follow <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref> to generate the ground truth labels for training process including starting labels, ending labels for TAM training and duration labels for PAM training.</p><p>The starting and ending labels are generated for every snippet of the video, which are called L S = {l s n } T n=1 and L E = {l e n } T n=1 , respectively. The boundaries timestamps (starting and ending) of every action instance a i = (s i , e i ) are rescaled into T -snippet range by multiplying them with T ?fps L where fps is the frame rate of the video and the action instance a i ? A, A = {a i } M i=1 . After rescaling, the action instance a i becomes a new action instance a ? i = (s ? i , e ? i ). For every snippet t n ? T , we denote a temporal region r n = [t n ? 1 2 , t n + </p><formula xml:id="formula_7">= {r s i } M i=1 and R E = {r e i } M i=1</formula><p>for starting and ending boundaries, respectively. Finally, starting label l s n and ending label l e n of a snippet t n are calculated by the following functions:</p><formula xml:id="formula_8">l s n = ? ? ? 1, M i=1 rn?r s i r s i ? 0.5 0, otherwise l e n = ? ? ? 1, M i=1 rn?r e i r e i ? 0.5 0, otherwise</formula><p>The duration labels for a video are gathered into a matrix L D ? [0, 1] D?T where D is the maximum length of proposals being considered in number of snippets, as suggested in <ref type="bibr" target="#b5">[6]</ref>, we set D = T in all of our experiments. With an element at position (t i , t j ) stands for a proposal action a p = (t s = tj ?T tv , t e = (tj +ti)?T tv ), it will be assigned by 1 if its Interaction-over-Union with any ground truth action in A = {a i } M i=1 reach a local maximum, or 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Loss function</head><p>As mentioned in section III-B2, TAM will generate probabilities vectors of starting and ending boundaries (P S ? R T and P E ? R T ), while PAM will generate two actionness scores matrices P cc D ? R D?T and P cr D ? R D?T . These four outputs are trained simultaneously by different loss functions as following:</p><formula xml:id="formula_9">L T AM = L bin (P S , L S ) + L bin (P E , L E ) (5) L P AM = L bin (P cc D , L D ) + ? reg ? L 2 (P cr D , L D ) (6) L = ? 1 ? L T AM + ? 2 ? L P AM<label>(7)</label></formula><p>As proposed by <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we set ? reg = 10 and ? 1 = ? 2 = 1, furthermore, L bin is a weighted binary log-likelihood function to deal with imbalanced number of negative and positive examples in groundtruth labels. Generally, L bin (? , Y ) <ref type="bibr">VOLUME 4, 2016</ref> between prediction? ? R N and groundtruth Y ? R N is defined as follows: <ref type="bibr" target="#b7">(8)</ref> where ? is multiplication operator. The weighting parameters are automatically set by number of positives and negatives, specifically, ? + = N N + and ? ? = N N ? , with N , N ? and N + are total number of examples and total number of positive and negative examples, respectively.</p><formula xml:id="formula_10">1 N N i=1 ? + ? Y i ? log? i + ? ? ? (1 ? Y i ) ? log (1 ?? i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. INFERENCE PHASE</head><p>During inference, four outputs are generated by the boundary generation network from the features sequence extracted by our ABN, including P S , P E from TAM output (output of layer 4 in <ref type="table" target="#tab_2">Table 1</ref>) and P cc D , P cr D from PAM output (output of layer 10 in <ref type="table" target="#tab_2">Table 1</ref>). Peaking probabilities of starting and ending boundaries from P S and P E , which are local maximums, are selected to form initial proposals by pairing every peak starting point with peak ending points behind them and within a pre-defined range. For a proposal formed by t s and t e boundaries with duration d p = t e ? t s , its score s p , as proposed in <ref type="bibr" target="#b5">[6]</ref>, are computed as follows:</p><formula xml:id="formula_11">s p = P S [t s ] ? P E [t e ] ? P cc D [d p , t s ] ? P cr D [d p ,</formula><p>t s ] (9) Then, with a list of proposals and their scores, a Soft-NMS <ref type="bibr" target="#b52">[53]</ref> is applied to eliminate highly overlapped proposals before outputting the final list of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. DATASETS &amp; METRICS 1) Datasets</head><p>We evaluate our proposed method on two benchmark datasets, namely ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> and THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>. ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> is a large scale dataset for benchmarking methods in human activity understanding problems, in which, action proposals and action detection are the centers of attention. The dataset contains 200 distinct activity classes and a total of 849 hours of videos collected from YouTube. ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> contains roughly 20K untrimmed videos which are divided into training, validation and test sets with the ratio of 0.5, 0.25 and 0.25, respectively. Each video in ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> is annotated with one or more temporal intervals accommodating any activity out of 200 activities of interest. Due to the unavailability of annotations on test splits, we compare and report performances of our approach and other state-of-the-art methods on the validation set, unless otherwise stated. THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>, on the other hand, is primarily a dataset for action recognition. Fortunately, a track of action localization and detection are derived from a portion of its videos. Concretely, 200 and 214 untrimmed videos are extracted from the validation and test sets of THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>, respectively, for training and testing methods in action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Metrics</head><p>To comprehensively evaluate the performance of the proposed ABN, we not only evaluate it in action proposals generation task, but also in action detection task.</p><p>For action proposals generation, on both ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> and THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>, we measure AR with different Average Numbers (ANs) of proposals, denoted as AR@AN. AN is defined as the average number of proposals kept by every video in the dataset. Temporal intersection over union (tIoU) is used as the sole metric to classify a proposal. We follow the traditional practice, tIoU thresholds set from 0.5 to 0.95 with a step size of 0.05 are used on ActivityNet-1.3, while tIoU thresholds set from 0.5 to 1.0 with a step size of 0.05 are used on THUMOS-14. On ActivityNet-1.3 particularly, we report the score of area under the Average Recall (AR) versus Average Number of Proposals per Video curve (AUC), with the average number of proposals ranges from 0 to 100.</p><p>For action detection task, following previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we mainly evaluate our method by Mean Average Precision at tIoU (mAP@tIoU), with the tIoU in ranges [0.5, 0.75, 0.95] and [0.3, 0.4, 0.5, 0.6, 0.7] for ActivityNet-1.3 and THUMOS-14, respectively. At a specified tIoU, Average Precision is calculated for every action class and then averaged up to mAP@tIoU. On ActivityNet-1.3 particularly, we also report the Average mAP which is averaged among all mAP@tIoU scores.</p><p>For comparability purposes, we follow the same setting up which was described in <ref type="bibr" target="#b5">[6]</ref>. We re-scaled all videos to 1600 frames by linear interpolation and extracted features for every separate snippet with length ? = 16 frames. Therefore, every video sequence will be represented by a feature sequence with the length of exactly 100 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IMPLEMENTATION DETAILS</head><p>On AcitivityNet <ref type="bibr" target="#b50">[51]</ref>, we benchmark our proposed ABN on C3D <ref type="bibr" target="#b24">[25]</ref>, SlowFast <ref type="bibr" target="#b15">[16]</ref> and Two-Stream <ref type="bibr" target="#b25">[26]</ref> backbones, the first two backbones are pre-trained on Kinetics-400 <ref type="bibr" target="#b63">[64]</ref> dataset, while the last one is pre-trained on recognition track of ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref>. The feature map S N of each backbone is extracted for the local feature extraction step, which has 2048 dimensions, 2304 dimensions and 400 dimensions for C3D, SlowFast and Two-Stream, respectively. We always keep this feature size through out our proposed network and output the Contextual Agent-Environment Feature of the same size as that of backbone feature.</p><p>On THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>, for fair comparisons with prior works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we employ C3D <ref type="bibr" target="#b24">[25]</ref> and Two-Stream <ref type="bibr" target="#b25">[26]</ref> networks as the backbones of our proposed ABN. The C3D <ref type="bibr" target="#b24">[25]</ref> backbone is pre-trained on Kinetics-400 <ref type="bibr" target="#b63">[64]</ref>, whereas Two-Stream backbone is pre-trained on the action recognition track of ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref>. Output feature map S 4 from C3D and Two-Stream backbones are 2048 and 400 dimensions, respectively.</p><p>In the local feature extraction step, we adopt a Faster R-CNN [28] model pre-trained on COCO dataset <ref type="bibr" target="#b47">[48]</ref> to detect The Transformer Encoders we used in Self-Attention Module for contextually merging local features into the local agent-aware feature or merging the local agent-aware feature with the global feature together share the same architecture of 4 attention heads and 1 transformer layer.</p><p>For every experiment, we trained our model for 10 epochs with initial learning rate of 0.0001 and Adam optimizer, the best performed model on validation set is chosen for further comparison.</p><p>In addition, we apply an augmentation where any groundtruth video, whose groundtruth actions having average length higher than a factor of ? upper of its length, will be discarded. Contrarily, any groundtruth video, whose groundtruth actions having average length lower than a factor of ? lower of its length, will be duplicated. We empirically observe that with ? upper = 0.98 and ? lower = 0.3, those augmentations during training will help the network achieve better performance and more robust on both datasets of ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> and THUMOS-14 <ref type="bibr" target="#b51">[52]</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the comparison in terms of AR@AN (AN = 100) and AUC between the ABN against other state-of-theart methods on both validation set and test set of ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref> dataset. Our performance is given in the last three rows of <ref type="table" target="#tab_4">Table 2</ref>. Compared against other state-of-the-art approaches, our proposed ABN obtains better performance on both AR@AN and AUC metrics regardless the backbone network. Concretely, the ABN outperforms BMN with 2.21%, 2.11% and 2.07% in terms of AUC on test set, when using Two-Stream <ref type="bibr" target="#b25">[26]</ref>, SlowFast <ref type="bibr" target="#b15">[16]</ref> and C3D <ref type="bibr" target="#b24">[25]</ref> backbones, respectively. With the most recent state-of-the-art, namely DBG <ref type="bibr" target="#b13">[14]</ref>, our proposed network makes the gaps of 0.83%, 0.73% and 0.69% on AUC on testing set with Two-Stream, SlowFast and C3D backbones, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PERFORMANCE ON TAPG 1) Compare with state-of-the-art methods</head><p>Additionally, <ref type="table" target="#tab_5">Table 3</ref> summarizes the performances of our proposed ABN and other state-of-the-arts on testing set of THUMOS-14 <ref type="bibr" target="#b51">[52]</ref> in terms of AR@AN (AN is in a set of <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">100,</ref><ref type="bibr">200,</ref><ref type="bibr">500,</ref><ref type="bibr">1000]</ref>). Our experiments are conducted on C3D <ref type="bibr" target="#b24">[25]</ref> and Two-Stream <ref type="bibr" target="#b25">[26]</ref> backbone networks following previous works for fair comparisons. Besides, inspired by <ref type="bibr" target="#b13">[14]</ref>, we also measure the performance of our method with both NMS and Soft-NMS in post-processing phase. Suprisingly, our proposed ABN outperforms all the previous works with very large margins as shown in <ref type="table" target="#tab_5">Table 3</ref>. We also noticed that using NMS will help the method to have better performances on top 200 proposals, while Soft-NMS will help the method to have better performances on more proposals e.g. 1000 ones. <ref type="figure">Fig. 4</ref> illustrates some qualitative results of the generated proposals by ABN and BMN <ref type="bibr" target="#b5">[6]</ref> on ActivityNet-1.3 <ref type="bibr" target="#b50">[51]</ref>. The experimental results show that Agent-Aware Boundary Network generates much better proposals, which almost perfectly cover the groundtruth events and tightly fit with their boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Generalizability of Proposals</head><p>One of the most important properties of a TAPG method is generating high quality proposals for unseen action categories. We follow the protocol defined in BSN <ref type="bibr" target="#b11">[12]</ref> and BMN <ref type="bibr" target="#b5">[6]</ref> to evaluate the generalizability of our proposed ABN. There are two un-overlapped action subsets: "Sports, Exercise, and Recreation" and "Socializing, Relaxing, and Leisure" of ActivityNet-1.3 are chosen as seen and unseen subsets separately. With such selection, there are 87 and   <ref type="table" target="#tab_6">Table 4</ref> shows the performance of ABN along with the comparison with BSN <ref type="bibr" target="#b11">[12]</ref> and BMN <ref type="bibr" target="#b5">[6]</ref>. Compare with BSN and BMN, the ABN achieves better generalizability on both seen and unseen validation sets. This proves that our method can be used to generate proposals for activities and actions that it never met during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. PERFORMANCE ON TAD</head><p>Another important aspect worth considering is the utilization of proposals in action detection. Following BSN <ref type="bibr" target="#b11">[12]</ref> and BMN <ref type="bibr" target="#b5">[6]</ref> for a fair comparison, we adopt top-1 video-level classification results generated by method in <ref type="bibr" target="#b57">[58]</ref> on ActivityNet-1.3 to label the proposals generated by our method. Meanwhile, we use top-2 video-level classification results generated by UntrimmedNet <ref type="bibr" target="#b64">[65]</ref> to label proposals generated by our method on THUMOS-14. The labeled proposals are then evaluated on mAP@tIoU metric as described in Sec. IV-A2. <ref type="table" target="#tab_8">Table 5</ref> illustrates the performance of ABN and comparison with other state-of-the-art methods on ActivityNet-1.3 validation set. As we can see, our method outperforms BSN <ref type="bibr" target="#b11">[12]</ref>, BMN <ref type="bibr" target="#b5">[6]</ref> on all settings with a large margin and keeps a good distance with the most recent state-of-the-art mehtod in action detection named GTAD <ref type="bibr" target="#b59">[60]</ref> on all settings except mAP@0.75.</p><p>The experiment results on THUMOS-14 shown in when compare with other methods including the state-of-theart method in action detecion, namely GTAD <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ABLATION STUDY</head><p>We conduct several ablation studies on the validation set of ActivityNet-1.3 dataset to analyze the contribution of individual feature in the ABN. In addition to different backbone networks, i.e. C3D <ref type="bibr" target="#b24">[25]</ref>, SlowFast <ref type="bibr" target="#b15">[16]</ref> and Two-Stream <ref type="bibr" target="#b25">[26]</ref>, we have investigated the following ablation configurations for each backbone network.  ally our proposed ABN. <ref type="table" target="#tab_10">Table 7</ref> provides the results in terms of AR@AN (AN = 1, 10, 100) and AUC metrics of the ABN under different feature configurations on ActivityNet-1.3. In Environment Feature Only (Env.) configuration, there is only a global information about the environment of entire video frames, whereas in Agent Feature Only (Agent) configuration, there is only a local information about the agents. <ref type="table" target="#tab_10">Table 7</ref> has shown that each feature has its own contribution and the combination of both features exhibits the best performance. This result proves that our aforementioned observations are valid and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this paper, we proposed a novel contextual Agent-Aware Boundary Network (ABN) for the TAPG. Our ABN contains two components corresponding to Agent-Environment representation network and boundary generation network. The first component extracts the contextual visual representation of the video whereas the second component with boundary-based mechanism aims at evaluating confidence scores of densely distributed proposals. Different from the previous works, which apply backbone network into the entire video frame, the video visual representation in the proposed ABN involves two parallel pathways: (i) the local pathway, which plays at the agents level and tells about where the agents are and what the agents are doing; (ii) the global pathway, which plays at a environment level and tells about how the environment affects after receiving the actions from the agents as well as the relationship between the agents, actions, and the environment. The experiments have demonstrated that our proposed ABN outperforms state-of-the-art proposal generation methods with C3D, SlowFast and Two-Stream backbone networks on both ActivityNet-1.3 and THUMOS-14 datasets. Our superior performance relies on both global feature and local feature, and demonstrates the robustness of the proposed ABN regardless of the backbone network as well as the effectiveness of our two-pathway contextual Agent-Environment visual representation. Additionally, our proposed method can also be generalized well to generate  proposals for activities and actions that it never sees in training phase. Therefore, our method also shows the superior results in further applications like action detection task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 2 .</head><label>2</label><figDesc>Approaches summarization on TAPG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Inspired by the success of ResNet in image classification, Hara, et al. extended ResNet architecture to 3D CNN and proposed 3D ResNet [42]. In their work, they examined various 3D CNN architecture under different backbone such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152, ResNet-200, DenseNet-121 and ResNeXt-101. The mainstream networks fall into three categories: Two-Stream networks, Recurrent Neural Network (RNN) with its popular variant named Long Short Term Memory (LSTM), and 3D networks. Two-Stream networks were first introduced by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .</head><label>3</label><figDesc>An overall architecture of our proposed contextual Agent-Aware representation network which contains four steps. Given a ?-frame snippet, the final video visual feature is conducted by both global feature and local feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>? 5 FIGURE 5 .</head><label>55</label><figDesc>Environment Feature Only (Env.): the network relies solely on global features to generate proposals. ? Agent Feature Only (Agent): the network relies completely on the contextual agent-aware features and does not use global feature. ? Both Environment and Agent Feature: both global feature and local feature are used and fused by Self-Attention Module. This network configuration is actu-Qualitative results of proposals generated by our proposed ABN on THUMOS-14<ref type="bibr" target="#b51">[52]</ref>, we use our best performed configuration which includes Two-Stream<ref type="bibr" target="#b14">[15]</ref> as backbone feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>The detailed architecture of the boundary generation network which takes the contextual Agent-Aware visual feature F as the input. T and D are the temporal length of the video and maximum duration of proposals in terms of number of snippets. The obtained outputs are O T and O P , which are corresponding to boundary-predictions and proposal actionness scores.</figDesc><table><row><cell>ID</cell><cell></cell><cell>Layer</cell><cell>Input</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell cols="2">Base Module</cell><cell></cell></row><row><cell>1</cell><cell>1DConv.</cell><cell>256 ? 3/1, ReLU</cell><cell>I : F ? T</cell><cell>O 1 : 256 ? T</cell></row><row><cell>2</cell><cell>1DConv.</cell><cell>128 ? 3/1, ReLU</cell><cell>O 1 : 256 ? T</cell><cell>O 2 : 128 ? T</cell></row><row><cell>3</cell><cell>1DConv.</cell><cell>256 ? 3/1, ReLU</cell><cell>O 2 : 128 ? T</cell><cell>O 3 : 256 ? T</cell></row><row><cell></cell><cell></cell><cell cols="2">Temporal Assessment Module (TAM)</cell><cell></cell></row><row><cell>4</cell><cell>1DConv.</cell><cell>2 ? 3/1 , Sigmoid</cell><cell>O 3 : 256 ? T</cell><cell>O T : 2 ? T</cell></row><row><cell></cell><cell></cell><cell cols="2">Proposal Assessment Module (PAM)</cell><cell></cell></row><row><cell>5</cell><cell></cell><cell>Matching layer</cell><cell>O 2 : 128 ? T</cell><cell>O 5 : 128 ? 32 ? D ? T</cell></row><row><cell>6</cell><cell cols="2">3DConv.512 ? 32 ? 1 ? 1/(32, 0, 0) , ReLU O</cell><cell></cell><cell></cell></row></table><note>5 : 128 ? 32</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison in terms of AR@AN and AUC between our proposed ABN against other state-of-the-art TAPG methods on validation set and test set of ActivityNet-1.3 dataset. The best performance is shown in bold. The second best performance is shown in italic</figDesc><table><row><cell>Methods</cell><cell>Year</cell><cell>Feature</cell><cell cols="3">AR@100 (val) AUC (val) AUC (test)</cell></row><row><cell>TCN [3]</cell><cell>ICCV2017</cell><cell>Two-Stream [15]</cell><cell>-</cell><cell>59.58</cell><cell>61.56</cell></row><row><cell>MSRA [4]</cell><cell>CVPRW2017</cell><cell>P3D [39]</cell><cell>-</cell><cell>63.12</cell><cell>64.18</cell></row><row><cell>SSTAD [36]</cell><cell>BMVC2017</cell><cell>C3D [25]</cell><cell>73.01</cell><cell>64.40</cell><cell>64.80</cell></row><row><cell>CTAP [2]</cell><cell>ECCV2017</cell><cell>Two-Stream [15]</cell><cell>73.17</cell><cell>65.72</cell><cell>-</cell></row><row><cell>BSN [12]</cell><cell>ECCV2018</cell><cell>Two-Stream [15]</cell><cell>74.16</cell><cell>66.17</cell><cell>66.26</cell></row><row><cell>SRG [47]</cell><cell cols="2">TCSVT 2019 Two-Stream [15]</cell><cell>74.65</cell><cell>66.06</cell><cell>-</cell></row><row><cell>MGG [13]</cell><cell>CVPR2019</cell><cell>I3D [41]</cell><cell>74.54</cell><cell>66.43</cell><cell>66.47</cell></row><row><cell>BMN [6]</cell><cell>ICCV2019</cell><cell>Two-Stream [15]</cell><cell>75.01</cell><cell>67.10</cell><cell>67.19</cell></row><row><cell>DBG [14]</cell><cell>AAAI2020</cell><cell>Two-Stream [15]</cell><cell>76.65</cell><cell>68.23</cell><cell>68.57</cell></row><row><cell>BSN++ [54]</cell><cell>ACCV2020</cell><cell>Two-Stream [15]</cell><cell>76.52</cell><cell>68.26</cell><cell>-</cell></row><row><cell>TSI++ [55]</cell><cell>ACCV2020</cell><cell>Two-Stream [15]</cell><cell>76.31</cell><cell>68.35</cell><cell>68.85</cell></row><row><cell>MR [56]</cell><cell>ECCV2020</cell><cell>Two-Stream [15]</cell><cell>75.27</cell><cell>66.51</cell><cell>-</cell></row><row><cell>SSTAP [57]</cell><cell>CVPR2021</cell><cell>Two-Stream [15]</cell><cell>75.20</cell><cell>67.23</cell><cell>-</cell></row><row><cell></cell><cell>-</cell><cell>Two-Stream</cell><cell>76.39</cell><cell>68.84</cell><cell>69.40</cell></row><row><cell>Our Proposed ABN</cell><cell>-</cell><cell>Slow-Fast</cell><cell>76.64</cell><cell>69.08</cell><cell>69.30</cell></row><row><cell></cell><cell>-</cell><cell>C3D</cell><cell>76.72</cell><cell>69.16</cell><cell>69.26</cell></row><row><cell cols="3">human bounding boxes for generating local features later by</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RoI alignment with the feature map S 4 .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Comparisons with other state-of-the-art TAPG methods on testing set of THUMOS-14 dataset in terms of AR@AN, where SNMS stands for Soft-NMS. The best performance is shown in bold. The second best performance is shown in italic</figDesc><table><row><cell>Feature</cell><cell>Methods</cell><cell>Year</cell><cell cols="2">@50 @100 @200 @500 @1000</cell></row><row><cell></cell><cell>SCNN-prop [8]</cell><cell cols="3">CVPR 2016 17.22 26.17 37.01 51.57</cell><cell>58.20</cell></row><row><cell></cell><cell>SST [1]</cell><cell cols="3">CVPR 2017 19.90 28.36 37.90 51.58</cell><cell>60.27</cell></row><row><cell></cell><cell>TURN-TAP [9]</cell><cell cols="3">ICCV 2017 19.63 27.96 38.34 53.52</cell><cell>60.75</cell></row><row><cell></cell><cell>BSN [12]</cell><cell cols="3">ECCV 2018 29.58 37.38 45.55 54.67</cell><cell>59.48</cell></row><row><cell>C3D</cell><cell>MGG [13] BMN [6]</cell><cell cols="3">CVPR 2019 29.11 36.31 44.32 54.95 ICCV 2019 32.73 40.68 47.86 56.42</cell><cell>60.98 60.44</cell></row><row><cell></cell><cell>DBG+SNMS [14]</cell><cell cols="3">AAAI 2020 30.55 38.82 46.59 56.42</cell><cell>62.17</cell></row><row><cell></cell><cell>DBG [14]</cell><cell cols="3">AAAI 2020 32.55 41.07 48.83 57.58</cell><cell>59.55</cell></row><row><cell></cell><cell>Our Proposed ABN + SNMS</cell><cell>-</cell><cell cols="2">34.25 44.01 52.05 60.57</cell><cell>65.39</cell></row><row><cell></cell><cell>Our Proposed ABN + NMS</cell><cell>-</cell><cell cols="2">36.01 45.41 52.74 59.91</cell><cell>62.47</cell></row><row><cell></cell><cell>TURN-TAG [9]</cell><cell cols="2">ICCV 2017 18.55 29.00 39.61</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CTAP [2]</cell><cell cols="2">ECCV 2018 32.49 42.61 51.97</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BSN [12]</cell><cell cols="3">ECCV 2018 37.46 46.06 53.21 60.64</cell><cell>64.52</cell></row><row><cell></cell><cell>MGG [13]</cell><cell cols="3">CVPR 2019 39.93 47.75 54.65 61.36</cell><cell>64.06</cell></row><row><cell>Two-Stream</cell><cell>BMN [6]</cell><cell cols="3">ICCV 2019 39.36 47.72 54.70 62.07</cell><cell>65.49</cell></row><row><cell></cell><cell>DBG+SNMS [14]</cell><cell cols="3">AAAI 2020 37.32 46.67 54.50 62.21</cell><cell>66.40</cell></row><row><cell></cell><cell>DBG+NMS [14]</cell><cell cols="3">AAAI 2020 40.89 49.24 55.76 61.43</cell><cell>61.95</cell></row><row><cell></cell><cell>SSTAP [57]</cell><cell cols="2">CVPR2021 41.01 50.12 56.69</cell><cell>-</cell><cell>68.81</cell></row><row><cell></cell><cell>Our Proposed ABN + SNMS</cell><cell>-</cell><cell cols="2">40.87 49.09 56.24 63.53</cell><cell>67.29</cell></row><row><cell></cell><cell>Our Proposed ABN + NMS</cell><cell>-</cell><cell cols="2">44.89 51.86 57.36 61.67</cell><cell>62.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Generalizability evaluation on ActivityNet 1.3. The best performance is shown in bold. We first train our ABN on both seen training set and seen+unseen training set and then evaluate it on seen validation set and unseen validation set separately.</figDesc><table><row><cell>Seen</cell><cell>Unseen</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 Groundtruth Ours BMN 91.46 18.15 15.12 92.86 76.61 106.83 FIGURE 4.</head><label>6</label><figDesc>again emphasizes the superior performance of our ABN Qualitative results of proposals by BMN<ref type="bibr" target="#b5">[6]</ref> and our proposed ABN on ActivityNet-1.3<ref type="bibr" target="#b50">[51]</ref>, we use our best performed configuration which includes C3D<ref type="bibr" target="#b24">[25]</ref> as backbone feature extractor.</figDesc><table><row><cell></cell><cell>Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Groundtruth</cell><cell></cell><cell>15.74</cell><cell></cell><cell cols="2">31.47 37.23</cell><cell></cell><cell cols="2">58.34 63.72 71.2 76.58</cell><cell>87.13 92.31</cell><cell>106.32</cell></row><row><cell>BMN</cell><cell>0.0</cell><cell>12.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>14.76</cell><cell cols="3">57.82 36.91</cell><cell></cell><cell cols="2">57.82 63.97 71.35 77.5 87.34</cell><cell>93.5</cell><cell>105.8</cell></row><row><cell>Ours</cell><cell></cell><cell>14.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.82</cell></row><row><cell>Groundtruth</cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.38</cell></row><row><cell>BMN</cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.26</cell></row><row><cell>Ours</cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19.54</cell></row><row><cell>Groundtruth</cell><cell></cell><cell cols="2">10.86 13.46</cell><cell>18.66</cell><cell>21.77</cell><cell>27.8</cell><cell>30.19</cell><cell>35.6</cell><cell>41.21</cell></row><row><cell>BMN</cell><cell cols="2">*No proposal*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>17.97</cell><cell>21.3</cell><cell>27.29</cell><cell>30.62</cell><cell>35.95</cell><cell>44.6</cell></row><row><cell>Groundtruth</cell><cell></cell><cell>12.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.72</cell></row><row><cell></cell><cell>0.0</cell><cell>11.02 12.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.99</cell></row><row><cell>BMN</cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.99</cell></row><row><cell>Ours</cell><cell></cell><cell>11.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>TAD results on ActivityNet-1.3 in terms of mAP@tIoU and average mAP, where our proposals are combined with video-level classification results generated by [58] CVPR2021 50.72 35.28 7.87 34.48 Our Proposed ABN -51.78 34.18 10.29 34.22</figDesc><table><row><cell>Method</cell><cell></cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell>CDC [59]</cell><cell cols="4">CVPR2017 43.83 25.88 0.21</cell><cell>22.77</cell></row><row><cell>BSN [12]</cell><cell cols="4">ECCV2018 46.45 29.96 8.02</cell><cell>30.03</cell></row><row><cell>BMN [6]</cell><cell>ICCV2019</cell><cell cols="3">50.07 34.78 8.29</cell><cell>33.85</cell></row><row><cell>GTAD [60]</cell><cell cols="3">CVPR2020 50.36 34.6</cell><cell>9.02</cell><cell>34.09</cell></row><row><cell>P-GCN [61]</cell><cell>ICCV2019</cell><cell>42.9</cell><cell>28.1</cell><cell>2.5</cell><cell>27.0</cell></row><row><cell>MR [56]</cell><cell cols="2">ECCV2020 43.5</cell><cell>33.9</cell><cell>9.2</cell><cell>30.1</cell></row><row><cell>BC-GNN [62]</cell><cell cols="4">ECCV2020 50.56 34.75 9.37</cell><cell>34.26</cell></row><row><cell>TSI++ [55]</cell><cell cols="2">ACCV2020 51.2</cell><cell>35.0</cell><cell>6.6</cell><cell>34.2</cell></row><row><cell>SSTAP [57]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Performance comparisons between our proposed ABN and the other proposal generation methods in terms of TAD on the testing set of THUMOS-14, where mAP is reported with tIoU set from 0.3 to 0.7 and Unet classifier is used</figDesc><table><row><cell>Method</cell><cell></cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>SST [1]</cell><cell cols="2">CVPR2017 4.7</cell><cell>10.9</cell><cell>20.0</cell><cell>31.5</cell><cell>41.2</cell></row><row><cell>TURN-TAP [9]</cell><cell cols="2">ICCV2017 6.3</cell><cell>14.1</cell><cell>24.5</cell><cell>35.3</cell><cell>46.3</cell></row><row><cell>BSN [12]</cell><cell cols="2">ECCV2018 20.0</cell><cell>28.4</cell><cell>36.9</cell><cell>45.0</cell><cell>53.5</cell></row><row><cell>BMN [6]</cell><cell cols="2">ICCV2019 20.5</cell><cell>29.7</cell><cell>38.8</cell><cell>47.4</cell><cell>56.0</cell></row><row><cell>MGG [13]</cell><cell cols="2">CVPR2019 21.3</cell><cell>29.5</cell><cell>37.4</cell><cell>46.8</cell><cell>53.9</cell></row><row><cell>DBG [14]</cell><cell cols="2">AAAI2019 21.7</cell><cell>30.2</cell><cell>39.8</cell><cell>49.4</cell><cell>57.8</cell></row><row><cell>GTAN [63]</cell><cell cols="2">CVPR2019 -</cell><cell>-</cell><cell>38.8</cell><cell>47.2</cell><cell>57.8</cell></row><row><cell>GTAD [60]</cell><cell cols="2">CVPR2020 23.4</cell><cell>30.8</cell><cell>40.2</cell><cell>47.6</cell><cell>54.5</cell></row><row><cell>BC-GNN [62]</cell><cell cols="2">ECCV2020 23.1</cell><cell>31.2</cell><cell>40.4</cell><cell>49.1</cell><cell>57.1</cell></row><row><cell>SSTAP [57]</cell><cell cols="2">CVPR2021 22.8</cell><cell>32.8</cell><cell>42.3</cell><cell>51.5</cell><cell>58.4</cell></row><row><cell cols="2">Our proposed ABN -</cell><cell cols="5">25.56 37.04 46.12 53.95 59.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>Ablation studies on the effectiveness of each component in the proposed ABN on ActivityNet-1.3 dataset with environment feature (Env.) and agent feature (Agent) in terms of AR@AN (AN = 100) and AUC. The ablation study is conducted on various features i.e C3D, SlowFast and Two-Stream</figDesc><table><row><cell>Feature</cell><cell></cell><cell cols="4">AR@1 AR@10 AR@100 AUC</cell></row><row><cell></cell><cell>Env.</cell><cell>33.58</cell><cell>57.50</cell><cell>75.07</cell><cell>67.55</cell></row><row><cell>C3D [25]</cell><cell>Agent</cell><cell>30.14</cell><cell>53.80</cell><cell>72.76</cell><cell>64.54</cell></row><row><cell></cell><cell cols="2">Agent-Env. (Ours) 33.87</cell><cell>59.21</cell><cell>76.72</cell><cell>69.16</cell></row><row><cell></cell><cell>Env</cell><cell>33.74</cell><cell>57.11</cell><cell>75.59</cell><cell>67.85</cell></row><row><cell>SlowFast [16]</cell><cell>Agent</cell><cell>32.24</cell><cell>52.77</cell><cell>72.67</cell><cell>64.27</cell></row><row><cell></cell><cell cols="2">Agent-Env. (Ours) 34.09</cell><cell>58.95</cell><cell>76.64</cell><cell>69.08</cell></row><row><cell></cell><cell>Env.</cell><cell>32.59</cell><cell>56.72</cell><cell>74.94</cell><cell>67.14</cell></row><row><cell>Two-Stream [15]</cell><cell>Agent</cell><cell>32.27</cell><cell>52.61</cell><cell>72.72</cell><cell>64.16</cell></row><row><cell></cell><cell cols="2">Agent-Env. (Ours) 33.61</cell><cell>58.86</cell><cell>76.39</cell><cell>68.84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">VOLUME 4, 2016 Khoa Vo et al.: Preparation of Papers for Journal of IEEE Access</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Msr asia msm at activitynet challenge 2017: Trimmed action recognition, temporal action proposals and densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Turn tap: temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
	<note>in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Highlight detection with pairwise deep ranking for first-person video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object relational graph with teacher-recommended learning for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sibnet: Sibling convolutional encoder for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale content-only video recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Less is more: Learning highlight detection from video duration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An efficient dense and scaleinvariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Endto-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2329" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06750</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Srg: Snippet relatedness-based temporal action proposal generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">BSN++: complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tsi: Temporal scale invariant network for action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bottomup temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Selfsupervised learning for semi-supervised temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">CUHK &amp; ETHZ &amp; SIAT submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1608.00797</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">G-tad: Subgraph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1703.03329</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">He received his B.S. degree in Computer Science from Honors Program, University of Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has co-authored in publications appearing in conferences and journals including ICASSP, CVPR Workshops, Applied Sciences</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
		<respStmt>
			<orgName>Khoa Vo is currently a PhD student in Department of Computer Science and Computer Engineering at the University of Arkansas in Fayetteville</orgName>
		</respStmt>
	</monogr>
	<note>he was an internship student in National Institute of Informatics</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sang Truong: Sang Truong is currently a PhD student in Department of Computer Science and Computer Engineering at the University of Arkansas in Fayetteville. He received his B.S. degree in Automation and Control Engineering from International University, VNU-HCM in 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finance and Machine Learning. His research interests includes Electrocardiography, Objects Detection and Actions Detection</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note>He is currently a masjournals including ICASSP, ICPR, and Artificial Intelligence Review</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">he returned to Hitachi Advanced Research Laboratory where he lead a project on content-based image retrieval supported by the Ministry of International Trade and Industry in Japan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ho Chi Minh city. Akihiro Sugimoto: Dr. Sugimoto received his B.S, M.S, and Dr. Eng. degrees in mathematical engineering from the University of Tokyo in 1987, 1989, and 1996, respectively. He joined Hitachi Advanced Research Laboratory in 1989, and then temporally moved to Advanced Telecommunications Research Institute International (ATR)</title>
		<editor>Dr. Tran obtained his B.Sc., M.Sc., and Ph.D.</editor>
		<meeting><address><addrLine>NII, Japan; Paris-Est Marne-la-Vall?e, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="2015" to="2016" />
		</imprint>
		<respStmt>
			<orgName>degrees in computer science from University of Science, VNU-HCM ; He joined the University of Science ; VNU-HCM ; and at University of Illinois at Urbana-Champaign (UIUC ; He is currently Head of Software Engineering Laboratory and Deputy Head of Artificial Intelligence Laboratory, University of Science, VNU-HCM. He is also the Deputy Head of Software Engineering Department, Faculty of Information Technology, University of Science, VNU-HCM</orgName>
		</respStmt>
	</monogr>
	<note>He is a regular reviewer of international conferences/journals in computer vision, AI, and pattern recognition. He has published more than 150 peer-reviewed journal/international conference papers. He received Best Paper Awards from the Information Processing Society of Japan in 2001 and from the Institute of Electronics, Information and Communication Engineers (IEICE) in 2011. He is a member of IEEE</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">She was a research associate in the Department of Electrical and Computer Engineering (ECE) at Carnegie Mellon University (CMU) in 2018-2019. She received the Ph.D degree in ECE at CMU in 2018, ECE Master degree at CMU in 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">:</forename><surname>Dr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vietnam in 2005. Her current research interests focus on Image Understanding, Video Understanding, Computer Vision, Robotics, Machine Learning, Deep Learning</title>
		<meeting><address><addrLine>SingleCell-RNA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Le is the director of Artificial Intelligence and Computer Vision lab and an Assistant Professor in the Department of Computer Science and Computer Engineering at University of Arkansas ; CS Master Degree at University of Science ; CS Bachelor degree at University of Science</orgName>
		</respStmt>
	</monogr>
	<note>Reinforcement Learning</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">She co-organized the Deep Reinforcement Learning Tutorial for Medical Imaging at MICCAI 2018, Medical Image Learning with Less Labels and Imperfect Data workshop at MICCAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Dr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miccai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ijcv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesa</forename><surname>Ijcv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jdsp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tifs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaai</forename><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cvpr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miccai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Website</surname></persName>
		</author>
		<ptr target="https://www.nganle.net" />
	</analytic>
	<monogr>
		<title level="m">Scene Understanding in Autonomous (Frontier) and Artificial intelligence in Biomedicine and Healthcare (MDPI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>She has served as a reviewer for 10+ top-tier conferences and journals, including</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
