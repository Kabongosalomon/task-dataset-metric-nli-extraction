<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seasalt AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhou</forename><surname>Chai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CLSP &amp; HLTCOE</orgName>
								<orgName type="institution" key="instit2">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Qiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Weng</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xiaomi Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CLSP &amp; HLTCOE</orgName>
								<orgName type="institution" key="instit2">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xiaomi Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mingjie</roleName><forename type="first">Jin</forename><surname>?4</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CLSP &amp; HLTCOE</orgName>
								<orgName type="institution" key="instit2">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CLSP &amp; HLTCOE</orgName>
								<orgName type="institution" key="instit2">The Johns Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijiang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">KE Holdings Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">KE Holdings Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">KE Holdings Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seasalt AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xiaomi Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>You</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Xiaomi Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Yan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xiaomi Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: corpus</term>
					<term>forced alignment</term>
					<term>segmentation</term>
					<term>speech recogni- tion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Thanks to the rapid development of the neural network models, automatic speech recognition (ASR) has made tremendous progress in the past decade. Various system architectures, from hybrid <ref type="bibr" target="#b0">[1]</ref> to end-to-end <ref type="bibr" target="#b1">[2]</ref>, are proposed, and state-of-the-art results on standard benchmarks are being frequently updated.</p><p>The mainstream speech recognition corpora, on the other hand, have not changed much in decades. To take the English speech recognition task as an example, the Wall Street Journal corpus, which consists of 80 hours of narrated news articles <ref type="bibr" target="#b2">[3]</ref>, is almost 20 years old, and has a word error rate (WER) of 2.32% on its eval92 benchmark <ref type="bibr" target="#b3">[4]</ref>. The Switchboard and Fisher corpus, which consists of 262 and 1,698 hours of telephone conversational speech, is also around 20 years old, and has a WER of 5.5% on the Switchboard portion of the Hub5'00 benchmark <ref type="bibr" target="#b4">[5]</ref>. Even LibriSpeech <ref type="bibr" target="#b5">[6]</ref>, one of the most popular corpora for speech recognition tasks, is more than 5 years old, and has a WER of 1.9% on its test clean benchmark <ref type="bibr" target="#b6">[7]</ref>. It consists of 1,000 hours of read English speech. Due to the fast development of speech recognition techniques, ASR performance on those data sets appears to have saturated, making it difficult to track further improvements from new techniques.</p><p>There is some progress on creating better corpora/benchmarks for English speech recognition, from both academia and industry. TED-LIUM <ref type="bibr" target="#b7">[8]</ref> is a series of corpora created by the Ubiqus company and the University of Le Mans. It consists of 452 hours of audio from TED talks in its latest release TED-LIUM 3. The corpora size, however, is less than 1,000 hours, making it not suitable for algorithms which demand a large amount of data. People's Speech <ref type="bibr" target="#b8">[9]</ref> released by ML * Co-first authors, equal contribution. ? Authors listed in alphabetical order.</p><p>Commons consists of 87,000 hours of audio, covering 59 different languages. It's source, however, is mostly audiobook, lacking crucial acoustic diversity. Another work is SPGISpeech <ref type="bibr" target="#b9">[10]</ref>, a corpus released by Kensho Technologies. It consists of 5,000 hours of transcribed audio from earnings calls transcribed by S&amp;P Global, Inc. The corpus by its nature gives an emphasis to the business domain.</p><p>We release a complementary English speech recognition corpus named GigaSpeech, an evolving, multi-domain ASR corpus with 10,000 hours of transcribed Audio. The initial release of GigaSpeech is complementary to the existing corpora in the following ways:</p><p>? Extensible, the metadata is designed in a way that it can be easily reused for other tasks, such as speaker identification.</p><p>? Large scale, 10,000 hours of transcribed speech.</p><p>? Multi-source, covers audiobook, podcast and YouTube.</p><p>? Multi-style, covers both read and spontaneous speech.</p><p>? Multi-topic, covers a variety of topics, such as arts, science, sports, etc.</p><p>? Original/Normalized transcription pairs, suitable for training end-to-end systems with post-processing (punctuation, case/date/time normalization, etc) included.</p><p>We make two contributions in this work. First, we release an evolving, multi-domain speech recognition corpus with 10,000 hours of labeled audio. Second we provide a scalable, reliable pipeline for generating speech recognition corpora.</p><p>The rest of the paper is organized as follows. Section 2 introduces the GigaSpeech corpus, and Section 3 presents the full pipeline to create the GigaSpeech corpus. We describe the speech recognition baseline systems for various toolkits, and provide experiment setup and results in Section 4. Finally, acknowledgements are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GigaSpeech Corpus</head><p>This section explains the structure of the GigaSpeech corpus, including metadata, data partition, audio format, etc. Instructions and scripts for downloading GigaSpeech can be found on GigaSpeech's GitHub repository 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Metadata</head><p>We save all the metadata information to a single JSON file named GigaSpeech.json. <ref type="figure" target="#fig_0">Figure 1</ref> shows a snip of this file. For better presentation of this paper, we skip a lot of non-critical entries in the snip, such as "format", "md5", "source", etc.</p><p>To use the corpus, users are expected to extract the relevant information from GigaSpeech.json. For example, for the speech recognition task, one should first follow the "audios" entry, and work out a list of audio files. One can then follow the "url" entry to download the original audio file, or "path" if preprocessed audio files have been downloaded to the disk. After that, for each audio file, one can follow the "segments" entry, and work out the trainable audio segments, as  well as their corresponding transcripts. Of course, we also have various supplementary entries, such as "subsets", "md5", which will also be helpful for your task. The metadata file GigaSpeech.json is version controlled, and is supposed to get updated over the time. In future releases, we plan to add speaker information to the metadata file, so that it will be suitable for speaker identification/verification tasks. We also plan to add more data from different sources to increase the diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training Subsets</head><p>We provide 5 training subsets in GigaSpeech, namely XS, S, M, L and XL, listed here in order of increasing audio hours. <ref type="table" target="#tab_0">Table 1</ref> shows a detailed breakdown of the 5 GigaSpeech training subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation Sets</head><p>We provide 2 evaluation sets in GigaSpeech: a DEV set for development and tuning, which consists of 12.5 hours of audio, and a TEST set for final evaluation, which consists of 40.3 hours of audio.</p><p>A breakdown of our evaluation sets is illustrated in <ref type="table" target="#tab_1">Table 2</ref>. Note that our evaluation sets do not have a coverage for the audiobooks. We make sure that audio files from the LibriSpeech <ref type="bibr" target="#b5">[6]</ref> evaluation sets (dev-clean, dev-other, test-clean and test-other) are not presented in our corpus, therefore, the LibriSpeech evaluation sets can be used as our evaluation sets as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Audio Format</head><p>To reduce the file size of the GigaSpeech corpus, we compress the original audio using the Opus audio codec. Original audio files are first converted to 16k sampling rate, single channel and 16-bit signed-integer format. Opus compression is then applied to achieve an output bit rate of 32 kpbs, which results in a compression ratio of 8. <ref type="table" target="#tab_2">Table 3</ref> shows the impact of Opus audio compression in terms of WER (%). Kaldi systems (see Section 4.3, but without recurrent neural network language model rescoring) are built for our M (1000h) training subset, with or without Opus compression. These two systems are then  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GigaSpeech Creation Pipeline</head><p>This section presents the detailed pipeline for creating the GigaSpeech corpus, which can be applied to other data generation tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stage 1: Audio Collection</head><p>We start the task by manually defining the categories that we are interested in. We selected 24 categories in total, namely Arts, Business, For podcasts, we follow the above categories, and select episodes that come with manual transcriptions. For YouTube, we use the above categories as seed keywords, and select videos with human-generated closed captions. For audiobooks, we do not enforce those categories.</p><p>Once we have the list of audio files, we create tools and download all audio files with their corresponding transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stage 2: Text Normalization</head><p>The audio transcripts we download from various sources are created by different transcribers with diversified transcription standards and styles, therefore it is necessary to apply text normalization to the original transcripts. We perform standard text normalization, including case normalization, special symbol removal, number to word rewriting, date/time rewriting, etc.</p><p>For audiobooks and podcasts, transcripts are usually at the episodes or chapter/book level. For speech recognition, however, smaller segments less than 20 seconds are needed for training. The next step is to segment the long audio file into smaller segments. For YouTube, closed captions are provided at the sentence level, but unfortunately we find that the timestamps of closed captions are not reliable for segmentation. As a result, we decide to splice the closed captions all together, and perform the same segmentation as audiobooks and podcasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stage 3: Forced Alignment</head><p>Our aligner is implemented with Kaldi <ref type="bibr" target="#b10">[11]</ref>, and the alignment procedure follows the work here <ref type="bibr" target="#b11">[12]</ref>, which adopts a divide and conquer strategy to tackle the alignment problem. First, both audio and transcript are uniformly chunked into smaller pieces. Second, audio segments are decoded with a biased language model (LM), and hypotheses with timestamps are generated. Third, each hypothesis segment is matched to one transcript segment via TF-IDF similarity. For each matched pair, hypothesis is further aligned with the transcript segment using the Smith-Waterman algorithm <ref type="bibr" target="#b12">[13]</ref>. Finally, through this alignment, timestamps are attached to the transcript segments, and eventually to the whole transcripts by stitching the independently aligned segments together. Note that we modify the Smith-Waterman algorithm to handle silence and punctuation, and this is essential to enable the sentencebased segmentation in the next section.</p><p>To achieve better alignment performance, we first align and segment the downloaded audio with a close-domain acoustic model. We then train an in-domain acoustic model with the audio segments created (around 3,000 hours). This model is used to align the whole corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Stage 4: Audio Segmentation</head><p>We work out the audio segments from the alignment information above. Several rules are applied during the segmentation process:</p><p>? Split allowed at silence that is longer than 1 second.</p><p>? Split allowed at punctuation (",", ".", "!" or "?") that is longer than 0.2 seconds.</p><p>? Segments with alignment WER ? 75% re removed.</p><p>? Segments with length ? 20 seconds are removed.</p><p>? Silence at segment boundaries are truncated to 0.15 seconds.</p><p>It is worth pointing out that we keep 4 types of punctuation, namely comma, period, question mark and exclamation mark, so that split can happen at sentence boundaries (second rule above). We map them to special words "&lt;COMMA&gt;", "&lt;PERIOD&gt;", "&lt;QUESTIONMARK&gt;" and "&lt;EXCLAMATIONMARK&gt;" respectively. Besides, this also allows us to build end-to-end speech recognition systems that includes punctuation tagging, and end point detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Stage 5: Segment Validation</head><p>The segmentation stage generates a list of candidate segments, but potentially with high transcription error rate. We therefore apply segment validation to filter out bad segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Forced Alignment Graph</head><p>To better detect transcription errors made by human transcribers, we propose a variation of alignment graph in n-gram framework, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The bold arrow path represents a typical LM-free forced alignment graph. Each state on the forced alignment path has a dotted "leaky" arc (with weight) that allows the token to leak out the forced alignment path, from higher order n-gram states down to lower order n-gram states, until reaching the null state. A garbage word loop (containing top 1,000 uni-gram words) is added around the null state to consume additional acoustic frames. Besides, there are extra states and arcs that allow the token to return to the forced alignment path. In general, this alignment graph allows the decoder to perform insertion/deletion/substitution to the reference. This essentially brings more flexibility to the forced alignment stage, making it possible to capture the discrepancy between the audio and the corresponding transcript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Validation Decoding Pass</head><p>During the validation decoding pass, we detect transcription errors and filter out segments with high error rate. <ref type="figure" target="#fig_2">Figure 3</ref> gives two examples of how errors are being detected. In the first example, the transcriber misses a word "YOU" in the transcript, which is caught by our decoder. In the second example, the transcriber writes a typo which is also successfully detected.</p><p>For the podcast and YouTube portion of our XL training subset, we cap the maximum WER at 4%, and throw away all segments with higher WER. For the audiobook portion of the XL training subset, as well as all other smaller subsets, we cap the maximum WER at 0%, meaning we don't allow any transcription errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">Reference Rewriting</head><p>Investigation into the validated segments reveals three common types of transcriber errors:</p><p>? Fillers ignored, such as AH, UH, UM, ER, ERR, YOU KNOW, I MEAN, SORT OF, etc.</p><p>? Conjunctions ignored, such as AND, OR, BUT, etc.</p><p>? Disfluency removed, such as "It's it's it's a great thing!".</p><p>Discarding these segments will fundamentally limit the diversity of the corpus. To fix those common errors, we add a filler loop (see <ref type="figure" target="#fig_1">Figure 2</ref>) to the forced alignment graph, which contains the above common fillers and conjunctions. Besides, we also employ a disfluency detector. The filler loop and the disfluency detector may modify the reference (reference rewriting), and if that happens, those words will be counted as correct. We only apply reference rewriting to our XL subset (10,000h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Stage 6: Evaluation</head><p>Since our evaluation sets are manually processed by professional human transcribers, we take that as the ground truth, and use it to compute the frame level segmentation precision and recall, see <ref type="figure" target="#fig_3">Figure 4</ref>. Here recall tells us how many frames can be retrieved by our segmentation and validation pipeline, and precision tells us how many of these retrieved frames are correctly labeled (consistent with the human labels). <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the precision-recall curve on the podcast portion of the DEV evaluation set. For the XL training subset, we select a working point that gives us 10,000 hours of validated audio data, while keeping the maximum WER under 4%. And for all our other training subsets, we select the working point that keeps the maximum WER at 0% (the left most working point on <ref type="figure" target="#fig_4">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes the baseline systems and experimental results for four popular speech recognition toolkits, namely Athena, ESPnet <ref type="bibr" target="#b13">[14]</ref>, Kaldi <ref type="bibr" target="#b10">[11]</ref> and Pika <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Athena Baseline System 2</head><p>The Athena baseline implements an encoder-decoder based transformer model, which is similar to <ref type="bibr" target="#b15">[16]</ref>, with parameters e = 12, d = 6, d model = 1024, d ff = 2048 and d head = 8.</p><p>During the training, the output sequence of the encoder is also used for connectionist temporal classification (CTC) for joint training to enforce monotonic alignment between speech and label sequences. We use the Adam optimizer and varied learning rate with a warmup schedule (warmup steps = 8000). The model is trained with a total batch size of 128 for 5 epochs.</p><p>During the decoding, beam search with a beam size of 8 is used, which combines the scores of the decoder, the CTC with weight 0.5 and the LM with weight 0.2. The LM is a RNN based model with two long short-term memory (LSTM) layers, each with 1024 nodes.   The ESPnet baseline uses conformer (Convolution-Augmented Transformer) <ref type="bibr" target="#b6">[7]</ref>, which is a recently proposed architecture combining the local sensitivies of convolutional neural networks with the long-range interactions of transformers <ref type="bibr" target="#b16">[17]</ref>. We use the implementation provided in the ESPnet toolkit <ref type="bibr" target="#b17">[18]</ref>. We use a set of 5k BPE tokens generated by the SentencePiece tokenizer <ref type="bibr" target="#b18">[19]</ref>. The model has a 12 conformer blocks with an output dimension of 512 and a kernel size of 31 in the encoder, and 6 transformer blocks in the decoder. Both encoder and decoder have 8 attention heads with 2048 feed-forward unit dimension. Conformer was trained using four 24Gb memory Titan RTX GPUs. The max trainable epoch is 20. Mini-batch size is 35 million acoustic feature bins. Adam optimizer with no weight decay was used. Noam learning rate scheduler was set to 25k warmup steps with a learning rate of 0.0015. SpecAug used 2 frequency masks and 5 time masks. The last 10 best checkpoints were averaged as the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Kaldi Baseline System 4</head><p>The Kaldi baseline implements a typical chain model. First, a GMM-HMM model is trained to obtain the alignments with no data cleaning. Second, volume and speed augmentation techniques are applied. Ivectors are then extracted and pasted to the basic acoustic features, as most Kaldi's recipes do. Finally, a neural network is trained with  The Pika baseline adopts a convolution and transformer based architecture <ref type="bibr" target="#b14">[15]</ref> for the encoder of our RNN-T system. Five-layer transformer is used for the decoder and the hidden dimension of each layer is 512. We apply on-the-fly speed and volume perturbation during training where speed rates are set to 0.9/1.0/1.1/1.2 and the volume range is from -55dB to -10dB. For input features, we use 80 dimensional log Fbanks. The targets of our RNN-T system are a set of English wordpieces plus blank symbol which lead to an output dimension of 5000. The number of total parameters in the RNN-T is about 87M. MBR training and two extra two-layer transformer based forward/backward rescorers are also adopted. All training is conducted on 16 V100 GPUs. Our distributed training strategy is based on block-wise model-update filtering (BMUF) with a Nesterov momentum scheme but with different learning rate scheduling <ref type="bibr" target="#b14">[15]</ref> where both initial and final learning rates are set before training and number of training epochs are fixed (therefore there is no early stop and no development/validation set is used). One single sweep of the GigaSpeech XL training subset takes about 5hrs. For decoding, we set beam-size to 8 and the temperature of softmax to 1.25. <ref type="table" target="#tab_4">Table 4</ref> demonstrates baseline results for Athena, ESPnet, Kaldi and Pika. Results listed here are purely for the purpose of providing baseline systems for each toolkit. They do not reflect the state-of-theart performance of each toolkit, and cannot be used to compare the performance across toolkits. <ref type="table" target="#tab_5">Table 5</ref> illustrates the Kaldi baseline results for 4 training subsets. Generally speaking, as the training subset gets bigger, the performance goes up. Our smallest XS training subset (10h) is designed for system building and debugging only, and is not expected to give strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A snip of the metadata file GigaSpeech.json</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A forced alignment graph for the sentence "&lt;s&gt; A B C D E &lt;/s&gt;" (4-gram)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of transcription errors detected by forced alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Frame level segmentation accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Frame level precision-recall curve for the segmentation and validation pipeline 4.2. ESPnet Baseline System 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GigaSpeech training subsets</figDesc><table><row><cell>Subset</cell><cell cols="3">Audiobook Podcast YouTube</cell><cell>Total</cell></row><row><cell>XL</cell><cell>2,655h</cell><cell>3,499h</cell><cell>3,846h</cell><cell>10,000h</cell></row><row><cell>L</cell><cell>650h</cell><cell>875h</cell><cell>975h</cell><cell>2,500h</cell></row><row><cell>M</cell><cell>260h</cell><cell>350h</cell><cell>390h</cell><cell>1,000h</cell></row><row><cell>S</cell><cell>65h</cell><cell>87.5h</cell><cell>97.5h</cell><cell>250h</cell></row><row><cell>XS</cell><cell>2.6h</cell><cell>3.5h</cell><cell>3.9h</cell><cell>10h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>GigaSpeech evaluation sets</figDesc><table><row><cell>Sets</cell><cell cols="2">Podcast YouTube</cell><cell>Total</cell></row><row><cell>DEV</cell><cell>6.3h</cell><cell>6.2h</cell><cell>12.5h</cell></row><row><cell>TEST</cell><cell>16.1h</cell><cell>24.2h</cell><cell>40.3h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Impact of Opus audio compression (WER in %) DEV and TEST evaluation sets, with or without Opus compression. FromTable 3, it is clear that compressing the training data with the Opus codec at 32 kpbs output bit rate has very small impact on the DEV and TEST set (0.1 -0.2% WER degradation).</figDesc><table><row><cell></cell><cell>Eval</cell><cell cols="2">DEV</cell><cell cols="2">TEST</cell></row><row><cell>Train</cell><cell></cell><cell cols="2">Opus Wav</cell><cell cols="2">Opus Wav</cell></row><row><cell>M</cell><cell>Opus Wav</cell><cell>19.0 18.8</cell><cell>18.7 18.5</cell><cell>18.5 18.3</cell><cell>18.3 18.2</cell></row><row><cell cols="2">used to decode the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>GigaSpeech baselines for the XL training subset (WER in %)</figDesc><table><row><cell>Toolkit</cell><cell>Model</cell><cell>DEV</cell><cell>TEST</cell></row><row><cell>Athena</cell><cell>Transformer-AED + RNNLM</cell><cell>13.60</cell><cell>12.70</cell></row><row><cell>ESPnet</cell><cell>Conformer/Transformer-AED</cell><cell>10.90</cell><cell>10.80</cell></row><row><cell>Kaldi</cell><cell>Chain + RNNLM</cell><cell>14.78</cell><cell>14.84</cell></row><row><cell>Pika</cell><cell>RNN-T</cell><cell>12.30</cell><cell>12.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Kaldi baselines for GigaSpeech training subsets (WER in %)</figDesc><table><row><cell>Subset</cell><cell>DEV</cell><cell>TEST</cell></row><row><cell>XL</cell><cell>14.78</cell><cell>14.84</cell></row><row><cell>L</cell><cell>16.60</cell><cell>16.28</cell></row><row><cell>M</cell><cell>17.96</cell><cell>17.53</cell></row><row><cell>S</cell><cell>22.59</cell><cell>22.14</cell></row><row><cell>XS</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="3">both cross-entropy and LF-MMI criteria. Our neural network stacks</cell></row><row><cell cols="3">6 convolutional neural network (CNN) layers, 10 TDNN-F layers,</cell></row><row><cell cols="3">1 attention-relu-renorm-layer, 1 TDNN-F layer, 1 fast-lstmp-layer, 1</cell></row><row><cell cols="3">TDNN-F layer and finally 1 more fast-lstmp-layer. During the decoding</cell></row><row><cell cols="3">stage, a 4-gram LM is first used for decoding, followed by a Recurrent</cell></row><row><cell cols="3">Neural Network Language Model (RNNLM) rescoring pass.</cell></row></table><note>4.4. Pika Baseline System 5</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/SpeechColab/GigaSpeech arXiv:2106.06909v1 [cs.SD] 13 Jun 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/athena-team/athena/tree/ master/examples/asr/gigaspeech</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/espnet/espnet/tree/ master/egs2/gigaspeech/asr1 4 https://github.com/kaldi-asr/kaldi/tree/ master/egs/gigaspeech</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/tencent-ailab/pika/tree/ main/egs/gigaspeech</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to thank Xingyu Na for his various suggestions on the Kaldi baseline system. The authors also would like to thank Speechocean for transcribing the GigaSpeech evaluation sets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The design for the wall street journalbased csr corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Spoken Language Processing (ICSLP). ISCA</title>
		<meeting>International Conference on Spoken Language essing (ICSLP). ISCA</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2016. ISCA</title>
		<meeting>Interspeech 2016. ISCA</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>2015 IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Speech and Computer</title>
		<meeting>International Conference on Speech and Computer</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">People&apos;s Speech</title>
		<ptr target="https://mlcommons.org/en/peoples-speech/" />
		<imprint>
			<date type="published" when="2021-04-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SPGISpeech: 5,000 hours of transcribed financial audio for fully formattedend-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>O&amp;apos;neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dovzhenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Shulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kucsko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Submitted to Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<meeting>2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">JHU Kaldi system for Arabic MGB-3 ASR challenge using diarization, audiotranscript alignment and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="346" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching protein sequence libraries: Comparison of the sensitivity and selectivity of the Smith-Waterman and FASTA algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="635" to="650" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2018. ISCA</title>
		<meeting>Interspeech 2018. ISCA</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimum bayes risk training of RNN-transducer for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020. ISCA</title>
		<meeting>Interspeech 2020. ISCA</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="966" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13956</idno>
		<title level="m">Recent developments on espnet toolkit boosted by conformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
