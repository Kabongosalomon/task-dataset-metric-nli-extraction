<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controllable Person Image Synthesis with Attribute-Decomposed GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Men</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Controllable Person Image Synthesis with Attribute-Decomposed GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the Attribute-Decomposed GAN, a novel generative model for controllable person image synthesis, which can produce realistic person images with desired human attributes (e.g., pose, head, upper clothes and pants) provided in various source inputs. The core idea of the proposed model is to embed human attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. Specifically, a new architecture consisting of two encoding pathways with style block connections is proposed to decompose the original hard mapping into multiple more accessible subtasks. In source pathway, we further extract component layouts with an off-the-shelf human parser and feed them into a shared global texture encoder for decomposed latent codes. This strategy allows for the synthesis of more realistic output images and automatic separation of un-annotated attributes. Experimental results demonstrate the proposed method's superiority over the state of the art in pose transfer and its effectiveness in the brand-new task of component attribute transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person image synthesis (PIS), a challenging problem in areas of Computer Vision and Computer Graphics, has huge potential applications for image editing, movie making, person re-identification (Re-ID), virtual clothes try-on and so on. An essential task of this topic is pose-guided image generation <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">33]</ref>, rendering the photo-realistic images of people in arbitrary poses, which has become a new hot topic in the community. Actually, not only poses but also many other valuable human attributes can be used to guide the synthesis process.</p><p>In this paper, we propose a brand-new task that aims at synthesizing person images with controllable human attributes, including pose and component attributes such as head, upper clothes and pants. As depicted in <ref type="figure" target="#fig_1">Figure 1</ref>  to provide desired human attributes respectively. The proposed model embeds component attributes into the latent space to construct the style code and encodes the keypointsbased 2D skeleton extracted from the person image as the pose code, which enables intuitive component-specific (pose) control of the synthesis by freely editing the style (pose) code. Thus, our method can automatically synthesize high-quality person images in desired component attributes under arbitrary poses and can be widely applied in not only pose transfer and Re-ID, but also garment transfer and attribute-specific data augmentation (e.g., clothes commodity retrieval and recognition).</p><p>Due to the insufficiency of annotation for human attributes, the simplicity of keypoint representation and the diversity of person appearances, it is challenging to achieve the goal mentioned above using existing methods. Pose transfer methods firstly proposed by <ref type="bibr" target="#b23">[23]</ref> and later extended by <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b46">46]</ref> mainly focus on pose-guided person image synthesis and they do not provide user control of human attributes such as head, pants and upper clothes. Moreover, because of the non-rigid nature of human body, it is difficult to directly transform the spatially misaligned bodyparts via convolution neural networks and thus these methods are unable to produce satisfactory results. Appearance transfer methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b28">28]</ref> allow users to transfer clothes from one person to another by estimating a complicated 3D human mesh and warping the textures to fit for the body topology. Yet, these methods fail to model the intricate interplay of the inherent shape and appearance, and lead to unrealistic results with deformed textures. Another type of appearance transfer methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">45]</ref> try to model clothing textures by feeding the entire source person image into neural networks, but they cannot transfer human attributes from multiple source person images and lack the capability of component-level clothing editing.</p><p>The notion of attribute editing is commonly used in the field of facial attribute manipulation <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b39">39]</ref>, but to the best of our knowledge this work is the first to achieve attribute editing in the task of person image synthesis. Different from pervious facial attribute editing methods which require strict attribute annotation (e.g., smiling, beard and eyeglasses exist or not in the training dataset), the proposed method does not need any annotation of component attributes and enables automatic and unsupervised attribute separation via delicately-designed modules. In another aspect, our model is trained with only a partial observation of the person and needs to infer the unobserved body parts to synthesize images in different poses and views. It is more challenging than motion imitation methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">35]</ref>, which utilize all characters performing a series of same motions to disentangle the appearance and pose, or train one model for each character by learning a mapping from 2D pose to one specific domain.</p><p>To address the aforementioned challenges, we propose a novel controllable person image synthesis method via an Attribute-Decomposed GAN. In contrast to previous works <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">33]</ref> forcedly learn a mapping from concatenated conditions to the target image, we introduce a new architecture of generator with two independent pathways, one for pose encoding and the other for decomposed component encoding. For the latter, our model first separates component attributes automatically from the source person image via its semantic layouts which are extracted with a pretrained human parser. Component layouts are fed into a global texture encoder with multi-branch embeddings and their latent codes are recombined in a specific order to construct the style code. Then the cascaded style blocks, acting as a connection of two pathways, inject the component attributes represented by the style code into the pose code by controlling the affine transform parameters of AdaIN layer. Eventually, the desired image can be reconstructed from target features. In summary, our contributions are threefold:</p><p>? We propose a brand-new task that synthesizes person images with controllable human attributes by directly providing different source person images, and solve it by modeling the intricate interplay of the inherent pose and component-level attributes.</p><p>? We introduce the Attribute-Decomposed GAN, a neat and effective model achieving not only flexible and continuous user control of human attributes, but also a significant quality boost for the original PIS task.</p><p>? We tackle the challenge of insufficient annotation for human attributes by utilizing an off-the-shelf human parser to extract component layouts, making an automatic separation of component attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Synthesis</head><p>Due to their remarkable results, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">[13]</ref> have become powerful generative models for image synthesis <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b3">4]</ref> in the last few years. The image-to-image translation task was solved with conditional GANs <ref type="bibr" target="#b26">[26]</ref> in Pix2pix <ref type="bibr" target="#b16">[16]</ref> and extended to highresolution level in Pix2pixHD <ref type="bibr" target="#b36">[36]</ref>. Zhu et al. <ref type="bibr" target="#b44">[44]</ref> introduced an unsupervised method, CycleGAN, exploiting cycle consistency to generate the image from two domains with unlabeled images. Much of the work focused on improving the quality of GAN-synthesized images by stacked architectures <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b27">27]</ref>, more interpretable latent representations <ref type="bibr" target="#b6">[7]</ref> or self-attention mechanism <ref type="bibr" target="#b42">[42]</ref>. StyleGAN <ref type="bibr" target="#b18">[18]</ref> synthesized impressive images by proposing a brandnew generator architecture which controls generator via the adaptive instance normalization (AdaIN) <ref type="bibr" target="#b15">[15]</ref>, the outcome of style transfer literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">17]</ref>. However, these techniques have limited scalability in handling attributed-guided person synthesis, due to complex appearances and simple poses with only several keypoints. Our method built on GANs overcomes these challenges by a novel generator architecture designed with attribute decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Person Image Synthesis</head><p>Up to now, many techniques have been proposed to synthesize person images in arbitrary poses using adversarial learning. PG 2 <ref type="bibr" target="#b23">[23]</ref> firstly proposed a two-stage GAN architecture to generate person images, in which the person with the target pose is coarsely synthesized in the first stage, and then refined in the second stage. Esser et al. <ref type="bibr" target="#b8">[9]</ref> leveraged a variational autoencoder combined with the conditional U-Net <ref type="bibr" target="#b31">[31]</ref>to model the inherent shape and appearance. Siarohin et al. <ref type="bibr" target="#b33">[33]</ref> used a U-Net based generator with deformable skip connections to alleviate the pixel-to-pixel misalignments caused by pose differences. A later work by Zhu et al. <ref type="bibr" target="#b46">[46]</ref> introduced cascaded Pose-Attentional Transfer Blocks into generator to guide the deformable transfer process progressively. <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b34">34]</ref> utilized a bidirectional strategy for synthesizing person images in an unsupervised manner. However, these methods only focused on transferring the pose of target image to the reference person and our method achieved a controllable person image synthesis with not only pose guided, but also component attributes (e.g., head, upper clothes and pants) controlled. Moreover, more realistic person images with textural coherence and identical consistency can be produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method Description</head><p>Our goal is to synthesize high-quality person images with user-controlled human attributes, such as pose, head, upper clothes and pants. Different from previous attribute editing methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41]</ref> requiring labeled data with binary annotation for each attribute, our model achieves automatic and unsupervised separation of component attributes by introducing a well-designed generator. Thus, we only need the dataset that contains person images {I ? R 3?H?W } with each person in several poses. The corresponding keypoint-based pose P ? R 18?H?W of I, 18 channel heat map that encodes the locations of 18 joints of a human body, can be automatically extracted via an existing pose estimation method <ref type="bibr" target="#b4">[5]</ref>. During training, a target pose P t and a source person image I s are fed into the generator and a synthesized image I g following the appearance of I s but under the pose P t will be challenged for realness by the discriminators. In the following, we will give a detailed description for each part of our model. <ref type="figure" target="#fig_2">Figure 2</ref> shows the architecture of our generator, whose inputs are the target pose P t and source person image I s , and the output is the generated image I g with source person I s in the target pose P t . Unlike the generator in <ref type="bibr" target="#b23">[23]</ref> which directly concatenates the source image and target condition together as input to a U-Net architecture and forcedly learns a result under the supervision of the target image I t , our generator embeds the target pose P t and source person I s into two latent codes via two independent pathways, called pose encoding and decomposed component encoding, respectively. These two pathways are connected by a series of style blocks, which inject the texture style of source person into the pose feature. Finally, the desired person image I g is reconstructed from target features by a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pose encoding</head><p>In the pose pathway, the target pose P t is embedded into the latent space as the pose code C pose by a pose encoder, which consists of N down-sampling convolutional layers (N = 2 in our case), following the regular configuration of encoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decomposed component encoding</head><p>In the source pathway, the source person image I s is embedded into the latent space as the style code C sty via a module called decomposed component encoding (DCE). As depicted in <ref type="figure" target="#fig_2">Figure 2</ref>, this module first extracts the semantic map S of source person I s with an existing human parser <ref type="bibr" target="#b12">[12]</ref> and converts S into a K-channel heat map M ? R K?H?W . For each channel i, there is a binary mask M i ? R H?W for the corresponding component (e.g., upper clothes). The decomposed person image with component i is computed by multiplying the source person image with the component mask M i</p><formula xml:id="formula_0">I i s = I s M i ,<label>(1)</label></formula><p>where denotes element-wise product. I i s is then fed into the texture encoder T enc to acquire the corresponding style code C i sty in each branch by</p><formula xml:id="formula_1">C i sty = T enc (I i s ),<label>(2)</label></formula><p>where the texture encode T enc is shared for all branches and its detailed architecture will be described below. Then all C i sty , i = 1K will be concatenated together in a top-down manner to get the full style code C sty .</p><p>In contrast to the common solution that directly encodes the entire source person image, this intuitive DCE module decomposes the source person into multiple components and recombines their latent codes to construct the full style code. Such an intuitive strategy kills two birds with one stone: 1) It speeds up the convergence of model and achieves more realistic results in less time. Due to the complex structure of the manifold that is constituted of various person images with different clothes and poses, it is hard to encode the entire person with detailed textures, but much simpler to only learn the features of one component of the person. Also, different components can share the same network parameters for color encoding and thus DCE implicitly provides a data augmentation for texture learning. The   For the texture encoder, inspired by a style transfer method <ref type="bibr" target="#b15">[15]</ref> which directly extracts the image code via a pretrained VGG network to improve the generalization ability of texture encoding, we introduce an architecture of global texture encoding by concatenating the VGG features in corresponding layers to our original encoder, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The values of parameters in the original encoder are learnable while those in the VGG encoder are fixed. Since the fixed VGG network is pretrained on the COCO dataset <ref type="bibr" target="#b21">[21]</ref> and it has seen many images with various textures, it has a global property and strong generalization ability for in-the-wild textures. But unlike the typical style transfer task <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b10">11]</ref> requiring only a roughly reasonable result without tight constraints, our model needs to output the explicitly specified result for a given source person in the target pose. It is difficult for the network with a fixed encoder to fit such a complex model and thus the learnable encoder is introduced, combined with the fixed one. The effects of the global texture encoding (GTE) are shown in <ref type="figure" target="#fig_5">Figure 4</ref> (c)(d). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Texture style transfer</head><p>Texture style transfer aims to inject the texture pattern of source person into the feature of target pose, acting as a connection of the pose code and style code in two pathways. This transfer network consists of several cascaded style blocks, each one of which is constructed by a fusion module and residual conv-blocks equipped with AdaIN. For the t th style block, its inputs are deep features F t?1 at the output of the previous block and the style code C sty . The output of this block can be computed by</p><formula xml:id="formula_2">F t = ? t (F t?1 , A) + F t?1 ,<label>(3)</label></formula><p>where F t?1 firstly goes through conv-blocks ? t , whose output is added back to F t?1 to get the output F t , F 0 = C pose in the first block and 8 style blocks are adopted totally. A denotes learned affine transform parameters (scale ? and shift ?) required in the AdaIN layer and can be used to normalize the features into the desired style <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">15]</ref>. Those parameters are extracted from the style code C sty via a fusion module (FM), which is an important auxiliary module for DCE. Because component codes are concatenated in a specified order to construct the style code, making a high correlation between the position and component features, this imposes much human ruled intervention and leads to a conflict with the learning tendency of the network itself. Thus we introduce FM consisting of 3 fully connected layers with the first two allowing the network to flexibly select the desired features via linear recombination and the last one providing parameters in the required dimensionality. FM can effectively disentangle features and avoid conflicts between forward operation and backward feedback. The effects of FM are shown in <ref type="figure" target="#fig_7">Figure 6</ref>. When DCE is applied to our model without FM, the result (see <ref type="figure" target="#fig_7">Figure 6</ref> (d)) is even worse than that without DCE (see <ref type="figure" target="#fig_7">Figure 6</ref> (c)). The fusion module makes our model more flexible and guarantees the proper performance of DCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Person image reconstruction</head><p>With the final target features F T ?1 at the output of the last style block, the decoder generates the final image I g from F T ?1 via N deconvolutional layers, following the regular decoder configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminators</head><p>Following Zhu et al. <ref type="bibr" target="#b46">[46]</ref>, we adapt two discriminators D p and D t , where D p is used to guarantee the alignment of the pose of generated image I g with the target pose P t and D t is used to ensure the similarity of the appearance texture of I g with the source person I s . For D p , the target pose P t concatenated with the generated image I g (real target image I t ) is fed into D p as a fake (real) pair. For D t , the source person image I s concatenated with I g (I t ) is fed into D t as a fake (real) pair. Both D p and D t are implemented as PatchGAN and more details can be found in <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Our full training loss is comprised of an adversarial term, a reconstruction term, a perceptual term and a contextual term</p><formula xml:id="formula_3">L total = L adv + ? rec L rec + ? per L per + ? CX L CX ,<label>(4)</label></formula><p>where ? rec , ? per and ? CX denote the weights of corresponding losses, respectively. Adversarial loss. We employ an adversarial loss L adv with discriminators D p and D t to help the generator G synthesize the target person image with visual textures similar to the reference one, as well as following the target pose. It penalizes for the distance between the distribution of real pairs (I s (P t ), I t ) and the distribution of fake pairs (I s (P t ), I g ) containing generated images L adv =E Is,Pt,It [log(D t (I s , I t ) ? D p (P t , I t ))]+ E Is,Pt [log((1 ? D t (I s , G(I s , P t ))) ? (1 ? D p (P t , G(I s , P t ))))].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(5)</head><p>Reconstruction loss. The reconstruction loss is used to directly guide the visual appearance of the generated image similar to that of the target image I t , which can avoid obvious color distortions and accelerate the convergence process to acquire satisfactory results. L rec is formulated as the L1 distance between the generated image and target image I t</p><formula xml:id="formula_4">L rec = ||G(I s , P t ) ? I t || 1 .<label>(6)</label></formula><p>Perceptual loss. Except for low-level constraints in the RGB space, we also exploit deep features extracted from certain layers of the pretrained VGG network for texture matching, which has been proven to be effective in image synthesis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">33]</ref> tasks. The preceputal loss is computed as <ref type="bibr" target="#b46">[46]</ref>   where ? l is the output feature from layer l of VGG19 network, and W l , H l , C l are spatial width, height and depth of feature ? l . Contextual loss. The contextual loss proposed in <ref type="bibr" target="#b25">[25]</ref> is designed to measure the similarity between two non-aligned images for image transformation, which is also effective in our GAN-based person image synthesis task. Compared with the pixel-level loss requiring pixel-to-pixel alignment, the contextual loss allows spatial deformations with respect to the target, getting less texture distortion and more reasonable outputs. We compute the contextual loss L CX by</p><formula xml:id="formula_5">L per = 1 W l H l C l W l x=1 H l y=1 C l z=1 ? l (I g ) x,y,z ?? l (I t ) x,y,z 1 ,<label>(7)</label></formula><formula xml:id="formula_6">L CX = ?log(CX(F l (I g ), F l (I t ))),<label>(8)</label></formula><p>where F l (I g ) and F l (I t ) denote the feature maps extracted from layer l = relu{3 2, 4 2} of the pretrained VGG19 network for images I g and I t , respectively, and CX denotes the similarity metric between matched features, considering both the semantic meaning of pixels and the context of the entire image. More details can be found in <ref type="bibr" target="#b25">[25]</ref>. We show the effects of L CX in <ref type="figure" target="#fig_9">Figure 8</ref>, which enables the network to better preserve details with less distortion. Implementation details. Our method is implemented in PyTorch using two NVIDIA Tesla-V100 GPUs with 16GB memory. With the human parser <ref type="bibr" target="#b1">[2]</ref>, we acquire the semantic map of person image and merge original labels defined in <ref type="bibr" target="#b12">[12]</ref> into K(K = 8) categories (i.e., background, hair, face, upper clothes, pants, skirt, arm and leg). The weights for the loss terms are set to ? rec = 2, ? per = 2, and ? CX = 0.02. We adopt Adam optimizer <ref type="bibr" target="#b19">[19]</ref> with the momentum set to 0.5 to train our model for around 120k iterations. The initial learning rate is set to 0.001 and linearly decayed to 0 after 60k iterations. Following this configuration, we alternatively train the generator and two discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we verify the effectiveness of the proposed network for attributes-guided person image synthesis tasks (pose transfer and component attribute transfer), and illustrate its superiority over other state-of-the-art methods. Detailed results are shown in the following subsections and more are available in the supplemental materials (Supp). Dataset. We conduct experiments on the In-shop Clothes Retrieval Benchmark DeepFashion <ref type="bibr" target="#b22">[22]</ref>, which contains a large number of person images with various appearances and poses. There are totally 52,712 images with the resolution of 256 ? 256. Following the same data configuration in pose transfer <ref type="bibr" target="#b46">[46]</ref>, we randomly picked 101,966 pairs of images for training and 8,750 pairs for testing. Evaluation Metrics. Inception Score (IS) <ref type="bibr" target="#b32">[32]</ref> and Structural Similarity (SSIM) <ref type="bibr" target="#b37">[37]</ref> are two most commonly-used evaluation metrics in the person image synthesis task, which were firstly used in PG 2 <ref type="bibr" target="#b23">[23]</ref>. Later, Siarohin et al. <ref type="bibr" target="#b33">[33]</ref> introduced Detection Score (DS) to measure whether the person can be detected in the image. However, IS and DS only rely on an output image to judge the quality in itself and ignore its consistency with conditional images. Here, we introduce a new metric called contextual (CX) score, which is proposed for image transformation <ref type="bibr" target="#b25">[25]</ref> and uses the cosine distance between deep features to measure the similarity of two non-aligned images, ignoring the spatial position of the features. CX is able to explicitly assess the texture coherence between two images and it is suitable for our task to measure the appearance consistency between the generated image and source image (target image), recording as CX-GS (CX-GT). Except for these computed metrics, we also perform the user study to assess the realness of synthesized images by human. istic results even when the target poses are drastically different from the source in scale, viewpoints, etc. We show some results of our method in <ref type="figure" target="#fig_8">Figure 7</ref> and more are available in Supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with state-of-the-art methods</head><p>For pose transfer, we evaluate our proposed method with both qualitative and quantitative comparisons. Qualitative comparison. In <ref type="figure">Figure 9</ref>, we compare the synthesis results of our method with four state-of-the-art pose transfer methods: PG 2 <ref type="bibr" target="#b23">[23]</ref> , DPIG <ref type="bibr" target="#b24">[24]</ref>, Def-GAN <ref type="bibr" target="#b33">[33]</ref> and PATN <ref type="bibr" target="#b46">[46]</ref>. All the results of these methods are obtained by directly using the source codes and trained models released by authors. As we can see, our method produced more realistic results in both global structures and detailed textures. The facial identity is better preserved and even detailed muscles and clothing wrinkles are successfully synthesized. More results can be found in Supp. Quantitative comparison. In <ref type="table">Table 1</ref>, we show the quantitative comparison with abundant metrics described before. Since the data split information in experiments of <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33]</ref> is not given, we download their pre-trained models and evaluate their performance on our test set. Although it is inevitable that testing images may be contained in their training samples, our method still outperforms them in most metrics. The results show that our method generates not only more realistic details with the highest IS value, but also more similar and natural textures with respect to the source image and target image, respectively (lowest CX-GS and CX-GT values). Furthermore, our method has the highest confidence for person detection with the best DS value. For SSIM, we observe that when the value of IS increases,  this metric slightly decreases, meaning the sharper images may have lower SSIM, which also has been observed in other methods <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. User study. We conduct a user study to assess the realness and faithfulness of the generated images and compare the performance of our method with four pose transfer techniques. For the realness, participants are asked to judge whether a given image is real or fake within a second. Following the protocol of <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b46">46]</ref>, we randomly selected 55 real images and 55 generated images, first 10 of which are used for warming up and the remaining 100 images are used for evaluation. For the faithfulness, participants are shown a source image and 5 transferred outputs, and they are asked to select the most natural and reasonable image with respect to the source person image. We show 30 comparisons to each participant and finally 40 responses are collected per experiment. The results in <ref type="table" target="#tab_1">Table 2</ref> further validate that our generated images are more realistic, natural and faithful. It is worth noting that there is a significant quality boost of synthesis results obtained by our approach compared with other methods, where over 70% of our results are selected as the most realistic one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component Attribute Transfer</head><p>Our method also achieves controllable person image synthesis with user-specific component attributes, which can be provided by multiple source person images. For example, given 3 source person images with different component attributes, we can automatically synthesize the target image with the basic appearance of person 1, the upper clothes of person 2 and the pants of person 3. This also provides a powerful tool for editing component-level human attributes, such as pants to dress, T-shirt to waistcoat, and head of man to woman. By encoding the source person images to decomposed component codes and recombining their codes to construct the full style code, our method can synthesize the target image with desired attributes. In <ref type="figure" target="#fig_1">Figure 10</ref>, we edit the upper clothes or pants of target images by using additional source person images to provide desired attributes. Our method generates natural images with new attributes introduced harmoniously while preserving the textures of remaining components. Style Interpolation.</p><p>Using our Attribute-Decomposed GAN, we can travel along the manifold of all component attributes of the person in a given image, thus synthesizing an animation from one attribute to another. Take for example the codes of upper clothes from person1 and person2 (C uc1 and C uc2 ), we define their mixing result as</p><formula xml:id="formula_7">C mix = ?C uc1 + (1 ? ?)C uc2 ,<label>(9)</label></formula><p>where ? ? (0, 1) and ? decreases from 1 to 0 in specific steps. Results of style interpolation are available in Supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Failure cases</head><p>Although impressive results can be obtained by our method in most cases, it fails to synthesize images with pose and component attributes that extremely bias the manifold built upon the training data. The model constructs a complex manifold that is constituted of various pose and component attributes of person images, and we can travel along the manifold from one attribute to another. Thus, valid synthesis results are actually the mixtures of seen ones via the interpolation operation. As shown in <ref type="figure" target="#fig_1">Figure 11</ref>, the specific carton pattern in T-shirt of a woman fails to be interpolated with seen ones and the person in a rare pose cannot be synthesized seamlessly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a novel Attribute-Decomposed GAN for controllable person image synthesis, which allows flexible and continuous control of human attributes. Our method introduces a new generator architecture which embeds the source person image into the latent space as a series of decomposed component codes and recombines these codes in a specific order to construct the full style code. Experimental results demonstrated that this decomposition strategy enables not only more realistic images for output but also flexible user control of component attributes. We also believed that our solution using the offthe-shelf human parser to automatically separate component attributes from the entire person image could inspire future researches with insufficient data annotation. Furthermore, our method is not only well suited to generate person images but also can be potentially adapted to other image synthesis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, users are allowed to input multiple source person images * Corresponding author. E-mail: lianzhouhui@pku.edu.cn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Controllable person image synthesis with desired human attributes provided by multiple source images. Human attributes including pose and component attributes are embedded into the latent space as the pose code and decomposed style code. Target person images can be generated in user control with the editable style code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the network architecture of our generator. The target pose and source person are embedded into the latent space via two independent pathways, called pose encoding and decomposed component encoding, respectively. For the latter, we employ a human parser to separate component attributes and encode them via a global texture encoder. A series of style blocks equipped with a fusion module are introduced to inject the texture style of source person into the pose code by controlling the affine transform parameters in AdaIN layers. Finally, the desired image is reconstructed via a decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Details of the texture encoder in our generator. A global texture encoding is introduced by concatenating the output of learnable encoder and fixed VGG encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Target pose (c) w/o DCE&amp;GTE (d) w/o DCE (e) Full Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization effects of the DCE and GTE. (a) A source person and (b) a target pose for inputs. (c) The result generated without either DCE or GTE. (d) The result generated without only DCE. (e) The result generated with both two modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Loss curves for the effectiveness of our DCE module in the training process. loss curves for the effects of our DCE module in training are shown in Figure 5 and the visualization effects are provided in Figure 4 (d)(e). 2) It achieves an automatic and unsupervised attribute separation without any annotation in the training dataset, which utilizes an existing human parser for spatial decomposition. Specific attributes are learned in the fixed positions of the style code. Thus we can easily control component attributes by mixing desired component codes extracted from different source persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Target pose (c) w/o DCE (d)w/ DCE, w/o FM (e)w/ DCE&amp;FM Auxiliary effects of the fusion module (FM) for DCE. (a) A source person and (b) a target pose for inputs. (c) The result generated without DCE. (d) The result generated with DCE introduced but no FM contained in style blocks. (e) The result generated with both DCE and FM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Results of synthesizing person images in arbitrary poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>(a) Source (b) Target pose (c) Ground-truth (d) w/o CX (e) w/ CX Effects of the contextual loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4. 1 .Figure 9 :</head><label>19</label><figDesc>Pose transfer 4.1.1 Person image synthesis in arbitrary poses Pose is one of the most essential human attributes and our experiments verify the effectiveness of our model in posecontrolled person image synthesis. Given the same source person image and several poses extracted from person images in the test set, our model can generate natural and real-Qualitative comparison with state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Results of synthesizing person images with controllable component attributes. We show original person images in the first column and the images in the right are synthesized results whose pants (the first row) or upper clothes (the second row) are changed with corresponding source images in the left. Failure cases caused by component or pose attributes that extremely bias the manifold built upon training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the user study (%). R2G means the percentage of real images rated as generated w.r.t. all real images. G2R means the percentage of generated images rated as real w.r.t. all generated images. The user preference of the most realistic images w.r.t. source persons is shown in the last row.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning character-agnostic motion for motion retargeting in 2d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8340" to="8348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5933" to="5942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<title level="m">Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="8857" to="8866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structuresensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attgan: Facial attribute editing by only changing what you want</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative model of people in clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="853" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="406" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dropout-gan: Learning from a dynamic ensemble of discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon?alo</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11346</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clothcap: Seamless 4d clothing capture and retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonny</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised person image synthesis in arbitrary poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8620" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swapnet: Image based garment transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="679" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deformable gans for pose-based human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised person image generation with semantic parsing transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2357" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Video-tovideo synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Detailed garment recovery from a single-view image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Ambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zherong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-latent gan: Learning to generate and modify facial images from attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02166</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human appearance transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5391" to="5399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative adversarial network with spatial attention for face attribute editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Be your own prada: Fashion synthesis with structural coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1680" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Progressive pose attention transfer for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
