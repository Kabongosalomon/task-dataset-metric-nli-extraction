<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2017 DECOMPOSING MOTION AND CONTENT FOR NATURAL VIDEO SEQUENCE PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<postCode>95110</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Google Brain</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2017 DECOMPOSING MOTION AND CONTENT FOR NATURAL VIDEO SEQUENCE PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the proposed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos. * This work was done while SH and XL were visiting the University of Michigan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding videos has been one of the most important tasks in the field of computer vision. Compared to still images, the temporal component of videos provides much richer descriptions of the visual world, such as interaction between objects, human activities, and so on. Amongst the various tasks applicable on videos, the task of anticipating the future has recently received increased attention in the research community. Most prior works in this direction focus on predicting high-level semantics in a video such as action <ref type="bibr" target="#b23">(Vondrick et al., 2015;</ref><ref type="bibr" target="#b16">Ryoo, 2011;</ref><ref type="bibr" target="#b8">Lan et al., 2014)</ref>, event <ref type="bibr" target="#b29">(Yuen and Torralba, 2010;</ref><ref type="bibr" target="#b6">Hoai and Torre, 2013)</ref> and motion <ref type="bibr" target="#b14">(Pintea et al., 2014;</ref><ref type="bibr" target="#b25">Walker et al., 2014;</ref><ref type="bibr" target="#b13">Pickup et al., 2014;</ref><ref type="bibr" target="#b26">Walker et al., 2016)</ref>. Forecasting semantics provides information about what will happen in a video, and is essential to automate decision making. However, the predicted semantics are often specific to a particular task and provide only a partial description of the future. Also, training such models often requires heavily labeled training data which leads to tremendous annotation costs especially with videos.</p><p>In this work, we aim to address the problem of prediction of future frames in natural video sequences. Pixel-level predictions provide dense and direct description of the visual world, and existing video recognition models can be adopted on top of the predicted frames to infer various semantics of the future. Spatio-temporal correlations in videos provide a self-supervision for frame prediction, which enables purely unsupervised training of a model by observing raw video frames. Unfortunately, estimating frames is an extremely challenging task; not only because of the inherent uncertainty of the future, but also various factors of variation in videos leading to complicated dynamics in raw pixel values. There have been a number of recent attempts on frame prediction <ref type="bibr" target="#b22">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b11">Oh et al., 2015;</ref><ref type="bibr" target="#b3">Goroshin et al., 2015;</ref><ref type="bibr" target="#b9">Lotter et al., 2015;</ref><ref type="bibr" target="#b15">Ranzato et al., 2014)</ref>, which use a single encoder that needs to reason about all the different variations occurring in videos in order to make predictions of the future, or require extra information like foreground-background segmentation masks and static background <ref type="bibr" target="#b24">(Vondrick et al., 2016)</ref>.</p><p>We propose a Motion-Content Network (MCnet) for robust future frame prediction. Our intuition is to split the inputs for video prediction into two easily identifiable groups, motion and content, and independently capture each information stream with separate encoder pathways. In this architecture, the motion pathway encodes the local dynamics of spatial regions, while the content pathway encodes the spatial layout of the salient parts of an image. The prediction of the future frame is then achieved by transforming the content of the last observed frame given the identified dynamics up to the last observation. Somewhat surprisingly, we show that such a network is end-to-end trainable without individual path way supervision. Specifically, we show that an asymmetric architecture for the two pathways enables such decompositions without explicit supervision. The contributions of this paper are summarized below:</p><p>? We propose MCnet for the task of frame prediction, which separates the information streams (motion and content) into different encoder pathways.</p><p>? The proposed network is end-to-end trainable and naturally learns to decompose motion and content without separate training, and reduces the task of frame prediction to transforming the last observed frame into the next by the observed motion.</p><p>? We evaluate the proposed model on challenging real-world video datasets, and show that it outperforms previous approaches on frame prediction.</p><p>The rest of the paper is organized as follows. We briefly review related work in Section 2, and introduce an overview of the proposed algorithm in Section 3. The detailed configuration of the proposed network is described in Section 4. Section 5 describes training and inference procedure. Section 6 illustrates implementation details and experimental results on challenging benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of visual future prediction has received growing interests in the computer vision community. It has led to various tasks depending on the objective of future prediction, such as human activity <ref type="bibr" target="#b23">(Vondrick et al., 2015;</ref><ref type="bibr" target="#b16">Ryoo, 2011;</ref><ref type="bibr" target="#b8">Lan et al., 2014)</ref>, event <ref type="bibr" target="#b29">(Yuen and Torralba, 2010;</ref><ref type="bibr" target="#b6">Hoai and Torre, 2013)</ref> and geometric path <ref type="bibr" target="#b25">(Walker et al., 2014)</ref>. Although previous work achieved reasonable success in specific tasks, they are often limited to estimating predefined semantics, and require fully-labeled training data. To alleviate this issue, approaches predicting representation of the future beyond semantic labels have been proposed. <ref type="bibr" target="#b25">Walker et al. (2014)</ref> proposed a data-driven approach to predict the motion of a moving object, and coarse hallucination of the predicted motion. <ref type="bibr" target="#b23">Vondrick et al. (2015)</ref> proposed a deep regression network to predict feature representations of the future frames. These approaches are supervised and provide coarse predictions of how the future will look like. Our work also focuses on unsupervised learning for prediction of the future, but to a more direct visual prediction task: frame prediction.</p><p>Compared to predicting semantics, pixel-level prediction has been less investigated due to the difficulties in modeling evolution of raw pixels over time. Fortunately, recent advances in deep learning provide a powerful tool for sequence modeling, and enable the creation of novel architectures for modeling complex sequential data. <ref type="bibr" target="#b15">Ranzato et al. (2014)</ref> applied a recurrent neural network developed for language modeling to frame prediction by posing the task as classification of each image region to one of quantized patch dictionaries. <ref type="bibr" target="#b22">Srivastava et al. (2015)</ref> applied a sequence-tosequence model to video prediction, and showed that Long Short-Term Memory (LSTM) is able to capture pixel dynamics. <ref type="bibr" target="#b11">Oh et al. (2015)</ref> proposed an action-conditional encoder-decoder network to predict future frames in Atari games. In addition to the different choices of architecture, some other works addressed the importance of selecting right objective function: <ref type="bibr" target="#b9">Lotter et al. (2015)</ref> used adversarial loss with combined CNN and LSTM architectures, and  employed similar adversarial loss with additional regularization using a multi-scale encoder-decoder network. <ref type="bibr" target="#b0">Finn et al. (2016)</ref> constructed a network that predicts transformations on the input pixels for next frame prediction. <ref type="bibr" target="#b12">Patraucean et al. (2015)</ref> proposed a network that by explicitly predicting optical flow features is able to predict the next frame in a video. <ref type="bibr" target="#b24">Vondrick et al. (2016)</ref> proposed a generative adversarial network for video which, by generating a background-foreground mask, is able to generate realistic-looking video sequences. However, none of the previously mentioned approaches exploit spatial and temporal information separately in an unsupervised fashion. In terms of the way data is observed, the closest work to ours is <ref type="bibr" target="#b28">Xue et al. (2016)</ref>. The differences are (1) Our model is deterministic and theirs is probabilistic, (2) our motion encoder is based on convolutional LSTM <ref type="bibr" target="#b18">(Shi et al., 2015)</ref> which is a more natural module to model long-term dynamics, (3) our content encoder observes a single scale input and theirs observes many scales, and (4) we directly generate image pixels values, which is a more complicated task. We aim to exploit the existing spatio-temporal correlations in videos by decomposing the motion and content in our network architecture.</p><p>To the best of our knowledge, the idea of separating motion and content has not been investigated in the task of unsupervised deterministic frame prediction. The proposed architecture shares similarities to the two-stream CNN <ref type="bibr" target="#b19">(Simonyan and Zisserman, 2014)</ref>, which is designed for action recognition to jointly exploit the information from frames and their temporal dynamics. However, in contrast to their network we aim to learn features for temporal dynamics directly from the raw pixels, and we use the identified features from the motion in combination with spatial features to make pixel-level predictions of the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM OVERVIEW</head><p>In this section, we formally define the task of frame prediction and the role of each component in the proposed architecture. Let x t ? R w?h?c denote the t-th frame in an input video x, where w, h, and c denote width, height, and number of channels, respectively. The objective of frame prediction is to generate the future framex t+1 given the input frames x 1:t .</p><p>At the t-th time step, our network observes a history of previous consecutive frames up to frame t, and generates the prediction of the next framex t+1 as follows:</p><p>? Motion Encoder recurrently takes an image difference input between frame x t and x t?1 starting from t = 2, and produces the hidden representation d t encoding the temporal dynamics of the scene components (Section 4.1). ? Content Encoder takes the last observed frame x t as an input, and outputs the hidden representation s t that encodes the spatial layout of the scene (Section 4.2). ? Multi-Scale Motion-Content Residual takes the computed features, from both the motion and content encoders, at every scale right before pooling and computes residuals r t <ref type="bibr" target="#b4">(He et al., 2015)</ref> to aid the information loss caused by pooling in the encoding phase (Section 4.3). ? Combination Layers and Decoder takes the outputs from both encoder pathways and residual connections, d t , s t , and r t , and combines them to produce a pixel-level prediction of the next framex t+1 (Section 4.4).</p><p>The overall architecture of the proposed algorithm is described in <ref type="figure" target="#fig_0">Figure 1</ref>. The prediction of multiple frames,x t+1:t+T , can be achieved by recursively performing the above procedures over T time steps (Section 5). Each component in the proposed architecture is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ARCHITECTURE</head><p>This section describes the detailed configuration of the proposed architecture, including the two encoder pathways, multi-scale residual connections, combination layers, and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MOTION ENCODER</head><p>The motion encoder captures the temporal dynamics of the scene's components by recurrently observing subsequent difference images computed from x t?1 and x t , and outputs motion features by</p><formula xml:id="formula_0">[d t , c t ] = f dyn (x t ? x t?1 , d t?1 , c t?1 ) ,<label>(1)</label></formula><p>where x t ?x t?1 denotes element-wise subtraction between frames at time t and t?1, d t ? R w ?h ?c is the feature tensor encoding the motion across the observed difference image inputs, and c t ? R w ?h ?c is a memory cell that retains information of the dynamics observed through time. f dyn is implemented in a fully-convolutional way to allow our model to identify local dynamics of frames  illustrates MCnet with such connections. Our network observes a history of image differences through the motion encoder and last observed image through the content encoder. Subsequently, our network proceeds to compute motion-content features and communicates them to the decoder for the prediction of the next frame.</p><p>rather than complicated global motion. For this, we use an encoder CNN with a Convolutional LSTM <ref type="bibr" target="#b18">(Shi et al., 2015)</ref> layer on top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CONTENT ENCODER</head><p>The content encoder extracts important spatial features from a single frame, such as the spatial layout of the scene and salient objects in a video. Specifically, it takes the last observed frame x t as an input, and produces content features by</p><formula xml:id="formula_1">s t = f cont (x t ) ,<label>(2)</label></formula><p>where s t ? R w ?h ?c is the feature encoding the spatial content of the last observed frame, and f cont is implemented by a Convolutional Neural Network (CNN) that specializes on extracting features from single frame.</p><p>It is important to note that our model employs an asymmetric architecture for the motion and content encoder. The content encoder takes the last observed frame, which keeps the most critical clue to reconstruct spatial layout of near future, but has no information about dynamics. On the other hand, the motion encoder takes a history of previous image differences, which are less informative about the future spatial layout compared to the last observed frame, yet contain important spatio-temporal variations occurring over time. This asymmetric architecture encourages encoders to exploit each of two pieces of critical information to predict the future content and motion individually, and enables the model to learn motion and content decomposition naturally without any supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MULTI-SCALE MOTION-CONTENT RESIDUAL</head><p>To prevent information loss after the pooling operations in our motion and content encoders, we use residual connections <ref type="bibr" target="#b4">(He et al., 2015)</ref>. The residual connections in our network communicate motion-content features at every scale into the decoder layers after unpooling operations. The residual feature at layer l is computed by</p><formula xml:id="formula_2">r l t = f res s l t , d l t l ,<label>(3)</label></formula><p>where r l t is the residual output at layer l, s l t , d l t is the concatenation of the motion and content features along the depth dimension at layer l of their respective encoders, f res (.) l the residual function at layer l implemented as consecutive convolution layers and rectification with a final linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">COMBINATION LAYERS AND DECODER</head><p>The outputs from the two encoder pathways, d t and s t , encode a high-level representation of motion and content, respectively. Given these representations, the objective of the decoder is to generate a pixel-level prediction of the next framex t+1 ? R w?h?c . To this end, it first combines the motion and content back into a unified representation by</p><formula xml:id="formula_3">f t = g comb ([d t , s t ]) ,<label>(4)</label></formula><p>where [d t , s t ] ? R w ?h ?2c denotes the concatenation of the higher-level motion and content features in the depth dimension, and f t ? R w ?h ?c denotes the combined high-level representation of motion and content. g comb is implemented by a CNN with bottleneck layers <ref type="bibr" target="#b5">(Hinton and Salakhutdinov, 2006)</ref>; it first projects both d t and s t into a lower-dimensional embedding space, and then puts it back to the original size to construct the combined feature f t . Intuitively, f t can be viewed as the content feature of the next time step, s t+1 , which is generated by transforming s t using the observed dynamics encoded in d t . Then our decoder places f t back into the original pixel space b?</p><formula xml:id="formula_4">x t+1 = g dec (f t , r t ) ,<label>(5)</label></formula><p>where r t is a list containing the residual connections from every layer of the motion and content encoders before pooling sent to every layer of the decoder after unpooling. We employ the deconvolution network <ref type="bibr" target="#b30">(Zeiler et al., 2011)</ref> for our decoder network g dec , which is composed of multiple successive operations of deconvolution, rectification and unpooling with the addition of the motioncontent residual connections after each unpooling operation. The output layer is passed through a tanh (.) activation function. Unpooling with fixed switches are used to upsample the intermediate activation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFERENCE AND TRAINING</head><p>Section 4 describes the procedures for single frame prediction, while this section presents the extension of our algorithm for the prediction of multiple time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MULTI-STEP PREDICTION</head><p>Given an input video, our network observes the first n frames as image difference between frame x t and x t?1 , starting from t = 2 up to t = n, to encode initial temporal dynamics through the motion encoder. The last frame x n is given to the content encoder to be transformed into the first prediction x t+1 by the identified motion features.</p><p>For each time step t ? [n + 1, n + T ], where T is the desired number of prediction steps, our network takes the difference image between the first predictionx t+1 and the previous image x t , and the first predictionx t+1 itself to predict the next framex t+2 , and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING OBJECTIVE</head><p>To train our network, we use an objective function composed of different sub-losses similar to . Given the training data D = {x</p><formula xml:id="formula_5">(i) 1,...,T } N i=1 , our model is trained to minimize the prediction loss by L = ?L img + ?L GAN ,<label>(6)</label></formula><p>where ? and ? are hyper-parameters that control the effect of each sub-loss during optimization. L img is the loss in image space from  defined by</p><formula xml:id="formula_6">L img = L p (x t+k ,x t+k ) + L gdl (x t+k ,x t+k ) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">L p (y, z) = T k=1 ||y ? z|| p p ,<label>(8)</label></formula><formula xml:id="formula_8">L gdl (y, z) = h,w i,j | (|y i,j ? y i?1,j | ? |z i,j ? z i?1,j |) | ? (9) + | (|y i,j?1 ? y i,j | ? |z i,j?1 ? z i,j |) | ? .</formula><p>Here, x t+k andx t+k are the target and predicted frames, respectively, and p and ? are hyperparameters for L p and L gdl , respectively. Intuitively, L p guides our network to match the average pixel values directly, while L gdl guides our network to match the gradients of such pixel values. Overall, L img guides our network to learn parameters towards generating the correct average sequence given the input. Training to generate average sequences, however, results in somewhat blurry generations which is the reason we use an additional sub-loss. L GAN is the generator loss in adversarial training to allow our model to predict realistic looking frames and it is defined by</p><formula xml:id="formula_9">L GAN = ? log D ([x 1:t , G (x 1:t )]) ,<label>(10)</label></formula><p>where x 1:t is the concatenation of the input images, x t+1:t+T is the concatenation of the ground-truth future images, G (x 1:t ) =x t+1:t+T is the concatenation of all predicted images along the depth dimension, and D (.) is the discriminator in adversarial training. The discriminative loss in adversarial training is defined by</p><formula xml:id="formula_10">L disc = ? log D ([x 1:t , x t+1:t+T ]) ? log (1 ? D ([x 1:t , G (x 1:t )])</formula><p>) .</p><p>(11) L GAN , in addition to L img , allows our network to not only generate the target sequence, but also simultaneously enforce realism in the images through visual sharpness that fools the human eye. Note that our model uses its predictions as input for the next time-step during the training, which enables the gradients to flow through time and makes the network robust for error propagation during prediction. For more a detailed description about adversarial training, please refer to Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we present experiments using our network for video generation. We first evaluate our network, MCnet, on the KTH <ref type="bibr" target="#b17">(Schuldt et al., 2004)</ref> and Weizmann action <ref type="bibr" target="#b2">(Gorelick et al., 2007)</ref> datasets, and compare against a baseline convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b18">(Shi et al., 2015)</ref>. We then proceed to evaluate on the more challenging UCF-101 <ref type="bibr" target="#b21">(Soomro et al., 2012)</ref> dataset, in which we compare against the same ConvLSTM baseline and also the current state-of-the-art method by . For all our experiments, we use ? = 1, ? = 1, and p = 2 in the loss functions.</p><p>In addition to the results in this section, we also provide more qualitative comparisons in the supplementary material and in the videos on the project website: https://sites.google. com/a/umich.edu/rubenevillegas/iclr2017.</p><p>Architectures. The content encoder of MCnet is built with the same architecture as VGG16 (Simonyan and Zisserman, 2015) up to the third pooling layer. The motion encoder of MCnet is also similar to VGG16 up to the third pooling layer, except that we replace its consecutive 3x3 convolutions with single 5x5, 5x5, and 7x7 convolutions in each layer. The combination layers are composed of 3 consecutive 3x3 convolutions (256, 128, and 256 channels in each layer). The multi-scale residuals are composed of 2 consecutive 3x3 convolutions. The decoder is the mirrored architecture of the content encoder where we perform unpooling followed by deconvolution. For the baseline ConvLSTM, we use the same architecture as the motion encoder, residual connections, and decoder, except we increase the number of channels in the encoder in order to have an overall comparable number of parameters with MCnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">KTH AND WEIZMANN ACTION DATASETS</head><p>Experimental settings. The KTH human action dataset <ref type="bibr" target="#b17">(Schuldt et al., 2004)</ref> contains 6 categories of periodic motions on a simple background: running, jogging, walking, boxing, hand-clapping and hand-waiving. We use person 1-16 for training and 17-25 for testing, and also resize frames to 128x128 pixels. We train our network and baseline by observing 10 frames and predicting 10 frames into the future on the KTH dataset. We set ? = 0.02 for training. We also select the walking, running, one-hand waving, and two-hands waving sequences from the Weizmann action dataset <ref type="bibr" target="#b2">(Gorelick et al., 2007)</ref> for testing the networks' generalizability.</p><p>For all the experiments, we test the networks on predicting 20 time steps into the future. As for evaluation, we use the same SSIM and PSNR metrics as in . The evaluation on KTH was performed on sub-clips within each video in the testset. We sample sub-clips every 3 frames for running and jogging, and sample sub-clips every 20 frames (skipping the frames we have already predicted) for walking, boxing, hand-clapping, and hand-waving. Sub-clips for running, jogging, and walking were manually trimmed to ensure humans are always present in the frames. The evaluation on Weizmann was performed on all sub-clips in the selected sequences. Results. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the quantitative comparisons among our MCnet, ConvLSTM baseline and their residual variations. In the KTH test set, our network outperforms the ConvLSTM baseline by a small margin. However, when we test the residual versions of MCnet and ConvLSTM on the dataset <ref type="bibr" target="#b2">(Gorelick et al., 2007)</ref> with similar motions, we can see that our network can generalize well to the unseen contents by showing clear improvements, especially in long-term prediction. One reason for this result is that the test and training partitions of the KTH dataset have simple and similar image contents so that ConvLSTM can memorize the average background and human appearance to make reasonable predictions. However, when tested on unseen data, ConvLSTM has to internally take care of both scene dynamics and image contents in a mingled representation, which gives it a hard time for generalization. In contrast, the reason our network outperforms the ConvLSTM baseline on unseen data is that our network focuses on identifying general motion features and applying them to a learned content representation. <ref type="figure" target="#fig_2">Figure 3</ref> presents qualitative results of multi-step prediction by our network and ConvLSTM. As expected, prediction results by our full architecture preserves human shapes more accurately than the baseline. It is worth noticing that our network produces very sharp prediction over long-term time steps; it shows that MCnet is able to capture periodic motion cycles, which reduces the uncertainty of future prediction significantly. More qualitative comparisons are shown in the supplementary material and the project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">UCF-101 DATASET</head><p>Experimental settings. This section presents results on the challenging real-world videos in the UCF-101 <ref type="bibr" target="#b21">(Soomro et al., 2012)</ref> dataset. Having collected from YouTube, the dataset contains 101 realistic human actions taken in a wild and exhibits various challenges, such as background clutter, occlusion, and complicated motion. We employed the same network architecture as in the KTH dataset, but resized frames to 240x320 pixels, and trained the network to observe 4 frames and predict a single frame. We set ? = 0.001 for training. We also trained our convolutional LSTM baseline in the same way. Following the same protocol as  for data pre-processing and G.T. We display predictions starting from the 12 th frame, in every 3 timesteps. The first 3 rows correspond to KTH dataset for the action of jogging and the last 3 rows correspond to Weizmann dataset for the action of walking. evaluation metrics on full images, all networks were trained on Sports-1M <ref type="bibr" target="#b7">(Karpathy et al., 2014)</ref> dataset and tested on UCF-101 unless otherwise stated. 1</p><p>Results. <ref type="figure">Figure 4</ref> shows the quantitative comparisons between our network trained for single-stepprediction and . We can clearly see the advantage of our network over the baseline. The separation of motion and contents in two encoder pathways allows our network to identify key motion and content features, which are then fed into the decoder to yield predictions of higher quality compared to the baseline. 2 In other words, our network only moves what shows motion in the past, and leaves the rest untouched. We also trained a residual version of MCnet on UCF-101, indicated by "MCnet + RES UCF101", to compare how well our model generalizes when trained and tested on the same or different dataset(s). To our surprise, when tested with UCF-101, the MCnet trained on Sports-1M (MCnet + RES) roughly matches the performance of the MCnet trained on UCF-101 (MCnet + RES UCF101), which suggests that our model learns effective representations which can generalize to new datasets. <ref type="figure">Figure 5</ref> presents qualitative comparisons between frames generated by our network and . Since the ConvLSTM and  lack explicit motion and content modules, they lose sense of the dynamics in the video and therefore the contents become distorted quickly. More qualitative comparisons are shown in the supplementary material and the project website.  <ref type="figure">Figure 4</ref>: Quantitative comparison between our model, convolutional <ref type="bibr">LSTM Shi et al. (2015)</ref>, and . Given 4 input frames, the models predict 8 frames recursively, one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We proposed a motion-content network for pixel-level prediction of future frames in natural video sequences. The proposed model employs two separate encoding pathways, and learns to decompose motion and content without explicit constraints or separate training. Experimental results suggest that separate modeling of motion and content improves the quality of the pixel-level future prediction, and our model overall achieves state-of-the-art performance in predicting future frames in challenging real-world video datasets. A QUALITATIVE AND QUANTITATIVE COMPARISON WITH CONSIDERABLE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAMERA MOTION AND ANALYSIS</head><p>In this section, we show frame prediction examples in which considerable camera motion occurs. We analyze the effects of camera motion on our best network and the corresponding baselines. First, we analyze qualitative examples on UCF101 (more complicated camera motion) and then on KTH (zoom-in and zoom-out camera effect).</p><p>UCF101 Results. As seen in <ref type="figure">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>, our model handles foreground and camera motion for a few steps. We hypothesize that for the first few steps, motion signals from images are clear. However, as images are predicted, motion signals start to deteriorate due to prediction errors. When a considerable amount of camera motion is present in image sequences, the motion signals are very dense. As predictions evolve into the future, our motion encoder has to handle large motion deterioration due to prediction errors, which cause motion signals to get easily confused and lost quickly. KTH Results. We were unable to find videos with background motion in the KTH dataset, but we found videos where the camera is zooming in or out for the actions of boxing, handclapping, and handwaving. In <ref type="figure" target="#fig_0">Figure 11</ref>, we display qualitative for such videos. Our model is able to predict the zoom change in the cameras, while continuing the action motion. In comparison to the performance observed in UCF101, the background does not change much. Thus, the motion signals are well localized in the foreground motion (human), and do not get confused with the background and lost as quickly.</p><p>G.T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvLSTM</head><p>MCnet t=12 t=15 t=18 t=21</p><p>Boxing t=24 t=27 t=30 G.T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCnet</head><p>Handclapping <ref type="figure" target="#fig_0">Figure 11</ref>: Qualitative comparisons on KTH testset. We display predictions starting from the 12 th frame, in every 3 timesteps. More clear motion prediction can be seen in the project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXTENDED QUANTITATIVE EVALUATION</head><p>In this section, we show additional quantitative comparison with a baseline based on copying the last observed frame through time for KTH and UCF101 datasets. Copying the last observed frame through time ensures perfect background prediction in videos where most of the motion comes from foreground (i.e. person performing an action). However, if such foreground composes a small part of the video, it will result in high prediction quality score regardless of the simple copying action.</p><p>In <ref type="figure" target="#fig_0">Figure 12</ref> below, we can see the quantitative comparison in the datasets. Copying the last observed frame through time does a reasonable job in both datasets, however, the impact is larger in UCF101. Videos in the KTH dataset comprise simple background with minimal camera motion, which allows our network to easily predict both foreground and background motion, resulting in better image quality scores. However, videos in UCF101 contain more complicated and diverse background which in combination with camera motion present a much greater challenge to video prediction networks. From the qualitative results in Section A and <ref type="figure">Figures 5, 8, 9</ref>, and 10, we can see that our network performs better in videos that contain isolated areas of motion compared to videos with dense motion. A simple copy/paste operation of the last observed frame, ensures very high prediction scores in videos where very small motion occur. The considerable score boost by videos with small motion causes the simple copy/paste baseline to outperform MCnet in the overall performance on UCF101.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C UCF101 MOTION DISAMBIGUATION EXPERIMENTS</head><p>Due to the observed bias from videos with small motion, we perform experiments by measuring the image quality scores on areas of motion. These experiments are similar to the ones performed in . We compute DeepFlow optical flow <ref type="bibr" target="#b27">(Weinzaepfel et al., 2013)</ref> between the previous and the current groundtruth image of interest, compute the magnitude, and normalize it to [0, 1]. The computed optical flow magnitude is used to mask the pixels where motion was observed. We set the pixels where the optical flow magnitude is less than 0.2, and leave all other pixels untouched in both the groundtruth and predicted images. Additionally, we separate the test videos by the average 2 -norm of time difference between target frames. We separate the test videos into deciles based of the computed average 2 -norms, and compute image quality on each decile. Intuitively, the 1 st decile contains videos with the least overall of motion (i.e. frames that show the smallest change over time), and the 10 th decile contains videos with the most overall motion (i.e. frames that show the largest change over time).</p><p>As shown in <ref type="figure" target="#fig_0">Figure 13</ref>, when we only evaluate on pixels where rough motion is observed, MCnet reflects higher PSNR and SSIM, and clearly outperforms all the baselines in terms of SSIM. The SSIM results show that our network is able to predict a structure (i.e. textures, edges, etc) similar to the grountruth images within the areas of motion. The PSNR results, however, show that our method outperforms the simple copy/paste baseline for the first few steps, but then our method performs slightly worse. The discrepancies observed between PSNR and SSIM scores could be due to the fact that some of the predicted images may not reflect the exact pixel values of the groundtruth regardless of the structures being similar. SSIM scores are known to take into consideration features in the image that go beyond directly matching pixel values, reflecting more accurately how humans perceived image quality.  <ref type="figure" target="#fig_0">Figure 13</ref>: Extended quantitative comparison on UCF101 including a baseline based on copying the last observed frame through time using motion based pixel mask. <ref type="figure" target="#fig_0">Figures 15 and 14</ref> show the evaluation by separating the test videos into deciles based on the average 2 -norm of time difference between target frames. From this evaluation, it is proven that the copy last frame baseline scores higher in videos where motion is the smallest. The first few deciles (videos with small motion) show that our network is not just copying the last observed frame through time, otherwise it would perform similarly to the copy last frame baseline. The last deciles (videos with large motion) show our network outperforming all the baselines, including the copy last frame baseline, effectively confirming that our network does predict motion similar to the motion observed in the video.  D ADVERSARIAL TRAINING  proposed an adversarial training for frame prediction. Inspired by <ref type="bibr" target="#b1">Goodfellow et al. (2014)</ref>, they proposed a training procedure that involves a generative model G and a discriminative model D. The two models compete in a two-player minimax game. The discriminator D is optimized to correctly classify its inputs as either coming from the training data (real frame sequence) or from the generator G (synthetic frame sequence). The generator G is optimized to generate frames that fool the discriminator into believing that they come from the training data. At training time, D takes the concatenation of the input frames that go into G and the images produced by G. The adversarial training objective is defined as follows:</p><formula xml:id="formula_11">min G max D log D ([x 1:t , x t+1:t+T ]) + log (1 ? D ([x 1:t , G (x 1:t )])) ,</formula><p>where [., .] denotes concatenation in the depth dimension, x 1:t denotes the input frames to G, x t+1:t+T are the target frames, and G (x 1:t ) =x t+1:t+T are the frames predicted by G. In practice, we split the minimax objective into two separate, but equivalent, objectives: L GAN and L disc . During optimization, we minimize the adversarial objective alternating between L GAN and L disc . L GAN is defined by</p><formula xml:id="formula_12">L GAN = ? log D ([x 1:t , G (x 1:t )]) ,</formula><p>where we optimize the parameters of G to minimize L GAN while the parameters of D stay untouched. As a result, G is optimized to generate images that make D believe that they come from the training data. Thus, the generated images look sharper, and more realistic. L disc is defined by</p><formula xml:id="formula_13">L disc = ? log D ([x 1:t , x t+1:t+T ]) ? log (1 ? D ([x 1:t , G (x 1:t )])) ,</formula><p>where we optimize the parameters of D to minimize L disc , while the parameters of G stay untouched. D tells us whether its input came from the training data or the generator G. Alternating between the two objectives, causes G to generate very realistic images, and D not being able to distinguish between generated frames and frames from the training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of the proposed network. (a) illustrates MCnet without the Motion-Content Residual skip connections, and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Quantitative comparison between MCnet and ConvLSTM baseline with and without multiscale residual connections (indicated by "+ RES"). Given 10 input frames, the models predict 20 frames recursively, one by one. Left column: evaluation on KTH dataset<ref type="bibr" target="#b17">(Schuldt et al., 2004)</ref>. Right colum: evaluation on Weizmann<ref type="bibr" target="#b2">(Gorelick et al., 2007)</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison between our MCNet model and ConvLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :Figure 7 :Figure 8 :</head><label>5678</label><figDesc>Qualitative comparisons among MCnet and ConvLSTM and. We display predicted frames (in every other frame) starting from the 5 th frame. The green arrows denote the top-30 closest optical flow vectors within image patches between MCnet and ground-truth. More clear motion prediction can be seen in the project website. Qualitative comparisons on KTH testset. We display predictions starting from the 12 th frame, for every 3 timesteps. More clear motion prediction can be seen in the project website. Qualitative comparisons on KTH testset. We display predictions starting from the 12 th frame, for every 3 timesteps. More clear motion prediction can be seen in the project website. Qualitative comparisons on UCF-101. We display predictions (in every other frame) starting from the 5 th frame. The green arrows denote the top-30 closest optical flow vectors within image patches between MCnet and ground-truth. More clear motion prediction can be seen in the project website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Qualitative comparisons on UCF-101. We display predictions (in every other frame) starting from the 5 th frame. The green arrows denote the top-30 closest optical flow vectors within image patches between MCnet and ground-truth. More clear motion prediction can be seen in the project websiteQualitative comparisons on UCF-101. We display predictions (in every other frame) starting from the 5 th frame. The green arrows denote the top-30 closest optical flow vectors within image patches between MCnet and ground-truth. More clear motion prediction can be seen in the project website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Extended quantitative comparison including a baseline based on copying the last observed frame through time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 :</head><label>15</label><figDesc>Quantitative comparison on UCF101 using motion based pixel mask, and separating dataset by average 2 -norm of time difference between target frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Quantitative comparison on UCF101 using motion based pixel mask, and separating dataset by average 2 -norm of time difference between target frames.</figDesc><table><row><cell>Peak Signal to Noise Ratio Peak Signal to Noise Ratio</cell><cell>16 18 20 22 24 26 28 25 30 35 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame</cell><cell>Structural Similarity Structural Similarity</cell><cell>0.7 0.8 0.9 1.0 1.0 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame</cell></row><row><cell cols="2">1 1 1 1 1 1 1 1 1 Figure 14: 1 2 2 18 20 22 24 26 28 30 Peak Signal to Noise Ratio 2 20 25 30 Peak Signal to Noise Ratio 2 22 24 26 28 30 32 34 36 Peak Signal to Noise Ratio 2 20 25 30 35 Peak Signal to Noise Ratio 2 2 25 30 35 40 Peak Signal to Noise Ratio 2 25 30 35 40 45 Peak Signal to Noise Ratio 2 25 30 35 40 45 Peak Signal to Noise Ratio 2 30 35 40 45 50 Peak Signal to Noise Ratio</cell><cell>3 3 3 3 3 3 3 3 3 3</cell><cell>4 4 4 4 4 4 4 4 4 4</cell><cell>5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps</cell><cell>6 6 6 6 6 6 6 6 6 6</cell><cell>7 7 7 7 7 7 7 7 7 7</cell><cell cols="3">8 8 Conv LSTM Conv LSTM + RES 9 th decile MCnet MCnet + RES Matheiu et al Copy last frame 0.7 0.8 0.9 1.0 Structural Similarity 8 th decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.8 0.9 1.0 Structural Similarity 7 th decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.8 0.9 1.0 Structural Similarity 6 th decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.9 1.0 Structural Similarity 8 4 th decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.9 1.0 Structural Similarity 3 rd decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.9 1.0 Structural Similarity 2 nd decile 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 0.9 1.0 Structural Similarity 1 st decile 8 1.0 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame Structural Similarity</cell><cell>1 1 1 1 1 1 1 1 1 1</cell><cell>2 2 2 2 2 2 2 Conv LSTM Conv LSTM + RES 3 3 3 3 3 3 3 MCnet MCnet + RES Matheiu et al Copy last frame 2 3 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 2 3 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 2 3 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame</cell><cell>4 4 4 4 4 4 4 4 4 4</cell><cell>5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps 5 time steps</cell><cell>6 6 6 6 6 6 6 6 6 6</cell><cell>7 7 7 7 7 7 7 7 7 7</cell><cell>8 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 8 Conv LSTM Conv LSTM + RES MCnet MCnet + RES Matheiu et al Copy last frame 8 8 8 8 8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the code and model released by at https://github.com/coupriec/ VideoPredictionICLR2016 2 We were not able to get the model fine-tuned on UCF-101 from the authors so it is not included inFigure 4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>This work was supported in part by ONR N00014-13-1-0762, NSF CAREER IIS-1453651, gifts from the Bosch Research and Technology Center, and Sloan Research Fellowship. We also thank NVIDIA for donating K40c and TITAN X GPUs. We thank Ye Liu, Junhyuk Oh, Xinchen Yan, Lajanugen Logeswaran, Yuting Zhang, Sungryull Sohn, Kibok Lee, Rui Zhang, and other collaborators for helpful discussions. R. Villegas was partly supported by the Rackham Merit Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2247" to="2253" />
			<date type="published" when="2007-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to linearize under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Max-margin early event detectors. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Torre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual structure using predictive generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08023</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.06309</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seeing the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dejavu: Motion prediction in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Anticipating the future by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08023</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno>abs/1606.07873</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A data-driven approach for event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
