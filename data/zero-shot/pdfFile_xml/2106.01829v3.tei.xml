<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Atomistic Line Graph Neural Network for Improved Materials Property Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Choudhary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Materials Measurement Laboratory</orgName>
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<postCode>20899</postCode>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Theiss Research</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92037</postCode>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Decost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Materials Measurement Laboratory</orgName>
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<postCode>20899</postCode>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Atomistic Line Graph Neural Network for Improved Materials Property Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 3 DeepMaterials LLC, Silver Spring, MD, 20906, USA. * These authors contributed equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Corresponding author: Kamal Choudhary (kamalchoudhary@nistgov) 2</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNN) have been shown to provide substantial performance improvements for atomistic material representation and modeling compared with descriptor-based machine learning models. While most existing GNN models for atomistic predictions are based on atomic distance information, they do not explicitly incorporate bond angles, which are critical for distinguishing many atomic structures. Furthermore, many material properties are known to be sensitive to slight changes in bond angles. We present an Atomistic Line Graph Neural Network (ALIGNN), a GNN architecture that performs message passing on both the interatomic bond graph and its line graph corresponding to bond angles. We demonstrate that angle information can be explicitly and efficiently included, leading to improved performance on multiple atomistic prediction tasks. We ALIGNN models for predicting 52 solid-state and molecular properties available in the JARVIS-DFT, Materials project, and QM9 databases. ALIGNN can outperform some previously reported GNN models on atomistic prediction tasks with better or comparable model training speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graphs are a powerful non-Euclidean data structure method for establishing relationships between features (nodes) and their relationships (edges) 1,2 . Graph neural networks (GNN) <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> have immense potential for modeling complex phenomena. Common applications of GNNs include community detection and link prediction in social networks <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> , functional time series on brain structures <ref type="bibr" target="#b6">7</ref> , gene DNA on regulatory networks <ref type="bibr" target="#b7">8</ref> , information flow through telecommunications networks <ref type="bibr" target="#b8">9</ref> , and property prediction for molecular and solid materials <ref type="bibr" target="#b9">10</ref> . From a quantum chemistry point of view, GNNs provide a unique opportunity to predict properties of solids, molecules, and proteins in a much faster way rather than by solving the computationally expensive Schrodinger equation <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> .</p><p>There has been rapid progress in the development of GNN architectures for predicting material properties such as SchNet 10 , Crystal Graph Convolutional Neural Networks (CGCNN) <ref type="bibr" target="#b14">15</ref> , MatErials Graph Network (MEGNet) <ref type="bibr" target="#b15">16</ref> , improved Crystal Graph Convolutional Neural Networks (iCGCNN) <ref type="bibr" target="#b16">17</ref> , OrbNet <ref type="bibr" target="#b17">18</ref> and similar variants <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> . This family of models represents a molecule or crystalline material as a graph with one node for each constituent atom and edges corresponding to interatomic bonds. A common theme is the use of elemental properties as node features and interatomic distances and/or bond valences as edge features. Through multiple layers of graph convolution updating node features based on their local chemical environment, these models can implicitly represent many-body interactions. However, many important material properties (especially electronic properties such as band gaps) are highly sensitive to structural features such as bond angles and local geometric distortions. It is possible that these models are not able to efficiently learn the importance of such many-body interactions. Explicit inclusion of angle-based information has already been shown to improve models with hand-crafted features such as classical force-field inspired descriptors (CFID) <ref type="bibr" target="#b31">32</ref> . Recently, there has been growing interest in the explicit incorporation of bond angles and other many-body features <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20</ref> .</p><p>In this work, we use line graph neural networks inspired by those proposed in Ref <ref type="bibr" target="#b5">6</ref> to develop an alternative way to include angular information to provide high accuracy models. Briefly, the line graph L(g) is a graph derived from another graph g that describes the connectivity of the edges in g. While the nodes of an atomistic graph correspond to atoms and its edges correspond to bonds, the nodes of an atomistic line graph correspond to interatomic bonds and its edges correspond to bond angles. Our model alternates between graph convolution on these two graphs, propagating bond angle information through interatomic bond representations to the atom-wise representations and vice versa. We use both the bond distances and angles in the line graph to incorporate finer details of atomic structure which leads to higher model performance. Our Atomistic Line Graph Neural Network (ALIGNN) models are implemented using the deep graph library (DGL) <ref type="bibr" target="#b32">33</ref> which allows efficient construction and neural message passing for different types of graphs. ALIGNN is a part of the Joint Automated Repository for Various Integrated Simulations (JARVIS) infrastructure <ref type="bibr" target="#b33">34</ref> . We train ALIGNN models for several crystalline material properties from JARVIS-density functional theory (DFT) <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> and Materials project <ref type="bibr" target="#b44">45</ref> (MP) datasets as well as molecular properties from QM9 46 database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Atomistic graph representation</head><p>ALIGNN performs Edge-gated graph convolution 4 message passing updates on both the atomistic bond graph (atoms are nodes, bonds are edges) and its line graph (bonds are nodes, bond pairs with one common atom are edges). The Edge-gated graph convolution variant has the distinct advantage of updating both node and edge features. Because each edge in the bond graph directly corresponds to a node in the line graph, ALIGNN can aggregate features from bond pairs to efficiently update atom and bond representations by alternating between message passing updates on the bond graph and its line graph.</p><p>For crystals, we use a periodic 12-nearest-neighbor graph construction. We expand this nearestneighbor graph to include edges to all atoms in the neighbor shell of the 12th-nearest neighbor.</p><p>Each node in the atomistic graph is assigned 9 input node features based on its atomic species: electronegativity, group number, covalent radius, valence electrons, first ionization energy, electron affinity, block and atomic volume. This feature set is inspired by the CGCNN 15 model.</p><p>The initial edge features are interatomic bond distances. We use a radial basis function (RBF) expansion with support between 0 and 8 ? for crystals and up to 5 ? for molecules. This undirected graph then can be represented as G = (?, ?) where ? are nodes and ? are edges i.e., a collection of (?i, ?j) linking vertices from ?i to ?j. G has an associated node feature set H={h1,?,hN) , where hi is the feature vector associated with node ?i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Atomistic line graph representation</head><p>The atomistic line graph is derived from the atomistic graph. Each node in the line graph corresponds to an edge in the original atomistic graph; both entities represent interatomic bonds, and in our work, they share latent representations. Edges in the line graph correspond to triplets of atoms or pairs of interatomic bonds. The initial line graph edge features are an RBF expansion of the bond angle cosines: ? = ( . | || | ), where rij and rjk are atomic displacement vectors between atoms i, j, and k. A schematic of an atomistic graph and corresponding atomistic line graph is shown in <ref type="figure" target="#fig_9">Fig. 1</ref>. To avoid ambiguity between the node and edge features of the atomistic graph and its line graph, we write atom, bond, and triplet representations as h, e, and t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Schematic showing undirected crystal graph representation and corresponding line graph construction for a SiO4 polyhedron. For simplicity, only Si-O bonds are illustrated. The ALIGNN convolution layer alternates between message passing on the bond graph (left) and its line graph (or bond adjacency graph, right).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Edge gated graph convolution</head><p>ALIGNN uses Edge-gated graph convolution 4 convolution for updating both node and edge features. This convolution is similar to the CGCNN update, except that edge features are only incorporated into normalized edge gates. Furthermore, edge gated graph convolution uses the preaggregated edge messages to update the edge representations.</p><p>Edge gated graph convolution updates node representations h l from layer l according to the formula:</p><formula xml:id="formula_0">? +1 = (? {? } ? ) (1) ? +1 = ? + ( ( ? + ? ? ? ? )) (2) = ( ) ? ? ( )+?<label>(3)</label></formula><formula xml:id="formula_1">= ?1 + ( ( ? ?1 + ? ?1 + ?1 ))<label>(4)</label></formula><p>The edge messages in this equation <ref type="bibr" target="#b3">(4)</ref> are equivalent to the gating term in the CGCNN update <ref type="bibr" target="#b14">15</ref> , which coalesces the weight matrices A, B, and C into Wgate, and the augmented edge representation</p><formula xml:id="formula_2">zij = hi ? hj ? eij (5) = ?1 + ( ( ?1 ))<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ALIGNN update</head><p>One ALIGNN layer composes an edge-gated graph convolution on the bond graph (g) with an edge-gated graph convolution on the line graph (L(g)), as illustrated in <ref type="figure">Fig</ref> </p><p>? , = ? ( , ? ?1 , ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overall model architecture and training</head><p>We use N layers of ALIGNN updates followed by M layers of edge-gated graph convolution (GCN) updates on the bond graph. We use Sigmoid Linear Unit (SiLU, also known as Swish)</p><p>activations instead of rectified linear unit (ReLU) or Softplus because it is twice differentiable like Softplus but can result in a better empirical performance like ReLU on many tasks. After N + M graph convolution layers, our networks perform global average pooling over nodes and finally predict the target properties with a single fully connected regression or classification layers. <ref type="table" target="#tab_0">Table   1</ref> presents the default hyperparameters of the ALIGNN model used to train the models reported in Section F. These hyperparameters were selected through a combination of hypothesis-driven experiments and random hyperparameter search, as discussed in detail in the Methods section.</p><p>Section G provides a detailed analysis of the sensitivity of model performance and computational cost.  Similarly, we train ALIGNN models on the JARVIS-DFT <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> dataset which consists of data for 55722 materials. In addition to properties such as formation energies, and bandgaps it also consists several unique quantities such as solar-cell efficiency (spectroscopic limited maximum efficiency, SLME), topological spin-orbit spillage, dielectric constant with (?x (DFPT)) and without ionic contributions (?x (OPT, MBJ)), exfoliation energies for two-dimensional (2D), electric field gradients (EFG), Voigt bulk (Kv) and shear modulus (Gv), energy above convex hull (ehull), maximum piezoelectric stress (eij) and strain (dij) tensors, n-type and p-type Seebeck coefficient and power factors (PF), crystallographic averages of electron (me) and hole (mh) effective masses.</p><p>As we converge plane wave-cutoff (ENCUT) and k-points used in Brillouin zone integration (Kpoint-length), we attempt to make machine learning predictions on these unique quantities as In <ref type="table">Table.</ref> 3 we show the performance on regression models for different properties in the JARVIS-DFT database. We observe that ALIGNN models outperform CFID descriptors by up to 4 times, suggesting GNNs can be a very powerful method for multiple material property predictions. Also, ALIGNN outperforms CGCNN by more than 2 times (such as for OptB88vdW total energy).</p><p>Cross-dataset comparison of corresponding property entries in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table">Table 3</ref> shows that generally models generally obtain better performance on the MP dataset, which we attribute primarily to the larger size of MP. For example, the MAE for the formation energy target on MP dataset is 50% lower than for JARVIS-DFT. However, for some targets, the differences in the DFT method and settings, as well as potential differences in the material-space distribution, might significantly contribute to the difficulty of a prediction task. (1.63). The results indicate that there is still much room for improvement for the GNN models especially for electronic properties. <ref type="figure" target="#fig_4">Supplementary Figure S3</ref> to <ref type="figure" target="#fig_4">Supplementary Figure S31</ref>. As we notice above, the regression tasks for some of the electronic properties do not show very high MAD: MAE. we train classification models for some of them. Classification tasks predict labels such as high value/low value (based on a selected threshold) as 1 and 0 instead of predicting actual data in regression tasks. Such models can be useful for fast screening purposes <ref type="bibr" target="#b37">38</ref> for computationally expensive methods. We evaluate the performance of these classifiers using the receiver operating characteristic curve area under the curve (ROC AUC). A random guessing model has a ROC AUC of 0.5, while a perfect model would be a ROC AUC of 1.0. Interestingly, we notice most of our classification models (as shown in <ref type="table">Table 4</ref>) have high ROC AUCs, ranging up to a maximum value of 0.94 (for convex hull stability) showing their usefulness for material classification-based applications. All results are based on the performance of 10 % test data which is never used during the training or model selection procedures. <ref type="figure" target="#fig_0">Supplementary Fig. S32</ref> to <ref type="figure" target="#fig_9">Supplementary Fig.41</ref>. obtained by tuning to validation performance on the JARVIS-DFT bandgap target, suggesting that ALIGNN provides robust performance with respect to different datasets and material types. <ref type="figure" target="#fig_0">Supplementary Figure S42</ref> to <ref type="figure" target="#fig_0">Supplementary Figure S52</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3: Regression model performances on JARVIS-DFT dataset for 29 properties using CFID, CGCNN and ALIGNN models on 55722 materials. Predictions on test set are shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: Classification task ROC AUC performance on JARVIS-DFT dataset for ALIGNN models. The ROC curve plots for these models are provided in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5: Regression model performances on QM9 dataset for 11 properties using ALIGNN. These models were trained with same parameters as solid-state databases but for 1000 epochs. Predictions on test set are shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Model analysis</head><p>We ablate individual components of the ALIGNN model to evaluate their contribution to the overall architecture. Keeping other parameters intact in the ALIGNN model (as specified in <ref type="table" target="#tab_0">Table   1</ref>), we vary the number of ALIGNN and GCN layers as shown in <ref type="table">Table 6</ref> and Supplementary  <ref type="table">Table 6</ref>, performs four atom feature updates via graph convolution modules. At least two ALIGNN updates are needed to obtain peak performance.</p><p>Additional atom feature updates provide little marginal increase in performance. This is consistent with the widely reported difficulty of GCN architectures scaling in depth beyond a few layers <ref type="bibr" target="#b51">52</ref> .    <ref type="table">Table 5</ref>). Additionally, we tried three different node feature attributes such 1) CFID chemical features (total 438), only atomic number (total 1), and default CGCNN type attributes (total 92) and compared them for formation energy model in <ref type="table">Supplementary Table 6</ref>. We observe that the default node attributes have the lowest MAE.</p><p>Next, we study time taken per epoch of several models for QM9 and JARVIS-DFT formation energy dataset in <ref type="table">Supplementary Table 7</ref>. To help facilitate fair comparison, we train all models with the same computational resources using the reference implementations and configurations reported in the literature. We note that the timing code for the reference implementations of different methods may include differing amounts of overhead. For example, the ALIGNN timings reported in Supplementary <ref type="table">Table 7</ref>  While we report timing comparisons using our standard hyperparameter configuration used to train models reported in Section F, through subsequent model analysis we have identified several strategies that substantially reduce computational workload without incurring a large performance penalty. We observe in <ref type="figure" target="#fig_12">Supplementary Figure 54</ref> that model performance converges after 300 epochs; shorter training budgets incur a modest performance reduction and slightly increased variance with respect to the training split. The performance tradeoff presented in <ref type="table">Table 6</ref> and <ref type="figure" target="#fig_4">Figure 3</ref> indicates that switching from the default configuration of 4 layers each of ALIGNN and GCN updates to 2 layers each could offer a speedup of ~1.5x with negligible reduction in accuracy.</p><p>Finally, we performed a drop-in replacement study comparing batch normalization and layer normalization in <ref type="table">Supplementary Table 8</ref>, finding that switching to layer normalization provides an additional ~1.7x speedup with a slight degradation in validation loss and negligible degradation in validation MAE. Because the cost of retraining models for all targets reported is still high, and because some of these strategies equally apply to competing models, we defer a more comprehensive performance-cost study to future work.</p><p>Finally, we simultaneously investigate the effects of dataset size and different train-validation-test splits by performing a learning curve study in cross-validation for the JARVIS-DFT formation energy <ref type="figure" target="#fig_12">(Figure 4</ref> and <ref type="table">Supplementary Table 9</ref>) and bandgap ( <ref type="figure">Supplementary Figure 55</ref> and <ref type="table">Supplementary Table 9</ref>) targets. We perform the cross-validation splitting procedure by merging </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: learning curve for JARVIS-DFT formation energy regression target. Blue markers indicate validation set MAE scores for individual cross-validation iterates. Error bars indicate the mean cross-validation MAE +/-one standard error of the mean.</head><p>In summary, we have developed an ALIGNN model which uses the line graph neural network that improves the performance of GNN predictions for solids and molecules. We have demonstrated that explicit inclusion of angle-based networks in GNNs can significantly improve model performance.</p><p>A key contribution of this work is the inclusion and development of both the undirected atomistic graph and its line graph counterpart for solid-state and molecular materials.</p><p>We develop regression and classification ALIGNN models for some of the well-known preexisting databases and it can be easily applied for other datasets as well. Our model significantly improved accuracies over prior GNN models. We believe the ALIGNN model will rapidly improve the machine-learning prediction for several material properties and classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A: JARVIS-DFT dataset</head><p>The JARVIS-DFT <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> dataset is developed using Vienna Ab-initio simulation package (VASP) <ref type="bibr" target="#b52">53</ref> software <ref type="bibr" target="#b53">54</ref> . Most of the properties are calculated using the OptB88vdW functional <ref type="bibr" target="#b47">48</ref> . For a subset of the data we use TBmBJ 49 for getting better band gaps. We use density functional perturbation theory (DFPT) <ref type="bibr" target="#b54">55</ref> for predicting piezoelectric and dielectric constants with both electronic and ionic contributions. The linear response theory-based <ref type="bibr" target="#b55">56</ref>  Classical force-field inspired descriptors (CFID) <ref type="bibr" target="#b31">32</ref> are generated using the JARVIS-Tools package.</p><p>The CFID baseline models are trained using the LightGBM package 54 , 57 using the models developed in Ref. <ref type="bibr" target="#b31">32</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ALIGNN model implementation and training</head><p>The ALIGNN model is implemented in PyTorch 58 and deep graph library (DGL) <ref type="bibr" target="#b32">33</ref> ; the training code heavily relies on PyTorch-ignite <ref type="bibr" target="#b58">59</ref> . For regression targets we minimize the mean squared error (MSE) loss, and for classification targets we minimize the standard negative log likelihood loss.</p><p>We train all models for 300 epochs using the AdamW 60 optimizer with normalized weight decay of 10 -5 and a batch size of 64. The learning rate is scheduled according to the one-cycle policy <ref type="bibr" target="#b60">61</ref> with a maximum learning rate of 0.001. We use the same model configuration for each regression and classification target. We use the initial atom representations from the CGCNN paper, 80 initial  <ref type="table" target="#tab_0">Supplementary Table 10</ref>. The random search results indicate that model performance is most highly sensitive to the learning rate, weight decay, and convolution layer width, and beyond a relatively low threshold is insensitive to the sizes of the initial feature embedding layers.</p><p>We used NIST's Nisaba cluster to train all ALIGNN models, and we reproduce results from the literature using the reference implementations for each competing method on the same hardware.</p><p>Each model is trained on a single Tesla V100 SXM2 32 gigabyte Graphics processing unit (GPU), with 8 Intel Xeon E5-2698 v4 CPU cores for concurrently fetching and preprocessing batches of    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>. 2 .</head><label>2</label><figDesc>To avoid ambiguity between the node and edge features of the atomistic graph and its line graph, we write atom, bond, and triplet representations as h, e, and t. The line graph convolution produces bond messages m that are propagated to the atomistic graph, which further updates the bond features in combination with atom features h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of the ALIGNN layer structure. The ALIGNN layer first performs edgegated graph convolution on the line graph to update pair and triplet features. The newly updated pair features are propagated to the edges of the direct graph and further updated withthe atom features in a second edge-gated graph convolution applied to the direct graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For example, the MAE on high throughput band gaps is lower (by 35.7 %) for the JARVIS-DFT dataset, which is interesting in light of MP's dataset size advantage over JARVIS-DFT. One potential source of this discrepancy is the differing computational methodologies used, such as different functionals (PBE vs OptB88vdW), use of the DFT+U method, and settings for various DFT hyperparameters like smearing and k-point settings, all of which can influence the values of computed bandgaps as discussed in Ref. 37 . Another potential contributing factor could be differing levels of dataset bias in the MP and JARVIS-DFT datasets stemming from differing distributions in material space. Clarifying this situation is beyond the scope of the present work, though it is of great importance for the atomistic modeling community to resolve. Nevertheless, application of ALIGNN models on different datasets shows improvements for materials-property predictions. Both CFID, CGCNN and ALIGNN models' MAEs are lower than the corresponding MADs. The MAD:MAE ratios can vary for energy related quantities from a high value of 48.11 (total energy), and 26.06 (formation energy model) to low values such as for DFPT based piezoelectric strain coefficients (1.19) and dielectric constant with ionic contributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>shows in detail the tradeoff between the performance benefit of including ALIGNN layers and their computational overhead relative to GCN layers. Per-epoch timing for each configuration is reported in Supplementary Table 2. All GCN-only configurations (annotated with the number of GCN layers) are on the low-computation portion of the pareto frontier, but the high-accuracy portion of the pareto frontier is dominated by ALIGNN/GCN combinations with at least two ALIGNN updates. The ALIGNN-2/GCN-2 configuration obtains peak performance (again, relative reduction of MAE by 29.14 %) with a computational overhead of roughly 2x relative to the GCN-4 configuration. Supplementary Table 1 and Supplementary Figure 53 present layer ablation study results yielding similar conclusions on the JARVIS-DFT OptB88vdW band gap target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>ALIGNN layer study performance tradeoff, JARVIS-DFT formation energy target. This layer ablation study clearly demonstrates that inclusion of bond angle information and propagation of bond and pair features through the node updates improves the generalization ability of atomistic GCN models. This is satisfying from a materials science perspective, as interatomic bonding theory clearly motivates the notion that inclusion of bond angles should improve accuracy of the model. Similarly, we vary the number of hidden features (i.e., the width of the graph convolution layers), edge input features, and embedding input features to evaluate the MAE performance for JARVIS-DFT formation energy and bandgap model in comparison with the default model in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>amortize the overhead of initial atomistic graph construction across 300 epochs, and each epoch includes the overhead of evaluating the model on the full training and validation sets for performance tracking. Additionally, the computational cost of deep learning models in general is not independent of certain hyperparameters; in particular, larger batch sizes can better leverage modern accelerator hardware by exposing more parallelism. We find ALIGNN requires less training time per epoch time compared to other models except DimeNet++ and MEGNet. However, it is important to note that DimeNet++ and other models usually take around 1000 epochs or more to reach desired accuracy, while ALIGNN can converge in about 300 epochs, resulting in lower overall training cost for similar or better accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the standard JARVIS-DFT train and validation sets and randomly sampling without replacement Ntrain training samples and 5000 validation samples. The learning curve study shows no sign of diminishing marginal returns for additional data up to the full size of the JARVIS-DFT dataset. On the full training set size (44577) we obtain an average validation MAE of 0.0316 ? 0.0004 eV/at (uncertainty corresponds to the standard error of the mean over five cross-validation (CV) iterates). The standard deviation over CV iterates is 0.0009 eV/at, indicating that model performance is relatively insensitive to the dataset split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>frequency based dielectric function was calculated using both OptB88vdW and TBmBJ and the zero-energy values are trained for the machine learning model. Note that the linear response based dielectric constants lack ionic contributions. The TBmBJ frequency dependent dielectric functions are used to calculate the spectroscopic limited maximum efficiency (SLME) 38 . The magnetic moments are calculated using spin-polarized calculations considering only ferromagnetic initial configurations and neglecting any density functional theory (DFT)+U effects. The thermoelectric coefficients such as Seebeck coefficients and power factors are calculated using BoltzTrap 50 software using constant relaxation time approximation. Exfoliation energy for the van der Waals bonded two-dimensional materials are calculated as the energy per atom differences between the bulk and corresponding monolayer counterparts. The spin-orbit spillage 40 is calculated using the difference in wavefunctions of a material with and without inclusion of spin orbit coupling effects. All the JARVIS-DFT data and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>bond radial basis function (RBF) features, and 40 initial bond angle RBF features. The atom, bond, and bond angle feature embedding layers produce 64-dimensional inputs to the graph convolution layers. The main body of the network consists of 4 ALIGNN and 4 graph convolution (GCN) layers, each with hidden dimension 256. The final atom representations are reduced by atom-wise average pooling and mapped to regression or classification outputs by a single linear layer. These hyperparameters are selected to optimize validation MAE on the JARVIS-DFT band gap task through a combination of manual hypothesis-driven experiments and random hyperparameter search facilitated and scheduled through Ray Tune 62 ; hyperparameter ranges are given in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure captions:Figure 1 :</head><label>1</label><figDesc>Schematic showing undirected crystal graph representation and corresponding line graph construction for a SiO4 polyhedron. For simplicity, only Si-O bonds are illustrated. The ALIGNN convolution layer alternates between message passing on the bond graph (left) and its line graph (or bond adjacency graph, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of the ALIGNN layer structure. The ALIGNN layer first performs edgegated graph convolution on the line graph to update pair and triplet features. The newly updated pair features are propagated to the edges of the direct graph and further updated with the atom features in a second edge-gated graph convolution applied to the direct graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>ALIGNN layer study performance tradeoff, JARVIS-DFT formation energy target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>learning curve for JARVIS-DFT formation energy regression target. Blue markers indicate validation set MAE scores for individual cross-validation iterates. Error bars indicate the mean cross-validation MAE +/-one standard error of the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : ALIGNN model configuration used for both solid state and molecular machine learning models.</head><label>1</label><figDesc>Model performance can vary substantially depending on the dataset and task. To evaluate the performance of ALIGNN, we currently use two different solid-state property datasets (Materials Project and JARVIS-DFT) as well as molecular property dataset QM9. Because the solid-state datasets are continuously updated, we use time-versioned snapshots of them, specifically selecting the MP version used by previous works to facilitate a direct comparison of model performance with the literature. It is likely that as these dataset sizes increase in the future the performance of ALIGNN outperforms reported values of CGCNN, MEGNet and SchNet models by 43.6 %, 21.4 % and 37.1 % respectively. For Eg, ALIGNN outperforms CGCNN and MEGNet by 43.8 % and 33.9 % respectively. Good performance on well-known and well-characterized datasets ensures high prediction accuracy of ALIGNN models. Because each property has different units and in general a different variance, we also report the mean absolute deviation (MAD) for each property to facilitate an unbiased comparison of the model performance between different properties. The MAD values represent the performance of a random guessing model with average value prediction for each data point. We also report the CFID based predictions for comparison.Clearly, all the neural networks, especially ALIGNN, perform much better than the corresponding MAD of the dataset as well as CFID performance. Analyzing the MAD: MAE (ALIGNN) ratio, we observe that the ratio could be as high as 42.27 model. Generally, a model with high MAD:MAE ratio (such as 5 and above) is considered a good predictive model<ref type="bibr" target="#b50">51</ref> .</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>ALIGNN layers</cell><cell>4</cell></row><row><cell>GCN layers</cell><cell>4</cell></row><row><cell>Edge input features</cell><cell>80</cell></row><row><cell>Triplet input features</cell><cell>40</cell></row><row><cell>Embedding features</cell><cell>64</cell></row><row><cell>Hidden features</cell><cell>256</cell></row><row><cell>Normalization</cell><cell>Batch normalization</cell></row><row><cell>Batch size</cell><cell>64</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row></table><note>the model can be further improved. We select the MP 2018.6.1 version which consists of 69239 materials with properties such as Perdew Burke-Ernzerhof functional (PBE) 47 bandgaps and formation energies. Similarly, we use 2021.8.18 version of JARVIS-DFT dataset, which consists of 55722 materials with several properties such as van der Waals correction with optimized Becke88 functional (OptB88vdW) 48 bandgaps, formation energies, dielectric constants, Tran- Blaha modified Becke Johnson potential (MBJ) 49 bandgaps and dielectric constants, bulk, shear modulus, magnetic moment, density functional perturbation theory (DFPT) based maximum piezoelectric coefficients, Boltztrap 50 based Seebeck coefficient, power factor, maximum absolute value of electric field gradient and two-dimensional materials exfoliation energies. All of these properties are critical for functional materials design. For the MP dataset we use a train-validation- test split of 60000-5000-4239 as used by SchNet 10 and MEGNet 16 . For the JARVIS-DFT dataset and its properties, we use 80 %:10 %: 10 % splits. For QM9 dataset we use a train-validation-test split of 110000-10000-10829 as used by SchNet 10 , DimeNet++ 20 , and MEGNet 16 . Performance of ALIGNN models on MP is shown in Table 2, which shows the regression model performance in terms of mean absolute error metric (MAE). The best MAEs for formation energy (Ef) and band gap (Eg) with ALIGNN are 0.022 eV(atom) -1 and 0.218 eV respectively. In terms of Ef,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Test set performance on the Materials Project dataset. Predictions on test set are shown in parity plots in Supplementary Figure S1 and Supplementary Figure S2.</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Prop Unit</cell><cell>MAD CFID</cell><cell cols="2">CGCNN MEGNet</cell><cell cols="3">SchNet ALIGNN MAD:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell></row><row><cell>Ef</cell><cell>eV(at</cell><cell cols="2">0.93 0.104 0.039</cell><cell>0.028</cell><cell cols="2">0.035 0.022</cell><cell>42.27</cell></row><row><cell></cell><cell>om) -1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eg</cell><cell>eV</cell><cell cols="2">1.35 0.434 0.388</cell><cell>0.33</cell><cell>-</cell><cell>0.218</cell><cell>6.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>well. Such a large variety of properties allow a thorough testing of our ALIGNN models. More details for individual properties, its precision with respect to experimental measurements, applicability and limitations can be found in respective works. However, it is important to mention that many important issues such as tackling systematic underestimation of bandgaps by DFT methods, the inclusion of van der Waals bonding, and the inclusion of spin-orbit coupling interactions, all critically important for materials-design perspective have been key areas of improvements for the JARVIS-DFT dataset. For instance, meta-GGA (generalized gradient approximation) based Tran-Blaha modified Becke Johnson potential (TBmBJ) band gaps are more reliable and comparable to experimental data than Perdew Burke-Ernzerhof functional (PBE) or van der Waals correction with optimized Becke88 functional (OptB88vdW) bandgaps, but their calculations are computationally expensive and hence they are underrepresented in the dataset. In addition to the ALIGNN performance, we also include hand-crafted Classical force-field inspired descriptors (CFID) descriptor and CGCNN MAE performances for these properties using identical data-splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DimeNet++ 20 as shown inTable.5. The results from models other than ALIGNN are reported as given in corresponding papers, not necessarily reproduced by us. QM9 provides DFT calculated molecular properties such as highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), energy gap, zero-point vibrational energy (ZPVE), dipole moment, isotropic polarizability, electronic spatial extent, internal energy at 0 K, internal energy at 298 K, enthalpy at 298 K, and Gibbs free energy at 298 K. ALIGNN outperforms competing methods for HOMO and dipole moment tasks while other accuracies are similar to the SchNet model. Most importantly, all ALIGNN results reported here use the same set of hyperparameters</figDesc><table><row><cell></cell><cell>Threshold</cell><cell>ALIGNN</cell></row><row><cell>Metal/non-metal classifier (OPT)</cell><cell>0.01 eV</cell><cell>0.92</cell></row><row><cell>Metal/non-metal classifier (MBJ)</cell><cell>0.01 eV</cell><cell>0.92</cell></row><row><cell>Magnetic/non-Magnetic classifier</cell><cell>0.05 ?B</cell><cell>0.91</cell></row><row><cell>High/low SLME</cell><cell>10 %</cell><cell>0.83</cell></row><row><cell>High/low spillage</cell><cell>0.1</cell><cell>0.80</cell></row><row><cell>Stable/unstable (ehull)</cell><cell>0.1 eV</cell><cell>0.94</cell></row><row><cell>High/low-n-Seebeck</cell><cell>-100 ?VK -1</cell><cell>0.88</cell></row><row><cell>High/low-p-Seebeck</cell><cell>100 ?VK -1</cell><cell>0.92</cell></row><row><cell>High/low-n-powerfactor</cell><cell>1000 ?W(mK 2 ) -1</cell><cell>0.74</cell></row><row><cell>High/low-p-powerfactor</cell><cell>1000?W(mK 2 ) -1</cell><cell>0.74</cell></row><row><cell cols="3">Next, we evaluate the ALIGNN model on QM9 molecular property dataset (130829 molecules)</cell></row><row><cell cols="3">and compare it with other well-known models such as SchNet 10 , MatErials Graph Network</cell></row><row><cell>(MEGNet) 16 , and</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 for</head><label>1</label><figDesc>JARVIS-DFT OptB88vdW formation energies and bandgaps respectively. We find that without any graph convolution layers the MAE for the formation energy and bandgap are 1248.5 % and 453.6 % higher than the default model. Adding even a single ALIGNN or GCN layer can reduce the MAE by 102.9 % illustrating the importance of these layers. However, further increase in ALIGNN/GCN layers doesn't scale well and performance quickly saturates at a depth of 4. Excluding GCN layers and increasing ALIGNN layers and vice versa show the individual importance of these layers. Performance of GCN-only models saturates at 4 layers with 44 meV/atom MAE on the JARVIS-DFT formation energy task, while ALIGNN-only models</figDesc><table><row><cell>saturate at 34 meV(atom) -1 -a relative reduction of 29.14 %. Each of these models, along with the</cell></row><row><cell>other highlighted configurations in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 .</head><label>1</label><figDesc>In Supplementary Table 3, we observe that the marginal performance from increasing the hidden features saturates at 256 for both properties. Supplementary Table 4 shows that the number of edge input features is optimal at 80 for formation energy model, while for the bandgap model performance saturates at 40. Similarly, embedding features are optimized at 64 for formation energy while 32 for bandgap model (Supplementary</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>data during training <ref type="bibr" target="#b53">54</ref> . For the MP dataset we use a train-validation-test split of 60000-5000-4239.</p><p>For the JARVIS-DFT dataset, we use 80 %:10 %: 10 % splits. The 10 % test data is never used during training procedures. For QM9 dataset we use a train-validation-test split of 110000-10000-10829. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests:</head><p>The authors declare no competing interests. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<title level="m">Benchmarking graph neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">982</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep graph neural network-based mechanism for social recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Industr. Inform</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2776</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<title level="m">Supervised community detection with line graph neural networks. arXiv,1705</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8415</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpretable brain graph neural network for fmri analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braingnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">100057</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CoryneRegNet 4.0-A reference database for corynebacterial gene regulatory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baumbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel intrusion detection model for a massive network using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">50850</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1706</biblScope>
			<biblScope unit="page">8566</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">Convolutional networks on graphs for learning molecular fingerprints. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1509</biblScope>
			<biblScope unit="page">9292</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Message Passing for Quantum Chemistry</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">1263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prediction errors of molecular machine learning models lower than hybrid DFT error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph networks as a universal machine learning framework for molecules and crystals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Mater</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Developing an improved Crystal Graph Convolutional Neural Network framework for accelerated materials discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">63801</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OrbNet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Manby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124111</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MT-CGCNN: Integrating crystal graph convolutional neural network with multitask learning for material property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1811</biblScope>
			<biblScope unit="page">5660</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="page">14115</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">12710</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantumchemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cormorant</surname></persName>
		</author>
		<title level="m">Covariant molecular neural networks. arXiv</title>
		<imprint>
			<date type="published" when="1906" />
			<biblScope unit="page">4015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">7457</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical modeling of molecular energies using a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lubbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">241715</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SchNetPack: A deep learning toolbox for atomistic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schutt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">448</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Elemnet: Deep learning the chemistry of materials from only elemental composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining SchNet and SHARC: The SchNarc machine learning approach for excited-state dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Westermayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquetand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Chem. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3828</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BonDNet: a graph neural network for the prediction of bond dissociation energies for charged molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W C</forename><surname>Spotte-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwaraknath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1858</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal fragment descriptors for predicting properties of inorganic crystals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Machine learning with force-field-inspired descriptors for materials: Fast screening and mapping energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Decost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">83801</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
			<biblScope unit="page">1315</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Elastic properties of bulk and lowdimensional materials using van der Waals density functional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">14107</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-throughput identification and characterization of two-dimensional materials using density functional theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kalish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computational screening of high-performance optoelectronic materials using OptB88vdW and TB-mBJ formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerated discovery of efficient solar cell materials using quantum and machine-learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Mater</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">5900</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-throughput discovery of topologically non-trivial materials using spin-orbit spillage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Garrity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High-throughput search for magnetic topological materials using spin-orbit spillage, machine learning, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Garrity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">155131</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Density functional theory-based electric field gradient database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Mazin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data-driven discovery of 3D and 2D thermoelectric materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Garrity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Condens. Matter Phys</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">475501</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High-throughput density functional perturbation theory and machine learning predictions of infrared, piezoelectric, and dielectric responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convergence and machine learning predictions of Monkhorst-Pack k-points and plane-wave cut-off in high-throughput DFT calculations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="300" to="308" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Commentary: The Materials Project: A materials genome approach to accelerating materials innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APL materials</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11002</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalized gradient approximation made simple</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Perdew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernzerhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">3865</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chemical accuracy for the van der Waals density functional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klime?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaelides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Condens. Matter Phys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">22201</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accurate band gaps of semiconductors and insulators with a semilocal exchange-correlation potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">226401</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A code for calculating band-structure dependent quantities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boltztrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Phys. Commun</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="67" to="71" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A general-purpose machine learning framework for predicting properties of inorganic materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">5453</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kresse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furthm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Please note commercial software is identified to specify procedures. Such identification does not imply recommendation by National Institute of Standards and Technology (NIST)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ab initio calculation of the macroscopic dielectric constant in silicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Resta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7017</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Linear optical properties in the projector-augmented wave methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gajdo?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kresse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Furthm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bechstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">45112</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3146</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="page">1703</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">PyTorch-ignite documentation</title>
		<ptr target="https://pytorch.org/ignite/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1711</biblScope>
			<biblScope unit="page">5101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay. arXiv, 1803</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9820</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tune</surname></persName>
		</author>
		<title level="m">A research platform for distributed model selection and training. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1807</biblScope>
			<biblScope unit="page">5118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
