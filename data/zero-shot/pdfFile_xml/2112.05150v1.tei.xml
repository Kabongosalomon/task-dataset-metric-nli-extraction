<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Recurrent Neural Network with Multi-scale Bi-directional Propagation for Video Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xian Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ByteDance Intelligent Creation Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xian Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xian Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lean</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ByteDance Intelligent Creation Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xian Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Recurrent Neural Network with Multi-scale Bi-directional Propagation for Video Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of the state-of-the-art video deblurring methods stems mainly from implicit or explicit estimation of alignment among the adjacent frames for latent video restoration. However, due to the influence of the blur effect, estimating the alignment information from the blurry adjacent frames is not a trivial task. Inaccurate estimations will interfere the following frame restoration. Instead of estimating alignment information, we propose a simple and effective deep Recurrent Neural Network with Multi-scale Bidirectional Propagation (RNN-MBP) to effectively propagate and gather the information from unaligned neighboring frames for better video deblurring. Specifically, we build a Multi-scale Bi-directional Propagation (MBP) module with two U-Net RNN cells which can directly exploit the interframe information from unaligned neighboring hidden states by integrating them in different scales. Moreover, to better evaluate the proposed algorithm and existing state-of-the-art methods on real-world blurry scenes, we also create a Real-World Blurry Video Dataset (RBVD) by a well-designed Digital Video Acquisition System (DVAS) and use it as the training and evaluation dataset. Extensive experimental results demonstrate that the proposed RBVD dataset effectively improves the performance of existing algorithms on real-world blurry videos, and the proposed algorithm performs favorably against the state-of-the-art methods on three typical benchmarks. The code is available at https://github.com/XJTU-CVLAB-LOWLEVEL/RNN-MBP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of smart-phone, live-streaming businesses and cloud computing services, videos become one of the most popular communication mediums in our daily life, plenty of people enjoy using hand-held devices to record their moments and share online. However, the captured videos often suffer from motion blur effect which is usually caused by camera shaking and object moving. Therefore, how to remove the motion blur in the videos becomes an urgent issue in both the academic and industrial communities.</p><p>The goal of video deblurring algorithm is to recover a sharp video from a blurry one, which is highly ill-posed. Lots of traditional methods <ref type="bibr" target="#b0">(Bar et al. 2007;</ref><ref type="bibr">Lee 2014, 2015;</ref><ref type="bibr" target="#b29">Wulff and Black 2014)</ref> usually impose kinds of hand-crafted priors on both blur kernels and latent frames and alternatively estimate the blur kernels and latent frames. However, these methods highly rely on the accuracy of kernel estimation which leads to unstable deblurring performance. Instead of using hand-crafted priors and alternative solutions, the deep convolutional neural network (CNN), provides an effective way to directly estimate the clear videos from blurry ones <ref type="bibr" target="#b33">(Zhang et al. 2018a;</ref><ref type="bibr" target="#b38">Zhou et al. 2019b;</ref><ref type="bibr" target="#b6">Gast and Roth 2019;</ref><ref type="bibr" target="#b10">Kim et al. 2018;</ref><ref type="bibr" target="#b19">Nah, Son, and Lee 2019;</ref><ref type="bibr" target="#b28">Wang et al. 2019;</ref><ref type="bibr" target="#b22">Pan, Bai, and Tang 2020;</ref><ref type="bibr" target="#b7">Kim et al. 2017;</ref><ref type="bibr" target="#b36">Zhong et al. 2020;</ref><ref type="bibr" target="#b25">Suin and Rajagopalan 2021)</ref>. We note that when restoring the clear frames, most traditional methods <ref type="bibr" target="#b5">(Dai and Wu 2008;</ref><ref type="bibr">Lee 2014, 2015)</ref> and the CNN-based ones <ref type="bibr" target="#b10">(Kim et al. 2018</ref><ref type="bibr" target="#b28">Wang et al. 2019;</ref><ref type="bibr" target="#b22">Pan, Bai, and Tang 2020)</ref> usually involve the alignment estimations by optical flow, deformable convolution, or alignment filters and so on. For example, EDVR ) and CDVD-TSP (Pan, Bai, and Tang 2020) use multi-scale deformable convolution and optical flow estimation to align inter-frame information respectively. Intuitively, these video deblurring algorithms should outperform the image deblurring algorithms by taking advantages of both intra-frame (spatial) and inter-frame (temporal) information from the adjacent frames. However, it is not trivial to estimate the alignment from blurry videos and most existing methods may fail due to the presence of motion blur and occlusion <ref type="bibr" target="#b30">(Xue et al. 2019)</ref>. The inaccurate estimations of the alignment will affect the deblurred results. Therefore, on some popular public video deblurring dataset (e.g. GO-PRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref>), most existing video deblurring methods do not perform well while top three methods (HINet <ref type="bibr" target="#b3">(Chen et al. 2021)</ref>, <ref type="bibr">MIMO-UNet (Cho et al. 2021)</ref>, and MPRNet <ref type="bibr" target="#b31">(Zamir et al. 2021</ref>)) on GOPRO dataset are all image deblurring algorithms. <ref type="figure">Figure 1</ref> shows the intermediate and deblurred results of two video deblurring methods with explicit alignment using a video with significant blur from the GOPRO dataset. We feed the blurry frames and corresponding sharp frames to the flow-based method CDVD-TSP (Pan, Bai, and Tang 2020) and deformable-based method EDVR , <ref type="figure">Figure 1</ref>: Optical flow-based and deformable convolution-based methods on video deblurring. We evaluate the flow-based algorithm CDVD-TSP (Pan, Bai, and Tang 2020) and deformable convolution-based algorithm EDVR ) using a video with significant blur from the GOPRO dataset <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref>. The top row shows the blurry frames, the optical flows and warped blurry frame from the CDVD-TSP, the learned offsets from the EDVR, the deblurred result of the CDVD-TSP, and the proposed deblurred result, respectively. The bottom row shows the sharp frames (ground truths), optical flow and warped sharp frame which are estimated from sharp frames by CDVD-TSP, the learned offsets from sharp frames by EDVR, the deblurred result of the EDVR, and ground truth result, respectively. Due to the influence of blur effect, the methods based on the optical flow and deformable convolution do not restore sharp frames. Best viewed on a high-resolution display.</p><p>respectively. Note that the methods based on optical flow or the deformable convolution do not estimate the alignment information well (even though these alignment modules have been finetuned) due to the influence of the motion blur, thus leading to results with significant blur residual.</p><p>To overcome this problem, we propose a deep Recurrent Neural Network with Multi-scale Bi-directional Propagation (RNN-MBP) to exploit inter-frame information from neighbouring frames for video deblurring. The proposed Multiscale Bi-directional Propagation (MBP) module is built by two U-Net RNN cells and the bidirectional propagation scheme under a multi-scale framework. Thus, the MBP can directly gather the unaligned neighboring hidden states and exploit the inter-frame information in different scales. Moreover, instead of directly reconstructing the target frame from the output of the propagation module <ref type="bibr" target="#b2">(Chan et al. 2021)</ref>, we propose a Target Frame Re-constructor module to first fuse the features from the target frame and the multi-scale outputs from the MBP module. By re-introducing the features from the target frame with an additional sub-network, the proposed Target Frame Re-constructor module can reinforce the importance of intra-frame information, as the intra-frame information and its importance may gradually vanish during the propagation process. Benefiting from these designs, the inter-frame and intra-frame information features can be exploited thus facilitating the clear video restoration.</p><p>In addition, we note that most existing video deblurring algorithms are trained and evaluated on the synthetic datasets, such as GOPRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref> and REDS <ref type="bibr" target="#b17">(Nah et al. 2019b</ref>). Due to the domain gap between synthetic datasets and real-world ones <ref type="bibr" target="#b1">(Brooks and Barron 2019)</ref>, the methods trained on the synthetic datasets do not generalize well on the real-world scenes as demonstrated by <ref type="bibr" target="#b15">(Lai et al. 2016;</ref><ref type="bibr" target="#b12">K?hler et al. 2012;</ref><ref type="bibr">Rim et al. 2020;</ref><ref type="bibr" target="#b36">Zhong et al. 2020)</ref>. To overcome this problem, we create a Real-World Blurry Video Dataset (RBVD) by a well-designed Digital Video Acquisition System (DVAS) to evaluate the proposed method and the state-of-the-art ones. Quantitative and qualitative experiments demonstrate that the proposed RBVD improves the performance of existing methods on real-world blurry videos, and the proposed al-gorithm shows better performance than the state-of-the-art methods on the RBVD test-set.</p><p>The main contributions are summarized as follows: ? We propose a RNN-MBP to effectively exploit intraand inter-frame information by directly gathering the unaligned neighboring hidden states. ? We propose a Target Frame Re-constructor to reinforce the importance of intra-frame information by reintroducing the features from target frames. ? To better evaluate different video deblurring methods on real-world blurry scenes, we create a real-world blurry video dataset by a well-designed DVAS. ? Both quantitative and qualitative evaluations demonstrate that our model performs favorably against the state-ofthe-art methods in terms of accuracy and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Significant progress has been made in single image deblurring due to the use of deep learning <ref type="bibr">(Gong et al. 2017a;</ref><ref type="bibr" target="#b22">Nimisha, Kumar Singh, and Rajagopalan 2017;</ref><ref type="bibr" target="#b13">Kupyn et al. 2017</ref><ref type="bibr" target="#b14">Kupyn et al. , 2019</ref><ref type="bibr" target="#b31">Zamir et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021)</ref>. We refer readers to <ref type="bibr" target="#b20">(Nah et al. 2021</ref><ref type="bibr" target="#b21">(Nah et al. , 2020</ref><ref type="bibr" target="#b17">(Nah et al. , 2019a</ref> for a comprehensive review of recent advances. In the following, we mainly briefly introduce the most related methods. from adjacent frames in a cascading way.</p><p>In the meantime, some researchers choose the recurrent neural network (RNN) as the framework to solve the video deblurring tasks, as RNN is suitable for processing on the time sequential inputs. ) develops a dynamic temporal blending layer in their spatial-temporal recurrent network which can enforce the temporal consistency among consecutive outputs. <ref type="bibr" target="#b36">(Zhong et al. 2020</ref>) builds their RNN cells with residual dense blocks (RDBs) and applies a global spatio-temporal attention module to fuse features from adjacent frames. However, due to the difficulty in estimating the alignment from blurry videos, the existing video deblurring methods do not perform as well as the state-of-the-art image deblurring methods <ref type="bibr" target="#b31">(Zamir et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021</ref>) on some benchmarks. Real-World Blurry Datasets. To better evaluate deblurring algorithms, some real-world blurry datasets <ref type="bibr" target="#b15">(Lai et al. 2016;</ref><ref type="bibr" target="#b12">K?hler et al. 2012;</ref><ref type="bibr">Rim et al. 2020;</ref><ref type="bibr" target="#b36">Zhong et al. 2020</ref>) have been proposed, but all these works have their own drawbacks which limit their applications. In <ref type="bibr" target="#b15">(Lai et al. 2016</ref>), Lai collects 100 real blurry images without paired sharp images, thus, their dataset is not suitable for quantitative evaluation. <ref type="bibr" target="#b12">(K?hler et al. 2012)</ref> proposes 48 pairs of real-world blurry images and corresponding sharp images through a high-precision hexapod robot, but the number of their dataset is too few for supervised training. Recently, (Rim et al. 2020) proposes a real-world blurry dataset for single image deblurring, which contains 4554 pairs realworld blurry and sharp images of low-light static scenes. Specifically, their device uses a beam splitter to separate the input light equally to these two cameras and an external synchronization trigger signal to activate the capture processes simultaneously. With similar equipment and acqui-sition strategy, <ref type="bibr" target="#b36">(Zhong et al. 2020)</ref> proposes their Beam-Splitter Dataset (BSD) for video deblurring which consists of 100 paired real-world blurry and sharp videos. However, both datasets suffer from distinct misalignment due to their defective acquisition strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>Given consecutive blurry frames I <ref type="bibr">[t:t+n]</ref> , the goal of the proposed RNN-MBP method is to recover the corresponding sharp frames O [t:t+n] recurrently. <ref type="figure" target="#fig_0">Figure 2</ref> shows that the proposed RNN-MBP contains three modules:</p><p>? A feature extractor module to extract features from the target frame. ? A MBP module to gather the unaligned neighboring hidden states set and exploit the inter-frame information from different scales. ? A Target Frame Re-constructor module to reinforce the importance of intra-frame information and reconstruct sharp results. For a blurry frame I t , our method first extracts target features ? t via a feature extractor that contains a convolutional layer and a Channel Attention Block (CAB) from RCAN <ref type="bibr" target="#b35">(Zhang et al. 2018c)</ref>  <ref type="figure" target="#fig_0">(Figure 2 (b)</ref>). Then, a MBP module is developed to gather the extracted features ? t and unaligned neighboring multi-scale hidden states sets (F t?1 and B t+1 ) in both the forward direction and the backward direction, and then outputs the propagated multi-scale hidden state set (F t and B t ). Finally, the Target Frame Re-constructor module progressively fuses the target features (? t ) with the multi-scale hidden state sets (F t and B t ) and reconstruct the final deblurred results (O t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MBP Module</head><p>The proposed MBP is to exploit the inter-frame information by gathering target features and the unaligned neighbouring hidden states sets under a multi-scale framework. To facilitate the gathering process, the proposed propagation module adopts two U-Net RNN cells and a multi-scale bidirectional propagation scheme <ref type="figure" target="#fig_0">(Figure 2 (c)</ref>), where the unaligned neighbouring hidden states are integrated into different levels of the RNN cells. We adopt the multi-scale framework for two reasons: (1) the blur degradations and the displacement among neighboring hidden states are relatively small in the features of lower resolution, which can help the cell to exploit the inter-frame information without explicit alignment, (2) the high-frequency information is still reserved in the features of larger resolution, which can help to reconstruct sharper results.</p><p>The framework of one U-Net RNN cell can be divided into two parts: an encoder for gathering neighbouring hidden states and a decode for generating the propagated features. Taking the forward propagation process as an example, the encoder part in the forward U-Net RNN cell consists of three downsample operations and 6 CABs, which can progressively down-sample the target features ? t and gather them with the multi-scale hidden state set F t?1 propagated from t?1 iteration. Noted that the multi-scale hidden state set F t?1 contains 6 hidden states and can be formulated as:</p><formula xml:id="formula_0">F t?1 = (f e1 t?1 , f e2 t?1 , f e3 t?1 , f d1 t?1 , f d2 t?1 , f d3 t?1 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">(f e1 t?1 , f e2 t?1 , f e3 t?1 ) and (f d1 t?1 , f d2 t?1 , f d3 t?1</formula><p>) denote the hidden stats from different levels of the encoder and the decoder of the t ? 1 iteration RNN cell respectively.</p><p>Specifically, the gathering process in the different levels of the encoder can be formulated as:</p><formula xml:id="formula_2">f e1 t = ? ? (? t ) + Conv(f e1 t?1 ) + Conv(f d1 t?1 ),<label>(2)</label></formula><formula xml:id="formula_3">f e2 t = ? ? ((f e1 t ) ?2 ) + Conv(f e2 t?1 ) + Conv(f d2 t?1 ), (3) f e3 t = ? ? ((f e2 t ) ?2 ) + Conv(f e3 t?1 ) + Conv(f d3 t?1 ),<label>(4)</label></formula><p>where the ? ? refers to two CABs, Conv denotes a convolutional layer with filter size of 3 ? 3, and the ? refers to down-sample operation. Then, the decoder part progressively up-sample the features f e3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Target Frame Re-constructor</head><p>Although the inter-frame information is exploited in the MBP module, the intra-frame information and its importance may gradually vanish during the propagation process due to the introduction of the neighbouring hidden states. Therefore, we propose a Target Frame Re-constructor module to re-introduce the target features (? t ) and progressively fuse it with the propagated outputs from the forward and backward RNN cells (F t , B t ). We believe that the reintroduce of the target features can reinforce the importance of intra-frame information, which may further improve the performance of the re-constructor module.</p><p>The framework of the proposed Target Frame Reconstructor module is shown as <ref type="figure" target="#fig_0">Figure 2 (c)</ref>. First, we upsample all the hidden states to the same scale with target features and fused them level-by-level. This process can be formulated as: </p><formula xml:id="formula_4">f 1 t = ? ? (? t ) + Conv(f e1 t + b e1 t ) + Conv(f d1 t + b d1 t ), (8) f 2 t = ? ? (f 1 t )+(Conv(f e2 t +b d2 t )) ?2 +(Conv(f d2 t +b d2 t )) ?2 , (9) f 3 t = ? ? (f 2 t )+(Conv(f e3 t +b e3 t )) ?4 +(Conv(f d3 t +b d3 t )) ?4 ,<label>(10)</label></formula><formula xml:id="formula_5">O t = Conv 5 * 5 (f 3 t ) + I t ,<label>(11)</label></formula><p>where Conv 5 * 5 denotes a convolutional layer with filter size of 5 ? 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Real-World Blurry Video Dataset</head><p>Due to the domain gap between synthetic blur and real blur <ref type="bibr" target="#b1">(Brooks and Barron 2019)</ref>, models trained on synthetic blurry datasets perform not well on real-world blurry videos.</p><p>To collect a real-world deblurring dataset for both supervised training and evaluation, we propose a novel acquisition system to capture blurry videos and their corresponding ground truth. The details of the acquisition system and acquisition process are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Digital Video Acquisition System</head><p>In order to capture the paired real-world blurry video dataset (RBVD), we build our own Digital Video Acquisition System (DVAS). Different from the acquisition process of <ref type="bibr" target="#b36">(Zhong et al. 2020</ref>) and (Rim et al. 2020), in which blurry and sharp images are captured simultaneously, the proposed DVAS can capture paired blurry and sharp videos respectively. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our system consists of four components, a high accurate robot arm (AUBO I5), a controller case of the robot arm, a Sony ICLE 6000 camera, and a laptop to tele-control the camera. Since the AUBO robot arm can repeatedly reach the same location with an accuracy of 2mm, we can collect perfectly aligned blurry and sharp video frames at a series of locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Acquisition</head><p>Scene layout and system settings. To guarantee the generality of the proposed dataset, all scenes in our dataset  have been carefully selected. Scenes with rich and colorful textures are preferred as the primary goal of deblurring is to recover high-frequency information from blurry inputs. Specifically, our dataset contains 40 indoor scenes, 14 outdoor scene, and 9 scenes with high-frequency charts. Before running the acquisition process, we first fix the remote-controlled camera at the end of the robot arm by a customized mounting rack and set the velocity of the robot arm to less than 0.25m/s to avoid the undesired shake of camera. Then, the camera's exposure time is set to 0.5s and 0.025s to collect blurry videos and corresponding sharp videos, respectively. Since capturing blurry videos requires a long exposure time, we use a neutral density filter (ND5) to eliminate overexposure problems. To enrich the diversity of our dataset, we also design 16 pre-defined trajectories for the following acquisition process. More details about the optical parameters of the camera and robot arm settings can be found in the supplementary material. Acquisition process. After finishing all the preparatory works, we start a multi-threading program to pick a predefined trajectory and control the robot arm moving to a series of acquisition locations. The timeline of the acquisition process can be summarized as <ref type="figure" target="#fig_2">Figure 4</ref>. The program takes 0.55 second to initialize the robot arm and the acquisition program starts at 3 seconds after robot arm moves. During the movement, the multi-threading program controls the camera taking blurry frames at the desired timing according to the exposure time. Depending on which trajectories we used in the acquisition process, the number of acquisition points ranges from 30-60, which means one captured video may contain 30-60 consecutive frames. The blurry video acquisition process usually takes approximately 5 minutes, and then the robot arm is re-initialized to the starting point for capturing the sharp video. It is noted that the centers of the exposure time for two acquisition processes are coincided to guarantee the spatial alignment of the corresponding blurry and sharp frame. Finally, all the collected video pairs are post-processed (correction, cropping and downsampling) to get suitable size (1440 ? 960) and perfectly aligned video for training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Dataset and Details</head><p>We use two synthetic datasets (GOPRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref> and dataset by <ref type="bibr" target="#b23">(Su et al. 2017)</ref>) and the proposed RBVD dataset in our experiments. The proposed RBVD dataset contains 63 video sequences (2164 samples in total): 7 video sequences are divided into the test set (including 2 outdoor scenes, 4 indoor scenes, and 1 scene with highfrequency charts), and the remaining 56 sequences are used as the training set (including 12 outdoor scenes, 36 indoor scenes and 8 scenes with high-frequency charts).</p><p>For each dataset, we train the proposed model using its training set and evaluate it on the corresponding test set. In the training process, we adopt L1 Charbonnier loss and Adam optimizer <ref type="bibr" target="#b11">(Kingma and Ba 2014)</ref> with default parameters to train our model. The learning rate is initialized as 2 ? 10 ?4 and is updated with the Cosine Scheduler strategy <ref type="bibr" target="#b16">(Loshchilov and Hutter 2016)</ref>. We set the length of the input sequences to 8 as a trade-off between the performance and required GPU memory. The batch size is 4 and the patch size of input sequences is 256 ? 256. The proposed model is implemented by the PyTorch framework and trained on 4 NVIDIA Tesla V100 GPUs. The code and the proposed RBVD dataset will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>Evaluations on the synthetic datasets. We evaluate different video deblurring algorithms on two synthetic datasets (GOPRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref> and dataset by <ref type="bibr" target="#b23">(Su et al. 2017)</ref>) and use the PSNR and the SSIM as the evaluation metrics. For fair comparisons, all the deep learning-based methods are trained on the same settings for evaluation with the same machine (Nvidia RTX 8000).</p><p>For the GOPRO dataset, the evaluated methods include both state-of-the-art video deblurring methods (EDVR   <ref type="table" target="#tab_2">Table 1</ref> shows the quantitative results on the GOPRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref>. We note that the state-of-the-art video deblurring methods ; Pan, Bai, and Tang 2020) do not perform well compared with the image deblurring methods <ref type="bibr" target="#b31">(Zamir et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021)</ref> due to the inaccurate estimations of the alignment. In contrast, our method generates the best results and its PSNR value is at least 0.61dB higher than the evaluated methods. This result demonstrates that the proposed method can effectively exploit intra-and inter-frame information by directly gathering the unaligned neighboring hidden states. In addition, the proposed network involves fewer model parameters and      <ref type="bibr" target="#b23">(Su et al. 2017)</ref>. Instead of randomly selected 30 frames from each dataset of <ref type="bibr" target="#b23">(Su et al. 2017)</ref>, all frames of the test set are used for evaluation. <ref type="figure">Figure 6</ref>: Deblurred results on test set of the RBVD dataset. Best viewed on a high-resolution display.</p><p>fewer running time, which further demonstrate its effectiveness. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of the visual results. We note that the evaluated methods produce ghost artifacts on the iron fence of the door (the blue box in <ref type="figure" target="#fig_4">Figure 5</ref>). In contrast, the proposed RNN-MBP generates much clearer images with sharper characters, clear graffiti on the pillar, and correct iron fence ( <ref type="figure" target="#fig_4">Figure 5(g)</ref>).</p><p>We further compare our algorithm with several stateof-the-art video deblurring algorithms including GVD-DS <ref type="bibr" target="#b9">(Kim and Lee 2015)</ref>, <ref type="bibr">DL-HMB (Gong et al. 2017b</ref>), SRNN <ref type="bibr" target="#b27">(Tao et al. 2018)</ref>, DVD-HC <ref type="bibr" target="#b23">(Su et al. 2017)</ref>, DTBN , EDVR , STFAN <ref type="bibr" target="#b37">(Zhou et al. 2019a)</ref>, and CDVD-TSP (Pan, Bai, and Tang 2020). <ref type="table" target="#tab_4">Table 2</ref> shows the quantitative results on the dataset by <ref type="bibr" target="#b23">(Su et al. 2017)</ref>. The proposed RNN-MBP achieves the most competitive results against the state-ofthe-art video deblurring methods (by a margin of 0.36db) due to its effectiveness on exploiting the inter-and intraframe information. More qualitative comparisons are included in the supplementary material.</p><p>Evaluations on the RBVD. Due to the domain gap between synthetic blur and real-world blur <ref type="bibr" target="#b1">(Brooks and Barron 2019)</ref>, the methods trained on the synthetic datasets do not perform well on the real-world ones. To better illustrate the difference between the real-world blur and synthetic blur, we generate a synthetic blurry video by averaging a series   of the clear scenes from a video in the proposed RBVD according to the protocol of <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref> and compare the generated blurry videos with our real captured blurry videos. <ref type="figure" target="#fig_5">Figure 7</ref> shows that the synthetic blurry frame contains significant discontinuous artifacts while the blurry frame collected by DVAS is more realistic. Therefore, the model trained on such synthetic blurry datasets may not perform well on the real-world blurry videos.</p><p>To demonstrate whether the proposed RBVD dataset facilitates the performance of existing deblurring methods, we train and evaluate the proposed RNN-MBP and two representative video deblurring methods ; Pan, Bai, and Tang 2020) on the proposed RBVD dataset. In addition, we also evaluate our method trained on the GOPRO and dataset by <ref type="bibr" target="#b23">(Su et al. 2017)</ref> using the test set of the RBVD. <ref type="table" target="#tab_6">Table 3</ref> shows that the proposed RNN-MBP performs better than EDVR ) and CDVD-TSP (Pan, Bai, and Tang 2020) with a marked improvement of 0.589dB and 1.115dB in terms of PSNR. In addition, the huge performance gaps among the RNN-MBP models trained on the RBVD dataset and synthetic datasets demonstrate the training set of the proposed RBVD is more suitable for training a real-world oriented deblurring models. <ref type="figure">Figure 6</ref> shows an example of the visual results on the RBVD dataset. As shown in <ref type="figure">Figure 6</ref> (e), only the RNN-MBP trained on the RBVD can generate clear results with sufficient details, while other deblurring methods and the RNN-MBP trained on the synthetic datasets suffer from irregular edges and blurry results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>We demonstrate the effectiveness of each key component in the proposed RNN-MBP. All the methods mentioned below are trained on the GOPRO <ref type="bibr" target="#b18">(Nah, Kim, and Lee 2017)</ref> dataset with the same settings for fairness. Since the Ba-sicVSR <ref type="bibr" target="#b2">(Chan et al. 2021</ref>) adopts RNN architecture and achieves promising results in the video super-resolution task, we remove the upsampling module and flow-based alignment module to form a basic RNN for video deblur-ring, which denotes as Baseline. Then, to better exploit interframe information from the neighboring hidden states, we replace the RNN cell in the Baseline with the proposed MBP module. We also introduce explicit alignments modules to the Baseline, denoted as Baseline+SpyNet and Base-line+deformable respectively, to evaluate the influence of explicit alignments in the video deblurring task. Finally, we deploy the proposed Target Frame Re-constructor module in the Baseline+MBP model to reinforce the intra-frame from the target features, which form our final model RNN-MBP. <ref type="table">Table 4</ref> shows the effectiveness of the proposed MBP and Target Frame Re-constructor module. Compared with the state-of-the-art image deblurring algorithms, the Base-line+SpyNet and Baseline+deformable cannot achieve promising results due to the inaccurate alignments. The introduction of the MBP module achieves 1.398 dB performance improvement over the Baseline+SpyNet model, which verifies that directly gathering the unaligned neighboring hidden states with a multi-scale propagation scheme is effective in exploiting inter-frame information. Although the Baseline+deformable contains an extra alignment module, it shows much worse performance (5.853dB) than the Baseline+MBP, which demonstrates that the unsatisfying warped features will deteriorate the performance of the video deblurring methods. Applying the proposed Target Frame Re-constructor module can further exploit the intrainformation by re-introducing the target features, which outperforms the Baseline+MBP model by a margin of 1.232 dB. Overall, the success of the RNN-MBP demonstrates that a well-designed recurrent neural network with the multi-scale propagation scheme is sufficient for exploiting the inter-and intra-frame information and obtaining favorable results.  <ref type="table">Table 4</ref>: The ablation study of the proposed algorithm.</p><p>All the methods are trained on the GOPRO dataset using the same training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed an effective Recurrent Neural Network with Multi-scale Bi-directional Propagation (RNN-MBP) for video deblurring. By adopting the U-Net RNN cells and multi-scale bidirectional propagation scheme, the proposed RNN-MBP can effectively exploit the inter-frame information from the unaligned neighboring hidden states without explicit alignment. In addition, we also developed a Target Frame Re-constructor module to reinforce the importance of the intra-frame information by re-introducing the target features into the reconstruction process. The ablation studies verify that the proposed modules are effective for video deblurring. To better evaluate the performance of video deblurring methods on real-world scenes, we also collect a Real-World Blurry Video Dataset (RBVD) by a welldesigned Digital Video Acquisition System (DVAS). Extensive evaluations on the synthetic and real-world datasets show that the proposed RNN-MBP performs favorably against the state-of-the-art deblurring methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Video Deblurring Algorithm. As the convolutional neural networks have achieved favorable results in image deblurring(Gong et al. 2017a;<ref type="bibr" target="#b34">Zhang et al. 2018b;</ref><ref type="bibr" target="#b14">Kupyn et al. 2019;</ref><ref type="bibr" target="#b31">Zamir et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021)</ref>, researchers begin to focus on solving video deblurring tasks with deep learning techniques. To better exploit the inter-frame (temporal) information in the adjacent frames, many implicit or explicit alignment methods are proposed.<ref type="bibr" target="#b33">(Zhang et al. 2018a</ref>) proposes a video deblurring network which use modified 3D convolutional layers to learn spatial temporal information. In the NTIRE19 video Deblurring Challenge<ref type="bibr" target="#b17">(Nah et al. 2019a</ref>), (Wang et al. 2019) proposes a pyramid cascading deformable (PCD) alignment module to compensate and fuse the temporal information in the feature-level which achieves the best results. In (Pan, Bai, and Tang 2020), Pan et al. use a pre-trained PWC-Net (Sun et al. 2018) to estimate the optical flows to exploit the inter-frame information Framework of the proposed RNN-MBP in video deblurring. (a) Overall framework of the proposed algorithm. (b) Details of the Feature extractor module. (c) Details of the Multi-scale Bi-directional Propagation module. (c) Details of the Target Frame Re-constructor module. The forward and backward hidden states with the same color have the same spatial scale. Best viewed on a high-resolution display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Digital Video Acquisition System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Timeline of our acquisition process. To capture the pairs of blurry and corresponding sharp videos, the camera will move twice along a pre-defined trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>), DVD-HC (Su et al. 2017), ES-TRNN (Zhong et al. 2020), CDVD-TSP (Pan, Bai, and Tang 2020)) and image deblurring methods (DMPHN (Zhang et al. 2019), SPN (Suin, Purohit, and Rajagopalan 2020), MPRNet (Zamir et al. 2021), HINet (Chen et al. 2021)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visual results on the GOPRO dataset. The results in (b)-(f) contain blur residual and ghost artifacts, while the deblurred image in (g) by our method is much clearer. Best viewed on a high-resolution display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The difference between real blur (a) and synthetic blur (b). Best viewed on a high-resolution display.Methods EDVR CDVD-TSP RNN-MBP(GOPRO) RNN-MBP (DVD) RNN-MBP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where ? ? denotes 8 CABs. Finally, the deblurred result O</figDesc><table /><note>t is restored by:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluations on the GORPO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluations on the dataset by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Quantitative and qualitative evaluations on the RBVD dataset. Models are trained on the RBVD train set unless mentioned specially.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t and generate the propagated features with skip-connections from the encoder part (f e2 t , f e1 t ), which can be formulated as:f d3 t = ? ? (f e3 t ) (5) f d2 t = ? ? ((f d3 t ) ?2 ) + ? ? (f e2 t ) (6) f d1 t = ? ? ((f d2 t ) ?2 ) + ? ? (f e1 t ) (7)where ? denotes the up-sample operation. Finally, all the six outputs in Eqs.(2)-(7) will be grouped as the propagated multi-scale hidden state set F t and sent to the Target Frame Re-constructor and the next iteration. Correspondingly, the backward propagation process also produce a multi-scale hidden state set B t which consists of b e3 t , b e2 t , b e1 t , b d1 t , b d2 t , and b d3 t .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Variational Framework for Simultaneous Motion Estimation and Restoration of Motion-Blurred Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berkels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Synthesize Motion Blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6833" to="6841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HINet: Half Instance Normalization Network for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking Coarse-to-Fine Approach in Single Image Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion from blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">; D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Video Deblurring: The Devil is in the Details. CoRR, abs/1909.12196. Gong</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3806" to="3815" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online Video Deblurring via Dynamic Temporal Blending Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation-Free Dynamic Scene Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spatio-Temporal Transformer Network for Video Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1701" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition Workshops</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8102" to="8111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NTIRE 2021 Challenge on Image Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NTIRE 2020 Challenge on Image and Video Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascaded Deep Video Deblurring Using Temporal Sharpness Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition. Rim, J.; et al. 2020. Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Video Deblurring for Hand-Held Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated Spatio-Temporal Attention-Guided Video Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7802" to="7811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale-Recurrent Network for Deep Image Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling Blurred Video with Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video Enhancement with Task-Oriented Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-Stage Progressive Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Stacked Hierarchical Multi-Patch Network for Image Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5971" to="5979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial Spatio-Temporal Learning for Video Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Gated Fusion Network for Joint Image Deblurring and Super-Resolution. In British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Filter Adaptive Network for Video Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DAVANet: Stereo Deblurring with View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
