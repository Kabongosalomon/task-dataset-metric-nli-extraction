<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glancing Transformer for Non-Autoregressive Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Qian</surname></persName>
							<email>qianlihua@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
							<email>baoy@smail.nju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<email>wangmingxuan.89@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
							<email>lqiu@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Glancing Transformer for Non-Autoregressive Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM), a method to learn word interdependency for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate highquality translation with 8?-15? speedup. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer has been the most widely used architecture for machine translation <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>. Despite its strong performance, the decoding of Transformer is inefficient as it adopts the sequential auto-regressive factorization for its probability model <ref type="figure">(Figure 1a</ref>). Recent work such as non-autoregressive transformer (NAT), aim to decode target tokens in parallel to speed up the generation <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>. However, the vanilla NAT still lags behind Transformer in the translation quality -with a gap about 7.0 BLEU score. NAT assumes the conditional independence of the target tokens given the source sentence. We suspect that NAT's conditional independence assumption prevents learning word interdependency in the target sentence. Notice that such word interdependency is crucial, as the Transformer explicitly captures that via decoding from left to right <ref type="figure">(Figure 1a</ref>). * The work was done when the first author was an intern at Bytedance.? 1 y 2 y 4 y 5 Several remedies are proposed <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b8">Gu et al., 2019)</ref> to capture word interdependency while keeping parallel decoding. Their common idea is to decode the target tokens iteratively while each pass of decoding is trained using the masked language model <ref type="figure">(Figure 1c</ref>). Since these methods require multiple passes of decoding, its generation speed is measurably slower than the vanilla NAT. With single-pass generation only, these methods still largely lag behind the autoregressive Transformer.</p><p>One open question is whether a complete parallel decoding model can achieve comparable machine translation performance to the Transformer.</p><p>It should be non-autoregressive and take only one pass of decoding during the inference time.</p><p>To address the quest, we propose glancing language model (GLM), a new method to train a probabilistic sequence model. Based on GLM, we develop the glancing Transformer (GLAT) for neural machine translation. It achieves parallel text generation with only single decoding pass. Yet, it outperforms previous NAT methods and achieves comparable performance as the strong Transformer baseline in multiple cases. Intuitively, GLM adopts a adaptive glancing sampling strategy, which glances at some fragments of the reference if the reference is too difficult to fit in the training of GLAT. Correspondingly, when the model is well tuned, it will adaptively reduce the percentage of glancing sampling, making sure that the resulting model could learn to generate the whole sentence in the singlepass fashion.</p><p>Specifically, our proposed GLM differs from MLM in two aspects. Firstly, GLM proposes an adaptive glancing sampling strategy, which enables GLAT to generate sentences in a one-iteration way, working by gradual training instead of iterative inference (see <ref type="figure">Figure 1d</ref>). Generally, GLM is quite similar to curriculum learning <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref> in spirit, namely first learning to generate some fragments and gradually moving to learn the whole sentences (from easy to hard). To achieve the adaptive glancing sampling, GLM performs decoding twice in training. The first decoding is the same as the vanilla NAT, and the prediction accuracy indicates whether the current reference is "difficult" for fitting. In the second decoding, GLM gets words of the reference via glancing sampling according to the first decoding, and learn to predict the remaining words that are not sampled. Note that only the second decoding will update the model parameters. Secondly, instead of using the [MASK] token, GLM directly uses representations from the encoder at corresponding positions, which is more natural and could enhance the interactions between sampled words and signals from the encoder.</p><p>Experimental results show that GLAT obtains significant improvements (about 5 BLEU) on standard benchmarks compared to the vanilla NAT, without losing inference speed-up. GLAT achieves competitive results against iterative approaches like Mask-Predict <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref>, even outperforming the Mask-Predict model on WMT14 DE-EN and WMT16 RO-EN. Compared to the strong AT baseline, GLAT can still close the performance gap within 0.9 BLEU point while keeping 7.9? speed-up. Empirically, we even find that GLAT outperforms AT when the length of the reference is less than 20 on WMT14 DE-EN. We speculate this is because GLM could capture bidirectional context for generation while its left-to-right counterpart is only unidirectional, which indicates the potential of parallel generation approaches like GLAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probability Models of Machine Translation</head><p>We state and compare different probability models for machine translation. A machine translation task can be formally defined as a sequence to sequence generation problem: given the source sentence X = {x 1 , x 2 , ..., x N }, to generate the target sentence Y = {y 1 , y 2 , ..., y T } according to the conditional probability P (Y |X; ?), where ? denotes the parameter set of a network. Different methods factorize the conditional probability differently. The Transformer uses the autoregressive factorization to maximize the following likelihood:</p><formula xml:id="formula_0">L AT = log P (Y |X; ?) = T t=1 log p(y t |y &lt;t , X; ?),</formula><p>where y &lt;t = {[BOS], y 1 , ..., y t?1 }. For simplicity, we omit the number of samples in the equation. Note the training of AT adopts left-to-right teacher forcing on the target tokens <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>. The word interdependency is learned in a unidirectional way. During inference, the preceding predicted token is fed into the decoder to generate the next token.</p><p>The vanilla NAT consists of the same encoder as Transformer and a parallel decoder with layers of multi-head attention <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>. During training, it uses the conditional independent factorization for the target sentence:</p><formula xml:id="formula_1">L NAT = T t=1 log P (y t |X; ?).</formula><p>Notice that, NAT's log-likelihood is an approximation to the full log-likelihood log P (Y |X; ?). During inference, the encoder representation is copied as the input to the decoder, therefore all tokens on the target side can be generated in parallel. Such a conditional independence assumption does not y 5 y 5 ? 5 0.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H H?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glancing Encoder</head><p>Parallel Decoder hold in general, which explains the inferior performance of NAT.</p><formula xml:id="formula_2">? 5 h 5 h 1 h 2 h 3 h 4 h 5 X Compute loss L GLM y 1 y 3 y 5 h 2 h 4 Compute Distanc? y 1 y 2 y 4 y 5 y 3 y 1 y 2 y 3 y 4 y 5 Sample words S(Y,? ) Replace Input ? Y Y</formula><p>Multi-pass iterative decoding approaches such as Mask-Predict <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref> extends the vanilla NAT. It still uses the conditional independent factorization, together with the random masking scheme:</p><formula xml:id="formula_3">L MLM = yt?RM(Y ) log p y t |? Y, RM(Y ) , X; ? ,</formula><p>where RM(Y ) is a set of randomly selected words from Y , and ?(?) replaces these selected words in Y with the [MASK] token. For example in <ref type="figure">Figure 1c</ref>, <ref type="bibr">[MASK]</ref>, y 4 , y 5 }. The training objective is to learn a refinement model ? that can predict the masked tokens given the source sentence X and words generated in the previous iteration.</p><formula xml:id="formula_4">RM(Y ) = {y 2 , y 3 }, ? Y, RM(Y ) = {y 1 , [MASK],</formula><p>The vanilla NAT breaks word interdependency, while MLM requires multiple passes of decoding to re-establish the word interdependency. Our goal in this work is to design a better probability model and a training objective to enable word interdependency learning for single-pass parallel generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Glancing Transformer</head><p>In this section, we present GLAT in detail. GLAT uses the same encoder-decoder architecture as the vanilla NAT <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>. GLAT differs from the vanilla NAT in that it explicitly encourages word interdependency via training with glancing language model (GLM). It differs from the iterative NAT with MLM in that it is trained to produce single pass parallel decoding while MLM is used for prediction refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Glancing Language Model</head><p>Given the input source sentence X = {x 1 , x 2 , ..., x N }, the task is to predict Y = {y 1 , y 2 , ..., y T }. The glancing Transformer (GLAT) formulates a glancing language model (GLM) during training. It maximizes the following:</p><formula xml:id="formula_5">L GLM = yt?GS(Y,? ) log p(y t |GS(Y,? ), X; ?)</formula><p>(1) Where,? is an initial predicted tokens, and GS(Y,? ) is a subset of tokens selected via the glancing sampling strategy ( <ref type="figure" target="#fig_0">Figure 2</ref>, described in detail in the next section). The glancing sampling strategy selects those words from the target sentence by comparing the initial prediction against the ground-truth tokens. It selects more tokens and feeds them into the decoder input if the network's initial prediction is less accurate. GS(Y,? ) is the remaining subset of tokens within the target Y but not selected. The training loss above is calculated against these remaining tokens.</p><p>GLAT adopts similar encoder-decoder architecture as the Transformer with some modification ( <ref type="figure">Figure 1d</ref>). Its encoder f enc is the same multihead attention layers. Its decoder f dec include multiple layers of multi-head attention where each layer attends to the full sequence of both encoder representation and the previous layer of decoder representation.</p><p>During the initial prediction, the input to the decoder H = {h 1 , h 2 , ..., h T } are copied from the encoder output using either uniform copy or soft copy <ref type="bibr" target="#b29">(Wei et al., 2019)</ref>. The initial tokens ? are predicted using argmax decoding with f dec (f enc (X; ?), H; ?).</p><p>To calculate the loss L GLM , we compare the initial prediction? against the ground-truth to select tokens within the target sentence, i.e. GS(Y,? ). We then replace those sampled indices of h's with corresponding target word embeddings, H = RP(Emb yt?GS(Y,? ) (y t ), H), where RP replaces the corresponding indices. Namely, if a token in the target is sampled, its word embedding replaces the corresponding h. Here the word embeddings are obtained from the softmax embedding matrix of the decoder. The updated H is then fed into the decoder f dec again to calculate the output token probability. Specifically, the output probabilities of remaining tokens p(y t |GS(Y,? ), X; ?) are computed with f dec (H , f enc (X; ?); ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Glancing Sampling Strategy</head><p>One important component of GLM is to adaptively select the positions of tokens from the target sentence. Those selected tokens provide "correct" information from the ground-truth target, therefore it helps training the decoder to predict the rest nonselected tokens. Intuitively, our adaptive sampling strategy guides the model to first learn the generation of fragments and then gradually turn to the whole sentences. Our glancing sampling strategy selects many words at the start of the training, when the model is not yet well tuned. As the model gets better progressively, the sampling strategy will sample fewer words to enable the model to learn the parallel generation of the whole sentence. Note that the sampling strategy is crucial in the training of GLAT.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the glancing sampling could be divided into two steps: first deciding a sampling number S, and then randomly selecting S words from the reference. The sampling number S will be larger when the model is poorly trained and decreases along the training process. Note that we choose to randomly select the S words from the reference. The random reference word selection is simple and yields good performance empirically.</p><p>Formally, given the input X, its predicted sen-tence? and its reference Y , the goal of glancing sampling function GS(Y,? ) is to obtain a subset of words sampled from Y :</p><formula xml:id="formula_6">GS(Y,? ) = Random(Y, S(Y,? ))<label>(2)</label></formula><p>Here, Random(Y, S) is randomly selecting S tokens from Y , and S is computed by comparing the difference between? and Y , S(Y,? ) = ? ? d(Y,? ). The sampling ratio ? is a hyper-parameter to more flexibly control the number of sampled tokens. d(Y,? ) is a metric for measuring the differences between Y and? . We adopt the Hamming distance <ref type="bibr" target="#b12">(Hamming, 1950)</ref> as the metric, which is computed as d(Y,? ) = T t=1 (y t =? t ). With d(Y,? ), the sampling number can be decided adaptively considering the current trained model's prediction capability. For situations that Y and? have different lengths, d(Y,? ) could be other distances such as Levenshtein distance <ref type="bibr" target="#b16">(Levenshtein, 1966)</ref>.</p><p>Alternative glancing sampling strategy can be adopted as well. For example, one simple alternative strategy is to set the number of sampled tokens to be proportional to the target sentence length, i.e. S = ? * T . We will evaluate the effects of these variations in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>GLAT only modifies the training procedure. Its inference is fully parallel with only a single pass. For parallel generation, we need to decide the output lengths before decoding. A simple way to decide the output lengths is predicting length with representations from the encoder.</p><p>In GLAT, the length prediction is implemented as in <ref type="bibr" target="#b4">Ghazvininejad et al. (2019)</ref>. An additional [LENGTH] token is added to the source input, and the encoder output for the [LENGTH] token is used to predict the length.</p><p>We also use two more complex methods to better decide the output lengths: noisy parallel decoding (NPD) and connectionist temporal classification (CTC). For NPD <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>, we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate. Then we use a pre-trained transformer to rank these sequences and identify the best overall output as the final output. For CTC <ref type="bibr" target="#b6">(Graves et al., 2006)</ref>, following <ref type="bibr" target="#b18">Libovick? and Helcl (2018)</ref>, we first set the max output length to twice the source input length, and remove the blanks and repeated tokens after generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines. Ablation studies and further analysis are also included to verify the effects of different components used in GLAT.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs). These datasets are tokenized and segmented into subword units using BPE encodings <ref type="bibr" target="#b22">(Sennrich et al., 2016)</ref>. We preprocess WMT14 EN-DE by following the data preprocessing in <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref>. For WMT16 EN-RO and IWSLT16 DE-EN, we use the processed data provided in <ref type="bibr" target="#b15">Lee et al. (2018)</ref>.</p><p>Knowledge Distillation Following previous work <ref type="bibr" target="#b7">(Gu et al., 2018;</ref><ref type="bibr" target="#b15">Lee et al., 2018;</ref>, we also use sequence-level knowledge distillation for all datasets. We employ the transformer with the base setting in <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref> as the teacher for knowledge distillation. Then, we train our GLAT on distilled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Setup</head><p>We compare our method with the base Transformer and strong representative NAT baselines in <ref type="table" target="#tab_2">Table 1</ref>. For all our tasks, we obtain other NAT models' performance by directly using the performance figures reported in their papers if they are available.</p><p>We adopt the vanilla model which copies source input uniformly in <ref type="bibr" target="#b7">Gu et al. (2018)</ref> as our base model (NAT-base) and replace the Uni-formCopy with attention mechanism using positions. Note that the output length does not equal the length of reference in models using CTC. Therefore, for GLAT with CTC, we adopt longest common subsequence distance for comparing Y and? , and the glancing target is the target alignment that maximize the output probability arg max a?B ?1 (Y ) P (a|X; ?). B ?1 is the mapping proposed in <ref type="bibr" target="#b6">(Graves et al., 2006)</ref>, which expand the reference to the length of output by inserting blanks or repeating words.</p><p>For WMT datasets, we follow the hyperparameters of the base Transformer in <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref>. And we choose a smaller setting for IWSLT16, as IWSLT16 is a smaller dataset. For IWSLT16, we use 5 layers for encoder and decoder, and set the model size d model to 256. Using Nvidia V100 GPUs, We train the model with batches of 64k/8k tokens for WMT/IWSLT datasets, respectively. We set the dropout rate to 0.1 and use Adam optimizer (Kingma and Ba, 2014) with ? = (0.9, 0.999). For WMT datasets, the learning rate warms up to 5e ? 4 in 4k steps and gradually  <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref>. As for IWSLT16 DE-EN, we adopt linear annealing (from 3e ? 4 to 1e ? 5) as in <ref type="bibr" target="#b15">Lee et al. (2018)</ref>. For the hyper-parameter ?, we adopt linear annealing from 0.5 to 0.3 for WMT datasets and a fixed value of 0.5 for IWSLT16. The final model is created by averaging the 5 best checkpoints chosen by validation BLEU scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>The main results on the benchmarks are presented in <ref type="table" target="#tab_2">Table 1</ref>. GLAT significantly improves the translation quality and outperforms strong baselines by a large margin. Our method introduces explicit word interdependency modeling for the decoder and gradually learns simultaneous generation of whole sequences, enabling the model to better capture the underlying data structure. Compared to models with iterative decoding, our method completely maintains the inference efficiency advantage of fully non-autoregressive models, since GLAT generate with a single pass. Compared with the baselines, we highlight our empirical advantages:</p><p>? GLAT is highly effective. Compared with the vanilla NAT-base models, GLAT obtains significant improvements (about 5 BLEU) on EN-DE/DE-EN. Additionally, GLAT also outperforms other fully non-autoregressive models with a substantial margin (almost +2 BLEU score on average). The results are even very close to those of the AT model, which shows great potential.</p><p>? GLAT is simple and can be applied to other NAT models flexibly, as we only modify the training process by reference glancing while keeping inference unchanged. For comparison, NAT-DCRF utilizes CRF to generate sequentially; NAT-IR and Mask-Predict models ? CTC and NPD use different approaches to determine the best output length, and they have their own advantages and disadvantages. CTC requires the output length to be longer than the exact target length. With longer output lengths, the training will consume more time and GPU memory. As for NPD, with a certain number of length reranking candidates, the inference speed will be slower than models using CTC. Note that NPD can use pretrained AT models or the non-autoregressive model itself to rerank multiple outputs.</p><p>We also present a scatter plot in <ref type="figure" target="#fig_1">Figure 3</ref>, displaying the trend of speed-up and BLEU with different NAT models. It is shown that the point of GLAT is located on the top-right of the competing methods. Obviously, GLAT outperforms our competitors in BLEU if speed-up is controlled, and in speed-up if BLEU is controlled. This indicates that GLAT outperforms previous state-of-the-art NAT methods. Although iterative models like Mask-Predict achieves competitive BLEU scores, they only maintain minor speed advantages over AT. In contrast, fully non-autoregressive models remarkably improve the inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Effect of Source Input Length To analyze the effect of source input length on the models' performance, we split the source sentences into different intervals by length after BPE and compute the BLEU score for each interval. The histogram of results is presented in <ref type="figure" target="#fig_2">Figure 4</ref>. NAT-base's performance drops sharply for long sentences, while the gradual learning process enables GLAT to boost the performance by a large margin, especially for  long sentences. We also find that GLAT outperforms autoregressive Transformer when the source input length is smaller than 20.</p><p>GLAT Reduces Repetition We also measure the percentage of repeated tokens on test set of WMT14 EN-DE and WMT14 DE-EN. <ref type="table" target="#tab_4">Table 2</ref> presents the token repetition ratio of sentences generated by NAT-base and GLAT. The results show that GLAT significantly reduces the occurrence of repetition, and the repetition ratio can be further reduced with NPD. We think an important cause of the improvement is better interdependency modeling. Since GLAT explicitly encourages word interdependency modeling to better capture the dependency between target tokens, wrong generation patterns, such as repetition, can be largely avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Effectiveness of the Adaptive Sampling Number To validate the effectiveness of the adaptive sampling strategy for the sampling number S(Y,? ), we also introduce two fixed approaches for comparison. The first one decides the sampling number with ? * T , where T is the length of Y , and ? is a constant ratio. The second one is relatively flexible, which sets a start ratio of ? s and an end ratio ? e , and linearly reduces the sampling number from ? s * T to ? e * T along the training process. As shown in <ref type="table" target="#tab_6">Table 3</ref> and <ref type="table" target="#tab_7">Table 4</ref>, clearly, our adaptive approach (Adaptive in the table) outperforms the baseline models with big margins. The results confirm our intuition that the sampling schedule affects the generation performance of our NAT model. The sampling strategy, which first offers relatively easy generation problems and then turns harder, benefits the final performance. Besides, even with the simplest constant ratio, GLAT still achieves remarkable results. When set ? = 0.2, it even outperforms the baseline ? = 0.0 by 2.5 BLEU score.</p><p>The experiments potentially support that it is beneficial to learn the generation of fragments at the   Similar to the word selection strategy for masking words during inference in Mask-Predict, we also add two strategies related to the prediction confidence: "most certain" and "most uncertain." We choose the positions where predictions have higher confidence for "most certain", and vise versa for "most uncertain." The results for different selection methods are listed in <ref type="table" target="#tab_9">Table 5</ref>.</p><p>In comparisons, the model with the selection strategy 1 ? p ref outperforms the one with p ref , indicating that words hard to predict are more important for glancing in training. And we find that the random strategy performs a little better than the two confidence-based strategies. We think this indicates that introducing more randomness in sam-   pling enable GLAT to explore more interdependency among target words. We adopt the random strategy for its simplicity and good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of GLAT over Mask-Predict</head><p>To study the effects of sampling strategy and decoder inputs of GLAT, we conduct experiments for replacing these two modules in GLAT with the corresponding part in Mask-Predict, respectively. The results are presented in <ref type="table" target="#tab_10">Table 6</ref>. GLAT employs glancing sampling strategy instead of the uniform sampling strategy used in Mask-Predict, and replaces the [MASK] token inputs with source representations from the encoder. The results show that the glancing sampling strategy outperforms the uniform sampling strategy by 5?6 BLEU points, and feeding representations from the encoder as the decoder input could still improve the strong baseline by 0.2?0.3 BLEU points after adopting glancing sampling. To sum up, the adaptive glancing sampling approach contributes the most to the final improvement, and the use of representations from the encoder also helps a bit.</p><p>More Analysis We also conduct experiments for: a) comparison of different distance metrics for glancing sampling, b) GLAT with more than one decoding iteration. The details are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model's burden of dealing with dependencies among output words <ref type="bibr" target="#b7">(Gu et al., 2018;</ref><ref type="bibr" target="#b19">Ma et al., 2019;</ref><ref type="bibr" target="#b0">Bao et al., 2019;</ref><ref type="bibr" target="#b20">Ran et al., 2019)</ref>.</p><p>Another branch of work considers transferring the knowledge from autoregressive models to nonautoregressive models <ref type="bibr" target="#b29">(Wei et al., 2019;</ref><ref type="bibr" target="#b10">Guo et al., 2019b;</ref><ref type="bibr" target="#b26">Sun and Yang, 2020)</ref>. Besides, there are also some work that apply different training objectives to train non-autoregressive models <ref type="bibr" target="#b18">(Libovick? and Helcl, 2018;</ref><ref type="bibr" target="#b23">Shao et al., 2020;</ref><ref type="bibr" target="#b3">Ghazvininejad et al., 2020a)</ref>, add regularization terms <ref type="bibr" target="#b9">Guo et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Autoregressive Models with Structured</head><p>Decoding To model the dependencies between words,  introduces a CRF inference module in NAT and performs additional sequential decoding after the non-autoregressive computation in inference. <ref type="bibr" target="#b2">Deng and Rush (2020)</ref> proposes cascaded CRF decoding. Since GLAT only performs single-pass non-autoregressive generation, our approach is orthogonal to the method proposed in . We can also combine our approach with the structured decoding methods.</p><p>Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding <ref type="bibr" target="#b15">(Lee et al., 2018;</ref><ref type="bibr" target="#b8">Gu et al., 2019;</ref><ref type="bibr" target="#b4">Ghazvininejad et al., 2019</ref><ref type="bibr" target="#b5">Ghazvininejad et al., , 2020b</ref><ref type="bibr" target="#b13">Kasai et al., 2020)</ref>. <ref type="bibr" target="#b15">Lee et al. (2018)</ref> proposed a method of iterative refinement based on denoising autoencoder. <ref type="bibr" target="#b8">Gu et al. (2019)</ref> utilized insertion and deletion to refine the outputs in inference. <ref type="bibr" target="#b4">Ghazvininejad et al. (2019)</ref> trained the model with the masked language model, and the model iteratively replaces masked tokens with new outputs. Despite the relatively better accuracy, the multiple decoding iterations vastly reduce the inference efficiency of non-autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel generation models.</p><p>With the glancing language model, the model starts from learning the generation of sequence fragments and gradually moving to whole sequences. Experimental results show that our approach significantly improves the performance of non-autoregressive machine translation with single-pass parallel generation. As GLAT achieves competitive performance compared with autoregressive models, applying our approach to other generation tasks is a promising direction for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The training procedure with glancing sampling in GLAT. H is the representation computed by the encoder.?'s are the initial predicted tokens of the parallel decoder. y's are the ground-truth target tokens. H is fed into the decoder again to calculate the training loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The trade-off between speed-up and BLEU on WMT14 DE-EN decays according to inverse square root schedule in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Performance under different source input length on WMT14 DE-EN need multiple decoding iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Results on WMT14 EN-DE/DE-EN and WMT16 EN-RO/RO-EN benchmarks. I dec is the number of decoding iterations and m is the number of length reranking candidates. NPD represents noisy parallel decoding, CTC represents connectionist temporal classification. * indicate the results are obtained by our implementation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Token repetition ratio on WMT14 EN-DE and WMT14 DE-EN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performances on IWSLT16 with fixed sampling ratio.</figDesc><table><row><cell cols="2">Sampling Number ?s</cell><cell cols="2">?e BLEU</cell></row><row><cell></cell><cell>0.5</cell><cell>0</cell><cell>27.80</cell></row><row><cell>Decreasing</cell><cell cols="2">0.5 0.1</cell><cell>28.21</cell></row><row><cell></cell><cell cols="2">0.5 0.2</cell><cell>27.15</cell></row><row><cell></cell><cell cols="2">0.5 0.3</cell><cell>23.37</cell></row><row><cell>Adaptive</cell><cell>-</cell><cell></cell><cell>29.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Influence of Reference Word SelectionTo analyze how the strategies of selecting reference words affect glancing sampling, we conduct experiments with different selection strategies. By default, we assume all the words in the reference are equally important and randomly choose reference words for glancing. Besides the random strategy, we devise four other selection methods considering the prediction of first decoding. For p ref and 1?p ref , the sampling probability of each reference word is proportional to the output probability for the reference word p ref and the probability 1 ? p ref , respectively.</figDesc><table><row><cell>: Performances on IWSLT16 with decreasing</cell></row><row><cell>sampling ratio.</cell></row><row><cell>start and gradually transfer to the whole sequence.</cell></row><row><cell>The flexible decreasing ratio method works better</cell></row><row><cell>than the constant one, and our proposed adaptive</cell></row><row><cell>approaches achieve the best results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance on WMT14 EN-DE with different reference word selection strategies.</figDesc><table><row><cell>Method</cell><cell cols="2">WMT14 EN-DE DE-EN</cell></row><row><cell cols="2">GLAT w/ uniform sampling 19.16</cell><cell>23.56</cell></row><row><cell>GLAT w/ [MASK] inputs</cell><cell>24.99</cell><cell>29.48</cell></row><row><cell>GLAT</cell><cell>25.21</cell><cell>29.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study for comparing GLAT and Mask-Predict on WMT14 EN-DE and DE-EN.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10677</idno>
		<title level="m">Non-autoregressive transformer by position learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascaded text generation with markov transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01112</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Aligned cross entropy for non-autoregressive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01655</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6114" to="6123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08785</idno>
		<title level="m">Semi-autoregressive training improves mask-predict decoding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11179" to="11189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation with enhanced decoder input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3723" to="3730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fine-tuning by curriculum learning for non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08717</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Error detecting and error correcting codes. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="147" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with disentangled context transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5144" to="5155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hint-based training for nonautoregressive translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Endto-end non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jind?ich</forename><surname>Libovick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jind?ich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlowSeq: Nonautoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1437</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4273" to="4283" />
		</imprint>
	</monogr>
	<note>Graham Neubig, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Guiding non-autoregressive neural machine translation decoding with reordering information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Qiu Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Non-autoregressive machine translation with latent alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07437</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimizing the bag-ofngrams difference for non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenze</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="198" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8846" to="8853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast structured decoding for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3016" to="3026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An em approach to non-autoregressive conditional sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9249" to="9258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with auxiliary regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imitation learning for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
